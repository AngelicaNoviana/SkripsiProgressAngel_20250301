Title,Summary,Published,Link,Authors
"Generalized Grade-of-Membership Estimation for High-dimensional Locally
  Dependent Data","This work focuses on the mixed membership models for multivariate categorical
data widely used for analyzing survey responses and population genetics data.
These grade of membership (GoM) models offer rich modeling power but present
significant estimation challenges for high-dimensional polytomous data. Popular
existing approaches, such as Bayesian MCMC inference, are not scalable and lack
theoretical guarantees in high-dimensional settings. To address this, we first
observe that data from this model can be reformulated as a three-way
(quasi-)tensor, with many subjects responding to many items with varying
numbers of categories. We introduce a novel and simple approach that flattens
the three-way quasi-tensor into a ""fat"" matrix, and then perform a singular
value decomposition of it to estimate parameters by exploiting the singular
subspace geometry. Our fast spectral method can accommodate a broad range of
data distributions with arbitrarily locally dependent noise, which we formalize
as the generalized-GoM models. We establish finite-sample entrywise error
bounds for the generalized-GoM model parameters. This is supported by a new
sharp two-to-infinity singular subspace perturbation theory for locally
dependent and flexibly distributed noise, a contribution of independent
interest. Simulations and applications to data in political surveys, population
genetics, and single-cell sequencing demonstrate our method's superior
performance.",2024-12-27T18:51:15Z,http://arxiv.org/abs/2412.19796v1,"Ling Chen, Chengzhu Huang, Yuqi Gu"
"Data-driven analysis of anomalous transport and three-wave-coupling
  effects in E x B plasma discharges","Collisionless cross-field electron transport in an E x B configuration
relevant for electric propulsion is studied using data from a (z, {\theta})
full-PIC simulation. Higher-order spectral analysis shows that transport is
dominated by the in-phase interaction of the oscillations of the azimuthal
electric field and the electron density associated to the first electron
cyclotron drift instability (ECDI) mode. A secondary contribution emanates from
a lower-frequency mode, not predicted by linear ECDI theory, while higher modes
have a minor direct impact on transport. However, a bicoherence analysis
reveals that strong phase couplings exist among the ECDI modes, and a sparse
symbolic regression spectral model, based on the three-wave coupling equations,
suggests an inverse energy cascade as the most likely explanation, thus
suggesting that higher modes contribute indirectly to transport by quadratic
power transfer to the first mode. This work provides new insights into the
dynamics of anomalous plasma transport in E x B sources and the underlying
processes governing energy distribution across different scales, and supports
the validity of weak turbulence theory to examine their behavior.",2024-12-27T18:43:24Z,http://arxiv.org/abs/2412.19789v1,"Borja Bayón-Buján, Enrique Bello-Benítez, Jiewei Zhou, Mario Merino"
"Machine Learning for Sentiment Analysis of Imported Food in Trinidad and
  Tobago","This research investigates the performance of various machine learning
algorithms (CNN, LSTM, VADER, and RoBERTa) for sentiment analysis of Twitter
data related to imported food items in Trinidad and Tobago. The study addresses
three primary research questions: the comparative accuracy and efficiency of
the algorithms, the optimal configurations for each model, and the potential
applications of the optimized models in a live system for monitoring public
sentiment and its impact on the import bill. The dataset comprises tweets from
2018 to 2024, divided into imbalanced, balanced, and temporal subsets to assess
the impact of data balancing and the COVID-19 pandemic on sentiment trends. Ten
experiments were conducted to evaluate the models under various configurations.
Results indicated that VADER outperformed the other models in both multi-class
and binary sentiment classifications. The study highlights significant changes
in sentiment trends pre- and post-COVID-19, with implications for import
policies.",2024-12-27T18:25:08Z,http://arxiv.org/abs/2412.19781v1,"Cassandra Daniels, Koffka Khan"
Tensor Network Estimation of Distribution Algorithms,"Tensor networks are a tool first employed in the context of many-body quantum
physics that now have a wide range of uses across the computational sciences,
from numerical methods to machine learning. Methods integrating tensor networks
into evolutionary optimization algorithms have appeared in the recent
literature. In essence, these methods can be understood as replacing the
traditional crossover operation of a genetic algorithm with a tensor
network-based generative model. We investigate these methods from the point of
view that they are Estimation of Distribution Algorithms (EDAs). We find that
optimization performance of these methods is not related to the power of the
generative model in a straightforward way. Generative models that are better
(in the sense that they better model the distribution from which their training
data is drawn) do not necessarily result in better performance of the
optimization algorithm they form a part of. This raises the question of how
best to incorporate powerful generative models into optimization routines. In
light of this we find that adding an explicit mutation operator to the output
of the generative model often improves optimization performance.",2024-12-27T18:22:47Z,http://arxiv.org/abs/2412.19780v1,"John Gardiner, Javier Lopez-Piqueres"
"Analysis of Premature Death Rates in Texas Counties: The Impact of Air
  Quality, Socioeconomic Factors, and COPD Prevalence","Understanding factors contributing to premature mortality is critical for
public health planning. This study examines the relationships between premature
death rates and multiple risk factors across several Texas counties, utilizing
EPA air quality data, Census information, and county health records from recent
years. We analyze the impact of air quality (PM2.5 levels), socioeconomic
factors (median household income), and health conditions (COPD prevalence)
through statistical analysis and modeling techniques. Results reveal COPD
prevalence as a strong predictor of premature death rates, with higher
prevalence associated with a substantial increase in years of potential life
lost. While socioeconomic factors show a significant negative correlation, air
quality demonstrates more complex indirect relationships. These findings
emphasize the need for integrated public health interventions that prioritize
key health conditions while addressing underlying socioeconomic disparities.",2024-12-27T18:12:04Z,http://arxiv.org/abs/2412.19774v1,"Richard Rich, Ernesto Diaz"
Direct estimates of irreversibility from time series,"The arrow of time can be quantified through the Kullback-Leibler divergence
($D_{KL}$) between the distributions of forward and reverse trajectories in a
system. Many approaches to estimate this rely on specific models, but the use
of incorrect models can introduce uncontrolled errors. Here, we describe a
model-free method that uses trajectory data directly to estimate the evidence
for irreversibility over finite windows of time. To do this we build on
previous work to identify and correct for errors that arise from limited sample
size. Importantly, our approach accurately recovers $D_{KL} = 0$ in systems
that adhere to detailed balance, and the correct nonzero $D_{KL}$ for data
generated by well understood models of nonequilibrium systems. We apply our
method to trajectories of neural activity in the retina as it responds to
naturalistic inputs, and find evidence of irreversibility in single neurons,
emphasizing the non-Markovian character of these data. These results open new
avenues for investigating how the brain represents the arrow of time.",2024-12-27T18:10:53Z,http://arxiv.org/abs/2412.19772v1,"Trevor GrandPre, Gianluca Teza, William Bialek"
"On Universally Free First-Order Extensions of Belnap-Dunn's Four-Valued
  Logic and Nelson's Paraconsistent Logic N4","The aim of this paper is to introduce the logics FFDE and FN4, which are
universally free versions of Belnap-Dunn's four-valued logic, also known as the
logic of first-degree entailment (FDE), and Nelson's paraconsistent logic QN4
(N-). Both FDE and QN4 are suitable to be interpreted as information-based
logics, that is, logics that are capable of representing the deductive behavior
of possibly inconsistent and incomplete information in a database. Like QN4 and
some non-free first-order extensions of FDE, FFDE and FN4 are endowed with
Kripke-style variable domain semantics, which allows representing the dynamic
aspect of information processing, that is, how a database receives new
information over time, including information about new individuals. We argue,
however, that FFDE and FN4 can better represent the development of inconsistent
and incomplete information states (i.e., configurations of a database) over
time than their non-free versions. First, because they allow for empty domains,
which corresponds to the idea that a database may acknowledge no individual at
all at an early stage of its development. Second, because they allow for empty
names, which get interpreted as information about new individuals is inserted
into the database. Also, both systems include an identity predicate that is
interpreted along the same lines of the other logical operators, viz., in terms
of independent positive and negative rules.",2024-12-27T17:57:18Z,http://arxiv.org/abs/2412.19767v1,"Henrique Antunes, Abilio Rodrigues"
Generative Video Propagation,"Large-scale video generation models have the inherent ability to
realistically model natural scenes. In this paper, we demonstrate that through
a careful design of a generative video propagation framework, various video
tasks can be addressed in a unified way by leveraging the generative power of
such models. Specifically, our framework, GenProp, encodes the original video
with a selective content encoder and propagates the changes made to the first
frame using an image-to-video generation model. We propose a data generation
scheme to cover multiple video tasks based on instance-level video segmentation
datasets. Our model is trained by incorporating a mask prediction decoder head
and optimizing a region-aware loss to aid the encoder to preserve the original
content while the generation model propagates the modified region. This novel
design opens up new possibilities: In editing scenarios, GenProp allows
substantial changes to an object's shape; for insertion, the inserted objects
can exhibit independent motion; for removal, GenProp effectively removes
effects like shadows and reflections from the whole video; for tracking,
GenProp is capable of tracking objects and their associated effects together.
Experiment results demonstrate the leading performance of our model in various
video tasks, and we further provide in-depth analyses of the proposed
framework.",2024-12-27T17:42:29Z,http://arxiv.org/abs/2412.19761v1,"Shaoteng Liu, Tianyu Wang, Jui-Hsien Wang, Qing Liu, Zhifei Zhang, Joon-Young Lee, Yijun Li, Bei Yu, Zhe Lin, Soo Ye Kim, Jiaya Jia"
"""Did my figure do justice to the answer?"" : Towards Multimodal Short
  Answer Grading with Feedback (MMSAF)","Personalized feedback plays a vital role in a student's learning process.
While existing systems are adept at providing feedback over MCQ-based
evaluation, this work focuses more on subjective and open-ended questions,
which is similar to the problem of Automatic Short Answer Grading (ASAG) with
feedback. Additionally, we introduce the Multimodal Short Answer grading with
Feedback (MMSAF) problem over the traditional ASAG feedback problem to address
the scenario where the student answer and reference answer might contain
images. Moreover, we introduce the MMSAF dataset with 2197 data points along
with an automated framework for generating such data sets. Our evaluations on
existing LLMs over this dataset achieved an overall accuracy of 55\% on Level
of Correctness labels, 75\% on Image Relevance labels and a score of 4.27 out
of 5 in correctness level of LLM generated feedback as rated by experts. As per
experts, Pixtral achieved a rating of above 4 out of all metrics, indicating
that it is more aligned to human judgement, and that it is the best solution
for assisting students.",2024-12-27T17:33:39Z,http://arxiv.org/abs/2412.19755v1,"Pritam Sil, Bhaskaran Raman, Pushpak Bhattacharyya"
Complement or substitute? How AI increases the demand for human skills,"The question of whether AI substitutes or complements human work is central
to debates on the future of work. This paper examines the impact of AI on skill
demand and compensation in the U.S. economy, analysing 12 million online job
vacancies from 2018 to 2023. It investigates internal effects (within-job
substitution and complementation) and external effects (across occupations,
industries, and regions). Our findings reveal a significant increase in demand
for AI-complementary skills, such as digital literacy, teamwork, and
resilience, alongside rising wage premiums for these skills in AI roles like
Data Scientist. Conversely, substitute skills, including customer service and
text review, have declined in both demand and value within AI-related
positions. Examining external effects, we find a notable rise in demand for
complementary skills in non-AI roles linked to the growth of AI-related jobs in
specific industries or regions. At the same time, there is a moderate decline
in non-AI roles requiring substitute skills. Overall, AI's complementary effect
is up to 50% larger than its substitution effect, resulting in net positive
demand for skills. These results, replicated for the UK and Australia,
highlight AI's transformative impact on workforce skill requirements. They
suggest reskilling efforts should prioritise not only technical AI skills but
also complementary skills like ethics and digital literacy.",2024-12-27T17:26:30Z,http://arxiv.org/abs/2412.19754v1,"Elina Mäkelä, Fabian Stephany"
"IMAGINE: An 8-to-1b 22nm FD-SOI Compute-In-Memory CNN Accelerator With
  an End-to-End Analog Charge-Based 0.15-8POPS/W Macro Featuring
  Distribution-Aware Data Reshaping","Charge-domain compute-in-memory (CIM) SRAMs have recently become an enticing
compromise between computing efficiency and accuracy to process sub-8b
convolutional neural networks (CNNs) at the edge. Yet, they commonly make use
of a fixed dot-product (DP) voltage swing, which leads to a loss in effective
ADC bits due to data-dependent clipping or truncation effects that waste
precious conversion energy and computing accuracy. To overcome this, we present
IMAGINE, a workload-adaptive 1-to-8b CIM-CNN accelerator in 22nm FD-SOI. It
introduces a 1152x256 end-to-end charge-based macro with a multi-bit DP based
on an input-serial, weight-parallel accumulation that avoids power-hungry DACs.
An adaptive swing is achieved by combining a channel-wise DP array split with a
linear in-ADC implementation of analog batch-normalization (ABN), obtaining a
distribution-aware data reshaping. Critical design constraints are relaxed by
including the post-silicon equivalent noise within a CIM-aware CNN training
framework. Measurement results showcase an 8b system-level energy efficiency of
40TOPS/W at 0.3/0.6V, with competitive accuracies on MNIST and CIFAR-10.
Moreover, the peak energy and area efficiencies of the 187kB/mm2 macro
respectively reach up to 0.15-8POPS/W and 2.6-154TOPS/mm2, scaling with the
8-to-1b computing precision. These results exceed previous charge-based designs
by 3-to-5x while being the first work to provide linear in-memory rescaling.",2024-12-27T17:18:15Z,http://arxiv.org/abs/2412.19750v1,"Adrian Kneip, Martin Lefebvre, Pol Maistriaux, David Bol"
"AAM-SEALS: Developing Aerial-Aquatic Manipulators in SEa, Air, and Land
  Simulator","Current simulators lack the ability to accurately model integrated
environments that encompass sea, air, and land. To address this gap, we
introduce Aerial-Aquatic Manipulators (AAMs) in SEa, Air, and Land Simulator
(SEALS), a comprehensive and photorealistic simulator designed for AAMs to
operate and learn in these diverse environments. The development of AAM-SEALS
tackles several significant challenges, including the creation of integrated
controllers for flying, swimming, and manipulation, and the high-fidelity
simulation of aerial dynamics and hydrodynamics leveraging particle physics.
Our evaluation demonstrates smooth operation and photorealistic transitions
across air, water, and their interfaces. We quantitatively validate the
fidelity of particle-based hydrodynamics by comparing position-tracking errors
across real-world and simulated systems. AAM-SEALS promises to benefit a broad
range of robotics communities, including robot learning, aerial robotics,
underwater robotics, mobile manipulation, and robotic simulators. We will
open-source our code and data to foster the advancement of research in these
fields. Please access our project website at: https:
//aam-seals.github.io/aam-seals-v1/",2024-12-27T17:13:14Z,http://arxiv.org/abs/2412.19744v1,"William Wang Yang, Karthikeya Kona, Yashveer Jain, Abhinav Bhamidipati, Tomer Atzili, Xiaomin Lin, Yantian Zha"
Hard Photon Triggered Jets in $p$-$p$ and $A$-$A$ Collisions,"An investigation of high transverse momentum (high-$p_T$) photon triggered
jets in proton-proton ($p$-$p$) and ion-ion ($A$-$A$) collisions at
$\sqrt{s_{NN}} = 0.2$ and $5.02~\mathrm{TeV}$ is carried out, using the
multistage description of in-medium jet evolution. Monte Carlo simulations of
hard scattering and energy loss in heavy-ion collisions are performed using
parameters tuned in a previous study of the nuclear modification factor
($R_{AA}$) for inclusive jets and high-$p_T$ hadrons. We obtain a good
reproduction of the experimental data for photon triggered jet $R_{AA}$, as
measured by the ATLAS detector, the distribution of the ratio of jet to photon
$p_T$ ($X_{\rm J \gamma}$), measured by both CMS and ATLAS, and the photon-jet
azimuthal correlation as measured by CMS. We obtain a moderate description of
the photon triggered jet $I_{AA}$, as measured by STAR. A noticeable
improvement in the comparison is observed when one goes beyond prompt photons
and includes bremsstrahlung and decay photons, revealing their significance in
certain kinematic regions, particularly at $X_{J\gamma} &gt; 1$. Moreover,
azimuthal angle correlations demonstrate a notable impact of non-prompt photons
on the distribution, emphasizing their role in accurately describing
experimental results. This work highlights the success of the multistage model
of jet modification to straightforwardly predict (this set of) photon triggered
jet observables. This comparison, along with the role played by non-prompt
photons, has important consequences on the inclusion of such observables in a
future Bayesian analysis.",2024-12-27T16:56:15Z,http://arxiv.org/abs/2412.19738v1,"C. Sirimanna, Y. Tachibana, A. Majumder, A. Angerami, R. Arora, S. A. Bass, Y. Chen, R. Datta, L. Du, R. Ehlers, H. Elfner, R. J. Fries, C. Gale, Y. He, B. V. Jacak, P. M. Jacobs, S. Jeon, Y. Ji, F. Jonas, L. Kasper, M. Kordell II, A. Kumar, R. Kunnawalkam-Elayavalli, J. Latessa, Y. -J. Lee, R. Lemmon, M. Luzum, S. Mak, A. Mankolli, C. Martin, H. Mehryar, T. Mengel, C. Nattrass, J. Norman, C. Parker, J. -F. Paquet, J. H. Putschke, H. Roch, G. Roland, B. Schenke, L. Schwiebert, A. Sengupta, C. Shen, M. Singh, D. Soeder, R. A. Soltz, I. Soudi, J. Velkovska, G. Vujanovic, X. -N. Wang, X. Wu, W. Zhao"
"Adaptive Context-Aware Multi-Path Transmission Control for VR/AR
  Content: A Deep Reinforcement Learning Approach","This paper introduces the Adaptive Context-Aware Multi-Path Transmission
Control Protocol (ACMPTCP), an efficient approach designed to optimize the
performance of Multi-Path Transmission Control Protocol (MPTCP) for
data-intensive applications such as augmented and virtual reality (AR/VR)
streaming. ACMPTCP addresses the limitations of conventional MPTCP by
leveraging deep reinforcement learning (DRL) for agile end-to-end path
management and optimal bandwidth allocation, facilitating path realignment
across diverse network environments.",2024-12-27T16:56:12Z,http://arxiv.org/abs/2412.19737v1,"Shakil Ahmed, Saifur Rahman Sabuj, Ashfaq Khokhar"
"A General Framework of Brain Region Detection And Genetic Variants
  Selection in Imaging Genetics","Imaging genetics is a growing field that employs structural or functional
neuroimaging techniques to study individuals with genetic risk variants
potentially linked to specific illnesses. This area presents considerable
challenges to statisticians due to the heterogeneous information and different
data forms it involves. In addition, both imaging and genetic data are
typically high-dimensional, creating a ""big data squared"" problem. Moreover,
brain imaging data contains extensive spatial information. Simply vectorizing
tensor images and treating voxels as independent features can lead to
computational issues and disregard spatial structure. This paper presents a
novel statistical method for imaging genetics modeling while addressing all
these challenges. We explore a Canonical Correlation Analysis based linear
model for the joint modeling of brain imaging, genetic information, and
clinical phenotype, enabling the simultaneous detection of significant brain
regions and selection of important genetic variants associated with the
phenotype outcome. Scalable algorithms are developed to tackle the ""big data
squared"" issue. We apply the proposed method to explore the reaction speed, an
indicator of cognitive functions, and its associations with brain MRI and
genetic factors using the UK Biobank database. Our study reveals a notable
connection between the caudate nucleus region of brain and specific significant
SNPs, along with their respective regulated genes, and the reaction speed.",2024-12-27T16:54:11Z,http://arxiv.org/abs/2412.19735v1,"Siqiang Su, Zhenghao Li, Long Feng, Ting Li"
"Dynamics, data and reconstruction","Data-driven learning is prevalent in many fields of science, mathematics and
engineering. The goal of data-driven learning of dynamical systems is to
interpret timeseries as a continuous observation of an underlying dynamical
system. This task is not well-posed for a variety of reasons. A dynamical
system may have multiple sub-systems co-existing within it. The nature of the
dataset depends on the portion of the phase space being viewed, and may thus my
confined to a sub-system. Secondly these sub-systems may be topologically
inter-weaved, so may be inseparable computationally. Thirdly, two timeseries
sampled separately from different dynamical systems may be close or even
indistinguishable. So there is no unqiue source for the timeseries. We show how
these ambiguities are circumvented if one considers dynamical systems and
measurement maps collectively. This is made possible in a category theoretical
framework, in which reconstruction is unique up to equivalences. We introduce
two categories of observed dynamical systems and timeseries-data. These are
related to the well known category of dynamical systems via functors. This
enables a functorial interpretation of the task of reconstruction as well.",2024-12-27T16:49:52Z,http://arxiv.org/abs/2412.19734v1,"Suddhasattwa Das, Tomoharu Suda"
"Generative Pretrained Embedding and Hierarchical Irregular Time Series
  Representation for Daily Living Activity Recognition","Within the evolving landscape of smart homes, the precise recognition of
daily living activities using ambient sensor data stands paramount. This paper
not only aims to bolster existing algorithms by evaluating two distinct
pretrained embeddings suited for ambient sensor activations but also introduces
a novel hierarchical architecture. We delve into an architecture anchored on
Transformer Decoder-based pre-trained embeddings, reminiscent of the GPT
design, and contrast it with the previously established state-of-the-art (SOTA)
ELMo embeddings for ambient sensors. Our proposed hierarchical structure
leverages the strengths of each pre-trained embedding, enabling the discernment
of activity dependencies and sequence order, thereby enhancing classification
precision. To further refine recognition, we incorporate into our proposed
architecture an hour-of-the-day embedding. Empirical evaluations underscore the
preeminence of the Transformer Decoder embedding in classification endeavors.
Additionally, our innovative hierarchical design significantly bolsters the
efficacy of both pre-trained embeddings, notably in capturing inter-activity
nuances. The integration of temporal aspects subtly but distinctively augments
classification, especially for time-sensitive activities. In conclusion, our
GPT-inspired hierarchical approach, infused with temporal insights, outshines
the SOTA ELMo benchmark.",2024-12-27T16:43:52Z,http://arxiv.org/abs/2412.19732v1,"Damien Bouchabou, Sao Mai Nguyen"
"Learning to Forget: Bayesian Time Series Forecasting using Recurrent
  Sparse Spectrum Signature Gaussian Processes","The signature kernel is a kernel between time series of arbitrary length and
comes with strong theoretical guarantees from stochastic analysis. It has found
applications in machine learning such as covariance functions for Gaussian
processes. A strength of the underlying signature features is that they provide
a structured global description of a time series. However, this property can
quickly become a curse when local information is essential and forgetting is
required; so far this has only been addressed with ad-hoc methods such as
slicing the time series into subsegments. To overcome this, we propose a
principled, data-driven approach by introducing a novel forgetting mechanism
for signatures. This allows the model to dynamically adapt its context length
to focus on more recent information. To achieve this, we revisit the recently
introduced Random Fourier Signature Features, and develop Random Fourier
Decayed Signature Features (RFDSF) with Gaussian processes (GPs). This results
in a Bayesian time series forecasting algorithm with variational inference,
that offers a scalable probabilistic algorithm that processes and transforms a
time series into a joint predictive distribution over time steps in one pass
using recurrence. For example, processing a sequence of length $10^4$ steps in
$\approx 10^{-2}$ seconds and in $&lt; 1\text{GB}$ of GPU memory. We demonstrate
that it outperforms other GP-based alternatives and competes with
state-of-the-art probabilistic time series forecasting algorithms.",2024-12-27T16:31:09Z,http://arxiv.org/abs/2412.19727v1,"Csaba Tóth, Masaki Adachi, Michael A. Osborne, Harald Oberhauser"
EEG-Reptile: An Automatized Reptile-Based Meta-Learning Library for BCIs,"Meta-learning, i.e., ""learning to learn"", is a promising approach to enable
efficient BCI classifier training with limited amounts of data. It can
effectively use collections of in some way similar classification tasks, with
rapid adaptation to new tasks where only minimal data are available. However,
applying meta-learning to existing classifiers and BCI tasks requires
significant effort. To address this issue, we propose EEG-Reptile, an automated
library that leverages meta-learning to improve classification accuracy of
neural networks in BCIs and other EEG-based applications. It utilizes the
Reptile meta-learning algorithm to adapt neural network classifiers of EEG data
to the inter-subject domain, allowing for more efficient fine-tuning for a new
subject on a small amount of data. The proposed library incorporates an
automated hyperparameter tuning module, a data management pipeline, and an
implementation of the Reptile meta-learning algorithm. EEG-Reptile automation
level allows using it without deep understanding of meta-learning. We
demonstrate the effectiveness of EEG-Reptile on two benchmark datasets (BCI IV
2a, Lee2019 MI) and three neural network architectures (EEGNet, FBCNet,
EEG-Inception). Our library achieved improvement in both zero-shot and few-shot
learning scenarios compared to traditional transfer learning approaches.",2024-12-27T16:24:31Z,http://arxiv.org/abs/2412.19725v1,"Daniil A. Berdyshev, Artem M. Grachev, Sergei L. Shishkin, Bogdan L. Kozyrskiy"
"Exploring low-rank structure for an inverse scattering problem with
  far-field data","The inverse scattering problem exhibits an inherent low-rank structure due to
its ill-posed nature; however developing low-rank structures for the inverse
scattering problem remains challenging. In this work, we introduce a novel
low-rank structure tailored for solving the inverse scattering problem. The
particular low-rank structure is given by the generalized prolate spheroidal
wave functions, computed stably and accurately via a Sturm-Liouville problem.
We first process the far-field data to obtain a post-processed data set within
a disk domain. Subsequently, the post-processed data are projected onto a
low-rank space given by the low-rank structure. The unknown is approximately
solved in this low-rank space, by dropping higher-order terms. The low-rank
structure leads to a H\""{o}lder-logarithmic type stability estimate for
arbitrary unknown functions, and a Lipschitz stability estimate for unknowns
belonging to a finite dimensional low-rank space. Various numerical experiments
are conducted to validate its performance, encompassing assessments of
resolution capability, robustness against randomly added noise and modeling
errors, and demonstration of increasing stability.",2024-12-27T16:24:20Z,http://arxiv.org/abs/2412.19724v1,"Yuyuan Zhou, Lorenzo Audibert, Shixu Meng, Bo Zhang"
"OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse
  Task Synthesis","Graphical User Interface (GUI) agents powered by Vision-Language Models
(VLMs) have demonstrated human-like computer control capability. Despite their
utility in advancing digital automation, a critical bottleneck persists:
collecting high-quality trajectory data for training. Common practices for
collecting such data rely on human supervision or synthetic data generation
through executing pre-defined tasks, which are either resource-intensive or
unable to guarantee data quality. Moreover, these methods suffer from limited
data diversity and significant gaps between synthetic data and real-world
environments. To address these challenges, we propose OS-Genesis, a novel GUI
data synthesis pipeline that reverses the conventional trajectory collection
process. Instead of relying on pre-defined tasks, OS-Genesis enables agents
first to perceive environments and perform step-wise interactions, then
retrospectively derive high-quality tasks to enable trajectory-level
exploration. A trajectory reward model is then employed to ensure the quality
of the generated trajectories. We demonstrate that training GUI agents with
OS-Genesis significantly improves their performance on highly challenging
online benchmarks. In-depth analysis further validates OS-Genesis's efficiency
and its superior data quality and diversity compared to existing synthesis
methods. Our codes, data, and checkpoints are available at
\href{https://qiushisun.github.io/OS-Genesis-Home/}{OS-Genesis Homepage}.",2024-12-27T16:21:58Z,http://arxiv.org/abs/2412.19723v1,"Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, Ben Kao, Guohao Li, Junxian He, Yu Qiao, Zhiyong Wu"
"Quantum correlations in a gravitational collapse simulation with
  SpheriCo.jl","We report on work using a newly developed code, SpheriCo.jl, that computes
the gravitational collapse of a spherical scalar field, where the scalar can be
either a classical field, or a quantum field operator. By utilising
summation-by-parts methods for the numerical derivatives we are able to
simulate the collapse longer than was possible previously due to enhanced
numerical stability. We present a suite of tests for the code that tests its
accuracy and stability, both for the classical and quantum fields. We are able
to observe critical behavior of gravitational collapse for the classical setup,
in agreement with expected results. The code is also used to compute two-point
correlation functions, with results that hint at a non-trivial correlation
across the horizon of Hawking quanta.",2024-12-27T16:20:27Z,http://arxiv.org/abs/2412.19722v1,"Benjamin Berczi, Magdalena Eriksson, Thanasis Giannakopoulos, Paul M. Saffin"
Sharpening Neural Implicit Functions with Frequency Consolidation Priors,"Signed Distance Functions (SDFs) are vital implicit representations to
represent high fidelity 3D surfaces. Current methods mainly leverage a neural
network to learn an SDF from various supervisions including signed distances,
3D point clouds, or multi-view images. However, due to various reasons
including the bias of neural network on low frequency content, 3D unaware
sampling, sparsity in point clouds, or low resolutions of images, neural
implicit representations still struggle to represent geometries with high
frequency components like sharp structures, especially for the ones learned
from images or point clouds. To overcome this challenge, we introduce a method
to sharpen a low frequency SDF observation by recovering its high frequency
components, pursuing a sharper and more complete surface. Our key idea is to
learn a mapping from a low frequency observation to a full frequency coverage
in a data-driven manner, leading to a prior knowledge of shape consolidation in
the frequency domain, dubbed frequency consolidation priors. To better
generalize a learned prior to unseen shapes, we introduce to represent
frequency components as embeddings and disentangle the embedding of the low
frequency component from the embedding of the full frequency component. This
disentanglement allows the prior to generalize on an unseen low frequency
observation by simply recovering its full frequency embedding through a
test-time self-reconstruction. Our evaluations under widely used benchmarks or
real scenes show that our method can recover high frequency component and
produce more accurate surfaces than the latest methods. The code, data, and
pre-trained models are available at \url{https://github.com/chenchao15/FCP}.",2024-12-27T16:18:46Z,http://arxiv.org/abs/2412.19720v1,"Chao Chen, Yu-Shen Liu, Zhizhong Han"
"Text2Insight: Transform natural language text into insights seamlessly
  using multi-model architecture","The growing demand for dynamic, user-centric data analysis and visualization
is evident across domains like healthcare, finance, and research. Traditional
visualization tools often fail to meet individual user needs due to their
static and predefined nature. To address this gap, Text2Insight is introduced
as an innovative solution that delivers customized data analysis and
visualizations based on user-defined natural language requirements. Leveraging
a multi-model architecture, Text2Insight transforms user inputs into actionable
insights and dynamic visualizations.
  The methodology begins with analyzing the input dataset to extract structural
details such as columns and values. A pre-trained Llama3 model converts the
user's natural language query into an SQL query, which is further refined using
a Named Entity Recognition (NER) model for accuracy. A chart predictor
determines the most suitable visualization type, while the Llama3 model
generates insights based on the SQL query's results. The output is a
user-friendly and visually informative chart. To enhance analysis capabilities,
the system integrates a question-answering model and a predictive model using
the BERT framework. These models provide insights into historical data and
predict future trends.
  Performance evaluation of Text2Insight demonstrates its effectiveness,
achieving high accuracy (99%), precision (100%), recall (99%), and F1-score
(99%), with a BLEU score of 0.5. The question-answering model attained an
accuracy of 89% and the predictive model achieved 70% accuracy. These results
validate Text2Insight as a robust and viable solution for transforming natural
language text into dynamic, user-specific data analysis and visualizations.",2024-12-27T16:17:22Z,http://arxiv.org/abs/2412.19718v1,Pradeep Sain
Low-Regularity Global solution for fractional NLS in modulation spaces,"We establish global well-posedness for the mass sub-critical nonlinear
fractional Schr\""odinger equation
  $$iu_t + (-\Delta)^\frac{\beta}{2} u \pm (|u|^{\alpha}u)=0$$
  with radial initial data in modulation spaces $M^{p,\frac{p}{p-1}}(\mathbb
R^n)$ with $2&lt;p$ sufficiently close to $2.$ Our order of dispersion $\beta$
lies in $(2n/ (2n-1), 2)$ for $n \geq 2$.",2024-12-27T16:15:10Z,http://arxiv.org/abs/2412.19714v1,"Divyang G. Bhimani, Diksha Dhingra, Vijay Kumar Sohani"
"ProKAN: Progressive Stacking of Kolmogorov-Arnold Networks for Efficient
  Liver Segmentation","The growing need for accurate and efficient 3D identification of tumors,
particularly in liver segmentation, has spurred considerable research into deep
learning models. While many existing architectures offer strong performance,
they often face challenges such as overfitting and excessive computational
costs. An adjustable and flexible architecture that strikes a balance between
time efficiency and model complexity remains an unmet requirement. In this
paper, we introduce proKAN, a progressive stacking methodology for
Kolmogorov-Arnold Networks (KANs) designed to address these challenges. Unlike
traditional architectures, proKAN dynamically adjusts its complexity by
progressively adding KAN blocks during training, based on overfitting behavior.
This approach allows the network to stop growing when overfitting is detected,
preventing unnecessary computational overhead while maintaining high accuracy.
Additionally, proKAN utilizes KAN's learnable activation functions modeled
through B-splines, which provide enhanced flexibility in learning complex
relationships in 3D medical data. Our proposed architecture achieves
state-of-the-art performance in liver segmentation tasks, outperforming
standard Multi-Layer Perceptrons (MLPs) and fixed KAN architectures. The
dynamic nature of proKAN ensures efficient training times and high accuracy
without the risk of overfitting. Furthermore, proKAN provides better
interpretability by allowing insight into the decision-making process through
its learnable coefficients. The experimental results demonstrate a significant
improvement in accuracy, Dice score, and time efficiency, making proKAN a
compelling solution for 3D medical image segmentation tasks.",2024-12-27T16:14:06Z,http://arxiv.org/abs/2412.19713v1,"Bhavesh Gyanchandani, Aditya Oza, Abhinav Roy"
"Causal machine learning for heterogeneous treatment effects in the
  presence of missing outcome data","When estimating heterogeneous treatment effects, missing outcome data can
complicate treatment effect estimation, causing certain subgroups of the
population to be poorly represented. In this work, we discuss this commonly
overlooked problem and consider the impact that missing at random (MAR) outcome
data has on causal machine learning estimators for the conditional average
treatment effect (CATE). We then propose two de-biased machine learning
estimators for the CATE, the mDR-learner and mEP-learner, which address the
issue of under-representation by integrating inverse probability of censoring
weights into the DR-learner and EP-learner respectively. We show that under
reasonable conditions, these estimators are oracle efficient, and illustrate
their favorable performance through simulated data settings, comparing them to
existing CATE estimators, including comparison to estimators which use common
missing data techniques. Guidance on the implementation of these estimators is
provided and we present an example of their application using the ACTG175
trial, exploring treatment effect heterogeneity when comparing Zidovudine
mono-therapy against alternative antiretroviral therapies among HIV-1-infected
individuals.",2024-12-27T16:10:03Z,http://arxiv.org/abs/2412.19711v1,"Matthew Pryce, Karla Diaz-Ordaz, Ruth H. Keogh, Stijn Vansteelandt"
"Noise Sensitivity of the Semidefinite Programs for Direct Data-Driven
  LQR","In this paper, we study the noise sensitivity of the semidefinite program
(SDP) proposed for direct data-driven infinite-horizon linear quadratic
regulator (LQR) problem for discrete-time linear time-invariant systems. While
this SDP is shown to find the true LQR controller in the noise-free setting, we
show that it leads to a trivial solution with zero gain matrices when data is
corrupted by noise, even when the noise is arbitrarily small. We then study a
variant of the SDP that includes a robustness promoting regularization term and
prove that regularization does not fully eliminate the sensitivity issue. In
particular, the solution of the regularized SDP converges in probability also
to a trivial solution.",2024-12-27T15:59:42Z,http://arxiv.org/abs/2412.19705v1,"Xiong Zeng, Laurent Bako, Necmiye Ozay"
"Search for the double Dalitz decays $η/η' \to e^+e^-μ^+μ^-$
  and $η' \to μ^+μ^-μ^+μ^-$","Using a data sample of $(10087 \pm 44) \times {10^{6}}$ $J/{\psi}$ events
collected with the BESIII detector, we search for the decays $\eta/\eta'\to
e^+e^-\mu^+\mu^-$ and $\eta' \to \mu^+\mu^-\mu^+\mu^-$ via the radiative decays
$J/{\psi}\to\gamma\eta$/$\gamma\eta'$. No excess of events over expected
background is observed for any of the decays of interest. At 90% confidence
level, we report the first upper limits on the branching fractions of $\eta'
\to e^{+}e^{-}\mu^{+}\mu^{-}$ and $\eta' \to \mu^{+}\mu^{-}\mu^{+}\mu^{-}$ to
be $ 1.75 \times {10^{-6}}$ and $5.28 \times {10^{-7}}$, respectively. In
addition, we set an upper limit on the branching fraction of $\eta \to
e^{+}e^{-}\mu^{+}\mu^{-}$ to be $6.88 \times {10^{-6}}$, which improves the
previous result by about two orders of magnitude.",2024-12-27T15:55:02Z,http://arxiv.org/abs/2412.19702v1,"BESIII Collaboration, M. Ablikim, M. N. Achasov, P. Adlarson, O. Afedulidis, X. C. Ai, R. Aliberti, A. Amoroso, Y. Bai, O. Bakina, I. Balossino, Y. Ban, H. -R. Bao, V. Batozskaya, K. Begzsuren, N. Berger, M. Berlowski, M. Bertani, D. Bettoni, F. Bianchi, E. Bianco, A. Bortone, I. Boyko, R. A. Briere, A. Brueggemann, H. Cai, X. Cai, A. Calcaterra, G. F. Cao, N. Cao, S. A. Cetin, X. Y. Chai, J. F. Chang, G. R. Che, Y. Z. Che, G. Chelkov, C. Chen, C. H. Chen, Chao Chen, G. Chen, H. S. Chen, H. Y. Chen, M. L. Chen, S. J. Chen, S. L. Chen, S. M. Chen, T. Chen, X. R. Chen, X. T. Chen, Y. B. Chen, Y. Q. Chen, Z. J. Chen, Z. Y. Chen, S. K. Choi, G. Cibinetto, F. Cossio, J. J. Cui, H. L. Dai, J. P. Dai, A. Dbeyssi, R. E. de Boer, D. Dedovich, C. Q. Deng, Z. Y. Deng, A. Denig, I. Denysenko, M. Destefanis, F. De Mori, B. Ding, X. X. Ding, Y. Ding, Y. Ding, J. Dong, L. Y. Dong, M. Y. Dong, X. Dong, M. C. Du, S. X. Du, Y. Y. Duan, Z. H. Duan, P. Egorov, Y. H. Fan, J. Fang, J. Fang, S. S. Fang, W. X. Fang, Y. Fang, Y. Q. Fang, R. Farinelli, L. Fava, F. Feldbauer, G. Felici, C. Q. Feng, J. H. Feng, Y. T. Feng, M. Fritsch, C. D. Fu, J. L. Fu, Y. W. Fu, H. Gao, X. B. Gao, Y. N. Gao, Yang Gao, S. Garbolino, I. Garzia, L. Ge, P. T. Ge, Z. W. Ge, C. Geng, E. M. Gersabeck, A. Gilman, K. Goetzen, L. Gong, W. X. Gong, W. Gradl, S. Gramigna, M. Greco, M. H. Gu, Y. T. Gu, C. Y. Guan, A. Q. Guo, L. B. Guo, M. J. Guo, R. P. Guo, Y. P. Guo, A. Guskov, J. Gutierrez, K. L. Han, T. T. Han, F. Hanisch, X. Q. Hao, F. A. Harris, K. K. He, K. L. He, F. H. Heinsius, C. H. Heinz, Y. K. Heng, C. Herold, T. Holtmann, P. C. Hong, G. Y. Hou, X. T. Hou, Y. R. Hou, Z. L. Hou, B. Y. Hu, H. M. Hu, J. F. Hu, Q. P. Hu, S. L. Hu, T. Hu, Y. Hu, G. S. Huang, K. X. Huang, L. Q. Huang, X. T. Huang, Y. P. Huang, Y. S. Huang, T. Hussain, F. Hölzken, N. Hüsken, N. in der Wiesche, J. Jackson, S. Janchiv, J. H. Jeong, Q. Ji, Q. P. Ji, W. Ji, X. B. Ji, X. L. Ji, Y. Y. Ji, X. Q. Jia, Z. K. Jia, D. Jiang, H. B. Jiang, P. C. Jiang, S. S. Jiang, T. J. Jiang, X. S. Jiang, Y. Jiang, J. B. Jiao, J. K. Jiao, Z. Jiao, S. Jin, Y. Jin, M. Q. Jing, X. M. Jing, T. Johansson, S. Kabana, N. Kalantar-Nayestanaki, X. L. Kang, X. S. Kang, M. Kavatsyuk, B. C. Ke, V. Khachatryan, A. Khoukaz, R. Kiuchi, O. B. Kolcu, B. Kopf, M. Kuessner, X. Kui, N. Kumar, A. Kupsc, W. Kühn, L. Lavezzi, T. T. Lei, Z. H. Lei, M. Lellmann, T. Lenz, C. Li, C. Li, C. H. Li, Cheng Li, D. M. Li, F. Li, G. Li, H. B. Li, H. J. Li, H. N. Li, Hui Li, J. R. Li, J. S. Li, K. Li, K. L. Li, L. J. Li, L. K. Li, Lei Li, M. H. Li, P. R. Li, Q. M. Li, Q. X. Li, R. Li, S. X. Li, T. Li, W. D. Li, W. G. Li, X. Li, X. H. Li, X. L. Li, X. Y. Li, X. Z. Li, Y. G. Li, Z. J. Li, Z. Y. Li, C. Liang, H. Liang, H. Liang, Y. F. Liang, Y. T. Liang, G. R. Liao, Y. P. Liao, J. Libby, A. Limphirat, C. C. Lin, C. X. Lin, D. X. Lin, T. Lin, B. J. Liu, B. X. Liu, C. Liu, C. X. Liu, F. Liu, F. H. Liu, Feng Liu, G. M. Liu, H. Liu, H. B. Liu, H. H. Liu, H. M. Liu, Huihui Liu, J. B. Liu, J. Y. Liu, K. Liu, K. Y. Liu, Ke Liu, L. Liu, L. C. Liu, Lu Liu, M. H. Liu, P. L. Liu, Q. Liu, S. B. Liu, T. Liu, W. K. Liu, W. M. Liu, X. Liu, X. Liu, Y. Liu, Y. Liu, Y. B. Liu, Z. A. Liu, Z. D. Liu, Z. Q. Liu, X. C. Lou, F. X. Lu, H. J. Lu, J. G. Lu, X. L. Lu, Y. Lu, Y. P. Lu, Z. H. Lu, C. L. Luo, J. R. Luo, M. X. Luo, T. Luo, X. L. Luo, X. R. Lyu, Y. F. Lyu, F. C. Ma, H. Ma, H. L. Ma, J. L. Ma, L. L. Ma, L. R. Ma, M. M. Ma, Q. M. Ma, R. Q. Ma, T. Ma, X. T. Ma, X. Y. Ma, Y. M. Ma, F. E. Maas, I. MacKay, M. Maggiora, S. Malde, Y. J. Mao, Z. P. Mao, S. Marcello, Z. X. Meng, J. G. Messchendorp, G. Mezzadri, H. Miao, T. J. Min, R. E. Mitchell, X. H. Mo, B. Moses, N. Yu. Muchnoi, J. Muskalla, Y. Nefedov, F. Nerling, L. S. Nie, I. B. Nikolaev, Z. Ning, S. Nisar, Q. L. Niu, W. D. Niu, Y. Niu, S. L. Olsen, S. L. Olsen, Q. Ouyang, S. Pacetti, X. Pan, Y. Pan, A. Pathak, Y. P. Pei, M. Pelizaeus, H. P. Peng, Y. Y. Peng, K. Peters, J. L. Ping, R. G. Ping, S. Plura, V. Prasad, F. Z. Qi, H. Qi, H. R. Qi, M. Qi, T. Y. Qi, S. Qian, W. B. Qian, C. F. Qiao, X. K. Qiao, J. J. Qin, L. Q. Qin, L. Y. Qin, X. P. Qin, X. S. Qin, Z. H. Qin, J. F. Qiu, Z. H. Qu, C. F. Redmer, K. J. Ren, A. Rivetti, M. Rolo, G. Rong, Ch. Rosner, M. Q. Ruan, S. N. Ruan, N. Salone, A. Sarantsev, Y. Schelhaas, K. Schoenning, M. Scodeggio, K. Y. Shan, W. Shan, X. Y. Shan, Z. J. Shang, J. F. Shangguan, L. G. Shao, M. Shao, C. P. Shen, H. F. Shen, W. H. Shen, X. Y. Shen, B. A. Shi, H. Shi, H. C. Shi, J. L. Shi, J. Y. Shi, Q. Q. Shi, S. Y. Shi, X. Shi, J. J. Song, T. Z. Song, W. M. Song, Y. J. Song, Y. X. Song, S. Sosio, S. Spataro, F. Stieler, S. S Su, Y. J. Su, G. B. Sun, G. X. Sun, H. Sun, H. K. Sun, J. F. Sun, K. Sun, L. Sun, S. S. Sun, T. Sun, W. Y. Sun, Y. Sun, Y. J. Sun, Y. Z. Sun, Z. Q. Sun, Z. T. Sun, C. J. Tang, G. Y. Tang, J. Tang, M. Tang, Y. A. Tang, L. Y. Tao, Q. T. Tao, M. Tat, J. X. Teng, V. Thoren, W. H. Tian, Y. Tian, Z. F. Tian, I. Uman, Y. Wan, S. J. Wang, B. Wang, B. L. Wang, Bo Wang, D. Y. Wang, F. Wang, H. J. Wang, J. J. Wang, J. P. Wang, K. Wang, L. L. Wang, M. Wang, N. Y. Wang, S. Wang, S. Wang, T. Wang, T. J. Wang, W. Wang, W. Wang, W. P. Wang, X. Wang, X. F. Wang, X. J. Wang, X. L. Wang, X. N. Wang, Y. Wang, Y. D. Wang, Y. F. Wang, Y. H. Wang, Y. L. Wang, Y. N. Wang, Y. Q. Wang, Yaqian Wang, Yi Wang, Z. Wang, Z. L. Wang, Z. Y. Wang, Ziyi Wang, D. H. Wei, F. Weidner, S. P. Wen, Y. R. Wen, U. Wiedner, G. Wilkinson, M. Wolke, L. Wollenberg, C. Wu, J. F. Wu, L. H. Wu, L. J. Wu, X. Wu, X. H. Wu, Y. Wu, Y. H. Wu, Y. J. Wu, Z. Wu, L. Xia, X. M. Xian, B. H. Xiang, T. Xiang, D. Xiao, G. Y. Xiao, S. Y. Xiao, Y. L. Xiao, Z. J. Xiao, C. Xie, X. H. Xie, Y. Xie, Y. G. Xie, Y. H. Xie, Z. P. Xie, T. Y. Xing, C. F. Xu, C. J. Xu, G. F. Xu, H. Y. Xu, M. Xu, Q. J. Xu, Q. N. Xu, W. Xu, W. L. Xu, X. P. Xu, Y. Xu, Y. C. Xu, Z. S. Xu, F. Yan, L. Yan, W. B. Yan, W. C. Yan, X. Q. Yan, H. J. Yang, H. L. Yang, H. X. Yang, J. H. Yang, T. Yang, Y. Yang, Y. F. Yang, Y. F. Yang, Y. X. Yang, Z. W. Yang, Z. P. Yao, M. Ye, M. H. Ye, J. H. Yin, Junhao Yin, Z. Y. You, B. X. Yu, C. X. Yu, G. Yu, J. S. Yu, M. C. Yu, T. Yu, X. D. Yu, Y. C. Yu, C. Z. Yuan, J. Yuan, J. Yuan, L. Yuan, S. C. Yuan, Y. Yuan, Z. Y. Yuan, C. X. Yue, A. A. Zafar, F. R. Zeng, S. H. Zeng, X. Zeng, Y. Zeng, Y. J. Zeng, Y. J. Zeng, X. Y. Zhai, Y. C. Zhai, Y. H. Zhan, A. Q. Zhang, B. L. Zhang, B. X. Zhang, D. H. Zhang, G. Y. Zhang, H. Zhang, H. Zhang, H. C. Zhang, H. H. Zhang, H. H. Zhang, H. Q. Zhang, H. R. Zhang, H. Y. Zhang, J. Zhang, J. Zhang, J. J. Zhang, J. L. Zhang, J. Q. Zhang, J. S. Zhang, J. W. Zhang, J. X. Zhang, J. Y. Zhang, J. Z. Zhang, Jianyu Zhang, L. M. Zhang, Lei Zhang, P. Zhang, Q. Y. Zhang, R. Y. Zhang, S. H. Zhang, Shulei Zhang, X. M. Zhang, X. Y Zhang, X. Y. Zhang, Y. Zhang, Y. Zhang, Y. T. Zhang, Y. H. Zhang, Y. M. Zhang, Yan Zhang, Z. D. Zhang, Z. H. Zhang, Z. L. Zhang, Z. Y. Zhang, Z. Y. Zhang, Z. Z. Zhang, G. Zhao, J. Y. Zhao, J. Z. Zhao, L. Zhao, Lei Zhao, M. G. Zhao, N. Zhao, R. P. Zhao, S. J. Zhao, Y. B. Zhao, Y. X. Zhao, Z. G. Zhao, A. Zhemchugov, B. Zheng, B. M. Zheng, J. P. Zheng, W. J. Zheng, Y. H. Zheng, B. Zhong, X. Zhong, H. Zhou, J. Y. Zhou, L. P. Zhou, S. Zhou, X. Zhou, X. K. Zhou, X. R. Zhou, X. Y. Zhou, Y. Z. Zhou, Z. C. Zhou, A. N. Zhu, J. Zhu, K. Zhu, K. J. Zhu, K. S. Zhu, L. Zhu, L. X. Zhu, S. H. Zhu, T. J. Zhu, W. D. Zhu, Y. C. Zhu, Z. A. Zhu, J. H. Zou, J. Zu"
"An Integrated Optimization and Deep Learning Pipeline for Predicting
  Live Birth Success in IVF Using Feature Optimization and Transformer-Based
  Models","In vitro fertilization (IVF) is a widely utilized assisted reproductive
technology, yet predicting its success remains challenging due to the
multifaceted interplay of clinical, demographic, and procedural factors. This
study develops a robust artificial intelligence (AI) pipeline aimed at
predicting live birth outcomes in IVF treatments. The pipeline uses anonymized
data from 2010 to 2018, obtained from the Human Fertilization and Embryology
Authority (HFEA). We evaluated the prediction performance of live birth success
as a binary outcome (success/failure) by integrating different feature
selection methods, such as principal component analysis (PCA) and particle
swarm optimization (PSO), with different traditional machine learning-based
classifiers including random forest (RF) and decision tree, as well as deep
learning-based classifiers including custom transformer-based model and a tab
transformer model with an attention mechanism. Our research demonstrated that
the best performance was achieved by combining PSO for feature selection with
the TabTransformer-based deep learning model, yielding an accuracy of 99.50%
and an AUC of 99.96%, highlighting its significant performance to predict live
births. This study establishes a highly accurate AI pipeline for predicting
live birth outcomes in IVF, demonstrating its potential to enhance personalized
fertility treatments.",2024-12-27T15:46:59Z,http://arxiv.org/abs/2412.19696v1,"Arezoo Borji, Hossam Haick, Birgit Pohn, Antonia Graf, Jana Zakall, S M Ragib Shahriar Islam, Gernot Kronreif, Daniel Kovatchki, Heinz Strohmer, Sepideh Hatamikia"
"Nonperturbative effects in triple-differential dijet and Z+jet
  production at the LHC","In comparisons of precision collider data to the most accurate highest-order
calculations in perturbative quantum chromodynamics (QCD), it is required to
correct for nonperturbative effects. Such effects are typically studied using
Monte Carlo event generators that complement fixed-order predictions with
perturbative parton showers and models for the nonperturbative effects of the
Underlying Event and hadronisation. Thereby, the final state of collision
events can be predicted at the level of stable particles, which serve as input
for full detector simulations.
  This article investigates the impact of nonperturbative effects on two
processes that may be used for precision determinations of the strong coupling
constant and the proton structure: the triple-differential dijet and Z+jet
production. While nonperturbative effects impact both processes, significant
differences among them are observed and further investigated. Indications are
found that the Underlying Event and hadronisation cannot fully explain these
differences and the perturbative modelling may play a significant role as well.",2024-12-27T15:42:07Z,http://arxiv.org/abs/2412.19694v1,"Stefan Gieseke, Maximilian Horzela, Manjit Kaur, Dari Leonardi, Klaus Rabbertz, Aayushi Singla, Cedric Verstege"
"A Review on the Integration of Artificial Intelligence and Medical
  Imaging in IVF Ovarian Stimulation","Artificial intelligence (AI) has emerged as a powerful tool to enhance
decision-making and optimize treatment protocols in in vitro fertilization
(IVF). In particular, AI shows significant promise in supporting
decision-making during the ovarian stimulation phase of the IVF process. This
review evaluates studies focused on the applications of AI combined with
medical imaging in ovarian stimulation, examining methodologies, outcomes, and
current limitations. Our analysis of 13 studies on this topic reveals that,
reveal that while AI algorithms demonstrated notable potential in predicting
optimal hormonal dosages, trigger timing, and oocyte retrieval outcomes, the
medical imaging data utilized predominantly came from two-dimensional (2D)
ultrasound which mainly involved basic quantifications, such as follicle size
and number, with limited use of direct feature extraction or advanced image
analysis techniques. This points to an underexplored opportunity where advanced
image analysis approaches, such as deep learning, and more diverse imaging
modalities, like three-dimensional (3D) ultrasound, could unlock deeper
insights. Additionally, the lack of explainable AI (XAI) in most studies raises
concerns about the transparency and traceability of AI-driven decisions - key
factors for clinical adoption and trust. Furthermore, many studies relied on
single-center designs and small datasets, which limit the generalizability of
their findings. This review highlights the need for integrating advanced
imaging analysis techniques with explainable AI methodologies, as well as the
importance of leveraging multicenter collaborations and larger datasets.
Addressing these gaps has the potential to enhance ovarian stimulation
management, paving the way for efficient, personalized, and data-driven
treatment pathways that improve IVF outcomes.",2024-12-27T15:29:08Z,http://arxiv.org/abs/2412.19688v1,"Jana Zakall, Birgit Pohn, Antonia Graf, Daniel Kovatchki, Arezoo Borji, Ragib Shahriar Islam, Hossam Haick, Heinz Strohmer, Sepideh Hatamikia"
"Boosting Private Domain Understanding of Efficient MLLMs: A Tuning-free,
  Adaptive, Universal Prompt Optimization Framework","Efficient multimodal large language models (EMLLMs), in contrast to
multimodal large language models (MLLMs), reduce model size and computational
costs and are often deployed on resource-constrained devices. However, due to
data privacy concerns, existing open-source EMLLMs rarely have access to
private domain-specific data during the pre-training process, making them
difficult to directly apply in device-specific domains, such as certain
business scenarios. To address this weakness, this paper focuses on the
efficient adaptation of EMLLMs to private domains, specifically in two areas:
1) how to reduce data requirements, and 2) how to avoid parameter fine-tuning.
Specifically, we propose a tun\textbf{\underline{I}}ng-free,
a\textbf{\underline{D}}aptiv\textbf{\underline{E}},
univers\textbf{\underline{AL}} \textbf{\underline{Prompt}} Optimization
Framework, abbreviated as \textit{\textbf{\ourmethod{}}} which consists of two
stages: 1) Predefined Prompt, based on the reinforcement searching strategy,
generate a prompt optimization strategy tree to acquire optimization priors; 2)
Prompt Reflection initializes the prompt based on optimization priors, followed
by self-reflection to further search and refine the prompt. By doing so,
\ourmethod{} elegantly generates the ``ideal prompts'' for processing private
domain-specific data. Note that our method requires no parameter fine-tuning
and only a small amount of data to quickly adapt to the data distribution of
private data. Extensive experiments across multiple tasks demonstrate that our
proposed \ourmethod{} significantly improves both efficiency and performance
compared to baselines.",2024-12-27T15:21:17Z,http://arxiv.org/abs/2412.19684v1,"Jiang Liu, Bolin Li, Haoyuan Li, Tianwei Lin, Wenqiao Zhang, Tao Zhong, Zhelun Yu, Jinghao Wei, Hao Cheng, Hao Jiang, Zheqi Lv, Juncheng Li, Siliang Tang, Yueting Zhuang"
"A Hybrid Technique for Plant Disease Identification and Localisation in
  Real-time","Over the past decade, several image-processing methods and algorithms have
been proposed for identifying plant diseases based on visual data. DNN (Deep
Neural Networks) have recently become popular for this task. Both traditional
image processing and DNN-based methods encounter significant performance issues
in real-time detection owing to computational limitations and a broad spectrum
of plant disease features. This article proposes a novel technique for
identifying and localising plant disease based on the Quad-Tree decomposition
of an image and feature learning simultaneously. The proposed algorithm
significantly improves accuracy and faster convergence in high-resolution
images with relatively low computational load. Hence it is ideal for deploying
the algorithm in a standalone processor in a remotely operated image
acquisition and disease detection system, ideally mounted on drones and robots
working on large agricultural fields. The technique proposed in this article is
hybrid as it exploits the advantages of traditional image processing methods
and DNN-based models at different scales, resulting in faster inference. The F1
score is approximately 0.80 for four disease classes corresponding to potato
and tomato crops.",2024-12-27T15:20:45Z,http://arxiv.org/abs/2412.19682v1,"Mahendra Kumar Gohil, Anirudha Bhattacharjee, Rwik Rana, Kishan Lal, Samir Kumar Biswas, Nachiketa Tiwari, Bishakh Bhattacharya"
Identifying clusters in Czekanowski's diagram,"Visualizing data through Czekanowski's diagram has as its aim the
illustration of the relationships between objects. Often, obvious clusters of
observations are directly visible. However, it is not straightforward to
precisely delineate these clusters. This paper presents the development of the
package RMaCzek, which now includes features for cluster identification in
Czekanowski diagrams.",2024-12-27T15:04:06Z,http://arxiv.org/abs/2412.19679v1,"Krzysztof Bartoszek, Ying Luo"
"Asymmetrical Reciprocity-based Federated Learning for Resolving
  Disparities in Medical Diagnosis","Geographic health disparities pose a pressing global challenge, particularly
in underserved regions of low- and middle-income nations. Addressing this issue
requires a collaborative approach to enhance healthcare quality, leveraging
support from medically more developed areas. Federated learning emerges as a
promising tool for this purpose. However, the scarcity of medical data and
limited computation resources in underserved regions make collaborative
training of powerful machine learning models challenging. Furthermore, there
exists an asymmetrical reciprocity between underserved and developed regions.
To overcome these challenges, we propose a novel cross-silo federated learning
framework, named FedHelp, aimed at alleviating geographic health disparities
and fortifying the diagnostic capabilities of underserved regions.
Specifically, FedHelp leverages foundational model knowledge via one-time API
access to guide the learning process of underserved small clients, addressing
the challenge of insufficient data. Additionally, we introduce a novel
asymmetric dual knowledge distillation module to manage the issue of asymmetric
reciprocity, facilitating the exchange of necessary knowledge between developed
large clients and underserved small clients. We validate the effectiveness and
utility of FedHelp through extensive experiments on both medical image
classification and segmentation tasks. The experimental results demonstrate
significant performance improvement compared to state-of-the-art baselines,
particularly benefiting clients in underserved regions.",2024-12-27T13:59:58Z,http://arxiv.org/abs/2412.19654v1,"Jiaqi Wang, Ziyi Yin, Quanzeng You, Lingjuan Lyu, Fenglong Ma"
"Distributed Download from an External Data Source in Faulty Majority
  Settings","We extend the study of retrieval problems in distributed networks, focusing
on improving the efficiency and resilience of protocols in the \emph{Data
Retrieval (DR) Model}. The DR Model consists of a complete network (i.e., a
clique) with $k$ peers, up to $\beta k$ of which may be Byzantine (for $\beta
\in [0, 1)$), and a trusted \emph{External Data Source} comprising an array $X$
of $n$ bits ($n \gg k$) that the peers can query. Additionally, the peers can
also send messages to each other. In this work, we focus on the Download
problem that requires all peers to learn $X$. Our primary goal is to minimize
the maximum number of queries made by any honest peer and additionally optimize
time.
  We begin with a randomized algorithm for the Download problem that achieves
optimal query complexity up to a logarithmic factor. For the stronger dynamic
adversary that can change the set of Byzantine peers from one round to the
next, we achieve the optimal time complexity in peer-to-peer communication but
with larger messages. In broadcast communication where all peers (including
Byzantine peers) are required to send the same message to all peers, with
larger messages, we achieve almost optimal time and query complexities for a
dynamic adversary. Finally, in a more relaxed crash fault model, where peers
stop responding after crashing, we address the Download problem in both
synchronous and asynchronous settings. Using a deterministic protocol, we
obtain nearly optimal results for both query complexity and message sizes in
these scenarios.",2024-12-27T13:55:00Z,http://arxiv.org/abs/2412.19649v1,"John Augustine, Soumyottam Chatterjee, Valerie King, Manish Kumar, Shachar Meir, David Peleg"
"Enhancing Vision-Language Tracking by Effectively Converting Textual
  Cues into Visual Cues","Vision-Language Tracking (VLT) aims to localize a target in video sequences
using a visual template and language description. While textual cues enhance
tracking potential, current datasets typically contain much more image data
than text, limiting the ability of VLT methods to align the two modalities
effectively. To address this imbalance, we propose a novel plug-and-play method
named CTVLT that leverages the strong text-image alignment capabilities of
foundation grounding models. CTVLT converts textual cues into interpretable
visual heatmaps, which are easier for trackers to process. Specifically, we
design a textual cue mapping module that transforms textual cues into target
distribution heatmaps, visually representing the location described by the
text. Additionally, the heatmap guidance module fuses these heatmaps with the
search image to guide tracking more effectively. Extensive experiments on
mainstream benchmarks demonstrate the effectiveness of our approach, achieving
state-of-the-art performance and validating the utility of our method for
enhanced VLT.",2024-12-27T13:54:32Z,http://arxiv.org/abs/2412.19648v1,"X. Feng, D. Zhang, S. Hu, X. Li, M. Wu, J. Zhang, X. Chen, K. Huang"
"Chimera: A Block-Based Neural Architecture Search Framework for
  Event-Based Object Detection","Event-based cameras are sensors that simulate the human eye, offering
advantages such as high-speed robustness and low power consumption. Established
Deep Learning techniques have shown effectiveness in processing event data.
Chimera is a Block-Based Neural Architecture Search (NAS) framework
specifically designed for Event-Based Object Detection, aiming to create a
systematic approach for adapting RGB-domain processing methods to the event
domain. The Chimera design space is constructed from various macroblocks,
including Attention blocks, Convolutions, State Space Models, and
MLP-mixer-based architectures, which provide a valuable trade-off between local
and global processing capabilities, as well as varying levels of complexity.
The results on the PErson Detection in Robotics (PEDRo) dataset demonstrated
performance levels comparable to leading state-of-the-art models, alongside an
average parameter reduction of 1.6 times.",2024-12-27T13:50:44Z,http://arxiv.org/abs/2412.19646v1,"Diego A. Silva, Ahmed Elsheikh, Kamilya Smagulova, Mohammed E. Fouda, Ahmed M. Eltawil"
A Brief Overlook on Magnetoplasmadynamic Thrusters,"This paper presents a comprehensive analysis of Magnetoplasmadynamic
Thrusters (MPDT), examining their working principles, performance
characteristics, and potential applications in space propulsion. The study
focuses on both self-field and applied-field MPDT variants, detailing the
fundamental physics of plasma generation, acceleration mechanisms through
Lorentz forces, and plasma detachment processes. Through mathematical modeling
and experimental data analysis, the paper demonstrates MPDTs' capability to
achieve high specific impulse and efficient propellant utilization compared to
chemical propulsion systems. While highlighting their advantages for deep space
missions and satellite operations, the study also addresses key challenges,
including high power requirements and thermal management issues. The research
concludes that despite current technological limitations, MPDTs show promising
potential for future space exploration, particularly for long-duration missions
requiring sustained thrust.",2024-12-27T13:30:13Z,http://arxiv.org/abs/2412.19636v1,Egemen Gover
"Primordial Black Hole Formation from the Upward Step Model: Avoiding
  Overproduction","We investigate the formation of primordial black holes (PBHs) in an upward
step inflationary model, where nonlinearities between curvature perturbations
and field fluctuations introduce a cutoff, deviating from the Gaussian case.
This necessitates a reevaluation of PBH formation, as $\mathcal{R}$ is not the
optimal variable for estimating abundance. Using the extended Press-Schechter
formalism, we show that non-Gaussianity modifies both the curvature
perturbation profile $\mathcal{R}(r)$ and the integration path in probability
space, significantly impacting PBH abundance. Our results reveal that the
abundance initially increases with the parameter $h$, which characterizes the
relaxation stage after the step. However, beyond a critical value ($h \simeq
5.9$), it sharply declines before rising again. Furthermore, we demonstrate
that non-Gaussianity introduces uncertainties in indirect PBH observations via
gravitational waves. Notably, we present an example where a positive $f_{\rm
NL}$ does not necessarily enhance PBH production, contrary to conventional
expectations. Finally, by accounting for non-perturbative effects, we resolve
the overproduction of PBHs suggested by pulsar timing array (PTA) data,
underscoring the critical importance of incorporating non-Gaussianity in future
studies.",2024-12-27T13:21:06Z,http://arxiv.org/abs/2412.19631v1,"Xiaoding Wang, Xiao-Han Ma, Yi-Fu Cai"
"Measurement of the branching fraction, polarization, and time-dependent
  $CP$ asymmetry in $B^0 \to ρ^+ρ^-$ decays and constraint on the CKM
  angle $φ_2$","We present a measurement of the branching fraction and fraction of
longitudinal polarization of $B^0 \to \rho^+ \rho^-$ decays, which have two
$\pi^0$'s in the final state. We also measure time-dependent $CP$ violation
parameters for decays into longitudinally polarized $\rho^+ \rho^-$ pairs. This
analysis is based on a data sample containing $(387\pm6) \times 10^6$ \BBbar
pairs collected with the Belle~II detector at the SuperKEKB asymmetric-energy
$e^+e^-$ collider in 2019-2022. We obtain ${B}(B^0\to\rho^+\rho^-) = (2.88
^{+0.23}_{-0.22} {}^{+0.29}_{-0.27}) \times 10^{-5}, f_{L} = 0.921
^{+0.024}_{-0.025} {}^{+0.017}_{-0.015}$, $S = -0.26\pm0.19\pm0.08$, and $C =
-0.02\pm0.12^{+0.06}_{-0.05}$, where the first uncertainties are statistical
and the second are systematic. We use these results to perform an isospin
analysis to constrain the CKM angle $\phi_2$ and obtain two solutions; the
result consistent with other Standard Model constraints is $\phi_2 =
(92.6^{+4.5}_{-4.8})^\circ$.",2024-12-27T13:00:10Z,http://arxiv.org/abs/2412.19624v1,"Belle II Collaboration, I. Adachi, L. Aggarwal, H. Ahmed, N. Akopov, M. Alhakami, A. Aloisio, N. Althubiti, N. Anh Ky, D. M. Asner, H. Atmacan, V. Aushev, M. Aversano, R. Ayad, V. Babu, N. K. Baghel, P. Bambade, Sw. Banerjee, M. Barrett, M. Bartl, J. Baudot, A. Baur, A. Beaubien, J. Becker, J. V. Bennett, V. Bertacchi, M. Bertemes, E. Bertholet, M. Bessner, S. Bettarini, B. Bhuyan, D. Biswas, A. Bobrov, D. Bodrov, A. Bolz, A. Bondar, J. Borah, A. Boschetti, A. Bozek, M. Bračko, P. Branchini, R. A. Briere, T. E. Browder, A. Budano, S. Bussino, Q. Campagna, M. Campajola, G. Casarosa, C. Cecchi, J. Cerasoli, M. -C. Chang, P. Chang, R. Cheaib, P. Cheema, B. G. Cheon, K. Chilikin, K. Chirapatpimol, H. -E. Cho, K. Cho, S. -J. Cho, S. -K. Choi, S. Choudhury, J. Cochran, L. Corona, J. X. Cui, E. De La Cruz-Burelo, S. A. De La Motte, G. De Nardo, G. De Pietro, R. de Sangro, M. Destefanis, S. Dey, F. Di Capua, J. Dingfelder, Z. Doležal, I. Domínguez Jiménez, T. V. Dong, X. Dong, M. Dorigo, D. Dossett, K. Dugic, G. Dujany, P. Ecker, J. Eppelt, P. Feichtinger, T. Ferber, T. Fillinger, C. Finck, G. Finocchiaro, A. Fodor, F. Forti, B. G. Fulsom, A. Gabrielli, E. Ganiev, M. Garcia-Hernandez, R. Garg, G. Gaudino, V. Gaur, A. Gaz, A. Gellrich, G. Ghevondyan, D. Ghosh, H. Ghumaryan, G. Giakoustidis, R. Giordano, A. Giri, P. Gironella Gironell, A. Glazov, B. Gobbo, R. Godang, O. Gogota, P. Goldenzweig, W. Gradl, E. Graziani, D. Greenwald, Z. Gruberová, Y. Guan, K. Gudkova, I. Haide, T. Hara, C. Harris, K. Hayasaka, S. Hazra, C. Hearty, M. T. Hedges, A. Heidelbach, I. Heredia de la Cruz, M. Hernández Villanueva, T. Higuchi, M. Hoek, M. Hohmann, R. Hoppe, P. Horak, C. -L. Hsu, T. Humair, T. Iijima, K. Inami, N. Ipsita, A. Ishikawa, R. Itoh, M. Iwasaki, D. Jacobi, W. W. Jacobs, E. -J. Jang, Y. Jin, A. Johnson, H. Junkerkalefeld, M. Kaleta, A. B. Kaliyar, J. Kandra, F. Keil, C. Ketter, C. Kiesling, C. -H. Kim, D. Y. Kim, J. -Y. Kim, K. -H. Kim, Y. -K. Kim, K. Kinoshita, P. Kodyš, T. Koga, S. Kohani, K. Kojima, A. Korobov, S. Korpar, E. Kovalenko, R. Kowalewski, P. Križan, P. Krokovny, T. Kuhr, Y. Kulii, R. Kumar, K. Kumara, T. Kunigo, A. Kuzmin, Y. -J. Kwon, S. Lacaprara, K. Lalwani, T. Lam, L. Lanceri, J. S. Lange, T. S. Lau, M. Laurenza, R. Leboucher, F. R. Le Diberder, M. J. Lee, C. Lemettais, P. Leo, L. K. Li, Q. M. Li, W. Z. Li, Y. Li, Y. B. Li, Y. P. Liao, J. Libby, J. Lin, S. Lin, M. H. Liu, Q. Y. Liu, Z. Q. Liu, D. Liventsev, S. Longo, T. Lueck, C. Lyu, Y. Ma, C. Madaan, M. Maggiora, S. P. Maharana, R. Maiti, G. Mancinelli, R. Manfredi, E. Manoni, M. Mantovano, D. Marcantonio, S. Marcello, C. Marinas, C. Martellini, A. Martens, A. Martini, T. Martinov, L. Massaccesi, M. Masuda, K. Matsuoka, D. Matvienko, S. K. Maurya, M. Maushart, J. A. McKenna, F. Meier, D. Meleshko, M. Merola, C. Miller, M. Mirra, S. Mitra, K. Miyabayashi, H. Miyake, G. B. Mohanty, S. Mondal, S. Moneta, H. -G. Moser, R. Mussa, I. Nakamura, M. Nakao, Y. Nakazawa, M. Naruki, Z. Natkaniec, A. Natochii, M. Nayak, G. Nazaryan, M. Neu, S. Nishida, S. Ogawa, R. Okubo, H. Ono, Y. Onuki, G. Pakhlova, S. Pardi, K. Parham, H. Park, J. Park, K. Park, S. -H. Park, A. Passeri, S. Patra, T. K. Pedlar, I. Peruzzi, R. Peschke, R. Pestotnik, L. E. Piilonen, P. L. M. Podesta-Lerma, T. Podobnik, S. Pokharel, C. Praz, S. Prell, E. Prencipe, M. T. Prim, H. Purwar, S. Raiz, K. Ravindran, J. U. Rehman, M. Reif, S. Reiter, M. Remnev, L. Reuter, D. Ricalde Herrmann, I. Ripp-Baudot, G. Rizzo, M. Roehrken, J. M. Roney, A. Rostomyan, N. Rout, Y. Sakai, D. A. Sanders, S. Sandilya, L. Santelj, V. Savinov, B. Scavino, C. Schwanda, A. J. Schwartz, Y. Seino, A. Selce, K. Senyo, J. Serrano, M. E. Sevior, C. Sfienti, W. Shan, X. D. Shi, T. Shillington, J. -G. Shiu, D. Shtol, B. Shwartz, A. Sibidanov, F. Simon, J. Skorupa, R. J. Sobie, M. Sobotzik, A. Soffer, A. Sokolov, E. Solovieva, S. Spataro, B. Spruck, W. Song, M. Starič, P. Stavroulakis, S. Stefkova, R. Stroili, J. Strube, M. Sumihama, K. Sumisawa, N. Suwonjandee, H. Svidras, M. Takizawa, U. Tamponi, K. Tanida, F. Tenchini, A. Thaller, O. Tittel, R. Tiwary, E. Torassa, K. Trabelsi, I. Tsaklidis, I. Ueda, T. Uglov, K. Unger, Y. Unno, K. Uno, S. Uno, P. Urquijo, Y. Ushiroda, S. E. Vahsen, R. van Tonder, K. E. Varvell, M. Veronesi, A. Vinokurova, V. S. Vismaya, L. Vitale, V. Vobbilisetti, R. Volpe, M. Wakai, S. Wallner, M. -Z. Wang, A. Warburton, M. Watanabe, S. Watanuki, C. Wessel, E. Won, X. P. Xu, B. D. Yabsley, S. Yamada, W. Yan, J. Yelton, J. H. Yin, K. Yoshihara, J. Yuan, Y. Yusa, L. Zani, V. Zhilich, J. S. Zhou, Q. D. Zhou, L. Zhu, R. Žlebčík"
Signatures of prediction during natural listening in MEG data?,"The brain uses contextual information and prior knowledge to anticipate
upcoming content during language comprehension. Recent research has shown
predictive signals can be revealed in pre-onset ECoG activity during
naturalistic narrative listening, by building encoding models based on word
embeddings from Large Language Models (LLMs). Similarly, evidence for
long-range predictive encoding has been observed in fMRI data, where
incorporating embeddings for multiple upcoming words in a narrative improves
alignment with brain activity. This study examines whether similar predictive
information can be detected in MEG, a technique with higher temporal resolution
than fMRI but a lower signal-to-noise ratio than ECoG. Our findings indicate
that MEG captures pre-onset representations up to 1 second before word onset,
consistent with ECoG results. However, unlike fMRI findings, incorporating
future word embeddings did not enhance MEG encoding, even for one word into the
future, which suggests that the pre-onset encoding may not reflect predictive
processing. This work demonstrates that MEG combined with LLMs is a valuable
approach for studying language processing in naturalistic narratives and
highlights the need to study further what constitutes evidence for prediction
during natural listening.",2024-12-27T12:49:03Z,http://arxiv.org/abs/2412.19622v1,"Sahel Azizpour, Britta U. Westner, Jakub Szewczyk, Umut Güçlü, Linda Geerligs"
"The Key Steps and Distinct Performance Trends of Pyrrolic vs. Pyridinic
  M-N-C Catalysts in Electrocatalytic Nitrate Reduction","Electrochemical nitrate reduction reaction(NO3RR)offers a sustainable route
for ambient ammonia synthesis. While metal-nitrogen-carbon (M-N-C) single-atom
catalysts have emerged as promising candidates for NO3RR, the
structure-activity relations underlying their catalytic behavior remain to be
elucidated. Through systematic analysis of reported experimental data and
pH-field coupled microkinetic modelling on a reversible hydrogen electrode
(RHE) scale, we reveal that the coordination-dependent activity originates from
distinct scaling relations governed by metal-intermediate interactions.
M-N-Pyrrolic catalysts demonstrate higher turnover frequencies for ammonia
production, whereas M-N-Pyridinic catalysts exhibit broader activity ranges
across the activity volcano plot. Meanwhile, the adsorption and protonation of
nitrate, which is a step often dismissed and/or assumed to be simultaneous in
many previous reports, is identified to be the rate-determining step (RDS) in
NO3RR. Remarkably, our subsequent experimental validation confirms the
theoretical predictions under both neutral and alkaline conditions. This study
offers a comprehensive mechanistic framework for interpreting the
electrocatalytic activity of M-N-C catalysts in NO3RR, showing that a classical
thermodynamic limiting-potential model is not sufficiently accurate to capture
the RDS and the catalytic performance trends of different materials (even on
M-N-Pyrrolic and M-N-Pyridinic catalysts). These findings provide brand new
insights into the reaction mechanism of NO3RR and establish fundamental design
principles for electrocatalytic ammonia synthesis.",2024-12-27T12:23:09Z,http://arxiv.org/abs/2412.19615v1,"Qiuling Jiang, Mingyao Gu, Tianyi Wang, Fangzhou Liu, Xin Yang, Di Zhang, Zhijian Wu, Ying Wang, Li Wei, Hao Li"
Super-bath Quantum Eigensolver,"Simulating the dynamics of a system coupled to a suitable environment is a
promising approach in quantum computing for determining the ground state of
physical systems. However, this approach requires not only the
$\textit{existence}$ of an environment that allows the system to dissipate
energy and evolve to its ground state, but also the environment's
characteristics to be $\textit{known}$ in detail. In this paper, we propose an
algorithm with a sufficient condition for achieving polynomial-time complexity
in ground state preparation: the existence of an environment that enables the
system to evolve to its ground state in polynomial time, while such
environment's details may remain $\textit{unknown}$. The proposed algorithm is
Super-bath Quantum Eigensolver, which solves the system's ground state by
utilizing quasi-steady state preparation and simulating the coupling between
the system and the super-bath. Supported by experimental lifetime data of
nuclear metastable states, we suggest that our algorithm is applicable to
determine nuclear ground states in polynomial time. These results highlight the
potential advantage of quantum computing in addressing ground state problems in
real-world physical systems.",2024-12-27T11:46:47Z,http://arxiv.org/abs/2412.19599v1,"Tianren Wang, Zongkang Zhang, Bing-Nan Lu, Mauro Cirio, Ying Li"
Composite nature of the $T_{cc}$ state,"In 2021, LHCb collaboration reported a very narrow state in the $D^0D^0\pi^+$
mass spectrum just below the $D^{*+}D^0$ mass threshold. We consider the
influence of the Castillejo-Dalitz-Dyson (CDD) pole in the scattering amplitude
to derive a general treatment for the two-body final state interaction near its
threshold. The line shape (or the energy dependent event distribution) are then
obtained, where the parameters can be fixed by fitting to the experimental data
on the $D^0D^0\pi^+$ mass spectrum. Within our method the data are quite well
reproduced. The pole structure in the complex energy plane indicates the bound
state structure of the $T_{cc}$ state. The compositeness as a measure of
molecule component in its hadron wave function is predicted to be
$0.23_{-0.09}^{+0.40}$. The non-molecular component, e.g., the compact
tetraquark also takes a non-negligible portion.",2024-12-27T11:42:44Z,http://arxiv.org/abs/2412.19597v1,"Xian-Wei Kang, Wen-Shuo Ding"
A counterexample to a Brenti-Carnevale conjecture,"Recently, F. Brenti put a preprint on the arXiv with several interesting open
problems on Coxeter groups and unimodality. In this note, we refute one of
these conjectures with a counterexample and provide supporting data related to
it. This work serves as an initial step toward further exploration of the
topic.",2024-12-27T11:26:53Z,http://arxiv.org/abs/2412.19593v1,"Nathan Chapelier-Laget, Jean Fromentin"
DAS3R: Dynamics-Aware Gaussian Splatting for Static Scene Reconstruction,"We propose a novel framework for scene decomposition and static background
reconstruction from everyday videos. By integrating the trained motion masks
and modeling the static scene as Gaussian splats with dynamics-aware
optimization, our method achieves more accurate background reconstruction
results than previous works. Our proposed method is termed DAS3R, an
abbreviation for Dynamics-Aware Gaussian Splatting for Static Scene
Reconstruction. Compared to existing methods, DAS3R is more robust in complex
motion scenarios, capable of handling videos where dynamic objects occupy a
significant portion of the scene, and does not require camera pose inputs or
point cloud data from SLAM-based methods. We compared DAS3R against recent
distractor-free approaches on the DAVIS and Sintel datasets; DAS3R demonstrates
enhanced performance and robustness with a margin of more than 2 dB in PSNR.
The project's webpage can be accessed via \url{https://kai422.github.io/DAS3R/}",2024-12-27T10:59:46Z,http://arxiv.org/abs/2412.19584v1,"Kai Xu, Tze Ho Elden Tse, Jizong Peng, Angela Yao"
"A Comparative Study of Machine Unlearning Techniques for Image and Text
  Classification Models","Machine Unlearning has emerged as a critical area in artificial intelligence,
addressing the need to selectively remove learned data from machine learning
models in response to data privacy regulations. This paper provides a
comprehensive comparative analysis of six state-of-theart unlearning techniques
applied to image and text classification tasks. We evaluate their performance,
efficiency, and compliance with regulatory requirements, highlighting their
strengths and limitations in practical scenarios. By systematically analyzing
these methods, we aim to provide insights into their applicability,
challenges,and tradeoffs, fostering advancements in the field of ethical and
adaptable machine learning.",2024-12-27T10:58:55Z,http://arxiv.org/abs/2412.19583v1,"Omar M. Safa, Mahmoud M. Abdelaziz, Mustafa Eltawy, Mohamed Mamdouh, Moamen Gharib, Salaheldin Eltenihy, Nagia M. Ghanem, Mohamed M. Ismail"
"Readout of strongly coupled NV center-pair spin states with deep neural
  networks","Optically addressable electron spin clusters are of interest for quantum
computation, simulation and sensing. However, with interaction length scales of
a few tens of nanometers in the strong coupling regime, they are unresolved in
conventional confocal microscopy, making individual readout problematic. Here
we show that when using a single shot readout technique, collective states of
the combined register space become accessible. By using spin to charge
conversion of the defects we draw the connection between the intricate photon
count statistics with spin state tomography using deep neural networks. This
approach is particularly versatile with further scaling the number of
constituent spins in a cluster due to complexity of the analytical treatment.
We perform a proof of concept measurement of the correlated classical signal,
paving the way for using our technique in realistic applications.",2024-12-27T10:56:04Z,http://arxiv.org/abs/2412.19581v1,"Matthew Joliffe, Vadim Vorobyov, Jörg Wrachtrup"
"Gauging or extending bulk and boundary conformal field theories:
  Application to bulk and domain wall problem in topological matter and their
  descriptions by (mock) modular covariant","We study gauging operations (or group extensions) in (smeared) boundary
conformal field theories (BCFTs) and bulk conformal field theories and their
applications to various phenomena in topologically ordered systems. We apply
the resultant theories to the correspondence between the renormalization group
(RG) flow of CFTs and the classification of topological quantum field theories
in the testable information of general classes of partition functions. One can
obtain the bulk topological properties of $2+1$ dimensional topological ordered
phase corresponding to the massive RG flow of $1+1$ dimensional systems, or
smeared BCFT. We present an obstruction of mass condensation for smeared BCFT
analogous to the Lieb-Shultz-Mattis theorem for noninvertible symmetry. Related
to the bulk topological degeneracies in $2+1$ dimensions and quantum phases in
$1+1$ dimensions we construct a new series of BCFT. We also investigate the
implications of the massless RG flow of $1+1$ dimensional CFT to $2+1$
dimensional topological order which corresponds to the earlier proposal by L.
Kong and H. Zheng in [Nucl. Phys. B 966 (2021), 115384], arXiv:1912.01760
closely related to the integer-spin simple current by Schellekens and
Gato-Rivera. We study the properties of the product of two CFTs connected by
the two kinds of massless flows. The (mock) modular covariants appearing in the
analysis seem to contain new ones. By applying the folding trick to the coupled
model, we provide a general method to solve the gapped and charged domain wall.
One can obtain the general phenomenology of the transportation of anyons
through the domain wall. Our work gives a unified direction for the future
theoretical and numerical studies of the topological phase based on the
established data of classifications of conformal field theories or modular
invariants.",2024-12-27T10:46:30Z,http://arxiv.org/abs/2412.19577v1,Yoshiki Fukusumi
"The possible long-term periodic variability of the extremely luminous
  quasar WISE J090924.01+000211.1","The extremely luminous infrared galaxy (ELIRG), WISE J090924.01+000211.1
(hereafter; WISE J0909+0002, $z=1.87$) is an extraordinary object with a quasar
aspect. This study performs monitoring observations of WISE J0909+0002 with the
105 cm Murikabushi telescope, Okayama and Akeno 50 cm telescopes/MITSuME ($g'$,
$R_{\rm c}$, and $I_{\rm c}$ bands), and the SaCRA 55 cm telescope/MuSaSHI
($r$, $i$, and $z$ bands). We obtain the following results by combining the
UV/optical light curves of the CRTS, Pan-STARRS, and ZTF archive data, and our
observational data: (1) the light curves of WISE J0909+0002 present
quasi-periodic (sinusoidal) oscillations with the rest-frame period of $\sim$
660$-$689 day; (2) the structure functions of WISE J0909+0002 do not show a
damped random walk (DRW) trend; (3) the mock DRW light curves present
periodic-like trend on rare occasions in 10000 simulations; (4) the
relativistic boost scenario is favored, since the relation between variability
amplitude and power-law slope ratio is consistent with the theoretical
prediction of this scenario, and a substantial parameter space exists between
the inclination angles and the black hole mass; (5) the circumbinary disk model
is difficult to explain the spectral energy distribution of our target; (6) the
significant radio flux density of WISE J0909+0002 is not detected from the VLA
FIRST Survey, thus the radio jet precession scenario is ruled out. From our
results, the Doppler boost scenario is likely as a cause of the periodic
variability, consequently the quasi-periodic oscillations in WISE J0909+0002 is
possibly interpreted by a supermassive blackhole binary. Additional
observations to investigate the continuity of the periodic trend would bring
new insights into mechanisms of the quasi-periodic oscillations and/or ELIRGs.",2024-12-27T10:33:11Z,http://arxiv.org/abs/2412.19573v1,"Takashi Horiuchi, Yoshiki Toba, Toru Misawa, Katsuhiro L. Murata, Keisuke Isogai, Yoichi Yatsu, Ichiro Takahashi, Mahito Sasada, Masafumi Niwano, Narikazu Higuchi, Shunsuke Hayatsu, Hibiki Seki, Yumiko Oasa, Rikuto Sato"
"Nonminimally coupled Dark Matter in Clusters of Galaxies: a fully
  comprehensive analysis","In this study, we explore how a non-minimal coupling between dark matter and
gravity can affect the behavior of dark matter in galaxy clusters. We have
considered the case of a disformal coupling, which leads to a modification of
the Poisson equation. Building on an earlier work, we expand the analysis
considering all possible disformal coupling scenarios and employing various
dark matter density profiles. In doing so, we aim to constrain the key
parameter in our model, the characteristic coupling length. To achieve this, we
analyze data from a combination of strong and weak lensing using three
statistical approaches: a single cluster fitting procedure, a joint analysis,
and one with stacked profiles. Our findings show that the coupling length is
typically very small, thus being fully consistent with general relativity,
although with an upper limit at $1\sigma$ which is of the order of $100$ kpc.",2024-12-27T10:12:29Z,http://arxiv.org/abs/2412.19569v1,"Saboura Zamani, Vincenzo Salzano, Dario Bettoni"
Quantiles under ambiguity and risk sharing,"Choquet capacities and integrals are central concepts in decision making
under ambiguity or model uncertainty, pioneered by Schmeidler. Motivated by
risk optimization problems for quantiles under ambiguity, we study the subclass
of Choquet integrals, called Choquet quantiles, which generalizes the usual
(probabilistic) quantiles, also known as Value-at-Risk in finance, from
probabilities to capacities. Choquet quantiles share many features with
probabilistic quantiles, in terms of axiomatic representation, optimization
formulas, and risk sharing. We characterize Choquet quantiles via only one
axiom, called ordinality. We prove that the inf-convolution of Choquet
quantiles is again a Choquet quantile, leading to explicit optimal allocations
in risk sharing problems for quantile agents under ambiguity. A new class of
risk measures, Choquet Expected Shortfall, is introduced, which enjoys most
properties of the coherent risk measure Expected Shortfall. Our theory is
complemented by optimization algorithms, numerical examples, and a stylized
illustration with financial data.",2024-12-27T09:22:19Z,http://arxiv.org/abs/2412.19546v1,"Peng Liu, Tiantian Mao, Ruodu Wang"
"TARGA: Targeted Synthetic Data Generation for Practical Reasoning over
  Structured Data","Semantic parsing, which converts natural language questions into logic forms,
plays a crucial role in reasoning within structured environments. However,
existing methods encounter two significant challenges: reliance on extensive
manually annotated datasets and limited generalization capability to unseen
examples. To tackle these issues, we propose Targeted Synthetic Data Generation
(TARGA), a practical framework that dynamically generates high-relevance
synthetic data without manual annotation. Starting from the pertinent entities
and relations of a given question, we probe for the potential relevant queries
through layer-wise expansion and cross-layer combination. Then we generate
corresponding natural language questions for these constructed queries to
jointly serve as the synthetic demonstrations for in-context learning.
Experiments on multiple knowledge base question answering (KBQA) datasets
demonstrate that TARGA, using only a 7B-parameter model, substantially
outperforms existing non-fine-tuned methods that utilize close-sourced model,
achieving notable improvements in F1 scores on GrailQA(+7.7) and
KBQA-Agent(+12.2). Furthermore, TARGA also exhibits superior sample efficiency,
robustness, and generalization capabilities under non-I.I.D. settings.",2024-12-27T09:16:39Z,http://arxiv.org/abs/2412.19544v1,"Xiang Huang, Jiayu Shen, Shanshan Huang, Sitao Cheng, Xiaxia Wang, Yuzhong Qu"
Diverse Rare Sample Generation with Pretrained GANs,"Deep generative models are proficient in generating realistic data but
struggle with producing rare samples in low density regions due to their
scarcity of training datasets and the mode collapse problem. While recent
methods aim to improve the fidelity of generated samples, they often reduce
diversity and coverage by ignoring rare and novel samples. This study proposes
a novel approach for generating diverse rare samples from high-resolution image
datasets with pretrained GANs. Our method employs gradient-based optimization
of latent vectors within a multi-objective framework and utilizes normalizing
flows for density estimation on the feature space. This enables the generation
of diverse rare images, with controllable parameters for rarity, diversity, and
similarity to a reference image. We demonstrate the effectiveness of our
approach both qualitatively and quantitatively across various datasets and GANs
without retraining or fine-tuning the pretrained GANs.",2024-12-27T09:10:30Z,http://arxiv.org/abs/2412.19543v1,"Subeen Lee, Jiyeon Han, Soyeon Kim, Jaesik Choi"
Interacted Object Grounding in Spatio-Temporal Human-Object Interactions,"Spatio-temporal Human-Object Interaction (ST-HOI) understanding aims at
detecting HOIs from videos, which is crucial for activity understanding.
However, existing whole-body-object interaction video benchmarks overlook the
truth that open-world objects are diverse, that is, they usually provide
limited and predefined object classes. Therefore, we introduce a new open-world
benchmark: Grounding Interacted Objects (GIO) including 1,098 interacted
objects class and 290K interacted object boxes annotation. Accordingly, an
object grounding task is proposed expecting vision systems to discover
interacted objects. Even though today's detectors and grounding methods have
succeeded greatly, they perform unsatisfactorily in localizing diverse and rare
objects in GIO. This profoundly reveals the limitations of current vision
systems and poses a great challenge. Thus, we explore leveraging
spatio-temporal cues to address object grounding and propose a 4D
question-answering framework (4D-QA) to discover interacted objects from
diverse videos. Our method demonstrates significant superiority in extensive
experiments compared to current baselines. Data and code will be publicly
available at https://github.com/DirtyHarryLYL/HAKE-AVA.",2024-12-27T09:08:46Z,http://arxiv.org/abs/2412.19542v1,"Xiaoyang Liu, Boran Wen, Xinpeng Liu, Zizheng Zhou, Hongwei Fan, Cewu Lu, Lizhuang Ma, Yulong Chen, Yong-Lu Li"
"Scalable Hierarchical Reinforcement Learning for Hyper Scale Multi-Robot
  Task Planning","To improve the efficiency of warehousing system and meet huge customer
orders, we aim to solve the challenges of dimension disaster and dynamic
properties in hyper scale multi-robot task planning (MRTP) for robotic mobile
fulfillment system (RMFS). Existing research indicates that hierarchical
reinforcement learning (HRL) is an effective method to reduce these challenges.
Based on that, we construct an efficient multi-stage HRL-based multi-robot task
planner for hyper scale MRTP in RMFS, and the planning process is represented
with a special temporal graph topology. To ensure optimality, the planner is
designed with a centralized architecture, but it also brings the challenges of
scaling up and generalization that require policies to maintain performance for
various unlearned scales and maps. To tackle these difficulties, we first
construct a hierarchical temporal attention network (HTAN) to ensure basic
ability of handling inputs with unfixed lengths, and then design multi-stage
curricula for hierarchical policy learning to further improve the scaling up
and generalization ability while avoiding catastrophic forgetting.
Additionally, we notice that policies with hierarchical structure suffer from
unfair credit assignment that is similar to that in multi-agent reinforcement
learning, inspired of which, we propose a hierarchical reinforcement learning
algorithm with counterfactual rollout baseline to improve learning performance.
Experimental results demonstrate that our planner outperform other
state-of-the-art methods on various MRTP instances in both simulated and
real-world RMFS. Also, our planner can successfully scale up to hyper scale
MRTP instances in RMFS with up to 200 robots and 1000 retrieval racks on
unlearned maps while keeping superior performance over other methods.",2024-12-27T09:07:11Z,http://arxiv.org/abs/2412.19538v1,"Xuan Zhou, Xiang Shi, Lele Zhang, Chen Chen, Hongbo Li, Lin Ma, Fang Deng, Jie Chen"
"Finger in Camera Speaks Everything: Unconstrained Air-Writing for
  Real-World","Air-writing is a challenging task that combines the fields of computer vision
and natural language processing, offering an intuitive and natural approach for
human-computer interaction. However, current air-writing solutions face two
primary challenges: (1) their dependency on complex sensors (e.g., Radar, EEGs
and others) for capturing precise handwritten trajectories, and (2) the absence
of a video-based air-writing dataset that covers a comprehensive vocabulary
range. These limitations impede their practicality in various real-world
scenarios, including the use on devices like iPhones and laptops. To tackle
these challenges, we present the groundbreaking air-writing Chinese character
video dataset (AWCV-100K-UCAS2024), serving as a pioneering benchmark for
video-based air-writing. This dataset captures handwritten trajectories in
various real-world scenarios using commonly accessible RGB cameras, eliminating
the need for complex sensors. AWCV-100K-UCAS2024 includes 8.8 million video
frames, encompassing the complete set of 3,755 characters from the GB2312-80
level-1 set (GB1). Furthermore, we introduce our baseline approach, the
video-based character recognizer (VCRec). VCRec adeptly extracts fingertip
features from sparse visual cues and employs a spatio-temporal sequence module
for analysis. Experimental results showcase the superior performance of VCRec
compared to existing models in recognizing air-written characters, both
quantitatively and qualitatively. This breakthrough paves the way for enhanced
human-computer interaction in real-world contexts. Moreover, our approach
leverages affordable RGB cameras, enabling its applicability in a diverse range
of scenarios. The code and data examples will be made public at
https://github.com/wmeiqi/AWCV.",2024-12-27T09:04:04Z,http://arxiv.org/abs/2412.19537v1,"Meiqi Wu, Kaiqi Huang, Yuanqiang Cai, Shiyu Hu, Yuzhong Zhao, Weiqiang Wang"
Is Your Text-to-Image Model Robust to Caption Noise?,"In text-to-image (T2I) generation, a prevalent training technique involves
utilizing Vision Language Models (VLMs) for image re-captioning. Even though
VLMs are known to exhibit hallucination, generating descriptive content that
deviates from the visual reality, the ramifications of such caption
hallucinations on T2I generation performance remain under-explored. Through our
empirical investigation, we first establish a comprehensive dataset comprising
VLM-generated captions, and then systematically analyze how caption
hallucination influences generation outcomes. Our findings reveal that (1) the
disparities in caption quality persistently impact model outputs during
fine-tuning. (2) VLMs confidence scores serve as reliable indicators for
detecting and characterizing noise-related patterns in the data distribution.
(3) even subtle variations in caption fidelity have significant effects on the
quality of learned representations. These findings collectively emphasize the
profound impact of caption quality on model performance and highlight the need
for more sophisticated robust training algorithm in T2I. In response to these
observations, we propose a approach leveraging VLM confidence score to mitigate
caption noise, thereby enhancing the robustness of T2I models against
hallucination in caption.",2024-12-27T08:53:37Z,http://arxiv.org/abs/2412.19531v1,"Weichen Yu, Ziyan Yang, Shanchuan Lin, Qi Zhao, Jianyi Wang, Liangke Gui, Matt Fredrikson, Lu Jiang"
"Exploiting Domain-Specific Parallel Data on Multilingual Language Models
  for Low-resource Language Translation","Neural Machine Translation (NMT) systems built on multilingual
sequence-to-sequence Language Models (msLMs) fail to deliver expected results
when the amount of parallel data for a language, as well as the language's
representation in the model are limited. This restricts the capabilities of
domain-specific NMT systems for low-resource languages (LRLs). As a solution,
parallel data from auxiliary domains can be used either to fine-tune or to
further pre-train the msLM. We present an evaluation of the effectiveness of
these two techniques in the context of domain-specific LRL-NMT. We also explore
the impact of domain divergence on NMT model performance. We recommend several
strategies for utilizing auxiliary parallel data in building domain-specific
NMT models for LRLs.",2024-12-27T08:25:52Z,http://arxiv.org/abs/2412.19522v1,"Surangika Ranathungaa, Shravan Nayak, Shih-Ting Cindy Huang, Yanke Mao, Tong Su, Yun-Hsiang Ray Chan, Songchen Yuan, Anthony Rinaldi, Annie En-Shiun Lee"
"Improved measurements of neutron lifetime with cold neutron beam at
  J-PARC","The ``neutron lifetime puzzle'' arises from the discrepancy between neutron
lifetime measurements obtained using the beam method, which measures decay
products, and the bottle method, which measures the disappearance of neutrons.
To resolve this puzzle, we conducted an experiment using a pulsed cold neutron
beam at J-PARC. In this experiment, the neutron lifetime is determined from the
ratio of neutron decay counts to $^3$He(n,p)$^3$H reactions in a gas detector.
This experiment belongs to the beam method but differs from previous
experiments that measured protons, as it instead detects electrons, enabling
measurements with distinct systematic uncertainties. By enlarging the beam
transport system and reducing systematic uncertainties, we achieved a fivefold
improvement in precision. Analysis of all acquired data yielded a neutron
lifetime of $\tau_{\rm n}=877.2~\pm~1.7_{\rm(stat.)}~^{+4.0}_{-3.6}{}_{\rm
(sys.)}$ s. This result is consistent with bottle method measurements but
exhibits a 2.3$\sigma$ tension with the average value obtained from the
proton-detection-based beam method.",2024-12-27T08:19:54Z,http://arxiv.org/abs/2412.19519v1,"Y. Fuwa, T. Hasegawa, K. Hirota, T. Hoshino, R. Hosokawa, G. Ichikawa, S. Ieki, T. Ino, Y. Iwashita, M. Kitaguchi, R. Kitahara, S. Makise, K. Mishima, T. Mogi, N. Nagakura, H. Oide, H. Okabe, H. Otono, Y. Seki, D. Sekiba, T. Shima, H. E. Shimizu, H. M. Shimizu, N. Sumi, H. Sumino, M. Tanida, H. Uehara, T. Yamada, S. Yamashita, K. Yano, T. Yoshioka"
"Estimation of System Parameters Including Repeated Cross-Sectional Data
  through Emulator-Informed Deep Generative Model","Differential equations (DEs) are crucial for modeling the evolution of
natural or engineered systems. Traditionally, the parameters in DEs are
adjusted to fit data from system observations. However, in fields such as
politics, economics, and biology, available data are often independently
collected at distinct time points from different subjects (i.e., repeated
cross-sectional (RCS) data). Conventional optimization techniques struggle to
accurately estimate DE parameters when RCS data exhibit various
heterogeneities, leading to a significant loss of information. To address this
issue, we propose a new estimation method called the emulator-informed
deep-generative model (EIDGM), designed to handle RCS data. Specifically, EIDGM
integrates a physics-informed neural network-based emulator that immediately
generates DE solutions and a Wasserstein generative adversarial network-based
parameter generator that can effectively mimic the RCS data. We evaluated EIDGM
on exponential growth, logistic population models, and the Lorenz system,
demonstrating its superior ability to accurately capture parameter
distributions. Additionally, we applied EIDGM to an experimental dataset of
Amyloid beta 40 and beta 42, successfully capturing diverse parameter
distribution shapes. This shows that EIDGM can be applied to model a wide range
of systems and extended to uncover the operating principles of systems based on
limited data.",2024-12-27T08:19:23Z,http://arxiv.org/abs/2412.19517v1,"Hyunwoo Cho, Sung Woong Cho, Hyeontae Jo, Hyung Ju Hwang"
"Confidence v.s. Critique: A Decomposition of Self-Correction Capability
  for LLMs","Large Language Models (LLMs) can correct their self-generated responses, but
a decline in accuracy after self-correction is also witnessed. To have a deeper
understanding of self-correction, we endeavor to decompose, evaluate, and
analyze the self-correction behaviors of LLMs. By enumerating and analyzing
answer correctness before and after self-correction, we decompose the
self-correction capability into confidence (being confident to correct answers)
and critique (turning wrong answers to correct) capabilities, and propose two
metrics from a probabilistic perspective to measure these 2 capabilities, along
with another metric for overall self-correction capability evaluation. Based on
our decomposition and evaluation metrics, we conduct extensive experiments and
draw some empirical conclusions. For example, we find different models can
exhibit distinct behaviors: some models are confident while others are more
critical. We also find the trade-off between the two capabilities (i.e.
improving one can lead to a decline in the other) when manipulating model
self-correction behavior by prompts or in-context learning. Further, we find a
simple yet efficient strategy to improve self-correction capability by
transforming Supervision Fine-Tuning (SFT) data format, and our strategy
outperforms vanilla SFT in both capabilities and achieves much higher accuracy
after self-correction. Our code will be publicly available on GitHub.",2024-12-27T08:09:11Z,http://arxiv.org/abs/2412.19513v1,"Zhe Yang, Yichang Zhang, Yudong Wang, Ziyao Xu, Junyang Lin, Zhifang Sui"
Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging,"Fine-tuning large language models (LLMs) for downstream tasks is a widely
adopted approach, but it often leads to safety degradation in safety-aligned
LLMs. Currently, many solutions address this issue by incorporating additional
safety data, which can be impractical in many cases. In this paper, we address
the question: How can we improve downstream task performance while preserving
safety in LLMs without relying on additional safety data? We propose a simple
and effective method that maintains the inherent safety of LLMs while enhancing
their downstream task performance: merging the weights of pre- and
post-fine-tuned safety-aligned models. Experimental results across various
downstream tasks, models, and merging methods demonstrate that this approach
effectively mitigates safety degradation while improving downstream task
performance, offering a practical solution for adapting safety-aligned LLMs.",2024-12-27T08:03:22Z,http://arxiv.org/abs/2412.19512v1,"Hua Farn, Hsuan Su, Shachi H Kumar, Saurav Sahay, Shang-Tse Chen, Hung-yi Lee"
Hybrid Local Causal Discovery,"Local causal discovery aims to learn and distinguish the direct causes and
effects of a target variable from observed data. Existing constraint-based
local causal discovery methods use AND or OR rules in constructing the local
causal skeleton, but using either rule alone is prone to produce cascading
errors in the learned local causal skeleton, and thus impacting the inference
of local causal relationships. On the other hand, directly applying score-based
global causal discovery methods to local causal discovery may randomly return
incorrect results due to the existence of local equivalence classes. To address
the above issues, we propose a Hybrid Local Causal Discovery algorithm, called
HLCD. Specifically, HLCD initially utilizes a constraint-based approach
combined with the OR rule to obtain a candidate skeleton and then employs a
score-based method to eliminate redundant portions in the candidate skeleton.
Furthermore, during the local causal orientation phase, HLCD distinguishes
between V-structures and equivalence classes by comparing the local structure
scores between the two, thereby avoiding orientation interference caused by
local equivalence classes. We conducted extensive experiments with seven
state-of-the-art competitors on 14 benchmark Bayesian network datasets, and the
experimental results demonstrate that HLCD significantly outperforms existing
local causal discovery algorithms.",2024-12-27T07:53:59Z,http://arxiv.org/abs/2412.19507v1,"Zhaolong Ling, Honghui Peng, Yiwen Zhang, Peng Zhou, Xingyu Wu, Kui Yu, Xindong Wu"
"Multi-P$^2$A: A Multi-perspective Benchmark on Privacy Assessment for
  Large Vision-Language Models","Large Vision-Language Models (LVLMs) exhibit impressive potential across
various tasks but also face significant privacy risks, limiting their practical
applications. Current researches on privacy assessment for LVLMs is limited in
scope, with gaps in both assessment dimensions and privacy categories. To
bridge this gap, we propose Multi-P$^2$A, a comprehensive benchmark for
evaluating the privacy preservation capabilities of LVLMs in terms of privacy
awareness and leakage. Privacy awareness measures the model's ability to
recognize the privacy sensitivity of input data, while privacy leakage assesses
the risk of the model unintentionally disclosing privacy information in its
output. We design a range of sub-tasks to thoroughly evaluate the model's
privacy protection offered by LVLMs. Multi-P$^2$A covers 26 categories of
personal privacy, 15 categories of trade secrets, and 18 categories of state
secrets, totaling 31,962 samples. Based on Multi-P$^2$A, we evaluate the
privacy preservation capabilities of 21 open-source and 2 closed-source LVLMs.
Our results reveal that current LVLMs generally pose a high risk of
facilitating privacy breaches, with vulnerabilities varying across personal
privacy, trade secret, and state secret.",2024-12-27T07:33:39Z,http://arxiv.org/abs/2412.19496v1,"Jie Zhang, Xiangkui Cao, Zhouyu Han, Shiguang Shan, Xilin Chen"
"Disparate Model Performance and Stability in Machine Learning Clinical
  Support for Diabetes and Heart Diseases","Machine Learning (ML) algorithms are vital for supporting clinical
decision-making in biomedical informatics. However, their predictive
performance can vary across demographic groups, often due to the
underrepresentation of historically marginalized populations in training
datasets. The investigation reveals widespread sex- and age-related inequities
in chronic disease datasets and their derived ML models. Thus, a novel
analytical framework is introduced, combining systematic arbitrariness with
traditional metrics like accuracy and data complexity. The analysis of data
from over 25,000 individuals with chronic diseases revealed mild sex-related
disparities, favoring predictive accuracy for males, and significant
age-related differences, with better accuracy for younger patients. Notably,
older patients showed inconsistent predictive accuracy across seven datasets,
linked to higher data complexity and lower model performance. This highlights
that representativeness in training data alone does not guarantee equitable
outcomes, and model arbitrariness must be addressed before deploying models in
clinical settings.",2024-12-27T07:31:14Z,http://arxiv.org/abs/2412.19495v1,"Ioannis Bilionis, Ricardo C. Berrios, Luis Fernandez-Luque, Carlos Castillo"
User Willingness-aware Sales Talk Dataset,"User willingness is a crucial element in the sales talk process that affects
the achievement of the salesperson's or sales system's objectives. Despite the
importance of user willingness, to the best of our knowledge, no previous study
has addressed the development of automated sales talk dialogue systems that
explicitly consider user willingness. A major barrier is the lack of sales talk
datasets with reliable user willingness data. Thus, in this study, we developed
a user willingness-aware sales talk collection by leveraging the ecological
validity concept, which is discussed in the field of human-computer
interaction. Our approach focused on three types of user willingness essential
in real sales interactions. We created a dialogue environment that closely
resembles real-world scenarios to elicit natural user willingness, with
participants evaluating their willingness at the utterance level from multiple
perspectives. We analyzed the collected data to gain insights into practical
user willingness-aware sales talk strategies. In addition, as a practical
application of the constructed dataset, we developed and evaluated a sales
dialogue system aimed at enhancing the user's intent to purchase.",2024-12-27T07:16:10Z,http://arxiv.org/abs/2412.19490v1,"Asahi Hentona, Jun Baba, Shiki Sato, Reina Akama"
Learning Radiance Fields from a Single Snapshot Compressive Image,"In this paper, we explore the potential of Snapshot Compressive Imaging (SCI)
technique for recovering the underlying 3D scene structure from a single
temporal compressed image. SCI is a cost-effective method that enables the
recording of high-dimensional data, such as hyperspectral or temporal
information, into a single image using low-cost 2D imaging sensors. To achieve
this, a series of specially designed 2D masks are usually employed, reducing
storage and transmission requirements and offering potential privacy
protection. Inspired by this, we take one step further to recover the encoded
3D scene information leveraging powerful 3D scene representation capabilities
of neural radiance fields (NeRF). Specifically, we propose SCINeRF, in which we
formulate the physical imaging process of SCI as part of the training of NeRF,
allowing us to exploit its impressive performance in capturing complex scene
structures. In addition, we further integrate the popular 3D Gaussian Splatting
(3DGS) framework and propose SCISplat to improve 3D scene reconstruction
quality and training/rendering speed by explicitly optimizing point clouds into
3D Gaussian representations. To assess the effectiveness of our method, we
conduct extensive evaluations using both synthetic data and real data captured
by our SCI system. Experimental results demonstrate that our proposed approach
surpasses the state-of-the-art methods in terms of image reconstruction and
novel view synthesis. Moreover, our method also exhibits the ability to render
high frame-rate multi-view consistent images in real time by leveraging SCI and
the rendering capabilities of 3DGS. Codes will be available at:
https://github.com/WU- CVGL/SCISplat.",2024-12-27T06:40:44Z,http://arxiv.org/abs/2412.19483v1,"Yunhao Li, Xiang Liu, Xiaodong Wang, Xin Yuan, Peidong Liu"
"Pre-training, Fine-tuning and Re-ranking: A Three-Stage Framework for
  Legal Question Answering","Legal question answering (QA) has attracted increasing attention from people
seeking legal advice, which aims to retrieve the most applicable answers from a
large-scale database of question-answer pairs. Previous methods mainly use a
dual-encoder architecture to learn dense representations of both questions and
answers. However, these methods could suffer from lacking domain knowledge and
sufficient labeled training data. In this paper, we propose a three-stage
(\underline{p}re-training, \underline{f}ine-tuning and \underline{r}e-ranking)
framework for \underline{l}egal \underline{QA} (called PFR-LQA), which promotes
the fine-grained text representation learning and boosts the performance of
dense retrieval with the dual-encoder architecture. Concretely, we first
conduct domain-specific pre-training on legal questions and answers through a
self-supervised training objective, allowing the pre-trained model to be
adapted to the legal domain. Then, we perform task-specific fine-tuning of the
dual-encoder on legal question-answer pairs by using the supervised learning
objective, leading to a high-quality dual-encoder for the specific downstream
QA task. Finally, we employ a contextual re-ranking objective to further refine
the output representations of questions produced by the document encoder, which
uses contextual similarity to increase the discrepancy between the anchor and
hard negative samples for better question re-ranking. We conduct extensive
experiments on a manually annotated legal QA dataset. Experimental results show
that our PFR-LQA method achieves better performance than the strong competitors
for legal question answering.",2024-12-27T06:33:42Z,http://arxiv.org/abs/2412.19482v1,"Shiwen Ni, Hao Cheng, Min Yang"
"An Overview of Machine Learning-Driven Resource Allocation in IoT
  Networks","In the wake of disruptive IoT technologies generating massive amounts of
diverse data, Machine Learning (ML) will play a crucial role in bringing
intelligence to Internet of Things (IoT) networks. This paper provides a
comprehensive analysis of the current state of resource allocation within IoT
networks, focusing specifically on two key categories: Low-Power IoT Networks
and Mobile IoT Networks. We delve into the resource allocation strategies that
are crucial for optimizing network performance and energy efficiency in these
environments. Furthermore, the paper explores the transformative role of
Machine Learning (ML), Deep Learning (DL), and Reinforcement Learning (RL) in
enhancing IoT functionalities. We highlight a range of applications and use
cases where these advanced technologies can significantly improve
decision-making and optimization processes. In addition to the opportunities
presented by ML, DL, and RL, we also address the potential challenges that
organizations may face when implementing these technologies in IoT settings.
These challenges include crucial accuracy, low flexibility and adaptability,
and high computational cost, etc. Finally, the paper identifies promising
avenues for future research, emphasizing the need for innovative solutions to
overcome existing hurdles and improve the integration of ML, DL, and RL into
IoT networks. By providing this holistic perspective, we aim to contribute to
the ongoing discourse on resource allocation strategies and the application of
intelligent technologies in the IoT landscape.",2024-12-27T06:11:28Z,http://arxiv.org/abs/2412.19478v1,Zhengdong Li
"Effects of Reynolds number and spatial resolution on the pressure source
  terms in turbulent boundary layers","The increase in wall-pressure fluctuations with increasing friction Reynolds
number ($Re_{\tau}$) of a turbulent boundary layer (TBL) is well known in the
literature. However, very few studies have investigated the
$Re_{\tau}$-variation of the source terms of the pressure fluctuations, which
are solely a function of the spatial velocity gradients within the TBL. This
study quantifies the pressure source terms in a zero-pressure gradient TBL by
utilizing a published direct numerical simulation (DNS; Sillero et al. 2013,
Phys. Fluids) database across 1000 $\lesssim$ $Re_{\tau}$ $\lesssim$ 2000. It
is found that the magnitude of all source terms increases with $Re_{\tau}$
across the entire TBL thickness, with the turbulence-turbulence (non-linear)
interaction terms growing faster than the mean-shear (linear) source terms.
Further, we use the simulation database to mimic the scenario of particle image
velocimetry (PIV) experiments that are typically spatially under-resolved
compared to DNS data. It is used to quantify the effect of spatial resolution
on the accuracy of pressure source terms, which are estimated here for two
common PIV scenarios: (i) planar PIV in the streamwise-wall-normal plane, and
(ii) stereo-PIV in the spanwise-wall-normal plane of a ZPG TBL. This exercise
reveals significant attenuation of all pressure source terms compared to those
estimated from the original DNS, highlighting the challenges of accurately
estimating these source terms in a high $Re_{\tau}$ PIV experiment.",2024-12-27T05:58:49Z,http://arxiv.org/abs/2412.19474v1,"Aditya Agarwal, Rahul Deshpande"
"Knowledge Graph-Based Multi-Agent Path Planning in Dynamic Environments
  using WAITR","This paper addresses the challenge of multi-agent path planning for efficient
data collection in dynamic, uncertain environments, exemplified by autonomous
underwater vehicles (AUVs) navigating the Gulf of Mexico. Traditional greedy
algorithms, though computationally efficient, often fall short in long-term
planning due to their short-sighted nature, missing crucial data collection
opportunities and increasing exposure to hazards. To address these limitations,
we introduce WAITR (Weighted Aggregate Inter-Temporal Reward), a novel
path-planning framework that integrates a knowledge graph with pathlet-based
planning, segmenting the environment into dynamic, speed-adjusted sub-regions
(pathlets). This structure enables coordinated, adaptive planning, as agents
can operate within time-bound regions while dynamically responding to
environmental changes. WAITR's cumulative scoring mechanism balances immediate
data collection with long-term optimization of Points of Interest (POIs),
ensuring safer navigation and comprehensive data coverage. Experimental results
show that WAITR substantially improves POI coverage and reduces exposure to
hazards, achieving up to 27.1\% greater event coverage than traditional greedy
methods.",2024-12-27T05:43:41Z,http://arxiv.org/abs/2412.19469v1,"Ted Edward Holmberg, Elias Ioup, Mahdi Abdelguerfi"
A Prototype Unit for Image De-raining using Time-Lapse Data,"We address the challenge of single-image de-raining, a task that involves
recovering rain-free background information from a single rain image. While
recent advancements have utilized real-world time-lapse data for training,
enabling the estimation of consistent backgrounds and realistic rain streaks,
these methods often suffer from computational and memory consumption, limiting
their applicability in real-world scenarios. In this paper, we introduce a
novel solution: the Rain Streak Prototype Unit (RsPU). The RsPU efficiently
encodes rain streak-relevant features as real-time prototypes derived from
time-lapse data, eliminating the need for excessive memory resources. Our
de-raining network combines encoder-decoder networks with the RsPU, allowing us
to learn and encapsulate diverse rain streak-relevant features as concise
prototypes, employing an attention-based approach. To ensure the effectiveness
of our approach, we propose a feature prototype loss encompassing cohesion and
divergence components. This loss function captures both the compactness and
diversity aspects of the prototypical rain streak features within the RsPU. Our
method evaluates various de-raining benchmarks, accompanied by comprehensive
ablation studies. We show that it can achieve competitive results in various
rain images compared to state-of-the-art methods.",2024-12-27T05:04:56Z,http://arxiv.org/abs/2412.19459v1,"Jaehoon Cho, Minjung Yoo, Jini Yang, Sunok Kim"
"DriveEditor: A Unified 3D Information-Guided Framework for Controllable
  Object Editing in Driving Scenes","Vision-centric autonomous driving systems require diverse data for robust
training and evaluation, which can be augmented by manipulating object
positions and appearances within existing scene captures. While recent
advancements in diffusion models have shown promise in video editing, their
application to object manipulation in driving scenarios remains challenging due
to imprecise positional control and difficulties in preserving high-fidelity
object appearances. To address these challenges in position and appearance
control, we introduce DriveEditor, a diffusion-based framework for object
editing in driving videos. DriveEditor offers a unified framework for
comprehensive object editing operations, including repositioning, replacement,
deletion, and insertion. These diverse manipulations are all achieved through a
shared set of varying inputs, processed by identical position control and
appearance maintenance modules. The position control module projects the given
3D bounding box while preserving depth information and hierarchically injects
it into the diffusion process, enabling precise control over object position
and orientation. The appearance maintenance module preserves consistent
attributes with a single reference image by employing a three-tiered approach:
low-level detail preservation, high-level semantic maintenance, and the
integration of 3D priors from a novel view synthesis model. Extensive
qualitative and quantitative evaluations on the nuScenes dataset demonstrate
DriveEditor's exceptional fidelity and controllability in generating diverse
driving scene edits, as well as its remarkable ability to facilitate downstream
tasks.",2024-12-27T04:49:36Z,http://arxiv.org/abs/2412.19458v1,"Yiyuan Liang, Zhiying Yan, Liqun Chen, Jiahuan Zhou, Luxin Yan, Sheng Zhong, Xu Zou"
Focusing Image Generation to Mitigate Spurious Correlations,"Instance features in images exhibit spurious correlations with background
features, affecting the training process of deep neural classifiers. This leads
to insufficient attention to instance features by the classifier, resulting in
erroneous classification outcomes. In this paper, we propose a data
augmentation method called Spurious Correlations Guided Synthesis (SCGS) that
mitigates spurious correlations through image generation model. This approach
does not require expensive spurious attribute (group) labels for the training
data and can be widely applied to other debiasing methods. Specifically, SCGS
first identifies the incorrect attention regions of a pre-trained classifier on
the training images, and then uses an image generation model to generate new
training data based on these incorrect attended regions. SCGS increases the
diversity and scale of the dataset to reduce the impact of spurious
correlations on classifiers. Changes in the classifier's attention regions and
experimental results on three different domain datasets demonstrate that this
method is effective in reducing the classifier's reliance on spurious
correlations.",2024-12-27T04:48:56Z,http://arxiv.org/abs/2412.19457v1,"Xuewei Li, Zhenzhen Nie, Mei Yu, Zijian Zhang, Jie Gao, Tianyi Xu, Zhiqiang Liu"
"NijiGAN: Transform What You See into Anime with Contrastive
  Semi-Supervised Learning and Neural Ordinary Differential Equations","Generative AI has transformed the animation industry. Several models have
been developed for image-to-image translation, particularly focusing on
converting real-world images into anime through unpaired translation.
Scenimefy, a notable approach utilizing contrastive learning, achieves high
fidelity anime scene translation by addressing limited paired data through
semi-supervised training. However, it faces limitations due to its reliance on
paired data from a fine-tuned StyleGAN in the anime domain, often producing
low-quality datasets. Additionally, Scenimefy's high parameter architecture
presents opportunities for computational optimization. This research introduces
NijiGAN, a novel model incorporating Neural Ordinary Differential Equations
(NeuralODEs), which offer unique advantages in continuous transformation
modeling compared to traditional residual networks. NijiGAN successfully
transforms real-world scenes into high fidelity anime visuals using half of
Scenimefy's parameters. It employs pseudo-paired data generated through
Scenimefy for supervised training, eliminating dependence on low-quality paired
data and improving the training process. Our comprehensive evaluation includes
ablation studies, qualitative, and quantitative analysis comparing NijiGAN to
similar models. The testing results demonstrate that NijiGAN produces
higher-quality images compared to AnimeGAN, as evidenced by a Mean Opinion
Score (MOS) of 2.192, it surpasses AnimeGAN's MOS of 2.160. Furthermore, our
model achieved a Frechet Inception Distance (FID) score of 58.71, outperforming
Scenimefy's FID score of 60.32. These results demonstrate that NijiGAN achieves
competitive performance against existing state-of-the-arts, especially
Scenimefy as the baseline model.",2024-12-27T04:46:44Z,http://arxiv.org/abs/2412.19455v1,"Kevin Putra Santoso, Anny Yuniarti, Dwiyasa Nakula, Dimas Prihady Setyawan, Adam Haidar Azizi, Jeany Aurellia P. Dewati, Farah Dhia Fadhila, Maria T. Elvara Bumbungan"
"Feature Alignment-Based Knowledge Distillation for Efficient Compression
  of Large Language Models","This study proposes a knowledge distillation algorithm based on large
language models and feature alignment, aiming to effectively transfer the
knowledge of large pre-trained models into lightweight student models, thereby
reducing computational costs while maintaining high model performance.
Different from the traditional soft label distillation method, this method
introduces a multi-layer feature alignment strategy to deeply align the
intermediate features and attention mechanisms of the teacher model and the
student model, maximally retaining the semantic expression ability and context
modeling ability of the teacher model. In terms of method design, a multi-task
loss function is constructed, including feature matching loss, attention
alignment loss, and output distribution matching loss, to ensure multi-level
information transfer through joint optimization. The experiments were
comprehensively evaluated on the GLUE data set and various natural language
processing tasks. The results show that the proposed model performs very close
to the state-of-the-art GPT-4 model in terms of evaluation indicators such as
perplexity, BLEU, ROUGE, and CER. At the same time, it far exceeds baseline
models such as DeBERTa, XLNet, and GPT-3, showing significant performance
improvements and computing efficiency advantages. Research results show that
the feature alignment distillation strategy is an effective model compression
method that can significantly reduce computational overhead and storage
requirements while maintaining model capabilities. Future research can be
further expanded in the directions of self-supervised learning, cross-modal
feature alignment, and multi-task transfer learning to provide more flexible
and efficient solutions for the deployment and optimization of deep learning
models.",2024-12-27T04:37:06Z,http://arxiv.org/abs/2412.19449v1,"Shuo Wang, Chihang Wang, Jia Gao, Zhen Qi, Hongye Zheng, Xiaoxuan Liao"
"Comparative Performance Analysis of Quantum Machine Learning
  Architectures for Credit Card Fraud Detection","As financial fraud becomes increasingly complex, effective detection methods
are essential. Quantum Machine Learning (QML) introduces certain capabilities
that may enhance both accuracy and efficiency in this area. This study examines
how different quantum feature map and ansatz configurations affect the
performance of three QML-based classifiers-the Variational Quantum Classifier
(VQC), the Sampler Quantum Neural Network (SQNN), and the Estimator Quantum
Neural Network (EQNN)-when applied to two non-standardized financial fraud
datasets. Different quantum feature map and ansatz configurations are
evaluated, revealing distinct performance patterns. The VQC consistently
demonstrates strong classification results, achieving an F1 score of 0.88,
while the SQNN also delivers promising outcomes. In contrast, the EQNN
struggles to produce robust results, emphasizing the challenges presented by
non-standardized data. These findings highlight the importance of careful model
configuration in QML-based financial fraud detection. By showing how specific
feature maps and ansatz choices influence predictive success, this work guides
researchers and practitioners in refining QML approaches for complex financial
applications.",2024-12-27T04:17:34Z,http://arxiv.org/abs/2412.19441v1,"Mansour El Alami, Nouhaila Innan, Muhammad Shafique, Mohamed Bennai"
"Residual Feature-Reutilization Inception Network for Image
  Classification","Capturing feature information effectively is of great importance in the field
of computer vision. With the development of convolutional neural networks
(CNNs), concepts like residual connection and multiple scales promote continual
performance gains in diverse deep learning vision tasks. In this paper, we
propose a novel CNN architecture that it consists of residual
feature-reutilization inceptions (ResFRI) or split-residual
feature-reutilization inceptions (Split-ResFRI). And it is composed of four
convolutional combinations of different structures connected by specially
designed information interaction passages, which are utilized to extract
multi-scale feature information and effectively increase the receptive field of
the model. Moreover, according to the network structure designed above,
Split-ResFRI can adjust the segmentation ratio of the input information,
thereby reducing the number of parameters and guaranteeing the model
performance. Specifically, in experiments based on popular vision datasets,
such as CIFAR10 ($97.94$\%), CIFAR100 ($85.91$\%) and Tiny Imagenet
($70.54$\%), we obtain state-of-the-art results compared with other modern
models under the premise that the model size is approximate and no additional
data is used.",2024-12-27T03:55:25Z,http://arxiv.org/abs/2412.19433v1,"Yuanpeng He, Wenjie Song, Lijian Li, Tianxiang Zhan, Wenpin Jiao"
Revisiting PCA for time series reduction in temporal dimension,"Revisiting PCA for Time Series Reduction in Temporal Dimension; Jiaxin Gao,
Wenbo Hu, Yuntian Chen; Deep learning has significantly advanced time series
analysis (TSA), enabling the extraction of complex patterns for tasks like
classification, forecasting, and regression. Although dimensionality reduction
has traditionally focused on the variable space-achieving notable success in
minimizing data redundancy and computational complexity-less attention has been
paid to reducing the temporal dimension. In this study, we revisit Principal
Component Analysis (PCA), a classical dimensionality reduction technique, to
explore its utility in temporal dimension reduction for time series data. It is
generally thought that applying PCA to the temporal dimension would disrupt
temporal dependencies, leading to limited exploration in this area. However,
our theoretical analysis and extensive experiments demonstrate that applying
PCA to sliding series windows not only maintains model performance, but also
enhances computational efficiency. In auto-regressive forecasting, the temporal
structure is partially preserved through windowing, and PCA is applied within
these windows to denoise the time series while retaining their statistical
information. By preprocessing time-series data with PCA, we reduce the temporal
dimensionality before feeding it into TSA models such as Linear, Transformer,
CNN, and RNN architectures. This approach accelerates training and inference
and reduces resource consumption. Notably, PCA improves Informer training and
inference speed by up to 40% and decreases GPU memory usage of TimesNet by 30%,
without sacrificing model accuracy. Comparative analysis against other
reduction methods further highlights the effectiveness of PCA in improving the
efficiency of TSA models.",2024-12-27T03:17:26Z,http://arxiv.org/abs/2412.19423v1,"Jiaxin Gao, Wenbo Hu, Yuntian Chen"
"A Matrix Logic Approach to Efficient Frequent Itemset Discovery in Large
  Data Sets","This paper proposes a frequent itemset mining algorithm based on the Boolean
matrix method, aiming to solve the storage and computational bottlenecks of
traditional frequent pattern mining algorithms in high-dimensional and
large-scale transaction databases. By representing the itemsets in the
transaction database as Boolean matrices, the algorithm uses Boolean logic
operations such as AND and OR to efficiently calculate the support of the
itemsets, avoiding the generation and storage of a large number of candidates
itemsets in traditional algorithms. The algorithm recursively mines frequent
itemsets through matrix operations and can flexibly adapt to different data
scales and support thresholds. In the experiment, the public Groceries dataset
was selected, and the running efficiency test and frequent itemset mining
effect test were designed to evaluate the algorithm's performance indicators
such as running time, memory usage, and number of frequent itemsets under
different transaction numbers and support thresholds. The experimental results
show that the algorithm can efficiently mine a large number of frequent
itemsets when the support threshold is low, and focus on strong association
rules with high support when the threshold is high. In addition, the changing
trends of running time and memory usage show that the Boolean matrix method can
still maintain good running efficiency when the number of transactions
increases significantly and has high scalability and robustness. Future
research can improve memory optimization and matrix block operations, and
combine distributed computing and deep learning models to further enhance the
algorithm's applicability and real-time processing capabilities in
ultra-large-scale data environments. The algorithm has broad application
potential and development prospects in the fields of market analysis,
recommendation systems, and network security.",2024-12-27T03:13:13Z,http://arxiv.org/abs/2412.19420v1,"Xuan Li, Tingyi Ruan, Yankaiqi Li, Quanchao Lu, Xiaoxuan Sun"
KALAHash: Knowledge-Anchored Low-Resource Adaptation for Deep Hashing,"Deep hashing has been widely used for large-scale approximate nearest
neighbor search due to its storage and search efficiency. However, existing
deep hashing methods predominantly rely on abundant training data, leaving the
more challenging scenario of low-resource adaptation for deep hashing
relatively underexplored. This setting involves adapting pre-trained models to
downstream tasks with only an extremely small number of training samples
available. Our preliminary benchmarks reveal that current methods suffer
significant performance degradation due to the distribution shift caused by
limited training samples. To address these challenges, we introduce
Class-Calibration LoRA (CLoRA), a novel plug-and-play approach that dynamically
constructs low-rank adaptation matrices by leveraging class-level textual
knowledge embeddings. CLoRA effectively incorporates prior class knowledge as
anchors, enabling parameter-efficient fine-tuning while maintaining the
original data distribution. Furthermore, we propose Knowledge-Guided Discrete
Optimization (KIDDO), a framework to utilize class knowledge to compensate for
the scarcity of visual information and enhance the discriminability of hash
codes. Extensive experiments demonstrate that our proposed method, Knowledge-
Anchored Low-Resource Adaptation Hashing (KALAHash), significantly boosts
retrieval performance and achieves a 4x data efficiency in low-resource
scenarios.",2024-12-27T03:04:54Z,http://arxiv.org/abs/2412.19417v1,"Shu Zhao, Tan Yu, Xiaoshuai Hao, Wenchao Ma, Vijaykrishnan Narayanan"
DIPS: Optimal Dynamic Index for Poisson $\boldsymbolπ$ps Sampling,"This paper addresses the Poisson $\pi$ps sampling problem, a topic of
significant academic interest in various domains and with practical data mining
applications, such as influence maximization. The problem includes a set
$\mathcal{S}$ of $n$ elements, where each element $v$ is assigned a weight
$w(v)$ reflecting its importance. The goal is to generate a random subset $X$
of $\mathcal{S}$, where each element $v \in \mathcal{S}$ is included in $X$
independently with probability $\frac{c\cdot w(v)}{\sum_{v \in \mathcal{S}}
w(v)}$, where $0&lt;c\leq 1$ is a constant. The subsets must be independent across
different queries. While the Poisson $\pi$ps sampling problem can be reduced to
the well-studied subset sampling problem, updates in Poisson $\pi$ps sampling,
such as adding a new element or removing an element, would cause the
probabilities of all $n$ elements to change in the corresponding subset
sampling problem, making this approach impractical for dynamic scenarios. To
address this, we propose a dynamic index specifically tailored for the Poisson
$\pi$ps sampling problem, supporting optimal expected $\mathcal{O}(1)$ query
time and $\mathcal{O}(1)$ index update time, with an optimal $\mathcal{O}(n)$
space cost. Our solution involves recursively partitioning the set by weights
and ultimately using table lookup. The core of our solution lies in addressing
the challenges posed by weight explosion and correlations between elements.
Empirical evaluations demonstrate that our approach achieves significant
speedups in update time while maintaining consistently competitive query time
compared to the subset-sampling-based methods.",2024-12-27T02:47:44Z,http://arxiv.org/abs/2412.19415v1,"Jinchao Huang, Sibo Wang"
"The Hobby-Eberly Telescope Dark Energy Experiment Survey (HETDEX) Active
  Galactic Nuclei Catalog: the Fourth Data Release","We present the Active Galactic Nuclei (AGN) catalog from the fourth data
release (HDR4) of the Hobby-Eberly Telescope Dark Energy Experiment Survey
(HETDEX). HETDEX is an untargeted spectroscopic survey. HDR4 contains 345,874
Integral Field Unit (IFU) observations from January 2017 to August 2023
covering an effective area of 62.9 deg2. With no imaging pre-selection, our
spectroscopic confirmed AGN sample includes low-luminosity AGN, narrow-line
AGN, and/or red AGN down to g~25. This catalog has 15,940 AGN across the
redshifts of z=0.1~4.6, giving a raw AGN number density of 253.4 deg-2. Among
them, 10,499 (66%) have redshifts either confirmed by line pairs or matched to
the Sloan Digital Sky Survey Quasar Catalog. For the remaining 5,441 AGN, 2,083
are single broad line AGN candidates, while the remaining 3,358 are single
intermediate broad line (full width at half maximum, FWHM ~ 1200 km s-1) AGN
candidates. A total of 4,060 (39%) of the 10,499 redshift-confirmed AGN have
emission-line regions $3\sigma$ more extended than the image quality which
could be strong outflows blowing into the outskirts of the host galaxies or
ionized intergalactic medium.",2024-12-27T02:45:20Z,http://arxiv.org/abs/2412.19414v1,"Chenxu Liu, Karl Gebhardt, Erin Mentuch Cooper, Dustin Davis, Donald P. Schneider, Matt J. Jarvis, Daniel J. Farrow, Steven L. Finkelstein, Oscar A. Chavez Ortiz, The HETDEX Collaboration"
MINIMA: Modality Invariant Image Matching,"Image matching for both cross-view and cross-modality plays a critical role
in multimodal perception. In practice, the modality gap caused by different
imaging systems/styles poses great challenges to the matching task. Existing
works try to extract invariant features for specific modalities and train on
limited datasets, showing poor generalization. In this paper, we present
MINIMA, a unified image matching framework for multiple cross-modal cases.
Without pursuing fancy modules, our MINIMA aims to enhance universal
performance from the perspective of data scaling up. For such purpose, we
propose a simple yet effective data engine that can freely produce a large
dataset containing multiple modalities, rich scenarios, and accurate matching
labels. Specifically, we scale up the modalities from cheap but rich RGB-only
matching data, by means of generative models. Under this setting, the matching
labels and rich diversity of the RGB dataset are well inherited by the
generated multimodal data. Benefiting from this, we construct MD-syn, a new
comprehensive dataset that fills the data gap for general multimodal image
matching. With MD-syn, we can directly train any advanced matching pipeline on
randomly selected modality pairs to obtain cross-modal ability. Extensive
experiments on in-domain and zero-shot matching tasks, including $19$
cross-modal cases, demonstrate that our MINIMA can significantly outperform the
baselines and even surpass modality-specific methods. The dataset and code are
available at https://github.com/LSXI7/MINIMA .",2024-12-27T02:39:50Z,http://arxiv.org/abs/2412.19412v1,"Xingyu Jiang, Jiangwei Ren, Zizhuo Li, Xin Zhou, Dingkang Liang, Xiang Bai"
Spectral-Temporal Fusion Representation for Person-in-Bed Detection,"This study is based on the ICASSP 2025 Signal Processing Grand Challenge's
Accelerometer-Based Person-in-Bed Detection Challenge, which aims to determine
bed occupancy using accelerometer signals. The task is divided into two tracks:
""in bed"" and ""not in bed"" segmented detection, and streaming detection, facing
challenges such as individual differences, posture variations, and external
disturbances. We propose a spectral-temporal fusion-based feature
representation method with mixup data augmentation, and adopt Intersection over
Union (IoU) loss to optimize detection accuracy. In the two tracks, our method
achieved outstanding results of 100.00% and 95.55% in detection scores,
securing first place and third place, respectively.",2024-12-27T02:05:09Z,http://arxiv.org/abs/2412.19404v1,"Xuefeng Yang, Shiheng Zhang, Jian Guan, Feiyang Xiao, Wei Lu, Qiaoxi Zhu"
"Fully Data-driven but Interpretable Human Behavioural Modelling with
  Differentiable Discrete Choice Model","Discrete choice models are essential for modelling various decision-making
processes in human behaviour. However, the specification of these models has
depended heavily on domain knowledge from experts, and the fully automated but
interpretable modelling of complex human behaviours has been a long-standing
challenge. In this paper, we introduce the differentiable discrete choice model
(Diff-DCM), a fully data-driven method for the interpretable modelling,
learning, prediction, and control of complex human behaviours, which is
realised by differentiable programming. Solely from input features and choice
outcomes without any prior knowledge, Diff-DCM can estimate interpretable
closed-form utility functions that reproduce observed behaviours. Comprehensive
experiments with both synthetic and real-world data demonstrate that Diff-DCM
can be applied to various types of data and requires only a small amount of
computational resources for the estimations, which can be completed within tens
of seconds on a laptop without any accelerators. In these experiments, we also
demonstrate that, using its differentiability, Diff-DCM can provide useful
insights into human behaviours, such as an optimal intervention path for
effective behavioural changes. This study provides a strong basis for the fully
automated and reliable modelling, prediction, and control of human behaviours.",2024-12-27T01:53:18Z,http://arxiv.org/abs/2412.19403v1,"Fumiyasu Makinoshima, Tatsuya Mitomi, Fumiya Makihara, Eigo Segawa"
"Comparing Few to Rank Many: Active Human Preference Learning using
  Randomized Frank-Wolfe","We study learning of human preferences from a limited comparison feedback.
This task is ubiquitous in machine learning. Its applications such as
reinforcement learning from human feedback, have been transformational. We
formulate this problem as learning a Plackett-Luce model over a universe of $N$
choices from $K$-way comparison feedback, where typically $K \ll N$. Our
solution is the D-optimal design for the Plackett-Luce objective. The design
defines a data logging policy that elicits comparison feedback for a small
collection of optimally chosen points from all ${N \choose K}$ feasible
subsets. The main algorithmic challenge in this work is that even fast methods
for solving D-optimal designs would have $O({N \choose K})$ time complexity. To
address this issue, we propose a randomized Frank-Wolfe (FW) algorithm that
solves the linear maximization sub-problems in the FW method on randomly chosen
variables. We analyze the algorithm, and evaluate it empirically on synthetic
and open-source NLP datasets.",2024-12-27T01:10:17Z,http://arxiv.org/abs/2412.19396v1,"Kiran Koshy Thekumparampil, Gaurush Hiranandani, Kousha Kalantari, Shoham Sabach, Branislav Kveton"
"An In-Depth Analysis of Adversarial Discriminative Domain Adaptation for
  Digit Classification","Domain adaptation is an active area of research driven by the growing demand
for robust machine learning models that perform well on real-world data.
Adversarial learning for deep neural networks (DNNs) has emerged as a promising
approach to improving generalization ability, particularly for image
classification. In this paper, we implement a specific adversarial learning
technique known as Adversarial Discriminative Domain Adaptation (ADDA) and
replicate digit classification experiments from the original ADDA paper. We
extend their findings by examining a broader range of domain shifts and provide
a detailed analysis of in-domain classification accuracy post-ADDA. Our results
demonstrate that ADDA significantly improves accuracy across certain domain
shifts with minimal impact on in-domain performance. Furthermore, we provide
qualitative analysis and propose potential explanations for ADDA's limitations
in less successful domain shifts. Code is at
https://github.com/eugenechoi2004/COS429_FINAL .",2024-12-27T00:36:40Z,http://arxiv.org/abs/2412.19391v1,"Eugene Choi, Julian Rodriguez, Edmund Young"
"A reduced-order framework for temperature estimation in food freezing
  from optimally located sensors, including turbulent conjugate flow scenarios","This article proposes a framework for estimating temperature fields in
food-freezing applications that significantly reduces computational load while
ensuring accurate temperature monitoring, representing a promising
technological tool for optimizing and controlling food engineering processes.
The strategy is based on (i) a mathematical model of a convection-dominated
problem coupling thermal convection and turbulence and (ii) a least-squares
approach for solving the inverse data assimilation problem, regularized by
projecting the governing dynamics onto a reduced-order model (ROM). The
unsteady freezing process considers an idealized salmon slice in a freezer
cabinet, modeled with temperature-dependent thermophysical properties. The
forward problem is approximated using a third-order WENO finite volume solver,
including an optimized second-order backward scheme for time discretization. We
employ our data assimilation framework to reconstruct the temperature field
from a limited number of sensor data and to estimate temperature distributions
within frozen food. Sensor placement is optimized using a new greedy algorithm,
relying on maximizing the observability of the reduced-order dynamics for a
fixed set of sensors. The proposed approach allows efficient extrapolation from
external sensor measurements to the internal temperature of the food, which is
crucial for maintaining food quality.",2024-12-27T00:26:36Z,http://arxiv.org/abs/2412.19387v1,"Felipe Galarce, Diego Rivera, Douglas Pacheco, Alfonso Caiazzo, Ernesto Castillo"
Resolvent-based estimation and control of a laminar airfoil wake,"We develop an optimal resolvent-based estimator and controller to predict and
attenuate unsteady vortex shedding fluctuations in the laminar wake of a NACA
0012 airfoil at an angle of attack of 6.5 degrees, chord-based Reynolds number
of 5000, and Mach number of 0.3. The resolvent-based estimation and control
framework offers several advantages over standard methods. Under equivalent
assumptions, the resolvent-based estimator and controller reproduce the Kalman
filter and LQG controller, respectively, but at substantially lower
computational cost using either an operator-based or data-driven
implementation. Unlike these methods, the resolvent-based approach can
naturally accommodate forcing terms (nonlinear terms from Navier-Stokes) with
colored-in-time statistics, significantly improving estimation accuracy and
control efficacy. Causality is optimally enforced using a Wiener-Hopf
formalism. We integrate these tools into a high-performance-computing-ready
compressible flow solver and demonstrate their effectiveness for estimating and
controlling velocity fluctuations in the wake of the airfoil immersed in clean
and noisy freestreams, the latter of which prevents the flow from falling into
a periodic limit cycle. Using four shear-stress sensors on the surface of the
airfoil, the resolvent-based estimator predicts a series of downstream targets
with approximately 3% and 30% error for the clean and noisy freestream
conditions, respectively. For the latter case, using four actuators on the
airfoil surface, the resolvent-based controller reduces the turbulent kinetic
energy in the wake by 98%.",2024-12-27T00:12:23Z,http://arxiv.org/abs/2412.19386v1,"Junoh Jung, Rutvij Bhagwat, Aaron Towne"
"The Internet of Value: Integrating Blockchain and Lightning Network
  Micropayments for Knowledge Markets","Q&amp;A websites rely on user-generated responses, with incentives such as
reputation scores or monetary rewards often offered. While some users may find
it intrinsically rewarding to assist others, studies indicate that payment can
improve the quality and speed of answers. However, traditional payment
processors impose minimum thresholds that many Q&amp;A inquiries fall below. The
introduction of Bitcoin enabled direct digital value transfer, yet frequent
micropayments remain challenging. Recent advancements like the Lightning
Network now allow frictionless micropayments by reducing costs and minimising
reliance on intermediaries. This development fosters an ""Internet of Value,""
where transferring even small amounts of money is as simple as sharing data.
This study investigates integrating Lightning Network-based micropayment
strategies into Q&amp;A platforms, aiming to create a knowledge market free of
minimum payment barriers. A survey was conducted to address the gap below the
$2 payment level identified in prior research. Responses confirmed that
incentives for asking and answering weaken as payments decrease. Findings
reveal even minimal payments, such as {\pounds}0.01, significantly encourage
higher quality and effort in responses. The study recommends micropayment
incentives for service-oriented applications, particularly Q&amp;A platforms. By
leveraging the Lightning Network to remove barriers, a more open marketplace
can emerge, improving engagement and outcomes. Further research is needed to
confirm if users follow through on reported intentions when spending funds.",2024-12-26T23:57:54Z,http://arxiv.org/abs/2412.19384v1,"Ellis Solaiman, Jorge Robins"
"Minimal Batch Adaptive Learning Policy Engine for Real-Time Mid-Price
  Forecasting in High-Frequency Trading","High-frequency trading (HFT) has transformed modern financial markets, making
reliable short-term price forecasting models essential. In this study, we
present a novel approach to mid-price forecasting using Level 1 limit order
book (LOB) data from NASDAQ, focusing on 100 U.S. stocks from the S&amp;P 500 index
during the period from September to November 2022. Expanding on our previous
work with Radial Basis Function Neural Networks (RBFNN), which leveraged
automated feature importance techniques based on mean decrease impurity (MDI)
and gradient descent (GD), we introduce the Adaptive Learning Policy Engine
(ALPE) - a reinforcement learning (RL)-based agent designed for batch-free,
immediate mid-price forecasting. ALPE incorporates adaptive epsilon decay to
dynamically balance exploration and exploitation, outperforming a diverse range
of highly effective machine learning (ML) and deep learning (DL) models in
forecasting performance.",2024-12-26T22:49:53Z,http://arxiv.org/abs/2412.19372v1,"Adamantios Ntakaris, Gbenga Ibikunle"
Large Language Models for Market Research: A Data-augmentation Approach,"Large Language Models (LLMs) have transformed artificial intelligence by
excelling in complex natural language processing tasks. Their ability to
generate human-like text has opened new possibilities for market research,
particularly in conjoint analysis, where understanding consumer preferences is
essential but often resource-intensive. Traditional survey-based methods face
limitations in scalability and cost, making LLM-generated data a promising
alternative. However, while LLMs have the potential to simulate real consumer
behavior, recent studies highlight a significant gap between LLM-generated and
human data, with biases introduced when substituting between the two. In this
paper, we address this gap by proposing a novel statistical data augmentation
approach that efficiently integrates LLM-generated data with real data in
conjoint analysis. Our method leverages transfer learning principles to debias
the LLM-generated data using a small amount of human data. This results in
statistically robust estimators with consistent and asymptotically normal
properties, in contrast to naive approaches that simply substitute human data
with LLM-generated data, which can exacerbate bias. We validate our framework
through an empirical study on COVID-19 vaccine preferences, demonstrating its
superior ability to reduce estimation error and save data and costs by 24.9\%
to 79.8\%. In contrast, naive approaches fail to save data due to the inherent
biases in LLM-generated data compared to human data. Another empirical study on
sports car choices validates the robustness of our results. Our findings
suggest that while LLM-generated data is not a direct substitute for human
responses, it can serve as a valuable complement when used within a robust
statistical framework.",2024-12-26T22:06:29Z,http://arxiv.org/abs/2412.19363v1,"Mengxin Wang, Dennis J. Zhang, Heng Zhang"
"Evaluating Convolutional Neural Networks for COVID-19 classification in
  chest X-ray images","Coronavirus Disease 2019 (COVID-19) pandemic rapidly spread globally,
impacting the lives of billions of people. The effective screening of infected
patients is a critical step to struggle with COVID-19, and treating the
patients avoiding this quickly disease spread. The need for automated and
scalable methods has increased due to the unavailability of accurate automated
toolkits. Recent researches using chest X-ray images suggest they include
relevant information about the COVID-19 virus. Hence, applying machine learning
techniques combined with radiological imaging promises to identify this disease
accurately. It is straightforward to collect these images once it is spreadly
shared and analyzed in the world. This paper presents a method for automatic
COVID-19 detection using chest Xray images through four convolutional neural
networks, namely: AlexNet, VGG-11, SqueezeNet, and DenseNet-121. This method
had been providing accurate diagnostics for positive or negative COVID-19
classification. We validate our experiments using a ten-fold cross-validation
procedure over the training and test sets. Our findings include the shallow
fine-tuning and data augmentation strategies that can assist in dealing with
the low number of positive COVID-19 images publicly available. The accuracy for
all CNNs is higher than 97.00%, and the SqueezeNet model achieved the best
result with 99.20%.",2024-12-26T22:05:30Z,http://arxiv.org/abs/2412.19362v1,"Leonardo Gabriel Ferreira Rodrigues, Danilo Ferreira da Silva, Larissa Ferreira Rodrigues, João Fernando Mari"
Dynamic Skill Adaptation for Large Language Models,"We present Dynamic Skill Adaptation (DSA), an adaptive and dynamic framework
to adapt novel and complex skills to Large Language Models (LLMs). Compared
with previous work which learns from human-curated and static data in random
orders, we propose to first automatically generate and organize the training
data by mimicking the learning pathways of human and then dynamically tailor
the training data based on the training dynamics. Specifically, inspired by the
learning structures and teaching strategies in the human education system, we
first construct a skill graph by decomposing complex skills into sub-skills and
arranging them based on their dependencies in human syllables. For every skill,
we utilize LLMs to generate both textbook-like data which contains detailed
descriptions of skills for pre-training and exercise-like data which targets at
explicitly utilizing the skills to solve problems for instruction-tuning.
Furthermore, during the instruction-tuning, we dynamically update the training
data which down-weight easy-to-learn examples, generate more complex examples,
and filter out data with errors. Experiments on large language models such as
LLAMA and Mistral demonstrate the effectiveness of our proposed methods in
adapting math reasoning skills and social study skills.",2024-12-26T22:04:23Z,http://arxiv.org/abs/2412.19361v1,"Jiaao Chen, Diyi Yang"
"Improving the network traffic classification using the Packet Vision
  approach","The network traffic classification allows improving the management, and the
network services offer taking into account the kind of application. The future
network architectures, mainly mobile networks, foresee intelligent mechanisms
in their architectural frameworks to deliver application-aware network
requirements. The potential of convolutional neural networks capabilities,
widely exploited in several contexts, can be used in network traffic
classification. Thus, it is necessary to develop methods based on the content
of packets transforming it into a suitable input for CNN technologies. Hence,
we implemented and evaluated the Packet Vision, a method capable of building
images from packets raw-data, considering both header and payload. Our approach
excels those found in state-of-the-art by delivering security and privacy by
transforming the raw-data packet into images. Therefore, we built a dataset
with four traffic classes evaluating the performance of three CNNs
architectures: AlexNet, ResNet-18, and SqueezeNet. Experiments showcase the
Packet Vision combined with CNNs applicability and suitability as a promising
approach to deliver outstanding performance in classifying network traffic.",2024-12-26T21:56:03Z,http://arxiv.org/abs/2412.19360v1,"Rodrigo Moreira, Larissa Ferreira Rodrigues, Pedro Frosi Rosa, Flávio de Oliveira Silva"
"Federated Hybrid Training and Self-Adversarial Distillation: Towards
  Robust Edge Networks","Federated learning (FL) is a distributed training technology that enhances
data privacy in mobile edge networks by allowing data owners to collaborate
without transmitting raw data to the edge server. However, data heterogeneity
and adversarial attacks pose challenges to develop an unbiased and robust
global model for edge deployment. To address this, we propose Federated hyBrid
Adversarial training and self-adversarial disTillation (FedBAT), a new
framework designed to improve both robustness and generalization of the global
model. FedBAT seamlessly integrates hybrid adversarial training and
self-adversarial distillation into the conventional FL framework from data
augmentation and feature distillation perspectives. From a data augmentation
perspective, we propose hybrid adversarial training to defend against
adversarial attacks by balancing accuracy and robustness through a weighted
combination of standard and adversarial training. From a feature distillation
perspective, we introduce a novel augmentation-invariant adversarial
distillation method that aligns local adversarial features of augmented images
with their corresponding unbiased global clean features. This alignment can
effectively mitigate bias from data heterogeneity while enhancing both the
robustness and generalization of the global model. Extensive experimental
results across multiple datasets demonstrate that FedBAT yields comparable or
superior performance gains in improving robustness while maintaining accuracy
compared to several baselines.",2024-12-26T21:32:08Z,http://arxiv.org/abs/2412.19354v1,"Yu Qiao, Apurba Adhikary, Kitae Kim, Eui-Nam Huh, Zhu Han, Choong Seon Hong"
ETTA: Elucidating the Design Space of Text-to-Audio Models,"Recent years have seen significant progress in Text-To-Audio (TTA) synthesis,
enabling users to enrich their creative workflows with synthetic audio
generated from natural language prompts. Despite this progress, the effects of
data, model architecture, training objective functions, and sampling strategies
on target benchmarks are not well understood. With the purpose of providing a
holistic understanding of the design space of TTA models, we set up a
large-scale empirical experiment focused on diffusion and flow matching models.
Our contributions include: 1) AF-Synthetic, a large dataset of high quality
synthetic captions obtained from an audio understanding model; 2) a systematic
comparison of different architectural, training, and inference design choices
for TTA models; 3) an analysis of sampling methods and their Pareto curves with
respect to generation quality and inference speed. We leverage the knowledge
obtained from this extensive analysis to propose our best model dubbed
Elucidated Text-To-Audio (ETTA). When evaluated on AudioCaps and MusicCaps,
ETTA provides improvements over the baselines trained on publicly available
data, while being competitive with models trained on proprietary data. Finally,
we show ETTA's improved ability to generate creative audio following complex
and imaginative captions -- a task that is more challenging than current
benchmarks.",2024-12-26T21:13:12Z,http://arxiv.org/abs/2412.19351v1,"Sang-gil Lee, Zhifeng Kong, Arushi Goel, Sungwon Kim, Rafael Valle, Bryan Catanzaro"
"Semi-Supervised Learning from Small Annotated Data and Large Unlabeled
  Data for Fine-grained PICO Entity Recognition","Objective: Extracting PICO elements -- Participants, Intervention,
Comparison, and Outcomes -- from clinical trial literature is essential for
clinical evidence retrieval, appraisal, and synthesis. Existing approaches do
not distinguish the attributes of PICO entities. This study aims to develop a
named entity recognition (NER) model to extract PICO entities with fine
granularities.
  Materials and Methods: Using a corpus of 2,511 abstracts with PICO mentions
from 4 public datasets, we developed a semi-supervised method to facilitate the
training of a NER model, FinePICO, by combining limited annotated data of PICO
entities and abundant unlabeled data. For evaluation, we divided the entire
dataset into two subsets: a smaller group with annotations and a larger group
without annotations. We then established the theoretical lower and upper
performance bounds based on the performance of supervised learning models
trained solely on the small, annotated subset and on the entire set with
complete annotations, respectively. Finally, we evaluated FinePICO on both the
smaller annotated subset and the larger, initially unannotated subset. We
measured the performance of FinePICO using precision, recall, and F1.
  Results: Our method achieved precision/recall/F1 of 0.567/0.636/0.60,
respectively, using a small set of annotated samples, outperforming the
baseline model (F1: 0.437) by more than 16\%. The model demonstrates
generalizability to a different PICO framework and to another corpus, which
consistently outperforms the benchmark in diverse experimental settings
(p-value \textless0.001).
  Conclusion: This study contributes a generalizable and effective
semi-supervised approach to named entity recognition leveraging large unlabeled
data together with small, annotated data. It also initially supports
fine-grained PICO extraction.",2024-12-26T20:24:35Z,http://arxiv.org/abs/2412.19346v1,"Fangyi Chen, Gongbo Zhang, Yilu Fang, Yifan Peng, Chunhua Weng"
Advanced Scheduling of Electrolyzer Modules for Grid Flexibility,"As the transition to sustainable power generation progresses, green hydrogen
production via electrolysis is expected to gain importance as a means for
energy storage and flexible load to complement variable renewable generation.
With the increasing need for cost-effective and efficient hydrogen production,
electrolyzer optimization is essential to improve both energy efficiency and
profitability. This paper analyzes how the efficiency and modular setup of
alkaline hydrogen electrolyzers can improve hydrogen output of systems linked
to a fluctuating renewable power supply. To explore this, we propose a
day-ahead optimal scheduling problem of a hybrid wind and electrolyzer system.
The novelty of our approach lies in modeling the number and capacity of
electrolyzer modules, and capturing the modules' impact on the hydrogen
production and efficiency. We solve the resulting mixed-integer optimization
problem with several different combinations of number of modules, efficiency
and operating range parameters, using day-ahead market data from a wind farm
generator in the ERCOT system as an input. Our results demonstrate that the
proposed approach ensures that electrolyzer owners can better optimize the
operation of their systems, achieving greater hydrogen production and higher
revenue. Key findings include that as the number of modules in a system with
the same overall capacity increases, hydrogen production and revenue increases.",2024-12-26T20:24:01Z,http://arxiv.org/abs/2412.19345v1,"Angelina Lesniak, Andrea Gloppen Johnsen, Noah Rhodes, Line Roald"
Identifying Split Vacancies with Foundation Models and Electrostatics,"Point defects are ubiquitous in solid-state compounds, dictating many
functional properties such as conductivity, catalytic activity and carrier
recombination. Over the past decade, the prevalence of metastable defect
geometries and their importance to relevant properties has been increasingly
recognised. A particularly striking example of this is split vacancies, where
an isolated atomic vacancy transforms to a stoichiometry-conserving complex of
two vacancies and an interstitial ($V_X \rightarrow [V_X + X_i + V_X]$), which
can be accompanied by a dramatic lowering of the defect energy and change in
behaviour. Such species are particularly challenging to identify from
computation, due to the `non-local' nature of this reconstruction. Here, I
present an approach for efficiently identifying such species in solid-state
compounds, through tiered screening which combines geometric analysis,
electrostatic energies and foundation machine learning (ML) models. This
approach allows the screening of all compounds in the Materials Project
database (including all entries in the ICSD, along with several thousand
predicted metastable materials), identifying thousands of split vacancy
configurations, hitherto unknown. This study highlights both the potential
utility of machine-learning potentials for defect investigations, with
important caveats, and the importance of global optimisation approaches for
correctly identifying stable defect geometries.",2024-12-26T18:58:52Z,http://arxiv.org/abs/2412.19330v1,Seán R. Kavanagh
"Deep learning and whole-brain networks for biomarker discovery: modeling
  the dynamics of brain fluctuations in resting-state and cognitive tasks","Background: Brain network models offer insights into brain dynamics, but the
utility of model-derived bifurcation parameters as biomarkers remains
underexplored. Objective: This study evaluates bifurcation parameters from a
whole-brain network model as biomarkers for distinguishing brain states
associated with resting-state and task-based cognitive conditions. Methods:
Synthetic BOLD signals were generated using a supercritical Hopf brain network
model to train deep learning models for bifurcation parameter prediction.
Inference was performed on Human Connectome Project data, including both
resting-state and task-based conditions. Statistical analyses assessed the
separability of brain states based on bifurcation parameter distributions.
Results: Bifurcation parameter distributions differed significantly across task
and resting-state conditions ($p &lt; 0.0001$ for all but one comparison).
Task-based brain states exhibited higher bifurcation values compared to rest.
Conclusion: Bifurcation parameters effectively differentiate cognitive and
resting states, warranting further investigation as biomarkers for brain state
characterization and neurological disorder assessment.",2024-12-26T18:58:38Z,http://arxiv.org/abs/2412.19329v1,"Facundo Roffet, Gustavo Deco, Claudio Delrieux, Gustavo Patow"
"Resolving the Ambiguity of Complete-to-Partial Point Cloud Registration
  for Image-Guided Liver Surgery with Patches-to-Partial Matching","In image-guided liver surgery, the initial rigid alignment between
preoperative and intraoperative data, often represented as point clouds, is
crucial for providing sub-surface information from preoperative CT/MRI images
to the surgeon during the procedure. Currently, this alignment is typically
performed using semi-automatic methods, which, while effective to some extent,
are prone to errors that demand manual correction. Point cloud
correspondence-based registration methods are promising to serve as a fully
automatic solution. However, they may struggle in scenarios with limited
intraoperative surface visibility, a common challenge in liver surgery,
particularly in laparoscopic procedures, which we refer to as
complete-to-partial ambiguity. We first illustrate this ambiguity by evaluating
the performance of state-of-the-art learning-based point cloud registration
methods on our carefully constructed in silico and in vitro datasets. Then, we
propose a patches-to-partial matching strategy as a plug-and-play module to
resolve the ambiguity, which can be seamlessly integrated into learning-based
registration methods without disrupting their end-to-end structure. It has
proven effective and efficient in improving registration performance for cases
with limited intraoperative visibility. The constructed benchmark and the
proposed module establish a solid foundation for advancing applications of
point cloud correspondence-based registration methods in image-guided liver
surgery.",2024-12-26T18:58:29Z,http://arxiv.org/abs/2412.19328v1,"Zixin Yang, Jon S. Heiselman, Cheng Han, Kelly Merrell, Richard Simon, Cristian. A. Linte"
"Performance Control in Early Exiting to Deploy Large Models at the Same
  Cost of Smaller Ones","Early Exiting (EE) is a promising technique for speeding up inference by
adaptively allocating compute resources to data points based on their
difficulty. The approach enables predictions to exit at earlier layers for
simpler samples while reserving more computation for challenging ones. In this
study, we first present a novel perspective on the EE approach, showing that
larger models deployed with EE can achieve higher performance than smaller
models while maintaining similar computational costs. As existing EE approaches
rely on confidence estimation at each exit point, we further study the impact
of overconfidence on the controllability of the compute-performance trade-off.
We introduce Performance Control Early Exiting (PCEE), a method that enables
accuracy thresholding by basing decisions not on a data point's confidence but
on the average accuracy of samples with similar confidence levels from a
held-out validation set. In our experiments, we show that PCEE offers a simple
yet computationally efficient approach that provides better control over
performance than standard confidence-based approaches, and allows us to scale
up model sizes to yield performance gain while reducing the computational cost.",2024-12-26T18:54:32Z,http://arxiv.org/abs/2412.19325v1,"Mehrnaz Mofakhami, Reza Bayat, Ioannis Mitliagkas, Joao Monteiro, Valentina Zantedeschi"
Electron efficiency in LHC Run-2 with the ATLAS experiment,"The document presents a general overview of the electron reconstruction,
identification and isolation performance in the ATLAS experiment. The results
are obtained using 13 TeV proton-proton collision data collected during the LHC
Run-2. The electron reconstruction efficiency is higher than 97%, and the ratio
of data to Monte Carlo simulation efficiency is close to unity, with associated
uncertainties generally smaller than 0.1%. The electron identification is shown
for three working points, and depending on the electron $E_T$, it can be as low
as 60%, increasing to more than 80% above 50 GeV. The correction factors are
close to one, generally within 5%. Five isolation working points are
recommended in the ATLAS experiment, to successfully reject fake/non-prompt
electrons. Their dependency on the electron identification working points is
shown and discussed, as well as their pile-up dependency, and their performance
versus electron $E_T$ and $\eta$.
  Document based on a presentation at the XI International Conference on New
Frontiers in Physics (ICNFP 2022).
  keywords; prompt electrons, reconstruction, identification, isolation,
fake/non-prompt electrons",2024-12-26T18:50:43Z,http://arxiv.org/abs/2412.19323v1,Otilia Ducu
Adaptive Conformal Inference by Betting,"Conformal prediction is a valuable tool for quantifying predictive
uncertainty of machine learning models. However, its applicability relies on
the assumption of data exchangeability, a condition which is often not met in
real-world scenarios. In this paper, we consider the problem of adaptive
conformal inference without any assumptions about the data generating process.
Existing approaches for adaptive conformal inference are based on optimizing
the pinball loss using variants of online gradient descent. A notable
shortcoming of such approaches is in their explicit dependence on and
sensitivity to the choice of the learning rates. In this paper, we propose a
different approach for adaptive conformal inference that leverages
parameter-free online convex optimization techniques. We prove that our method
controls long-term miscoverage frequency at a nominal level and demonstrate its
convincing empirical performance without any need of performing cumbersome
parameter tuning.",2024-12-26T18:42:08Z,http://arxiv.org/abs/2412.19318v1,"Aleksandr Podkopaev, Darren Xu, Kuang-Chih Lee"
"ATLAS searches for higgsinos with R-parity violating couplings in events
  with leptons","This document presents two searches for Supersymmetry through the direct
production of pairs of higgsinos decaying into final states with leptons and
($b$-) jets. The analyses are performed using 139~fb$^{-1}$ of the 13~TeV
proton-proton collision data collected with the ATLAS detector. The methods
used to estimate the Standard Model and detector backgrounds are discussed, as
well as their shortcomings. Finally, results in selected signal regions, and
some exclusion limits, are presented, illustrating the significant improvement
over the previous exclusion limits.
  Document based on a presentation at the XI International Conference on New
Frontiers in Physics (ICNFP 2022).",2024-12-26T18:41:55Z,http://arxiv.org/abs/2412.19317v1,Otilia Ducu
Towards a Single ASR Model That Generalizes to Disordered Speech,"This study investigates the impact of integrating a dataset of disordered
speech recordings ($\sim$1,000 hours) into the fine-tuning of a near
state-of-the-art ASR baseline system. Contrary to what one might expect,
despite the data being less than 1% of the training data of the ASR system, we
find a considerable improvement in disordered speech recognition accuracy.
Specifically, we observe a 33% improvement on prompted speech, and a 26%
improvement on a newly gathered spontaneous, conversational dataset of
disordered speech. Importantly, there is no significant performance decline on
standard speech recognition benchmarks. Further, we observe that the proposed
tuning strategy helps close the gap between the baseline system and
personalized models by 64% highlighting the significant progress as well as the
room for improvement. Given the substantial benefits of our findings, this
experiment suggests that from a fairness perspective, incorporating a small
fraction of high quality disordered speech data in a training recipe is an easy
step that could be done to make speech technology more accessible for users
with speech disabilities.",2024-12-26T18:39:15Z,http://arxiv.org/abs/2412.19315v1,"Jimmy Tobin, Katrin Tomanek, Subhashini Venugopalan"
"Theoretical models for longitudinal coupled-bunch instabilities driven
  by harmonic cavities in electron storage rings","We present a theoretical framework for analyzing longitudinal coupled-bunch
instabilities in double-rf systems with even filling patterns, accounting for
potential-well distortion and multiple azimuthal modes. The linearized Vlasov
equation is solved in the frequency-domain for an arbitrary rf potential to
derive the Lebedev equation. We unified different formulations, obtaining
results from recent publications as particular cases. Applications to Robinson
dipole-quadrupole mode coupling and the periodic transient beam loading
(PTBL)/mode-1 instability are presented. Notably, for the first time,
theoretical predictions of the mode-1 thresholds show excellent agreement with
experimental data. The analysis reveals that the PTBL instability is a
zero-frequency effect dependent on azimuthal mode interactions and resistant to
Landau damping, providing new insights into its mechanism. The methods are
implemented in the open-source package pycolleff, offering a useful
semi-analytical tool for studying instabilities in electron storage rings with
harmonic cavities.",2024-12-26T18:11:39Z,http://arxiv.org/abs/2412.19308v1,Murilo B. Alves
"Perceive, Query &amp; Reason: Enhancing Video QA with Question-Guided
  Temporal Queries","Video Question Answering (Video QA) is a challenging video understanding task
that requires models to comprehend entire videos, identify the most relevant
information based on contextual cues from a given question, and reason
accurately to provide answers. Recent advancements in Multimodal Large Language
Models (MLLMs) have transformed video QA by leveraging their exceptional
commonsense reasoning capabilities. This progress is largely driven by the
effective alignment between visual data and the language space of MLLMs.
However, for video QA, an additional space-time alignment poses a considerable
challenge for extracting question-relevant information across frames. In this
work, we investigate diverse temporal modeling techniques to integrate with
MLLMs, aiming to achieve question-guided temporal modeling that leverages
pre-trained visual and textual alignment in MLLMs. We propose T-Former, a novel
temporal modeling method that creates a question-guided temporal bridge between
frame-wise visual perception and the reasoning capabilities of LLMs. Our
evaluation across multiple video QA benchmarks demonstrates that T-Former
competes favorably with existing temporal modeling approaches and aligns with
recent advancements in video QA.",2024-12-26T17:53:14Z,http://arxiv.org/abs/2412.19304v1,"Roberto Amoroso, Gengyuan Zhang, Rajat Koner, Lorenzo Baraldi, Rita Cucchiara, Volker Tresp"
RecLM: Recommendation Instruction Tuning,"Modern recommender systems aim to deeply understand users' complex
preferences through their past interactions. While deep collaborative filtering
approaches using Graph Neural Networks (GNNs) excel at capturing user-item
relationships, their effectiveness is limited when handling sparse data or
zero-shot scenarios, primarily due to constraints in ID-based embedding
functions. To address these challenges, we propose a model-agnostic
recommendation instruction-tuning paradigm that seamlessly integrates large
language models with collaborative filtering. Our proposed Recommendation
Language Model (RecLM) enhances the capture of user preference diversity
through a carefully designed reinforcement learning reward function that
facilitates self-augmentation of language models. Comprehensive evaluations
demonstrate significant advantages of our approach across various settings, and
its plug-and-play compatibility with state-of-the-art recommender systems
results in notable performance enhancements.",2024-12-26T17:51:54Z,http://arxiv.org/abs/2412.19302v1,"Yangqin Jiang, Yuhao Yang, Lianghao Xia, Da Luo, Kangyi Lin, Chao Huang"
"Sample Complexity of Data-driven Multistage Stochastic Programming under
  Markovian Uncertainty","This work is motivated by the challenges of applying the sample average
approximation (SAA) method to multistage stochastic programming with an unknown
continuous-state Markov process. While SAA is widely used in static and
two-stage stochastic optimization, it becomes computationally intractable in
general multistage settings as the time horizon $T$ increases. Indeed, the
number of samples required to obtain a reasonably accurate solution grows
exponentially$\text{ -- }$a phenomenon known as the curse of dimensionality
with respect to the time horizon. To overcome this limitation, we propose a
novel data-driven approach, the Markov Recombining Scenario Tree (MRST) method,
which constructs an approximate problem using only two independent trajectories
of historical data. Our analysis demonstrates that the MRST method achieves
polynomial sample complexity in $T$, providing a more efficient alternative to
SAA. Numerical experiments on the Linear Quadratic Gaussian problem show that
MRST outperforms SAA, addressing the curse of dimensionality.",2024-12-26T17:46:27Z,http://arxiv.org/abs/2412.19299v1,"Hyuk Park, Grani A. Hanasusanto"
Spatio-Temporal Differences in Bike Sharing Usage: A Tale of Six Cities,"This study investigates the spatio-temporal patterns of Bike Sharing System
(BSS) usage in six major cities: New York, London, Tokyo, Boston, Chicago and
Washington D.C. By analyzing data over a 30-day period with comparable climate
and average temperatures, we explored differences in BSS usage between weekdays
and weekends in those cities using Jensen-Shannon divergence (JSD) and rank
distribution analysis. Our findings reveal significant temporal differences in
BSS usage that were commonly observed in all cities, with weekday patterns
dominated by commute peaks and weekend patterns reflecting recreational
activities. Friday emerges as a transitional day, sharing the characteristics
of both weekdays and weekends. Meanwhile, docking station usage rank
distributions show remarkable consistency between weekdays and weekends for
most cities, with London being a unique anomaly. This study highlights the
potential of BSS data to uncover urban mobility patterns and the underlying
structures of cities. The results suggest that BSS usage reflects both
intrinsic user behavior and external influences such as urban planning.",2024-12-26T17:35:28Z,http://arxiv.org/abs/2412.19294v1,"Shu-ichi Kinoshita, Yuya Bando, Hiroki Sayama"
RAG with Differential Privacy,"Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to
provide *Large Language Models* (LLM) with fresh and relevant context,
mitigating the risk of hallucinations and improving the overall quality of
responses in environments with large and fast moving knowledge bases. However,
the integration of external documents into the generation process raises
significant privacy concerns. Indeed, when added to a prompt, it is not
possible to guarantee a response will not inadvertently expose confidential
data, leading to potential breaches of privacy and ethical dilemmas. This paper
explores a practical solution to this problem suitable to general knowledge
extraction from personal data. It shows *differentially private token
generation* is a viable approach to private RAG.",2024-12-26T17:34:26Z,http://arxiv.org/abs/2412.19291v1,Nicolas Grislain
"ViPCap: Retrieval Text-Based Visual Prompts for Lightweight Image
  Captioning","Recent lightweight image captioning models using retrieved data mainly focus
on text prompts. However, previous works only utilize the retrieved text as
text prompts, and the visual information relies only on the CLIP visual
embedding. Because of this issue, there is a limitation that the image
descriptions inherent in the prompt are not sufficiently reflected in the
visual embedding space. To tackle this issue, we propose ViPCap, a novel
retrieval text-based visual prompt for lightweight image captioning. ViPCap
leverages the retrieved text with image information as visual prompts to
enhance the ability of the model to capture relevant visual information. By
mapping text prompts into the CLIP space and generating multiple randomized
Gaussian distributions, our method leverages sampling to explore randomly
augmented distributions and effectively retrieves the semantic features that
contain image information. These retrieved features are integrated into the
image and designated as the visual prompt, leading to performance improvements
on the datasets such as COCO, Flickr30k, and NoCaps. Experimental results
demonstrate that ViPCap significantly outperforms prior lightweight captioning
models in efficiency and effectiveness, demonstrating the potential for a
plug-and-play solution.",2024-12-26T17:29:38Z,http://arxiv.org/abs/2412.19289v1,"Taewhan Kim, Soeun Lee, Si-Woo Kim, Dong-Jin Kim"
Improving Generalization for AI-Synthesized Voice Detection,"AI-synthesized voice technology has the potential to create realistic human
voices for beneficial applications, but it can also be misused for malicious
purposes. While existing AI-synthesized voice detection models excel in
intra-domain evaluation, they face challenges in generalizing across different
domains, potentially becoming obsolete as new voice generators emerge. Current
solutions use diverse data and advanced machine learning techniques (e.g.,
domain-invariant representation, self-supervised learning), but are limited by
predefined vocoders and sensitivity to factors like background noise and
speaker identity. In this work, we introduce an innovative disentanglement
framework aimed at extracting domain-agnostic artifact features related to
vocoders. Utilizing these features, we enhance model learning in a flat loss
landscape, enabling escape from suboptimal solutions and improving
generalization. Extensive experiments on benchmarks show our approach
outperforms state-of-the-art methods, achieving up to 5.12% improvement in the
equal error rate metric in intra-domain and 7.59% in cross-domain evaluations.",2024-12-26T16:45:20Z,http://arxiv.org/abs/2412.19279v1,"Hainan Ren, Lin Li, Chun-Hao Liu, Xin Wang, Shu Hu"
Memory-Centric Computing: Recent Advances in Processing-in-DRAM,"Memory-centric computing aims to enable computation capability in and near
all places where data is generated and stored. As such, it can greatly reduce
the large negative performance and energy impact of data access and data
movement, by 1) fundamentally avoiding data movement, 2) reducing data access
latency &amp; energy, and 3) exploiting large parallelism of memory arrays. Many
recent studies show that memory-centric computing can largely improve system
performance &amp; energy efficiency. Major industrial vendors and startup companies
have recently introduced memory chips with sophisticated computation
capabilities. Going forward, both hardware and software stack should be
revisited and designed carefully to take advantage of memory-centric computing.
  This work describes several major recent advances in memory-centric
computing, specifically in Processing-in-DRAM, a paradigm where the operational
characteristics of a DRAM chip are exploited and enhanced to perform
computation on data stored in DRAM. Specifically, we describe 1) new techniques
that slightly modify DRAM chips to enable both enhanced computation capability
and easier programmability, 2) new experimental studies that demonstrate the
functionally-complete bulk-bitwise computational capability of real commercial
off-the-shelf DRAM chips, without any modifications to the DRAM chip or the
interface, and 3) new DRAM designs that improve access granularity &amp;
efficiency, unleashing the true potential of Processing-in-DRAM.",2024-12-26T16:31:40Z,http://arxiv.org/abs/2412.19275v1,"Onur Mutlu, Ataberk Olgun, Geraldo F. Oliveira, Ismail Emir Yuksel"
"Anvendelse av kunstig intelligens (KI) i Norge i norsk offentlig sektor
  2024","There are great expectations for the use of AI in Norway. On the other hand,
it is reported that the adoption of AI in Norway is slower than expected in
both the private and public sectors. Using responses from NOKIOS Technology
Radar 2017-2021, IT in Practice surveys conducted by Ramboll in 2021-2024, as
well as another national survey as part of a five-year cycle, this article
looks at reported and planned use of AI with a focus on local (municipalities)
and national government agencies. IT in practice is distributed to a large
number of Norwegian public agencies, with a response rate of over 5o percent.
The most recent data (2024) presented in this article is based on responses
from 335 public organizations, with 237 municipalities, and 98 public
organizations at the national or regional level. The survey confirms that the
use of AI is still at an early stage, although expectations are high for future
use.
  --
  Det er store forventninger til bruk av KI i Norge. P{\aa} den annen side
rapporteres det at adopsjonen av KI i Norge g{\aa}r tregere enn forventet
b{\aa}de i privat og offentlig sektor. Ved hjelp av svar fra NOKIOS
teknologiradar 2017-2021, IT i Praksis unders{\o}kelser utf{\o}rt av Ramb{\o}ll
i 2021-2024, samt en annen nasjonal unders{\o}kelse som en del av en
fem{\aa}rig syklus, ser vi i denne artikkelen p{\aa} rapportert og planlagt
bruk av KI med fokus p{\aa} lokale (kommuner) og nasjonale offentlige etater.
IT i praksis distribueres til en lang rekke norske offentlige virksomheter, med
en svarprosent p{\aa} over 50 prosent. De nyeste dataene (2024) presentert i
denne artikkelen er basert p{\aa} svar fra 335 offentlige organisasjoner, med
237 kommuner, og 98 offentlige organisasjoner p{\aa} nasjonalt eller regionalt
niv{\aa}. Unders{\o}kelsen bekrefter at bruken av KI fortsatt er p{\aa} et
tidlig stadium, selv om forventningene er h{\o}ye til fremtidig bruk.",2024-12-26T16:28:49Z,http://arxiv.org/abs/2412.19273v1,John Krogstie
Jet formation studies in AGN: a search for new targets,"In recent years, the jet formation region in active galaxies has been imaged
through mm-VLBI in few ideal targets, first and foremost M87. An important leap
forward for understanding jet launching could be made by identifying a larger
number of suitable objects, characterized by different accretion modes and jet
powers. In this article, we present 1 cm and 7 mm VLBI data of a sample of 16
poorly explored radio galaxies, comprising both High-Excitation (HEG) and
Low-Excitation Galaxies (LEG) and spanning a large range in radio power. The
combination of the sources vicinity (z&lt;0.1) with a large black hole mass
($\log{M_{\rm BH}}$&gt;8.5) results in a high spatial resolution in units of
Schwarzschild radii ($&lt;10^3-10^4$ $R_{\rm S}$), necessary for probing the
region where the jet is initially accelerated and collimated. We aim at
identifying the best candidates for follow-up observations with current and
future VLBI facilities. The observations were performed with the High
Sensitivity Array, including Effelsberg and the phased-VLA, which has allowed
us to characterize the sub-parsec properties of these faint jets and to
estimate their core brightness temperature and orientation. The number of
sources imaged on scales $\lesssim 10^3$ $R_{\rm S}$ is more than doubled by
our study. All targets were detected at both frequencies, and several present
two-sided jet structures. Several LEG jets show hints of limb-brightening. The
core brightness temperatures are generally below the equipartition value,
indicating that equipartition has not yet been reached and/or that the emission
is de-boosted. Among LEG, we identify 3C31, 3C66B, and 3C465 as the most
promising, combining a relatively high flux density (&gt;50 mJy) with superb
spatial resolution (&lt;500 $R_{\rm S}$) at 7 mm. The powerful HEG 3C452 is
interesting as well due to its highly symmetric, two-sided jet base.",2024-12-26T16:20:53Z,http://arxiv.org/abs/2412.19268v1,"B. Boccardi, L. Ricci, E. Madika, V. Bartolini, U. Bach, P. Grandi, E. Torresi, T. P. Krichbaum, J. A. Zensus"
"Search for a neutral gauge boson with nonuniversal fermion couplings in
  vector boson fusion processes in proton-proton collisions at $\sqrt{s}$ = 13
  TeV","The first search for a heavy neutral spin-1 gauge boson (Z') with
nonuniversal fermion couplings produced via vector boson fusion processes and
decaying to tau leptons or W bosons is presented. The analysis is performed
using LHC data at $\sqrt{s}$ = 13 TeV, collected from 2016 to 2018 and
corresponding to an integrated luminosity of 138 fb$^{-1}$. The data are
consistent with the standard model predictions. Upper limits are set on the
product of the cross section for production of the Z' boson and its branching
fraction to $\tau\tau$ or WW. The presence of a Z' boson decaying to
$\tau^+\tau^-$ (W$^+$W$^-$) is excluded for masses up to 2.45 (1.60) TeV,
depending on the Z' boson coupling to SM weak bosons, and assuming a Z' $\to$
$\tau^+\tau^-$ (W$^+$W$^-$) branching fraction of 50%.",2024-12-26T15:54:58Z,http://arxiv.org/abs/2412.19261v1,CMS Collaboration
"MEDEC: A Benchmark for Medical Error Detection and Correction in
  Clinical Notes","Several studies showed that Large Language Models (LLMs) can answer medical
questions correctly, even outperforming the average human score in some medical
exams. However, to our knowledge, no study has been conducted to assess the
ability of language models to validate existing or generated medical text for
correctness and consistency. In this paper, we introduce MEDEC
(https://github.com/abachaa/MEDEC), the first publicly available benchmark for
medical error detection and correction in clinical notes, covering five types
of errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal
Organism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes
from three US hospital systems that were not previously seen by any LLM. The
dataset has been used for the MEDIQA-CORR shared task to evaluate seventeen
participating systems [Ben Abacha et al., 2024]. In this paper, we describe the
data creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,
Claude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and
correcting medical errors requiring both medical knowledge and reasoning
capabilities. We also conducted a comparative study where two medical doctors
performed the same task on the MEDEC test set. The results showed that MEDEC is
a sufficiently challenging benchmark to assess the ability of models to
validate existing or generated notes and to correct medical errors. We also
found that although recent LLMs have a good performance in error detection and
correction, they are still outperformed by medical doctors in these tasks. We
discuss the potential factors behind this gap, the insights from our
experiments, the limitations of current evaluation metrics, and share potential
pointers for future research.",2024-12-26T15:54:10Z,http://arxiv.org/abs/2412.19260v1,"Asma Ben Abacha, Wen-wai Yim, Yujuan Fu, Zhaoyi Sun, Meliha Yetisgen, Fei Xia, Thomas Lin"
Swarm Contract: A Multi-Sovereign Agent Consensus Mechanism,"Traditional smart contracts on blockchains excel at on-chain, deterministic
logic. However, they have inherent limitations when dealing with large-scale
off-chain data, dynamic multi-step workflows, and scenarios requiring high
flexibility or iterative updates. In this paper, we propose the concept of a
""Swarm Contract"" (Swarm), a multi-agent mechanism wherein several digital life
forms (DLF) or Sovereign Agents (SA) collectively handle complex tasks in
Trusted Execution Environments (TEE). These digital entities are defined as
autonomous software agents that own their code, state, and possibly on-chain
assets, while operating free from centralized control.
  By leveraging a simple multi-signature wallet on-chain, Swarm moves most of
the logic off-chain, achieving trust minimization through multi-agent consensus
rather than a single monolithic on-chain contract. We illustrate these ideas
with a lightweight off-chain auction example - minting and selling 10,000
identical NFTs - to showcase how off-chain coordination can determine a
clearing price and finalize distribution, with each step performed collectively
by multiple agents in TEE. This approach broadens the scope of trustless and
decentralized solutions, potentially benefiting DAO governance, multi-modal
data processing, and cross-chain interoperability.",2024-12-26T15:46:56Z,http://arxiv.org/abs/2412.19256v1,Haowei Yang
"Leveraging Self-Training and Variational Autoencoder for Agitation
  Detection in People with Dementia Using Wearable Sensors","Dementia is a neurodegenerative disorder that has been growing among elder
people over the past decades. This growth profoundly impacts the quality of
life for patients and caregivers due to the symptoms arising from it. Agitation
and aggression (AA) are some of the symptoms of people with severe dementia
(PwD) in long-term care or hospitals. AA not only causes discomfort but also
puts the patients or others at potential risk. Existing monitoring solutions
utilizing different wearable sensors integrated with Artificial Intelligence
(AI) offer a way to detect AA early enough for timely and adequate medical
intervention. However, most studies are limited by the availability of
accurately labeled datasets, which significantly affects the efficacy of such
solutions in real-world scenarios. This study presents a novel comprehensive
approach to detect AA in PwD using physiological data from the Empatica E4
wristbands. The research creates a diverse dataset, consisting of three
distinct datasets gathered from 14 participants across multiple hospitals in
Canada. These datasets have not been extensively explored due to their limited
labeling. We propose a novel approach employing self-training and a variational
autoencoder (VAE) to detect AA in PwD effectively. The proposed approach aims
to learn the representation of the features extracted using the VAE and then
uses a semi-supervised block to generate labels, classify events, and detect
AA. We demonstrate that combining Self-Training and Variational Autoencoder
mechanism significantly improves model performance in classifying AA in PwD.
Among the tested techniques, the XGBoost classifier achieved the highest
accuracy of 90.16\%. By effectively addressing the challenge of limited labeled
data, the proposed system not only learns new labels but also proves its
superiority in detecting AA.",2024-12-26T15:34:25Z,http://arxiv.org/abs/2412.19254v1,"Abeer Badawi, Somayya Elmoghazy, Samira Choudhury, Khalid Elgazzar, Amer Burhan"
"Localized exploration in contextual dynamic pricing achieves
  dimension-free regret","We study the problem of contextual dynamic pricing with a linear demand
model. We propose a novel localized exploration-then-commit (LetC) algorithm
which starts with a pure exploration stage, followed by a refinement stage that
explores near the learned optimal pricing policy, and finally enters a pure
exploitation stage. The algorithm is shown to achieve a minimax optimal,
dimension-free regret bound when the time horizon exceeds a polynomial of the
covariate dimension. Furthermore, we provide a general theoretical framework
that encompasses the entire time spectrum, demonstrating how to balance
exploration and exploitation when the horizon is limited. The analysis is
powered by a novel critical inequality that depicts the
exploration-exploitation trade-off in dynamic pricing, mirroring its existing
counterpart for the bias-variance trade-off in regularized regression. Our
theoretical results are validated by extensive experiments on synthetic and
real-world data.",2024-12-26T15:29:58Z,http://arxiv.org/abs/2412.19252v1,"Jinhang Chai, Yaqi Duan, Jianqing Fan, Kaizheng Wang"
Network double autoregression,"Modeling high-dimensional time series with simple structures is a challenging
problem. This paper proposes a network double autoregression (NDAR) model,
which combines the advantages of network structure and the double
autoregression (DAR) model, to handle high-dimensional, conditionally
heteroscedastic, and network-structured data within a simple framework. The
parameters of the model are estimated using quasi-maximum likelihood
estimation, and the asymptotic properties of the estimators are derived. The
selection of the model's lag order will be based on the Bayesian information
criterion. Finite-sample simulations show that the proposed model performs well
even with moderate time dimensions and network sizes. Finally, the model is
applied to analyze three different categories of stock data.",2024-12-26T15:28:41Z,http://arxiv.org/abs/2412.19251v1,"Tingting Li, Hao Wang"
"A Space Lower Bound for Approximate Membership with Duplicate Insertions
  or Deletions of Nonelements","Designs of data structures for approximate membership queries with
false-positive errors that support both insertions and deletions stipulate the
following two conditions: (1) Duplicate insertions are prohibited, i.e., it is
prohibited to insert an element $x$ if $x$ is currently a member of the
dataset. (2) Deletions of nonelements are prohibited, i.e., it is prohibited to
delete $x$ if $x$ is not currently a member of the dataset. Under these
conditions, the space required for the approximate representation of a datasets
of cardinality $n$ with a false-positive probability of $\epsilon^{+}$ is at
most $(1+o(1))n\cdot\log_2 (1/\epsilon^{+}) + O(n)$ bits [Bender et al., 2018;
Bercea and Even, 2019].
  We prove that if these conditions are lifted, then the space required for the
approximate representation of datasets of cardinality $n$ from a universe of
cardinality $u$ is at least $\frac 12 \cdot (1-\epsilon^{+} -\frac 1n)\cdot
\log \binom{u}{n} -O(n)$ bits.",2024-12-26T15:21:42Z,http://arxiv.org/abs/2412.19249v1,"Aryan Agarwala, Guy Even"
Functional structural equation modeling with latent variables,"Handling latent variables in Structural Equation Models (SEMs) in a case
where both the latent variables and their corresponding indicators in the
measurement error part of the model are random curves presents significant
challenges, especially with sparse data. In this paper, we develop a novel
family of Functional Structural Equation Models (FSEMs) that incorporate latent
variables modeled as Gaussian Processes (GPs). The introduced FSEMs are built
upon functional regression models having response variables modeled as
underlying GPs. The model flexibly adapts to cases when the random curves'
realizations are observed only over a sparse subset of the domain, and the
inferential framework is based on a restricted maximum likelihood approach. The
advantage of this framework lies in its ability and flexibility in handling
various data scenarios, including regularly and irregularly spaced points and
thus missing data. To extract smooth estimates for the functional parameters,
we employ a penalized likelihood approach that selects the smoothing parameters
using a cross-validation method. We evaluate the performance of the proposed
model using simulation studies and a real data example, which suggests that our
model performs well in practice. The uncertainty associated with the estimates
of the functional coefficients is also assessed by constructing confidence
regions for each estimate. The goodness of fit indices that are commonly used
to evaluate the fit of SEMs are developed for the FSEMs introduced in this
paper. Overall, the proposed method is a promising approach for modeling
functional data in SEMs with functional latent variables.",2024-12-26T14:57:14Z,http://arxiv.org/abs/2412.19242v1,"Fatemeh Asgari, Valeria Vitelli, Uta Sailer"
"Effect of Peak Absolute Magnitude of Type Ia Supernovae and Sound
  Horizon Values on Hubble Tension using DESI results","We apply data-motivated priors on the peak absolute magnitude of Type Ia
supernovae ($M$), and on the sound horizon at the drag epoch ($r_d$), to study
their impact on the Hubble tension, when compared to the Planck estimated value
of the Hubble constant. We use the data from Pantheon$+$, cosmic chronometers,
and the latest DESI BAO results for this purpose. We reaffirm the fact that
there is a degeneracy between $M$ and $r_d$, and modifying the $r_d$ values to
reconcile the Hubble tension also requires a change in the peak absolute
magnitude $M$. For certain $M$ and $r_d$ priors, the tension is found to reduce
to as low as (1.2-2) $\sigma$.",2024-12-26T14:51:09Z,http://arxiv.org/abs/2412.19240v1,"Shubham Barua, Shantanu Desai"
FineVQ: Fine-Grained User Generated Content Video Quality Assessment,"The rapid growth of user-generated content (UGC) videos has produced an
urgent need for effective video quality assessment (VQA) algorithms to monitor
video quality and guide optimization and recommendation procedures. However,
current VQA models generally only give an overall rating for a UGC video, which
lacks fine-grained labels for serving video processing and recommendation
applications. To address the challenges and promote the development of UGC
videos, we establish the first large-scale Fine-grained Video quality
assessment Database, termed FineVD, which comprises 6104 UGC videos with
fine-grained quality scores and descriptions across multiple dimensions. Based
on this database, we propose a Fine-grained Video Quality assessment (FineVQ)
model to learn the fine-grained quality of UGC videos, with the capabilities of
quality rating, quality scoring, and quality attribution. Extensive
experimental results demonstrate that our proposed FineVQ can produce
fine-grained video-quality results and achieve state-of-the-art performance on
FineVD and other commonly used UGC-VQA datasets. Both Both FineVD and FineVQ
will be made publicly available.",2024-12-26T14:44:47Z,http://arxiv.org/abs/2412.19238v1,"Huiyu Duan, Qiang Hu, Jiarui Wang, Liu Yang, Zitong Xu, Lu Liu, Xiongkuo Min, Chunlei Cai, Tianxiao Ye, Xiaoyun Zhang, Guangtao Zhai"
SeaMo: A Multi-Seasonal and Multimodal Remote Sensing Foundation Model,"Remote Sensing (RS) data contains a wealth of multi-dimensional information
crucial for Earth observation. Owing to its vast volume, diverse sources, and
temporal properties, RS data is highly suitable for the development of large
Visual Foundation Models (VFMs). VFMs act as robust feature extractors,
learning from extensive RS data, and are subsequently fine-tuned for deployment
in various geoscientific tasks. However, current VFMs in the RS domain are
predominantly pretrained and tailored exclusively for specific characteristics
of RS imagery, neglecting the potential of utilizing the multi-dimensional
properties of RS data. Therefore, in this work, we propose SeaMo, a pioneering
visual foundation model that integrates multi-seasonal and multimodal
information in the RS field. SeaMo is designed to harness multiple properties
of RS data. Within the masked image modeling framework, we employ non-aligned
cropping techniques to extract spatial properties, use multi-source inputs for
multimodal integration, and incorporate temporal-multimodal fusion blocks for
effective assimilation of multi-seasonal data. SeaMo explicitly models the
multi-dimensional properties of RS data, making the model more comprehensive,
robust, and versatile. We applied SeaMo to several downstream geoscience tasks,
which demonstrated exceptional performance. Extensive ablation studies were
conducted to validate the model's superiority.",2024-12-26T14:40:38Z,http://arxiv.org/abs/2412.19237v1,"Xuyang Li, Danfeng Hong, Chenyu Li, Jocelyn Chanussot"
"Are Two Hidden Layers Still Enough for the Physics-Informed Neural
  Networks?","The article discusses the development of various methods and techniques for
initializing and training neural networks with a single hidden layer, as well
as training a separable physics-informed neural network consisting of neural
networks with a single hidden layer to solve physical problems described by
ordinary differential equations (ODEs) and partial differential equations
(PDEs). A method for strictly deterministic initialization of a neural network
with one hidden layer for solving physical problems described by an ODE is
proposed. Modifications to existing methods for weighting the loss function are
given, as well as new methods developed for training strictly
deterministic-initialized neural networks to solve ODEs (detaching, additional
weighting based on the second derivative, predicted solution-based weighting,
relative residuals). An algorithm for physics-informed data-driven
initialization of a neural network with one hidden layer is proposed. A neural
network with pronounced generalizing properties is presented, whose
generalizing abilities of which can be precisely controlled by adjusting
network parameters. A metric for measuring the generalization of such neural
network has been introduced. A gradient-free neuron-by-neuron fitting method
has been developed for adjusting the parameters of a single-hidden-layer neural
network, which does not require the use of an optimizer or solver for its
implementation. The proposed methods have been extended to 2D problems using
the separable physics-informed neural networks approach. Numerous experiments
have been carried out to develop the above methods and approaches. Experiments
on physical problems, such as solving various ODEs and PDEs, have demonstrated
that these methods for initializing and training neural networks with one or
two hidden layers (SPINN) achieve competitive accuracy and, in some cases,
state-of-the-art results.",2024-12-26T14:30:54Z,http://arxiv.org/abs/2412.19235v1,"Vasiliy A. Es'kin, Alexey O. Malkhanov, Mikhail E. Smorkalov"
"Virtual Nodes Can Help: Tackling Distribution Shifts in Federated Graph
  Learning","Federated Graph Learning (FGL) enables multiple clients to jointly train
powerful graph learning models, e.g., Graph Neural Networks (GNNs), without
sharing their local graph data for graph-related downstream tasks, such as
graph property prediction. In the real world, however, the graph data can
suffer from significant distribution shifts across clients as the clients may
collect their graph data for different purposes. In particular, graph
properties are usually associated with invariant label-relevant substructures
(i.e., subgraphs) across clients, while label-irrelevant substructures can
appear in a client-specific manner. The issue of distribution shifts of graph
data hinders the efficiency of GNN training and leads to serious performance
degradation in FGL. To tackle the aforementioned issue, we propose a novel FGL
framework entitled FedVN that eliminates distribution shifts through
client-specific graph augmentation strategies with multiple learnable Virtual
Nodes (VNs). Specifically, FedVN lets the clients jointly learn a set of shared
VNs while training a global GNN model. To eliminate distribution shifts, each
client trains a personalized edge generator that determines how the VNs connect
local graphs in a client-specific manner. Furthermore, we provide theoretical
analyses indicating that FedVN can eliminate distribution shifts of graph data
across clients. Comprehensive experiments on four datasets under five settings
demonstrate the superiority of our proposed FedVN over nine baselines.",2024-12-26T14:16:15Z,http://arxiv.org/abs/2412.19229v1,"Xingbo Fu, Zihan Chen, Yinhan He, Song Wang, Binchi Zhang, Chen Chen, Jundong Li"
Multi-view Fake News Detection Model Based on Dynamic Hypergraph,"With the rapid development of online social networks and the inadequacies in
content moderation mechanisms, the detection of fake news has emerged as a
pressing concern for the public. Various methods have been proposed for fake
news detection, including text-based approaches as well as a series of
graph-based approaches. However, the deceptive nature of fake news renders
text-based approaches less effective. Propagation tree-based methods focus on
the propagation process of individual news, capturing pairwise relationships
but lacking the capability to capture high-order complex relationships. Large
heterogeneous graph-based approaches necessitate the incorporation of
substantial additional information beyond news text and user data, while
hypergraph-based approaches rely on predefined hypergraph structures. To tackle
these issues, we propose a novel dynamic hypergraph-based multi-view fake news
detection model (DHy-MFND) that learns news embeddings across three distinct
views: text-level, propagation tree-level, and hypergraph-level. By employing
hypergraph structures to model complex high-order relationships among multiple
news pieces and introducing dynamic hypergraph structure learning, we optimize
predefined hypergraph structures while learning news embeddings. Additionally,
we introduce contrastive learning to capture authenticity-relevant embeddings
across different views. Extensive experiments on two benchmark datasets
demonstrate the effectiveness of our proposed DHy-MFND compared with a broad
range of competing baselines.",2024-12-26T14:05:51Z,http://arxiv.org/abs/2412.19227v1,"Rongping Ye, Xiaobing Pei"
"Completion as Enhancement: A Degradation-Aware Selective Image Guided
  Network for Depth Completion","In this paper, we introduce the Selective Image Guided Network (SigNet), a
novel degradation-aware framework that transforms depth completion into depth
enhancement for the first time. Moving beyond direct completion using
convolutional neural networks (CNNs), SigNet initially densifies sparse depth
data through non-CNN densification tools to obtain coarse yet dense depth. This
approach eliminates the mismatch and ambiguity caused by direct convolution
over irregularly sampled sparse data. Subsequently, SigNet redefines completion
as enhancement, establishing a self-supervised degradation bridge between the
coarse depth and the targeted dense depth for effective RGB-D fusion. To
achieve this, SigNet leverages the implicit degradation to adaptively select
high-frequency components (e.g., edges) of RGB data to compensate for the
coarse depth. This degradation is further integrated into a multi-modal
conditional Mamba, dynamically generating the state parameters to enable
efficient global high-frequency information interaction. We conduct extensive
experiments on the NYUv2, DIML, SUN RGBD, and TOFDC datasets, demonstrating the
state-of-the-art (SOTA) performance of SigNet.",2024-12-26T14:05:01Z,http://arxiv.org/abs/2412.19225v1,"Zhiqiang Yan, Zhengxue Wang, Kun Wang, Jun Li, Jian Yang"
"Interference-Robust Broadband Rapidly-Varying MIMO Communications: A
  Knowledge-Data Dual Driven Framework","A novel time-efficient framework is proposed for improving the robustness of
a broadband multiple-input multiple-output (MIMO) system against unknown
interference under rapidly-varying channels. A mean-squared error (MSE)
minimization problem is formulated by optimizing the beamformers employed.
Since the unknown interference statistics are the premise for solving the
formulated problem, an interference statistics tracking (IST) module is first
designed. The IST module exploits both the time- and spatial-domain
correlations of the interference-plus-noise (IPN) covariance for the future
predictions with data training. Compared to the conventional signal-free space
sampling approach, the IST module can realize zero-pilot and low-latency
estimation. Subsequently, an interference-resistant hybrid beamforming (IR-HBF)
module is presented, which incorporates both the prior knowledge of the
theoretical optimization method as well as the data-fed training. Taking
advantage of the interpretable network structure, the IR-HBF module enables the
simplified mapping from the interference statistics to the beamforming weights.
The simulations are executed in high-mobility scenarios, where the numerical
results unveil that: 1) the proposed IST module attains promising prediction
accuracy compared to the conventional counterparts under different snapshot
sampling errors; and 2) the proposed IR-HBF module achieves lower MSE with
significantly reduced computational complexity.",2024-12-26T13:59:08Z,http://arxiv.org/abs/2412.19221v1,"Jingjing Zhao, Jing Su, Xianchi Lv, Kaiquan Cai, Yanbo Zhu, Yuanwei Liu, Naofal Al-Dhahir"
"Applying the maximum entropy principle to multi-species neural networks
  improves species distribution models","The rapid expansion of citizen science initiatives has led to a significant
growth of biodiversity databases, and particularly presence-only (PO)
observations. PO data are invaluable for understanding species distributions
and their dynamics, but their use in Species Distribution Models (SDM) is
curtailed by sampling biases and the lack of information on absences. Poisson
point processes are widely used for SDMs, with Maxent being one of the most
popular methods. Maxent maximises the entropy of a probability distribution
across sites as a function of predefined transformations of environmental
variables, called features. In contrast, neural networks and deep learning have
emerged as a promising technique for automatic feature extraction from complex
input variables. In this paper, we propose DeepMaxent, which harnesses neural
networks to automatically learn shared features among species, using the
maximum entropy principle. To do so, it employs a normalised Poisson loss where
for each species, presence probabilities across sites are modelled by a neural
network. We evaluate DeepMaxent on a benchmark dataset known for its spatial
sampling biases, using PO data for calibration and presence-absence (PA) data
for validation across six regions with different biological groups and
environmental covariates. Our results indicate that DeepMaxent improves model
performance over Maxent and other state-of-the-art SDMs across regions and
taxonomic groups. The method performs particularly well in regions of uneven
sampling, demonstrating substantial potential to improve species distribution
modelling. The method opens the possibility to learn more robust environmental
features predicting jointly many species and scales to arbitrary large numbers
of sites without an increased memory demand.",2024-12-26T13:47:04Z,http://arxiv.org/abs/2412.19217v1,"Maxime Ryckewaert, Diego Marcos, Christophe Botella, Maximilien Servajean, Pierre Bonnet, Alexis Joly"
"Optimizing Fantasy Sports Team Selection with Deep Reinforcement
  Learning","Fantasy sports, particularly fantasy cricket, have garnered immense
popularity in India in recent years, offering enthusiasts the opportunity to
engage in strategic team-building and compete based on the real-world
performance of professional athletes. In this paper, we address the challenge
of optimizing fantasy cricket team selection using reinforcement learning (RL)
techniques. By framing the team creation process as a sequential
decision-making problem, we aim to develop a model that can adaptively select
players to maximize the team's potential performance. Our approach leverages
historical player data to train RL algorithms, which then predict future
performance and optimize team composition. This not only represents a huge
business opportunity by enabling more accurate predictions of high-performing
teams but also enhances the overall user experience. Through empirical
evaluation and comparison with traditional fantasy team drafting methods, we
demonstrate the effectiveness of RL in constructing competitive fantasy teams.
Our results show that RL-based strategies provide valuable insights into player
selection in fantasy sports.",2024-12-26T13:36:18Z,http://arxiv.org/abs/2412.19215v1,"Shamik Bhattacharjee, Kamlesh Marathe, Hitesh Kapoor, Nilesh Patil"
Primordial Power Spectrum of Five Dimensional Uniform Inflation,"Five dimensional (5D) uniform inflation describes a de Sitter (or
approximate) solution of 5D Einstein equations, with cosmological constant and
a 5D Planck scale $M_* \sim 10^9$ GeV. During the inflationary period all
dimensions (compact and non-compact) expand exponentially in terms of the 5D
proper time. This set-up requires about 40 $e$-folds to expand the fifth
dimension from the fundamental length to the micron size. At the end of 5D
inflation (or at any given moment during the inflationary phase) one can
interpret the solution in terms of 4D fields using 4D Planck units from the
relation $M_p^2 = 2 \pi R M_*^3$, which amounts going to the 4D Einstein frame.
This implies that if the compactification radius $R$ expands $N$ $e$-folds,
then the 3D space would expand $3N/2$ $e$-folds as a result of a uniform 5D
inflation. We reexamine the primordial power spectrum predicted by this model
and show that it is consistent with Planck's measurements of the comic
microwave background. The best-fit to Planck data corresponds to $R \sim
10~\mu$m. A departure of the angular power spectrum predicted by 4D cosmology
is visible at multipole moment $\ell \sim 7$.",2024-12-26T13:24:36Z,http://arxiv.org/abs/2412.19213v1,"Luis A. Anchordoqui, Ignatios Antoniadis"
"Towards Better Spherical Sliced-Wasserstein Distance Learning with
  Data-Adaptive Discriminative Projection Direction","Spherical Sliced-Wasserstein (SSW) has recently been proposed to measure the
discrepancy between spherical data distributions in various fields, such as
geology, medical domains, computer vision, and deep representation learning.
However, in the original SSW, all projection directions are treated equally,
which is too idealistic and cannot accurately reflect the importance of
different projection directions for various data distributions. To address this
issue, we propose a novel data-adaptive Discriminative Spherical
Sliced-Wasserstein (DSSW) distance, which utilizes a projected energy function
to determine the discriminative projection direction for SSW. In our new DSSW,
we introduce two types of projected energy functions to generate the weights
for projection directions with complete theoretical guarantees. The first type
employs a non-parametric deterministic function that transforms the projected
Wasserstein distance into its corresponding weight in each projection
direction. This improves the performance of the original SSW distance with
negligible additional computational overhead. The second type utilizes a neural
network-induced function that learns the projection direction weight through a
parameterized neural network based on data projections. This further enhances
the performance of the original SSW distance with less extra computational
overhead. Finally, we evaluate the performance of our proposed DSSW by
comparing it with several state-of-the-art methods across a variety of machine
learning tasks, including gradient flows, density estimation on real earth
data, and self-supervised learning.",2024-12-26T13:23:37Z,http://arxiv.org/abs/2412.19212v1,"Hongliang Zhang, Shuo Chen, Lei Luo, Jian Yang"
"Large Language Models Meet Graph Neural Networks: A Perspective of Graph
  Mining","Graph mining is an important area in data mining and machine learning that
involves extracting valuable information from graph-structured data. In recent
years, significant progress has been made in this field through the development
of graph neural networks (GNNs). However, GNNs are still deficient in
generalizing to diverse graph data. Aiming to this issue, Large Language Models
(LLMs) could provide new solutions for graph mining tasks with their superior
semantic understanding. In this review, we systematically review the
combination and application techniques of LLMs and GNNs and present a novel
taxonomy for research in this interdisciplinary field, which involves three
main categories: GNN-driving-LLM, LLM-driving-GNN, and GNN-LLM-co-driving.
Within this framework, we reveal the capabilities of LLMs in enhancing graph
feature extraction as well as improving the effectiveness of downstream tasks
such as node classification, link prediction, and community detection. Although
LLMs have demonstrated their great potential in handling graph-structured data,
their high computational requirements and complexity remain challenges. Future
research needs to continue to explore how to efficiently fuse LLMs and GNNs to
achieve more powerful graph learning and reasoning capabilities and provide new
impetus for the development of graph mining techniques.",2024-12-26T13:21:09Z,http://arxiv.org/abs/2412.19211v1,"Yuxin You, Zhen Liu, Xiangchao Wen, Yongtao Zhang, Wei Ai"
Context-Aware Deep Learning for Multi Modal Depression Detection,"In this study, we focus on automated approaches to detect depression from
clinical interviews using multi-modal machine learning (ML). Our approach
differentiates from other successful ML methods such as context-aware analysis
through feature engineering and end-to-end deep neural networks for depression
detection utilizing the Distress Analysis Interview Corpus. We propose a novel
method that incorporates: (1) pre-trained Transformer combined with data
augmentation based on topic modelling for textual data; and (2) deep 1D
convolutional neural network (CNN) for acoustic feature modeling. The
simulation results demonstrate the effectiveness of the proposed method for
training multi-modal deep learning models. Our deep 1D CNN and Transformer
models achieved state-of-the-art performance for audio and text modalities
respectively. Combining them in a multi-modal framework also outperforms
state-of-the-art for the combined setting. Code available at
https://github.com/genandlam/multi-modal-depression-detection",2024-12-26T13:19:26Z,http://arxiv.org/abs/2412.19209v1,"Genevieve Lam, Huang Dongyan, Weisi Lin"
"Open cluster dissolution rate and the initial cluster mass function in
  the solar neighbourhood. Modelling the age and mass distributions of clusters
  observed by Gaia","Context. The dissolution rate of open clusters (OCs) and integration of their
stars into the Milky Way's field population has been previously explored using
their age distribution. With the advent of the Gaia mission, we have an
exceptional opportunity to revisit and enhance these studies with ages and
masses from high quality data. Aims. To build a comprehensive Gaia-based OC
mass catalogue which, combined with the age distribution, allows a deeper
investigation of the disruption experienced by OCs within the solar
neighbourhood. Methods. Masses were determined by comparing luminosity
distributions to theoretical luminosity functions. The limiting and core radii
of the clusters were obtained by fitting the King function to their observed
density profiles. We examined the disruption process through simulations of the
build-up and mass evolution of a population of OCs which were compared to the
observed mass and age distributions. Results. Our analysis yielded an OC mass
distribution with a peak at $log(M)$ = 2.7 dex ($\sim 500 M_{\odot}$), as well
as radii for 1724 OCs. Our simulations showed that using a power-law Initial
Cluster Mass Function (ICMF) no parameters were able to reproduce the observed
mass distribution. Moreover, we find that a skew log-normal ICMF provides a
good match to the observations and that the disruption time of a $10^4
M{_\odot}$ OC is $t_4^{tot} = 2.9 \pm 0.4$ Gyr. Conclusions. Our results
indicate that the OC disruption time $t_4^{tot}$ is about twice longer than
previous estimates based solely on OC age distributions. We find that the shape
of the ICMF for bound OCs differs from that of embedded clusters, which could
imply a low typical star formation efficiency of $\leq 20\%$ in OCs. Our
results also suggest a lower limit of $\sim 60 M{_\odot}$ for bound OCs in the
solar neighbourhood.",2024-12-26T12:54:51Z,http://arxiv.org/abs/2412.19204v1,"Duarte Almeida, André Moitinho, Sandro Moreira"
"GAIS: A Novel Approach to Instance Selection with Graph Attention
  Networks","Instance selection (IS) is a crucial technique in machine learning that aims
to reduce dataset size while maintaining model performance. This paper
introduces a novel method called Graph Attention-based Instance Selection
(GAIS), which leverages Graph Attention Networks (GATs) to identify the most
informative instances in a dataset. GAIS represents the data as a graph and
uses GATs to learn node representations, enabling it to capture complex
relationships between instances. The method processes data in chunks, applies
random masking and similarity thresholding during graph construction, and
selects instances based on confidence scores from the trained GAT model.
Experiments on 13 diverse datasets demonstrate that GAIS consistently
outperforms traditional IS methods in terms of effectiveness, achieving high
reduction rates (average 96\%) while maintaining or improving model
performance. Although GAIS exhibits slightly higher computational costs, its
superior performance in maintaining accuracy with significantly reduced
training data makes it a promising approach for graph-based data selection.",2024-12-26T12:51:14Z,http://arxiv.org/abs/2412.19201v1,"Zahiriddin Rustamov, Ayham Zaitouny, Rafat Damseh, Nazar Zaki"
"Personalized Dynamic Music Emotion Recognition with Dual-Scale
  Attention-Based Meta-Learning","Dynamic Music Emotion Recognition (DMER) aims to predict the emotion of
different moments in music, playing a crucial role in music information
retrieval. The existing DMER methods struggle to capture long-term dependencies
when dealing with sequence data, which limits their performance. Furthermore,
these methods often overlook the influence of individual differences on emotion
perception, even though everyone has their own personalized emotional
perception in the real world. Motivated by these issues, we explore more
effective sequence processing methods and introduce the Personalized DMER
(PDMER) problem, which requires models to predict emotions that align with
personalized perception. Specifically, we propose a Dual-Scale Attention-Based
Meta-Learning (DSAML) method. This method fuses features from a dual-scale
feature extractor and captures both short and long-term dependencies using a
dual-scale attention transformer, improving the performance in traditional
DMER. To achieve PDMER, we design a novel task construction strategy that
divides tasks by annotators. Samples in a task are annotated by the same
annotator, ensuring consistent perception. Leveraging this strategy alongside
meta-learning, DSAML can predict personalized perception of emotions with just
one personalized annotation sample. Our objective and subjective experiments
demonstrate that our method can achieve state-of-the-art performance in both
traditional DMER and PDMER.",2024-12-26T12:47:35Z,http://arxiv.org/abs/2412.19200v1,"Dengming Zhang, Weitao You, Ziheng Liu, Lingyun Sun, Pei Chen"
"Implications for the non-Gaussianity of primordial gravitational waves
  from pulsar timing arrays","The detection of a stochastic signal by recent pulsar timing array (PTA)
collaborations, including NANOGrav, PPTA, EPTA+InPTA, CPTA and MPTA, has opened
a new window to explore gravitational waves (GWs) at nanohertz frequencies.
Motivated by the possibility that such a signal could arise from primordial
gravitational waves (PGWs), we investigate the implications of tensor
non-Gaussianity for the PGW power spectrum. Utilizing PTA data sets, we provide
constraints on local-type tensor non-Gaussianity parameter ${F}_{\mathrm{NL}}$.
We find $|{F}_{\mathrm{NL}}|\lesssim 7.97$ for a log-normal PGW power spectrum.
Our analysis reveals that even moderate tensor non-Gaussianity can lead to
significant deviations from standard predictions, thereby offering a novel
means to test inflationary scenarios and probe the underlying dynamics of the
early Universe. Future multi-band GW observatories, such as LISA, Taiji, and
TianQin, will be instrumental in complementing these efforts and further
refining our understanding of tensor non-Gaussianity.",2024-12-26T12:31:58Z,http://arxiv.org/abs/2412.19196v1,"Zhi-Zhang Peng, You Wu, Lang Liu"
"Game-Theoretically Secure Distributed Protocols for Fair Allocation in
  Coalitional Games","We consider game-theoretically secure distributed protocols for coalition
games that approximate the Shapley value with small multiplicative error. Since
all known existing approximation algorithms for the Shapley value are
randomized, it is a challenge to design efficient distributed protocols among
mutually distrusted players when there is no central authority to generate
unbiased randomness. The game-theoretic notion of maximin security has been
proposed to offer guarantees to an honest player's reward even if all other
players are susceptible to an adversary.
  Permutation sampling is often used in approximation algorithms for the
Shapley value. A previous work in 1994 by Zlotkin et al. proposed a simple
constant-round distributed permutation generation protocol based on commitment
scheme, but it is vulnerable to rushing attacks. The protocol, however, can
detect such attacks.
  In this work, we model the limited resources of an adversary by a violation
budget that determines how many times it can perform such detectable attacks.
Therefore, by repeating the number of permutation samples, an honest player's
reward can be guaranteed to be close to its Shapley value. We explore both high
probability and expected maximin security. We obtain an upper bound on the
number of permutation samples for high probability maximin security, even with
an unknown violation budget. Furthermore, we establish a matching lower bound
for the weaker notion of expected maximin security in specific permutation
generation protocols. We have also performed experiments on both synthetic and
real data to empirically verify our results.",2024-12-26T12:13:21Z,http://arxiv.org/abs/2412.19192v1,"T-H. Hubert Chan, Qipeng Kuang, Quan Xue"
New Physics in the 3-3-1 models,"Two main ingredients of current particle physics
  such as local gauge symmetry and mass generation via the Higgs mechanism
being basic ground of the Standard Model are widely confirmed by
  experimental data. However, some problems such as neutrino masses, dark
matter, baryon asymmetry of Universe have clearly indicated that the Standard
Model cannot be the ultimate theory of nature. To surpass the mentioned
puzzles,
  many extensions of the Standard Model (called beyond Standard Model) have
been proposed. Among beyond Standard Models, the 3-3-1 models have some
intriguing features and they get wide attention. The pioneer models develop in
some directions. In this paper, %some new main versions of the 3-3-1 models and
their consequences are presented.",2024-12-26T11:55:43Z,http://arxiv.org/abs/2412.19188v1,H. N. Long
"Outlier-Bias Removal with Alpha Divergence: A Robust Non-Convex
  Estimator for Linear Regression","Convex and penalized robust methods often suffer from bias induced by large
outliers, limiting their effectiveness in adversarial or heavy-tailed settings.
In this study, we propose a novel approach that eliminates this bias (when
possible) by leveraging a non-convex $M$-estimator based on the alpha
divergence. We address the problem of estimating the parameters vector in high
dimensional linear regression, even when a subset of the data has been
deliberately corrupted by an adversary with full knowledge of the dataset and
its underlying distribution.
  Our primary contribution is to demonstrate that the objective function,
although non-convex, exhibits convexity within a carefully chosen basin of
attraction, enabling robust and unbiased estimation. Additionally, we establish
three key theoretical guarantees for the estimator: (a) a deviation bound that
is minimax optimal up to a logarithmic factor, (b) an improved unbiased bound
when the outliers are large and (c) asymptotic normality as the sample size
increases. Finally, we validate the theoretical findings through empirical
comparisons with state-of-the-art estimators on both synthetic and real-world
datasets, highlighting the proposed method's superior robustness, efficiency,
and ability to mitigate outlier-induced bias.",2024-12-26T11:42:46Z,http://arxiv.org/abs/2412.19183v1,"Ilyes Hammouda, Mohamed Ndaoud, and Abd-Krim Seghouane"
"Unraveling the magnetic and electronic complexity of intermetallic
  ErPd$_2$Si$_2$: Anisotropic thermal expansion, phase transitions, and twofold
  magnetotransport behavior","We present a comprehensive investigation into the physical properties of
intermetallic ErPd$_2$Si$_2$, a compound renowned for its intriguing magnetic
and electronic characteristics. We confirm the tetragonal crystal structure of
ErPd$_2$Si$_2$ within the $I4/mmm$ space group. Notably, we observed
anisotropic thermal expansion, with the lattice constant $a$ expanding and $c$
contracting between 15 K and 300 K. This behavior is attributed to lattice
vibrations and electronic contributions. Heat capacity measurements revealed
three distinct temperature regimes: $T_1 \sim 3.0$ K, $T_\textrm{N} \sim 4.20$
K, and $T_2 \sim 15.31$ K. These correspond to the disappearance of
spin-density waves, the onset of an incommensurate antiferromagnetic (AFM)
structure, and the crystal-field splitting and/or the presence of short-range
spin fluctuations, respectively. Remarkably, the AFM phase transition anomaly
was observed exclusively in low-field magnetization data (120 Oe) at
$T_\textrm{N}$. A high magnetic field ($B =$ 3 T) effectively suppressed this
anomaly, likely due to spin-flop and spin-flip transitions. Furthermore, the
extracted effective PM moments closely matched the expected theoretical value,
suggesting a dominant magnetic contribution from localized 4$f$ spins of Er.
Additionally, significant differences in resistance ($R$) values at low
temperatures under applied $B$ indicated a magnetoresistance (MR) effect with a
minimum value of -4.36\%. Notably, the measured MR effect exhibited anisotropic
behavior, where changes in the strength or direction of the applied $B$ induced
variations in the MR effect. A twofold symmetry of $R$ was discerned at 3 T and
9 T, originating from the orientation of spin moments relative to the applied
$B$. Intriguingly, above $T_\textrm{N}$, short-range spin fluctuations also
displayed a preferred orientation along the $c$-axis due to single-ion
anisotropy.",2024-12-26T11:39:24Z,http://arxiv.org/abs/2412.19181v1,"Kaitong Sun, Si Wu, Guanping Xu, Lingwei Li, Hongyu Chen, Qian Zhao, Muqing Su, Wolfgang Schmidt, Chongde Cao, Hai-Feng Li"
"Mask Approximation Net: Merging Feature Extraction and Distribution
  Learning for Remote Sensing Change Captioning","Remote sensing image change description, as a novel multimodal task in the
field of remote sensing processing, not only enables the detection of changes
in surface conditions but also provides detailed descriptions of these changes,
thereby enhancing human interpretability and interactivity. However, previous
methods mainly employed Convolutional Neural Network (CNN) architectures to
extract bitemporal image features. This approach often leads to an overemphasis
on designing specific network architectures and limits the captured feature
distributions to the current dataset, resulting in poor generalizability and
robustness when applied to other datasets or real-world scenarios. To address
these limitations, this paper proposes a novel approach for remote sensing
image change detection and description that integrates diffusion models, aiming
to shift the focus from conventional feature learning paradigms to data
distribution learning. The proposed method primarily includes a simple
multi-scale change detection module, whose output features are subsequently
refined using a diffusion model. Additionally, we introduce a frequency-guided
complex filter module to handle high-frequency noise during the diffusion
process, which helps to maintain model performance. Finally, we validate the
effectiveness of our proposed method on several remote sensing change detection
description datasets, demonstrating its superior performance. The code
available at MaskApproxNet.",2024-12-26T11:35:57Z,http://arxiv.org/abs/2412.19179v1,"Dongwei Sun, Xiangyong Cao"
Physical nature of quasi-stable structures existing in antimony melt,"Equilibrium antimony melt near the melting temperature is characterised by
structural features that are not present in simple single-component liquids.
The cause of these features may be long-lived structural formations that are
not yet fully understood. The present work provides the detailed
characterization of the structures formed in liquid antimony near the melting
temperature based on the results of quantum chemical calculations and the
available neutron and X-ray diffraction data. The quasi-stable structures in
antimony melt are detected with lifetimes exceeding the structural relaxation
time of this melt. These structures are characterised by a low degree of order
and spatial localisation. It is shown for the first time that the elementary
units of these quasi-stable structures are triplets of atoms with
characteristic lengths of $3.07$\,\AA~and $4.7$\,\AA~and characteristic angles
of $45$ and $90$ degrees. It was found that these triplets can form chains and
percolating clusters up to $\sim15$\,\AA~in length. The characteristic lengths
of these triplets are fully consistent with the correlation lengths associated
with short-range order in the antimony melt as determined by diffraction
experiments.",2024-12-26T11:30:38Z,http://arxiv.org/abs/2412.19177v1,"Artem A. Tsygankov, Bulat N. Galimzyanov, Anatolii V. Mokshin"
"Accelerating Stochastic Gravitational Wave Backgrounds Parameter
  Estimation in Pulsar Timing Arrays with Flow Matching","Pulsar timing arrays (PTAs) are essential tools for detecting the stochastic
gravitational wave background (SGWB), but their analysis faces significant
computational challenges. Traditional methods like Markov-chain Monte Carlo
(MCMC) struggle with high-dimensional parameter spaces where noise parameters
often dominate, while existing deep learning approaches fail to model the
Hellings-Downs (HD) correlation or are validated only on synthetic datasets. We
propose a flow-matching-based continuous normalizing flow (CNF) for efficient
and accurate PTA parameter estimation. By focusing on the 10 most contributive
pulsars from the NANOGrav 15-year dataset, our method achieves posteriors
consistent with MCMC, with a Jensen-Shannon divergence below \(10^{-2}\) nat,
while reducing sampling time from 50 hours to 4 minutes. Powered by a versatile
embedding network and a reweighting loss function, our approach prioritizes the
SGWB parameters and scales effectively for future datasets. It enables precise
reconstruction of SGWB and opens new avenues for exploring vast observational
data and uncovering potential new physics, offering a transformative tool for
advancing gravitational wave astronomy.",2024-12-26T11:02:11Z,http://arxiv.org/abs/2412.19169v1,"Bo Liang, Chang Liu, Tianyu Zhao, Minghui Du, Manjia Liang, Ruijun Shi, Hong Guo, Yuxiang Xu, Li-e Qiang, Peng Xu, Wei-Liang Qian, Ziren Luo"
Master Stability Functions in Complex Networks,"Synchronization is an emergent phenomenon in coupled dynamical networks. The
Master Stability Function (MSF) is a highly elegant and powerful tool for
characterizing the stability of synchronization states. However, a significant
challenge lies in determining the MSF for complex dynamical networks driven by
nonlinear interaction mechanisms. These mechanisms introduce additional
complexity through the intricate connectivity of interacting elements within
the network and the intrinsic dynamics, which are governed by nonlinear
processes with diverse parameters and higher dimensionality of systems. Over
the past 25 years, extensive research has focused on determining the MSF for
pairwise coupled identical systems with diffusive coupling. Our literature
survey highlights two significant advancements in recent years: the
consideration of multilayer networks instead of single-layer networks and the
extension of MSF analysis to incorporate higher-order interactions alongside
pairwise interactions.
  In this review article, we revisit the analysis of the MSF for diffusively
pairwise coupled dynamical systems and extend this framework to more general
coupling schemes. Furthermore, we systematically derive the MSF for multilayer
dynamical networks and single-layer coupled systems by incorporating
higher-order interactions alongside pairwise interactions. The primary focus of
our review is on the analytical derivation and numerical computation of the MSF
for complex dynamical networks. Finally, we demonstrate the application of the
MSF in data science, emphasizing its relevance and potential in this rapidly
evolving field.",2024-12-26T10:47:00Z,http://arxiv.org/abs/2412.19163v1,"Suman Acharyya, Priodyuti Pradhan, Chandrakala Meena"
"Dual Channel Multi-Attention in ViT for Biometric Authentication using
  Forehead Subcutaneous Vein Pattern and Periocular Pattern","Traditional biometric systems, like face and fingerprint recognition, have
encountered significant setbacks due to wearing face masks and hygiene
concerns. To meet the challenges of the partially covered face due to face
masks and hygiene concerns of fingerprint recognition, this paper proposes a
novel dual-channel multi-attention Vision Transformer (ViT) framework for
biometric authentication using forehead subcutaneous vein patterns and
periocular patterns, offering a promising alternative to traditional methods,
capable of performing well even with face masks and without any physical touch.
The proposed framework leverages a dual-channel ViT architecture, designed to
handle two distinct biometric traits. It can capture long-range dependencies of
independent features from the vein and periocular patterns. A custom classifier
is then designed to integrate the independently extracted features, producing a
final class prediction. The performance of the proposed algorithm was
rigorously evaluated using the Forehead Subcutaneous Vein Pattern and
Periocular Biometric Pattern (FSVP-PBP) database. The results demonstrated the
superiority of the algorithm over state-of-the-art methods, achieving
remarkable classification accuracy of $99.3 \pm 0.02\%$ with the combined vein
and periocular patterns.",2024-12-26T10:40:15Z,http://arxiv.org/abs/2412.19160v1,"Arun K. Sharma, Shubhobrata Bhattacharya, Motahar Reza"
Advancements in Terahertz Antenna Design,"The promising way to provide sufficient transmission capacity is by accessing
transmission bands at higher carrier frequencies. This desire for higher
carrier frequency or more bandwidth led the researchers to take advantage of
the terahertz (THz) spectrum. The opportunity for large bandwidth in the THz
band leads to the possibility of easy, high data rate transmission. In spite of
the advantages, the THz band suffers from large free space path loss. In the
development of THz communication systems, the antenna is the most significant
component. The focus is especially on designing highly directive antennas
because they enhance the performance of the overall system by compensating for
the large path loss at THz and thus improving the signal-to-noise ratio. This
chapter presents different types of THz antennas, including planar,
reflectarray, horn antenna, and lens antenna. Emphasis has been made to present
the latest trend of designing THz antennas using carbon-based materials, such
as graphene and carbon nanotubes. The performance of these antennas has been
compared with that of traditional copper-based THz antennas by critically
analyzing their properties. A brief discussion on THz power sources is included
in this chapter for completeness. A comprehensive discussion on different
fabrication techniques has been provided to appraise the reader of the general
fabrication processes of THz components.",2024-12-26T10:21:58Z,http://arxiv.org/abs/2412.19156v1,"Sasmita Dash, Amalendu Patnaik"
"Evolutionary de-homogenization using a generative model for optimizing
  solid-porous infill structures considering the stress concentration issue","The design of porous infill structures presents significant challenges due to
their complex geometric configurations, such as the accurate representation of
geometric boundaries and the control of localized maximum stress. In current
mainstream design methods, such as topology optimization, the analysis is often
performed using pixel or voxel-based element approximations. These
approximations, constrained by the optimization framework, result in
substantial geometric discrepancies between the analysis model and the final
physical model. Such discrepancies can severely impact structural performance,
particularly for localized properties like stress response, where accurate
geometry is critical to mitigating stress concentration. To address these
challenges, we propose evolutionary de-homogenization, which is a design
framework based on the integration of de-homogenization and data-driven
multifidelity optimization. This framework facilitates the hybrid solid-porous
infill design by bridging the gap between low-fidelity analysis and
high-fidelity physical realizations, ensuring both geometric accuracy and
enhanced structural performance. The low-fidelity level utilizes commonly used
density control variables, while the high-fidelity level involves stress
analysis based on structures with precise geometric representations. By
employing a de-homogenization-based mapping method, a side-by-side
correspondence between low-fidelity and high-fidelity results is established.
The low-fidelity control variables are iteratively adjusted to optimize the
high-fidelity results by integrating deep generative model with multi-objective
evolutionary algorithm. Finally, numerical experiments demonstrate the
effectiveness of the proposed method.",2024-12-26T10:18:16Z,http://arxiv.org/abs/2412.19154v1,"Shuzhi Xu, Hiroki Kawabe, Kentaro Yaji"
"To Predict or Not To Predict? Proportionally Masked Autoencoders for
  Tabular Data Imputation","Masked autoencoders (MAEs) have recently demonstrated effectiveness in
tabular data imputation. However, due to the inherent heterogeneity of tabular
data, the uniform random masking strategy commonly used in MAEs can disrupt the
distribution of missingness, leading to suboptimal performance. To address
this, we propose a proportional masking strategy for MAEs. Specifically, we
first compute the statistics of missingness based on the observed proportions
in the dataset, and then generate masks that align with these statistics,
ensuring that the distribution of missingness is preserved after masking.
Furthermore, we argue that simple MLP-based token mixing offers competitive or
often superior performance compared to attention mechanisms while being more
computationally efficient, especially in the tabular domain with the inherent
heterogeneity. Experimental results validate the effectiveness of the proposed
proportional masking strategy across various missing data patterns in tabular
datasets. Code is available at: \url{https://github.com/normal-kim/PMAE}.",2024-12-26T10:12:08Z,http://arxiv.org/abs/2412.19152v1,"Jungkyu Kim, Kibok Lee, Taeyoung Park"
AskChart: Universal Chart Understanding through Textual Enhancement,"Chart understanding tasks such as ChartQA and Chart-to-Text involve
automatically extracting and interpreting key information from charts, enabling
users to query or convert visual data into structured formats. State-of-the-art
approaches primarily focus on visual cues from chart images, failing to
explicitly incorporate rich textual information (e.g., data labels and axis
labels) embedded within the charts. This textual information is vital for
intuitive human comprehension and interpretation of charts. Moreover, existing
models are often large and computationally intensive, limiting their practical
applicability. In this paper, we introduce AskChart, a universal model that
explicitly integrates both textual and visual cues from charts using a Mixture
of Experts (MoE) architecture. AskChart facilitates the learning of enhanced
visual-textual representations of charts for effectively handling multiple
chart understanding tasks, while maintaining a smaller model size. To capture
the synergy between visual and textual modalities, we curate a large-scale
dataset named ChartBank with about 7.5M data samples, which helps align textual
and visual information and facilitates the extraction of visual entities and
text. To effectively train AskChart, we design a three-stage training strategy
to align visual and textual modalities for learning robust visual-textual
representations and optimizing the learning of the MoE layer. Extensive
experiments across five datasets demonstrate the significant performance gains
of AskChart in four chart understanding tasks. Remarkably, AskChart with 4.6B
parameters outperforms state-of-the-art models with 13B parameters by 68.3% in
Open-ended ChartQA and 49.2% in Chart-to-Text tasks, while achieving comparable
performance in ChartQA and Chart-to-Table tasks.",2024-12-26T09:59:43Z,http://arxiv.org/abs/2412.19146v1,"Xudong Yang, Yifan Wu, Yizhang Zhu, Nan Tang, Yuyu Luo"
"Impact of color and mixing proportion of synthetic point clouds on
  semantic segmentation","Semantic segmentation of point clouds is essential for understanding the
built environment, and a large amount of high-quality data is required for
training deep learning models. Despite synthetic point clouds (SPC) having the
potential to compensate for the shortage of real data, how to exploit the
benefits of SPC is still open. Therefore, this study systematically
investigates how color and mixing proportion of SPC impact semantic
segmentation for the first time. First, a new method to mimic the scanning
process and generate SPC based on BIM is proposed, to create a synthetic
dataset with consistent colors of BIM (UniSPC) and a synthetic dataset with
real colors (RealSPC) respectively. Subsequently, by integrating with the S3DIS
dataset, further experiments on PointNet, PointNet++, and DGCNN are conducted.
Meanwhile, benchmark experiments and new evaluation metrics are introduced to
better evaluate the performance of different models. Experiments show that
synthetic color significantly impacts model performance, the performance for
common components of the models trained with pure RealSPC is comparable to
models with real data, and RealSPC contributes average improvements of 14.1% on
overall accuracy and 7.3% on mIoU than UniSPC. Furthermore, the proportion of
SPC also has a significant impact on the performance. In mixing training
experiments, adding more than 70% SPC achieves an average of 3.9% on overall
accuracy and 3.4% on mIoU better than benchmark on three models. It is also
revealed that for large flat elements such as floors, ceilings, and walls, the
SPC can even replace real point clouds without compromising model performance.",2024-12-26T09:58:04Z,http://arxiv.org/abs/2412.19145v1,"Shaojie Zhou, Jia-Rui Lin, Peng Pan, Yuandong Pan, Ioannis Brilakis"
"SILC-EFSA: Self-aware In-context Learning Correction for Entity-level
  Financial Sentiment Analysis","In recent years, fine-grained sentiment analysis in finance has gained
significant attention, but the scarcity of entity-level datasets remains a key
challenge. To address this, we have constructed the largest English and Chinese
financial entity-level sentiment analysis datasets to date. Building on this
foundation, we propose a novel two-stage sentiment analysis approach called
Self-aware In-context Learning Correction (SILC). The first stage involves
fine-tuning a base large language model to generate pseudo-labeled data
specific to our task. In the second stage, we train a correction model using a
GNN-based example retriever, which is informed by the pseudo-labeled data. This
two-stage strategy has allowed us to achieve state-of-the-art performance on
the newly constructed datasets, advancing the field of financial sentiment
analysis. In a case study, we demonstrate the enhanced practical utility of our
data and methods in monitoring the cryptocurrency market. Our datasets and code
are available at https://github.com/NLP-Bin/SILC-EFSA.",2024-12-26T09:53:01Z,http://arxiv.org/abs/2412.19140v1,"Senbin Zhu, Chenyuan He, Hongde Liu, Pengcheng Dong, Hanjie Zhao, Yuchen Yan, Yuxiang Jia, Hongying Zan, Min Peng"
SUTrack: Towards Simple and Unified Single Object Tracking,"In this paper, we propose a simple yet unified single object tracking (SOT)
framework, dubbed SUTrack. It consolidates five SOT tasks (RGB-based,
RGB-Depth, RGB-Thermal, RGB-Event, RGB-Language Tracking) into a unified model
trained in a single session. Due to the distinct nature of the data, current
methods typically design individual architectures and train separate models for
each task. This fragmentation results in redundant training processes,
repetitive technological innovations, and limited cross-modal knowledge
sharing. In contrast, SUTrack demonstrates that a single model with a unified
input representation can effectively handle various common SOT tasks,
eliminating the need for task-specific designs and separate training sessions.
Additionally, we introduce a task-recognition auxiliary training strategy and a
soft token type embedding to further enhance SUTrack's performance with minimal
overhead. Experiments show that SUTrack outperforms previous task-specific
counterparts across 11 datasets spanning five SOT tasks. Moreover, we provide a
range of models catering edge devices as well as high-performance GPUs,
striking a good trade-off between speed and accuracy. We hope SUTrack could
serve as a strong foundation for further compelling research into unified
tracking models. Code and models are available at
github.com/chenxin-dlut/SUTrack.",2024-12-26T09:41:36Z,http://arxiv.org/abs/2412.19138v1,"Xin Chen, Ben Kang, Wanting Geng, Jiawen Zhu, Yi Liu, Dong Wang, Huchuan Lu"
Semantic Residual for Multimodal Unified Discrete Representation,"Recent research in the domain of multimodal unified representations
predominantly employs codebook as representation forms, utilizing Vector
Quantization(VQ) for quantization, yet there has been insufficient exploration
of other quantization representation forms. Our work explores more precise
quantization methods and introduces a new framework, Semantic Residual
Cross-modal Information Disentanglement (SRCID), inspired by the numerical
residual concept inherent to Residual Vector Quantization (RVQ). SRCID employs
semantic residual-based information disentanglement for multimodal data to
better handle the inherent discrepancies between different modalities. Our
method enhances the capabilities of unified multimodal representations and
demonstrates exceptional performance in cross-modal generalization and
cross-modal zero-shot retrieval. Its average results significantly surpass
existing state-of-the-art models, as well as previous attempts with RVQ and
Finite Scalar Quantization (FSQ) based on these modals.",2024-12-26T09:08:52Z,http://arxiv.org/abs/2412.19128v1,"Hai Huang, Shulei Wang, Yan Xia"
"Advanced Knowledge Transfer: Refined Feature Distillation for Zero-Shot
  Quantization in Edge Computing","We introduce AKT (Advanced Knowledge Transfer), a novel method to enhance the
training ability of low-bit quantized (Q) models in the field of zero-shot
quantization (ZSQ). Existing research in ZSQ has focused on generating
high-quality data from full-precision (FP) models. However, these approaches
struggle with reduced learning ability in low-bit quantization due to its
limited information capacity. To overcome this limitation, we propose effective
training strategy compared to data generation. Particularly, we analyzed that
refining feature maps in the feature distillation process is an effective way
to transfer knowledge to the Q model. Based on this analysis, AKT efficiently
transfer core information from the FP model to the Q model. AKT is the first
approach to utilize both spatial and channel attention information in feature
distillation in ZSQ. Our method addresses the fundamental gradient exploding
problem in low-bit Q models. Experiments on CIFAR-10 and CIFAR-100 datasets
demonstrated the effectiveness of the AKT. Our method led to significant
performance enhancement in existing generative models. Notably, AKT achieved
significant accuracy improvements in low-bit Q models, achieving
state-of-the-art in the 3,5bit scenarios on CIFAR-10. The code is available at
https://github.com/Inpyo-Hong/AKT-Advanced-knowledge-Transfer.",2024-12-26T08:52:27Z,http://arxiv.org/abs/2412.19125v1,"Inpyo Hong, Youngwan Jo, Hyojeong Lee, Sunghyun Ahn, Sanghyun Park"
"Evaluating Self-Supervised Learning in Medical Imaging: A Benchmark for
  Robustness, Generalizability, and Multi-Domain Impact","Self-supervised learning (SSL) has emerged as a promising paradigm in medical
imaging, addressing the chronic challenge of limited labeled data in healthcare
settings. While SSL has shown impressive results, existing studies in the
medical domain are often limited in scope, focusing on specific datasets or
modalities, or evaluating only isolated aspects of model performance. This
fragmented evaluation approach poses a significant challenge, as models
deployed in critical medical settings must not only achieve high accuracy but
also demonstrate robust performance and generalizability across diverse
datasets and varying conditions. To address this gap, we present a
comprehensive evaluation of SSL methods within the medical domain, with a
particular focus on robustness and generalizability. Using the MedMNIST dataset
collection as a standardized benchmark, we evaluate 8 major SSL methods across
11 different medical datasets. Our study provides an in-depth analysis of model
performance in both in-domain scenarios and the detection of
out-of-distribution (OOD) samples, while exploring the effect of various
initialization strategies, model architectures, and multi-domain pre-training.
We further assess the generalizability of SSL methods through cross-dataset
evaluations and the in-domain performance with varying label proportions (1%,
10%, and 100%) to simulate real-world scenarios with limited supervision. We
hope this comprehensive benchmark helps practitioners and researchers make more
informed decisions when applying SSL methods to medical applications.",2024-12-26T08:51:56Z,http://arxiv.org/abs/2412.19124v1,"Valay Bundele, Oğuz Ata Çal, Bora Kargi, Karahan Sarıtaş, Kıvanç Tezören, Zohreh Ghaderi, Hendrik Lensch"
Discrete vs. Continuous Trade-offs for Generative Models,"This work explores the theoretical and practical foundations of denoising
diffusion probabilistic models (DDPMs) and score-based generative models, which
leverage stochastic processes and Brownian motion to model complex data
distributions. These models employ forward and reverse diffusion processes
defined through stochastic differential equations (SDEs) to iteratively add and
remove noise, enabling high-quality data generation. By analyzing the
performance bounds of these models, we demonstrate how score estimation errors
propagate through the reverse process and bound the total variation distance
using discrete Girsanov transformations, Pinsker's inequality, and the data
processing inequality (DPI) for an information theoretic lens.",2024-12-26T08:14:27Z,http://arxiv.org/abs/2412.19114v1,"Jathin Korrapati, Tanish Baranwal, Rahul Shah"
"SketchFill: Sketch-Guided Code Generation for Imputing Derived Missing
  Values","Missing value is a critical issue in data science, significantly impacting
the reliability of analyses and predictions. Missing value imputation (MVI) is
a longstanding problem because it highly relies on domain knowledge. Large
language models (LLMs) have emerged as a promising tool for data cleaning,
including MVI for tabular data, offering advanced capabilities for
understanding and generating content. However, despite their promise, existing
LLM techniques such as in-context learning and Chain-of-Thought (CoT) often
fall short in guiding LLMs to perform complex reasoning for MVI, particularly
when imputing derived missing values, which require mathematical formulas and
data relationships across rows and columns. This gap underscores the need for
further advancements in LLM methodologies to enhance their reasoning
capabilities for more reliable imputation outcomes. To fill this gap, we
propose SketchFill, a novel sketch-based method to guide LLMs in generating
accurate formulas to impute missing numerical values. Our experimental results
demonstrate that SketchFill significantly outperforms state-of-the-art methods,
achieving 56.2% higher accuracy than CoT-based methods and 78.8% higher
accuracy than MetaGPT. This sets a new standard for automated data cleaning and
advances the field of MVI for numerical values.",2024-12-26T08:13:34Z,http://arxiv.org/abs/2412.19113v1,"Yunfan Zhang, Changlun Li, Yuyu Luo, Nan Tang"
"Graph Mixture of Experts and Memory-augmented Routers for Multivariate
  Time Series Anomaly Detection","Multivariate time series (MTS) anomaly detection is a critical task that
involves identifying abnormal patterns or events in data that consist of
multiple interrelated time series. In order to better model the complex
interdependence between entities and the various inherent characteristics of
each entity, the GNN based methods are widely adopted by existing methods. In
each layer of GNN, node features aggregate information from their neighboring
nodes to update their information. In doing so, from shallow layer to deep
layer in GNN, original individual node features continue to be weakened and
more structural information,i.e., from short-distance neighborhood to
long-distance neighborhood, continues to be enhanced. However, research to date
has largely ignored the understanding of how hierarchical graph information is
represented and their characteristics that can benefit anomaly detection.
Existing methods simply leverage the output from the last layer of GNN for
anomaly estimation while neglecting the essential information contained in the
intermediate GNN layers. To address such limitations, in this paper, we propose
a Graph Mixture of Experts (Graph-MoE) network for multivariate time series
anomaly detection, which incorporates the mixture of experts (MoE) module to
adaptively represent and integrate hierarchical multi-layer graph information
into entity representations. It is worth noting that our Graph-MoE can be
integrated into any GNN-based MTS anomaly detection method in a plug-and-play
manner. In addition, the memory-augmented routers are proposed in this paper to
capture the correlation temporal information in terms of the global historical
features of MTS to adaptively weigh the obtained entity representations to
achieve successful anomaly estimation. Extensive experiments on five
challenging datasets prove the superiority of our approach and each proposed
module.",2024-12-26T07:49:51Z,http://arxiv.org/abs/2412.19108v1,"Xiaoyu Huang, Weidong Chen, Bo Hu, Zhendong Mao"
"The role of potential energy landscape research in the development of
  new electrolyte solutions","The development of new electrolyte solutions with improved characteristics is
a key challenge for creating high-performance batteries, fuel cells,
supercapacitors, and other electrochemical devices. The study of the potential
energy landscape (PEL) plays an important role in this process, providing
information about the interactions between solution components at the molecular
level. In this work, we review the practice of applying PEL research methods
based on classical and quantum-chemical algorithms to analyze the structure,
dynamics, and thermodynamic properties of electrolyte solutions. Intermolecular
and ion-molecular interactions at the microscopic level, which determine the
macroscopic properties of the electrolyte solution, are considered in detail.
The importance of identifying stable configurations of ions and their solvates
is emphasized. PEL analysis allows for the systematic determination of the most
probable structures and complexes formed in solution, which is important for
understanding ion transport mechanisms. The study of the PEL allows for the
determination of the energy barriers that must be overcome for ion migration,
which is related to the conductivity of the electrolyte. The application of PEL
research methods in combination with experimental data opens up new
possibilities for the rational design of electrolyte solutions with desired
physicochemical properties.",2024-12-26T07:45:29Z,http://arxiv.org/abs/2412.19103v1,Vitaly V. Chaban
"""I've Heard of You!"": Generate Spoken Named Entity Recognition Data for
  Unseen Entities","Spoken named entity recognition (NER) aims to identify named entities from
speech, playing an important role in speech processing. New named entities
appear every day, however, annotating their Spoken NER data is costly. In this
paper, we demonstrate that existing Spoken NER systems perform poorly when
dealing with previously unseen named entities. To tackle this challenge, we
propose a method for generating Spoken NER data based on a named entity
dictionary (NED) to reduce costs. Specifically, we first use a large language
model (LLM) to generate sentences from the sampled named entities and then use
a text-to-speech (TTS) system to generate the speech. Furthermore, we introduce
a noise metric to filter out noisy data. To evaluate our approach, we release a
novel Spoken NER benchmark along with a corresponding NED containing 8,853
entities. Experiment results show that our method achieves state-of-the-art
(SOTA) performance in the in-domain, zero-shot domain adaptation, and fully
zero-shot settings. Our data will be available at
https://github.com/DeepLearnXMU/HeardU.",2024-12-26T07:43:18Z,http://arxiv.org/abs/2412.19102v1,"Jiawei Yu, Xiang Geng, Yuang Li, Mengxin Ren, Wei Tang, Jiahuan Li, Zhibin Lan, Min Zhang, Hao Yang, Shujian Huang, Jinsong Su"
"Reconstruction Target Matters in Masked Image Modeling for Cross-Domain
  Few-Shot Learning","Cross-Domain Few-Shot Learning (CDFSL) requires the model to transfer
knowledge from the data-abundant source domain to data-scarce target domains
for fast adaptation, where the large domain gap makes CDFSL a challenging
problem. Masked Autoencoder (MAE) excels in effectively using unlabeled data
and learning image's global structures, enhancing model generalization and
robustness. However, in the CDFSL task with significant domain shifts, we find
MAE even shows lower performance than the baseline supervised models. In this
paper, we first delve into this phenomenon for an interpretation. We find that
MAE tends to focus on low-level domain information during reconstructing pixels
while changing the reconstruction target to token features could mitigate this
problem. However, not all features are beneficial, as we then find
reconstructing high-level features can hardly improve the model's
transferability, indicating a trade-off between filtering domain information
and preserving the image's global structure. In all, the reconstruction target
matters for the CDFSL task. Based on the above findings and interpretations, we
further propose Domain-Agnostic Masked Image Modeling (DAMIM) for the CDFSL
task. DAMIM includes an Aggregated Feature Reconstruction module to
automatically aggregate features for reconstruction, with balanced learning of
domain-agnostic information and images' global structure, and a Lightweight
Decoder module to further benefit the encoder's generalizability. Experiments
on four CDFSL datasets demonstrate that our method achieves state-of-the-art
performance.",2024-12-26T07:43:01Z,http://arxiv.org/abs/2412.19101v1,"Ran Ma, Yixiong Zou, Yuhua Li, Ruixuan Li"
"TrajGEOS: Trajectory Graph Enhanced Orientation-based Sequential Network
  for Mobility Prediction","Human mobility studies how people move to access their needed resources and
plays a significant role in urban planning and location-based services. As a
paramount task of human mobility modeling, next location prediction is
challenging because of the diversity of users' historical trajectories that
gives rise to complex mobility patterns and various contexts. Deep sequential
models have been widely used to predict the next location by leveraging the
inherent sequentiality of trajectory data. However, they do not fully leverage
the relationship between locations and fail to capture users' multi-level
preferences. This work constructs a trajectory graph from users' historical
traces and proposes a \textbf{Traj}ectory \textbf{G}raph \textbf{E}nhanced
\textbf{O}rientation-based \textbf{S}equential network (TrajGEOS) for
next-location prediction tasks. TrajGEOS introduces hierarchical graph
convolution to capture location and user embeddings. Such embeddings consider
not only the contextual feature of locations but also the relation between
them, and serve as additional features in downstream modules. In addition, we
design an orientation-based module to learn users' mid-term preferences from
sequential modeling modules and their recent trajectories. Extensive
experiments on three real-world LBSN datasets corroborate the value of graph
and orientation-based modules and demonstrate that TrajGEOS outperforms the
state-of-the-art methods on the next location prediction task.",2024-12-26T07:18:38Z,http://arxiv.org/abs/2412.19092v1,"Zhaoping Hu, Zongyuan Huang, Jinming Yang, Tao Yang, Yaohui Jin, Yanyan Xu"
From Coin to Data: The Impact of Object Detection on Digital Numismatics,"In this work we investigate the application of advanced object detection
techniques to digital numismatics, focussing on the analysis of historical
coins. Leveraging models such as Contrastive Language-Image Pre-training
(CLIP), we develop a flexible framework for identifying and classifying
specific coin features using both image and textual descriptions. By examining
two distinct datasets, modern Russian coins featuring intricate ""Saint George
and the Dragon"" designs and degraded 1st millennium AD Southeast Asian coins
bearing Hindu-Buddhist symbols, we evaluate the efficacy of different detection
algorithms in search and classification tasks. Our results demonstrate the
superior performance of larger CLIP models in detecting complex imagery, while
traditional methods excel in identifying simple geometric patterns.
Additionally, we propose a statistical calibration mechanism to enhance the
reliability of similarity scores in low-quality datasets. This work highlights
the transformative potential of integrating state-of-the-art object detection
into digital numismatics, enabling more scalable, precise, and efficient
analysis of historical artifacts. These advancements pave the way for new
methodologies in cultural heritage research, artefact provenance studies, and
the detection of forgeries.",2024-12-26T07:05:53Z,http://arxiv.org/abs/2412.19091v1,"Rafael Cabral, Maria De Iorio, Andrew Harris"
"Integrating Artificial Open Generative Artificial Intelligence into
  Software Supply Chain Security","While new technologies emerge, human errors always looming. Software supply
chain is increasingly complex and intertwined, the security of a service has
become paramount to ensuring the integrity of products, safeguarding data
privacy, and maintaining operational continuity. In this work, we conducted
experiments on the promising open Large Language Models (LLMs) into two main
software security challenges: source code language errors and deprecated code,
with a focus on their potential to replace conventional static and dynamic
security scanners that rely on predefined rules and patterns. Our findings
suggest that while LLMs present some unexpected results, they also encounter
significant limitations, particularly in memory complexity and the management
of new and unfamiliar data patterns. Despite these challenges, the proactive
application of LLMs, coupled with extensive security databases and continuous
updates, holds the potential to fortify Software Supply Chain (SSC) processes
against emerging threats.",2024-12-26T07:03:55Z,http://arxiv.org/abs/2412.19088v1,"Vasileios Alevizos, George A Papakostas, Akebu Simasiku, Dimitra Malliarou, Antonis Messinis, Sabrina Edralin, Clark Xu, Zongliang Yue"
MoPD: Mixture-of-Prompts Distillation for Vision-Language Models,"Soft prompt learning methods are effective for adapting vision-language
models (VLMs) to downstream tasks. Nevertheless, empirical evidence reveals a
tendency of existing methods that they overfit seen classes and exhibit
degraded performance on unseen classes. This limitation is due to the inherent
bias in the training data towards the seen classes. To address this issue, we
propose a novel soft prompt learning method, named Mixture-of-Prompts
Distillation (MoPD), which can effectively transfer useful knowledge from hard
prompts manually hand-crafted (a.k.a. teacher prompts) to the learnable soft
prompt (a.k.a. student prompt), thereby enhancing the generalization ability of
soft prompts on unseen classes. Moreover, the proposed MoPD method utilizes a
gating network that learns to select hard prompts used for prompt distillation.
Extensive experiments demonstrate that the proposed MoPD method outperforms
state-of-the-art baselines especially on on unseen classes.",2024-12-26T06:57:04Z,http://arxiv.org/abs/2412.19087v1,"Yang Chen, Shuai Fu, Yu Zhang"
Investigating the Temporal Dynamics of Cyber Threat Intelligence,"Indicators of Compromise (IoCs) play a crucial role in the rapid detection
and mitigation of cyber threats. However, the existing body of literature lacks
in-depth analytical studies on the temporal aspects of IoC publication,
especially when considering up-to-date datasets related to Common
Vulnerabilities and Exposures (CVEs). This paper addresses this gap by
conducting an analysis of the timeliness and comprehensiveness of Cyber Threat
Intelligence (CTI) pertaining to several recent CVEs. The insights derived from
this study aim to enhance cybersecurity defense strategies, particularly when
dealing with dynamic cyber threats that continually adapt their Tactics,
Techniques, and Procedures (TTPs). Utilizing IoCs sourced from multiple
providers, we scrutinize the IoC publication rate. Our analysis delves into how
various factors, including the inherent nature of a threat, its evolutionary
trajectory, and its observability over time, influence the publication rate of
IoCs. Our preliminary findings emphasize the critical need for cyber defenders
to maintain a constant state of vigilance in updating their IoCs for any given
vulnerability. This vigilance is warranted because the publication rate of IoCs
may exhibit fluctuations over time. We observe a recurring pattern akin to an
epidemic model, with an initial phase following the public disclosure of a
vulnerability characterized by sparse IoC publications, followed by a sudden
surge, and subsequently, a protracted period with a slower rate of IoC
publication.",2024-12-26T06:54:27Z,http://arxiv.org/abs/2412.19086v1,"Angel Kodituwakku, Clark Xu, Daniel Rogers, David K. Ahn, Errin W. Fulp"
"Assessing Pre-trained Models for Transfer Learning through Distribution
  of Spectral Components","Pre-trained model assessment for transfer learning aims to identify the
optimal candidate for the downstream tasks from a model hub, without the need
of time-consuming fine-tuning. Existing advanced works mainly focus on
analyzing the intrinsic characteristics of the entire features extracted by
each pre-trained model or how well such features fit the target labels. This
paper proposes a novel perspective for pre-trained model assessment through the
Distribution of Spectral Components (DISCO). Through singular value
decomposition of features extracted from pre-trained models, we investigate
different spectral components and observe that they possess distinct
transferability, contributing diversely to the fine-tuning performance.
Inspired by this, we propose an assessment method based on the distribution of
spectral components which measures the proportions of their corresponding
singular values. Pre-trained models with features concentrating on more
transferable components are regarded as better choices for transfer learning.
We further leverage the labels of downstream data to better estimate the
transferability of each spectral component and derive the final assessment
criterion. Our proposed method is flexible and can be applied to both
classification and regression tasks. We conducted comprehensive experiments
across three benchmarks and two tasks including image classification and object
detection, demonstrating that our method achieves state-of-the-art performance
in choosing proper pre-trained models from the model hub for transfer learning.",2024-12-26T06:54:22Z,http://arxiv.org/abs/2412.19085v1,"Tengxue Zhang, Yang Shu, Xinyang Chen, Yifei Long, Chenjuan Guo, Bin Yang"
A Microservice Graph Generator with Production Characteristics,"A production microservice application may provide multiple services, queries
of a service may have different call graphs, and a microservice may be shared
across call graphs. It is challenging to improve the resource efficiency of
such complex applications without proper benchmarks, while production traces
are too large to be used in experiments. To this end, we propose a Service
Dependency Graph Generator (DGG) that comprises a Data Handler and a Graph
Generator, for generating the service dependency graphs of benchmarks that
incorporate production-level characteristics from traces. The data handler
first constructs fine-grained call graphs with dynamic interface and repeated
calling features from the trace and merges them into dependency graphs, and
then clusters them into different categories based on the topological and
invocation types. Taking the organized data and the selected category, the
graph generator simulates the process of real microservices invoking downstream
microservices using a random graph model, generates multiple call graphs, and
merges the call graphs to form the small-scale service dependency graph with
production-level characteristics. Case studies show that DGG's generated graphs
are similar to real traces in terms of topologies. Moreover, the resource
scaling based on DGG's fine-grained call graph constructing increases the
resource efficiency by up to 44.8% while ensuring the required QoS.",2024-12-26T06:51:35Z,http://arxiv.org/abs/2412.19083v1,"Fanrong Du, Jiuchen Shi, Quan Chen, Li Li, Minyi Guo"
"Mask Factory: Towards High-quality Synthetic Data Generation for
  Dichotomous Image Segmentation","Dichotomous Image Segmentation (DIS) tasks require highly precise
annotations, and traditional dataset creation methods are labor intensive,
costly, and require extensive domain expertise. Although using synthetic data
for DIS is a promising solution to these challenges, current generative models
and techniques struggle with the issues of scene deviations, noise-induced
errors, and limited training sample variability. To address these issues, we
introduce a novel approach, \textbf{\ourmodel{}}, which provides a scalable
solution for generating diverse and precise datasets, markedly reducing
preparation time and costs. We first introduce a general mask editing method
that combines rigid and non-rigid editing techniques to generate high-quality
synthetic masks. Specially, rigid editing leverages geometric priors from
diffusion models to achieve precise viewpoint transformations under zero-shot
conditions, while non-rigid editing employs adversarial training and
self-attention mechanisms for complex, topologically consistent modifications.
Then, we generate pairs of high-resolution image and accurate segmentation mask
using a multi-conditional control generation method. Finally, our experiments
on the widely-used DIS5K dataset benchmark demonstrate superior performance in
quality and efficiency compared to existing methods. The code is available at
\url{https://qian-hao-tian.github.io/MaskFactory/}.",2024-12-26T06:37:25Z,http://arxiv.org/abs/2412.19080v1,"Haotian Qian, YD Chen, Shengtao Lou, Fahad Shahbaz Khan, Xiaogang Jin, Deng-Ping Fan"
"Graph-Enhanced Dual-Stream Feature Fusion with Pre-Trained Model for
  Acoustic Traffic Monitoring","Microphone array techniques are widely used in sound source localization and
smart city acoustic-based traffic monitoring, but these applications face
significant challenges due to the scarcity of labeled real-world traffic audio
data and the complexity and diversity of application scenarios. The DCASE
Challenge's Task 10 focuses on using multi-channel audio signals to count
vehicles (cars or commercial vehicles) and identify their directions
(left-to-right or vice versa). In this paper, we propose a graph-enhanced
dual-stream feature fusion network (GEDF-Net) for acoustic traffic monitoring,
which simultaneously considers vehicle type and direction to improve detection.
We propose a graph-enhanced dual-stream feature fusion strategy which consists
of a vehicle type feature extraction (VTFE) branch, a vehicle direction feature
extraction (VDFE) branch, and a frame-level feature fusion module to combine
the type and direction feature for enhanced performance. A pre-trained model
(PANNs) is used in the VTFE branch to mitigate data scarcity and enhance the
type features, followed by a graph attention mechanism to exploit temporal
relationships and highlight important audio events within these features. The
frame-level fusion of direction and type features enables fine-grained feature
representation, resulting in better detection performance. Experiments
demonstrate the effectiveness of our proposed method. GEDF-Net is our
submission that achieved 1st place in the DCASE 2024 Challenge Task 10.",2024-12-26T06:28:42Z,http://arxiv.org/abs/2412.19078v1,"Shitong Fan, Feiyang Xiao, Wenbo Wang, Shuhan Qi, Qiaoxi Zhu, Wenwu Wang, Jian Guan"
"Robust Speech and Natural Language Processing Models for Depression
  Screening","Depression is a global health concern with a critical need for increased
patient screening. Speech technology offers advantages for remote screening but
must perform robustly across patients. We have described two deep learning
models developed for this purpose. One model is based on acoustics; the other
is based on natural language processing. Both models employ transfer learning.
Data from a depression-labeled corpus in which 11,000 unique users interacted
with a human-machine application using conversational speech is used. Results
on binary depression classification have shown that both models perform at or
above AUC=0.80 on unseen data with no speaker overlap. Performance is further
analyzed as a function of test subset characteristics, finding that the models
are generally robust over speaker and session variables. We conclude that
models based on these approaches offer promise for generalized automated
depression screening.",2024-12-26T06:05:52Z,http://arxiv.org/abs/2412.19072v1,"Y. Lu, A. Harati, T. Rutowski, R. Oliveira, P. Chlebek, E. Shriberg"
Cross-Demographic Portability of Deep NLP-Based Depression Models,"Deep learning models are rapidly gaining interest for real-world applications
in behavioral health. An important gap in current literature is how well such
models generalize over different populations. We study Natural Language
Processing (NLP) based models to explore portability over two different corpora
highly mismatched in age. The first and larger corpus contains younger
speakers. It is used to train an NLP model to predict depression. When testing
on unseen speakers from the same age distribution, this model performs at
AUC=0.82. We then test this model on the second corpus, which comprises seniors
from a retirement community. Despite the large demographic differences in the
two corpora, we saw only modest degradation in performance for the
senior-corpus data, achieving AUC=0.76. Interestingly, in the senior
population, we find AUC=0.81 for the subset of patients whose health state is
consistent over time. Implications for demographic portability of speech-based
applications are discussed.",2024-12-26T05:54:24Z,http://arxiv.org/abs/2412.19070v1,"Tomek Rutowski, Elizabeth Shriberg, Amir Harati, Yang Lu, Ricardo Oliveira, Piotr Chlebek"
Effective and secure federated online learning to rank,"Online Learning to Rank (OLTR) optimises ranking models using implicit user
feedback, such as clicks. Unlike traditional Learning to Rank (LTR) methods
that rely on a static set of training data with relevance judgements to learn a
ranking model, OLTR methods update the model continually as new data arrives.
Thus, it addresses several drawbacks such as the high cost of human
annotations, potential misalignment between user preferences and human
judgments, and the rapid changes in user query intents. However, OLTR methods
typically require the collection of searchable data, user queries, and clicks,
which poses privacy concerns for users.
  Federated Online Learning to Rank (FOLTR) integrates OLTR within a Federated
Learning (FL) framework to enhance privacy by not sharing raw data. While
promising, FOLTR methods currently lag behind traditional centralised OLTR due
to challenges in ranking effectiveness, robustness with respect to data
distribution across clients, susceptibility to attacks, and the ability to
unlearn client interactions and data. This thesis presents a comprehensive
study on Federated Online Learning to Rank, addressing its effectiveness,
robustness, security, and unlearning capabilities, thereby expanding the
landscape of FOLTR.",2024-12-26T05:53:10Z,http://arxiv.org/abs/2412.19069v1,Shuyi Wang
"Attacking Voice Anonymization Systems with Augmented Feature and Speaker
  Identity Difference","This study focuses on the First VoicePrivacy Attacker Challenge within the
ICASSP 2025 Signal Processing Grand Challenge, which aims to develop speaker
verification systems capable of determining whether two anonymized speech
signals are from the same speaker. However, differences between feature
distributions of original and anonymized speech complicate this task. To
address this challenge, we propose an attacker system that combines Data
Augmentation enhanced feature representation and Speaker Identity Difference
enhanced classifier to improve verification performance, termed DA-SID.
Specifically, data augmentation strategies (i.e., data fusion and SpecAugment)
are utilized to mitigate feature distribution gaps, while probabilistic linear
discriminant analysis (PLDA) is employed to further enhance speaker identity
difference. Our system significantly outperforms the baseline, demonstrating
exceptional effectiveness and robustness against various voice anonymization
systems, ultimately securing a top-5 ranking in the challenge.",2024-12-26T05:52:44Z,http://arxiv.org/abs/2412.19068v1,"Yanzhe Zhang, Zhonghao Bi, Feiyang Xiao, Xuefeng Yang, Qiaoxi Zhu, Jian Guan"
Learning Monocular Depth from Events via Egomotion Compensation,"Event cameras are neuromorphically inspired sensors that sparsely and
asynchronously report brightness changes. Their unique characteristics of high
temporal resolution, high dynamic range, and low power consumption make them
well-suited for addressing challenges in monocular depth estimation (e.g.,
high-speed or low-lighting conditions). However, current existing methods
primarily treat event streams as black-box learning systems without
incorporating prior physical principles, thus becoming over-parameterized and
failing to fully exploit the rich temporal information inherent in event camera
data. To address this limitation, we incorporate physical motion principles to
propose an interpretable monocular depth estimation framework, where the
likelihood of various depth hypotheses is explicitly determined by the effect
of motion compensation. To achieve this, we propose a Focus Cost Discrimination
(FCD) module that measures the clarity of edges as an essential indicator of
focus level and integrates spatial surroundings to facilitate cost estimation.
Furthermore, we analyze the noise patterns within our framework and improve it
with the newly introduced Inter-Hypotheses Cost Aggregation (IHCA) module,
where the cost volume is refined through cost trend prediction and multi-scale
cost consistency constraints. Extensive experiments on real-world and synthetic
datasets demonstrate that our proposed framework outperforms cutting-edge
methods by up to 10\% in terms of the absolute relative error metric, revealing
superior performance in predicting accuracy.",2024-12-26T05:41:18Z,http://arxiv.org/abs/2412.19067v1,"Haitao Meng, Chonghao Zhong, Sheng Tang, Lian JunJia, Wenwei Lin, Zhenshan Bing, Yi Chang, Gang Chen, Alois Knoll"
Coarse-grained binning in Drell-Yan transverse momentum spectra,"We report a study of the determination of the intrinsic transverse momentum
of partons, the intrinsic $k_T$, from the dilepton transverse momentum $p_T$ in
Drell-Yan (DY) production at hadron colliders. The result shows that a good
sensitivity to the intrinsic $k_T$ distribution is achieved by measuring
relative ratios between the cross sections of suitably defined low-$p_T$ and
high-$p_T$ regions. The study is performed through both a pseudo-data test and
an extraction from measurements of the DY process by the CMS collaboration.
Since the methodology does not rely on any dedicated partition of bins, this
$p_T$-ratio observable requires less special treatment in very low $p_T$
regions, and propagates lower systematic uncertainties induced from unfolding
or momentum migration, in contrast with previous proposals of using a
fine-binning measurement of the differential cross section.",2024-12-26T05:13:39Z,http://arxiv.org/abs/2412.19060v1,"Wenxiao Zhan, Siqi Yang, Minghui Liu, Francesco Hautmann, Liang Han"
Faster Semi-streaming Matchings via Alternating Trees,"We design a deterministic algorithm for the $(1+\epsilon)$-approximate
maximum matching problem. Our primary result demonstrates that this problem can
be solved in $O(\epsilon^{-6})$ semi-streaming passes, improving upon the
$O(\epsilon^{-19})$ pass-complexity algorithm by [Fischer, Mitrovi\'c, and
Uitto, STOC'22]. This contributes substantially toward resolving Open
question~2 from [Assadi, SOSA'24]. Leveraging the framework introduced in
[FMU'22], our algorithm achieves an analogous round complexity speed-up for
computing a $(1+\epsilon)$-approximate maximum matching in both the Massively
Parallel Computation (MPC) and CONGEST models.
  The data structures maintained by our algorithm are formulated using blossom
notation and represented through alternating trees. This approach enables a
simplified correctness analysis by treating specific components as if operating
on bipartite graphs, effectively circumventing certain technical intricacies
present in prior work.",2024-12-26T04:59:27Z,http://arxiv.org/abs/2412.19057v1,"Slobodan Mitrović, Anish Mukherjee, Piotr Sankowski, Wen-Horng Sheu"
"Performance Characterization and Optimizations of Traditional ML
  Applications","Even in the era of Deep Learning based methods, traditional machine learning
methods with large data sets continue to attract significant attention.
However, we find an apparent lack of a detailed performance characterization of
these methods in the context of large training datasets. In this work, we study
the system's behavior of a number of traditional ML methods as implemented in
popular free software libraries/modules to identify critical performance
bottlenecks experienced by these applications. The performance characterization
study reveals several interesting insights on the performance of these
applications. Then we evaluate the performance benefits of applying some
well-known optimizations at the levels of caches and the main memory. More
specifically, we test the usefulness of optimizations such as (i) software
prefetching to improve cache performance and (ii) data layout and computation
reordering optimizations to improve locality in DRAM accesses. These
optimizations are implemented as modifications to the well-known scikit-learn
library, and hence can be easily leveraged by application programmers. We
evaluate the impact of the proposed optimizations using a combination of
simulation and execution on a real system. The software prefetching
optimization results in performance benefits varying from 5.2%-27.1% on
different ML applications while the data layout and computation reordering
approaches yield 6.16%-28.0% performance improvement.",2024-12-26T04:13:52Z,http://arxiv.org/abs/2412.19051v1,"Harsh Kumar, R. Govindarajan"
Jasper and Stella: distillation of SOTA embedding models,"A crucial component of many deep learning applications (such as FAQ and RAG)
is dense retrieval, in which embedding models are used to convert raw text to
numerical vectors and then get the most similar text by MIPS (Maximum Inner
Product Search). Some text embedding benchmarks (e.g. MTEB, BEIR, and
AIR-Bench) have been established to evaluate embedding models accurately.
Thanks to these benchmarks, we can use SOTA models; however, the deployment and
application of these models in industry were hampered by their large vector
dimensions and numerous parameters. To alleviate this problem, 1) we present a
distillation technique that can enable a smaller student model to achieve good
performance. 2) Inspired by MRL we present a training approach of reducing the
vector dimensions based on its own vectors or its teacher vectors. 3) We do
simple yet effective alignment training between images and text to make our
model a multimodal encoder. We trained Stella and Jasper models using the
technologies above and achieved high scores on the MTEB leaderboard. We release
the model and data at Hugging Face Hub
(https://huggingface.co/infgrad/jasper_en_vision_language_v1) and the training
logs are at https://api.wandb.ai/links/dunnzhang0/z8jqoqpb.",2024-12-26T04:05:28Z,http://arxiv.org/abs/2412.19048v1,"Dun Zhang, FulongWang"
Revealing the Self: Brainwave-Based Human Trait Identification,"People exhibit unique emotional responses. In the same scenario, the
emotional reactions of two individuals can be either similar or vastly
different. For instance, consider one person's reaction to an invitation to
smoke versus another person's response to a query about their sleep quality.
The identification of these individual traits through the observation of common
physical parameters opens the door to a wide range of applications, including
psychological analysis, criminology, disease prediction, addiction control, and
more. While there has been previous research in the fields of psychometrics,
inertial sensors, computer vision, and audio analysis, this paper introduces a
novel technique for identifying human traits in real time using brainwave data.
To achieve this, we begin with an extensive study of brainwave data collected
from 80 participants using a portable EEG headset. We also conduct a
statistical analysis of the collected data utilizing box plots. Our analysis
uncovers several new insights, leading us to a groundbreaking unified approach
for identifying diverse human traits by leveraging machine learning techniques
on EEG data. Our analysis demonstrates that this proposed solution achieves
high accuracy. Moreover, we explore two deep-learning models to compare the
performance of our solution. Consequently, we have developed an integrated,
real-time trait identification solution using EEG data, based on the insights
from our analysis. To validate our approach, we conducted a rigorous user
evaluation with an additional 20 participants. The outcomes of this evaluation
illustrate both high accuracy and favorable user ratings, emphasizing the
robust potential of our proposed method to serve as a versatile solution for
human trait identification.",2024-12-26T03:27:34Z,http://arxiv.org/abs/2412.19041v1,"Md Mirajul Islam, Md Nahiyan Uddin, Maoyejatun Hasana, Debojit Pandit, Nafis Mahmud Rahman, Sriram Chellappan, Sami Azam, A. B. M. Alim Al Islam"
"Unifying Tree-Reweighted Belief Propagation and Mean Field for Tracking
  Extended Targets","This paper proposes a unified tree-reweighted belief propagation (BP) and
mean field (MF) approach for scalable detection and tracking of extended
targets within the framework of factor graph. The factor graph is partitioned
into a BP region and an MF region so that the messages in each region are
updated according to the corresponding region rules. The BP region exploits the
tree-reweighted BP, which offers improved convergence than the standard BP for
graphs with massive cycles, to resolve data association. The MF region
approximates the posterior densities of the measurement rate, kinematic state
and extent. For linear Gaussian target models and gamma Gaussian inverse
Wishart distributed state density, the unified approach provides a closed-form
recursion for the state density. Hence, the proposed algorithm is more
efficient than particle-based BP algorithms for extended target tracking. This
method also avoids measurement clustering and gating since it solves the data
association problem in a probabilistic fashion. We compare the proposed
approach with algorithms such as the Poisson multi-Bernoulli mixture filter and
the BP-based Poisson multi-Bernoulli filter. Simulation results demonstrate
that the proposed algorithm achieves enhanced tracking performance.",2024-12-26T03:12:53Z,http://arxiv.org/abs/2412.19036v1,"Weizhen Ma, Zhongliang Jing, Peng Dong, Henry Leung"
"Reflection on Purpose Changes Students' Academic Interests: A Scalable
  Intervention in an Online Course Catalog","College students routinely use online course catalogs to explore a variety of
academic offerings. Course catalogs may therefore be an effective place to
encourage reflection on academic choices and interests. To test this, we
embedded a psychological intervention in an online course catalog to encourage
students to reflect on their purpose during course exploration. Results of a
randomized field experiment with over 4,000 students at a large U.S. university
show that a purpose intervention increased students' cognitive engagement in
describing their interests, but reduced search activities. Students became more
interested in courses related to creative arts and social change, but less in
computer and data science. The findings demonstrate the malleability of
students' interests during course exploration and suggest practical strategies
to support purpose reflection and guide students toward deliberate exploration
of their interests in higher education.",2024-12-26T03:12:50Z,http://arxiv.org/abs/2412.19035v1,"Youjie Chen, Pranathi Iyer, Rene F. Kizilcec"
Repository Structure-Aware Training Makes SLMs Better Issue Resolver,"Language models have been applied to various software development tasks, but
the performance varies according to the scale of the models. Large Language
Models (LLMs) outperform Small Language Models (SLMs) in complex tasks like
repository-level issue resolving, but raise concerns about privacy and cost. In
contrast, SLMs are more accessible but under-perform in complex tasks. In this
paper, we introduce ReSAT (Repository Structure-Aware Training), construct
training data based on a large number of issues and corresponding pull requests
from open-source communities to enhance the model's understanding of repository
structure and issue resolving ability. We construct two types of training data:
(1) localization training data, a multi-level progressive localization data to
improve code understanding and localization capability; (2) code edit training
data, which improves context-based code editing capability. The evaluation
results on SWE-Bench-verified and RepoQA demonstrate that ReSAT effectively
enhances SLMs' issue-resolving and repository-level long-context understanding
capabilities.",2024-12-26T03:01:32Z,http://arxiv.org/abs/2412.19031v1,"Zexiong Ma, Shengnan An, Zeqi Lin, Yanzhen Zou, Bing Xie"
"Modality-Projection Universal Model for Comprehensive Full-Body Medical
  Imaging Segmentation","The integration of deep learning in medical imaging has shown great promise
for enhancing diagnostic, therapeutic, and research outcomes. However, applying
universal models across multiple modalities remains challenging due to the
inherent variability in data characteristics. This study aims to introduce and
evaluate a Modality Projection Universal Model (MPUM). MPUM employs a novel
modality-projection strategy, which allows the model to dynamically adjust its
parameters to optimize performance across different imaging modalities. The
MPUM demonstrated superior accuracy in identifying anatomical structures,
enabling precise quantification for improved clinical decision-making. It also
identifies metabolic associations within the brain-body axis, advancing
research on brain-body physiological correlations. Furthermore, MPUM's unique
controller-based convolution layer enables visualization of saliency maps
across all network layers, significantly enhancing the model's
interpretability.",2024-12-26T02:23:27Z,http://arxiv.org/abs/2412.19026v1,"Yixin Chen, Lin Gao, Yajuan Gao, Rui Wang, Jingge Lian, Xiangxi Meng, Yanhua Duan, Leiying Chai, Hongbin Han, Zhaoping Cheng, Zhaoheng Xie"
"Nonparametric Estimation of Matching Efficiency and Elasticity in a Spot
  Gig Work Platform: 2019-2023","This paper provides new evidence on spot gig work platforms for unemployed
workers searching for occupations with minimal educational or experience
requirements in Japan. Using proprietary data from a private online spot work
matching platform, Timee, it examines trends in key variables such as the
numbers of unemployed users, vacancies, hires, and labor market tightness. The
study compares these trends with part-time worker data from the public
employment platform, Hello Work. The private platform shows a significant
market expansion from December 2019 to December 2023. Applying a novel
nonparametric approach, the paper finds greater variability in efficiency and
higher elasticity, with elasticity with respect to the number of users
fluctuating from below 0.7 to above 1.5, and elasticity with respect to the
number of vacancies often exceeding 1.0, which is higher than Hello Work.
Lastly, the study highlights that Tokyo's labor market exhibits higher
efficiency compared to Osaka and Aichi, while elasticities are similar,
indicating less geographical heterogeneity of the spot work compared to Hello
Work.",2024-12-26T02:16:58Z,http://arxiv.org/abs/2412.19024v1,"Hayato Kanayama, Suguru Otani"
"Nuclear matter properties from chiral-scale effective theory including a
  dilatonic scalar meson","Chiral effective theory has become a powerful tool for studying the
low-energy properties of QCD. In this work, we apply an extended chiral
effective theory -- chiral-scale effective theory -- including a dilatonic
scalar meson to study nuclear matter and find that the properties around
saturation density can be well reproduced. Compared to the traditionally used
Walecka-type models in nuclear matter studies, our approach improves the
behavior of symmetry energy and the incompressibility coefficient in describing
empirical data without introducing additional freedoms. Moreover, the predicted
neutron star structures fall within the constraints of GW170817, PSR
J0740+6620, and PSR J0030+0451, while the maximum neutron star mass can reach
about $~3M_{\odot}$ with a pure hadronic phase. Additionally, we find that
symmetry patterns of the effective theory significantly impact neutron star
structures. %In chiral-scale effective theory, effective operators are well
organized by chiral-scale orders and freedoms induced by QCD symmetry patterns.
We believe that introducing this type of theory into nuclear matter studies can
lead to a deeper understanding of QCD, nuclear matter, and compact
astrophysical objects.",2024-12-26T02:13:05Z,http://arxiv.org/abs/2412.19023v1,"Lu-Qi Zhang, Yao Ma, Yong-Liang Ma"
Adaptivity can help exponentially for shadow tomography,"In recent years there has been significant interest in understanding the
statistical complexity of learning from quantum data under the constraint that
one can only make unentangled measurements. While a key challenge in
establishing tight lower bounds in this setting is to deal with the fact that
the measurements can be chosen in an adaptive fashion, a recurring theme has
been that adaptivity offers little advantage over more straightforward,
nonadaptive protocols.
  In this note, we offer a counterpoint to this. We show that for the basic
task of shadow tomography, protocols that use adaptively chosen two-copy
measurements can be exponentially more sample-efficient than any protocol that
uses nonadaptive two-copy measurements.",2024-12-26T02:13:04Z,http://arxiv.org/abs/2412.19022v1,"Sitan Chen, Weiyuan Gong, Zhihan Zhang"
"Brain Ageing Prediction using Isolation Forest Technique and Residual
  Neural Network (ResNet)","Brain aging is a complex and dynamic process, leading to functional and
structural changes in the brain. These changes could lead to the increased risk
of neurodegenerative diseases and cognitive decline. Accurate brain-age
estimation utilizing neuroimaging data has become necessary for detecting
initial signs of neurodegeneration. Here, we propose a novel deep learning
approach using the Residual Neural Network 101 Version 2 (ResNet101V2) model to
predict brain age from MRI scans. To train, validate and test our proposed
model, we used a large dataset of 2102 images which were selected randomly from
the International Consortium for Brain Mapping (ICBM). Next, we applied data
preprocessing techniques, including normalizing the images and using outlier
detection via Isolation Forest method. Then, we evaluated various pre-trained
approaches (namely: MobileNetV2, ResNet50V2, ResNet101V2, Xception). The
results demonstrated that the ResNet101V2 model has higher performance compared
with the other models, attaining MAEs of 0.9136 and 0.8242 years for before and
after using Isolation Forest process. Our method achieved a high accuracy in
brain age estimation in ICBM dataset and it provides a reliable brain age
prediction.",2024-12-26T01:49:21Z,http://arxiv.org/abs/2412.19017v1,"Saadat Behzadi, Danial Sharifrazi, Roohallah Alizadehsani, Mojtaba Lotfaliany, Mohammadreza Mohebbi"
