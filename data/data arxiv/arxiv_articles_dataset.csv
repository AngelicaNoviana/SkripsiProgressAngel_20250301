Title,Summary,Published,Link,Authors
Concentration of ergotropy in many-body systems,"Ergotropy -- the maximal amount of unitarily extractable work -- measures the
``charge level'' of quantum batteries. We prove that in large many-body
batteries ergotropy exhibits a concentration of measure phenomenon. Namely, the
ergotropy of such systems is almost constant for almost all states sampled from
the Hilbert--Schmidt measure. We establish this by first proving that
ergotropy, as a function of the state, is Lipschitz-continuous with respect to
the Bures distance, and then applying Levy's measure concentration lemma. In
parallel, we showcase the analogous properties of von Neumann entropy,
compiling and adapting known results about its continuity and concentration
properties. Furthermore, we consider the situation with the least amount of
prior information about the state. This corresponds to the quantum version of
the Jeffreys prior distribution -- the Bures measure. In this case, there exist
no analytical bounds guaranteeing exponential concentration of measure.
Nonetheless, we provide numerical evidence that ergotropy, as well as von
Neumann entropy, concentrate also in this case.",2024-12-27T18:58:43Z,http://arxiv.org/abs/2412.19801v1,"Karen V. Hovhannisyan, Rick P. A. Simon, Janet Anders"
InfAlign: Inference-aware language model alignment,"Language model alignment has become a critical step in training modern
generative language models. The goal of alignment is to finetune a reference
model such that the win rate of a sample from the aligned model over a sample
from the reference model is high, subject to a KL divergence constraint. Today,
we are increasingly using inference-time algorithms (e.g., Best-of-N,
controlled decoding, tree search) to decode from language models rather than
standard sampling. However, the alignment objective does not capture such
inference-time decoding procedures. We show that the existing alignment
framework is sub-optimal in view of such inference-time methods. We then modify
the alignment objective and propose a framework for inference-aware alignment
(IAPO). We prove that for any inference-time decoding algorithm, the optimal
solution that optimizes the inference-time win rate of the aligned policy
against the reference policy is the solution to the typical RLHF problem with a
transformation of the reward. This motivates us to provide the KL-regularized
calibrate-and-transform RL (CTRL) algorithm to solve this problem, which
involves a reward calibration step and a KL-regularized reward maximization
step with a transformation of the calibrated reward. We particularize our study
to two important inference-time strategies: best-of-N sampling and best-of-N
jailbreaking, where N responses are sampled from the model and the one with the
highest or lowest reward is selected. We propose specific transformations for
these strategies and demonstrate that our framework offers significant
improvements over existing state-of-the-art methods for language model
alignment. Empirically, we outperform baselines that are designed without
taking inference-time decoding into consideration by 8-12% and 4-9% on
inference-time win rates over the Anthropic helpfulness and harmlessness dialog
benchmark datasets.",2024-12-27T18:45:36Z,http://arxiv.org/abs/2412.19792v1,"Ananth Balashankar, Ziteng Sun, Jonathan Berant, Jacob Eisenstein, Michael Collins, Adrian Hutter, Jong Lee, Chirag Nagpal, Flavien Prost, Aradhana Sinha, and Ananda Theertha Suresh, Ahmad Beirami"
Composite operators in $\mathcal{N}=4$ Super Yang-Mills,"We consider four-point functions of protected, double- and single-trace
operators in the large central charge limit. We use superconformal symmetry to
disentangle the contribution of protected operators in the partial wave
decomposition. With this information, we fix the non protected part of such
correlators up to subleading order in the large central charge expansion. We
particularly focus on the triple-trace sector of the correlator and comment on
the connection to the holographic description of these correlators.",2024-12-27T18:40:23Z,http://arxiv.org/abs/2412.19788v1,"Agnese Bissi, Giulia Fardelli, Andrea Manenti"
"Enhancing Whisper's Accuracy and Speed for Indian Languages through
  Prompt-Tuning and Tokenization","Automatic speech recognition has recently seen a significant advancement with
large foundational models such as Whisper. However, these models often struggle
to perform well in low-resource languages, such as Indian languages. This paper
explores two novel approaches to enhance Whisper's multilingual speech
recognition performance in Indian languages. First, we propose prompt-tuning
with language family information, which enhances Whisper's accuracy in
linguistically similar languages. Second, we introduce a novel tokenizer that
reduces the number of generated tokens, thereby accelerating Whisper's
inference speed. Our extensive experiments demonstrate that the tokenizer
significantly reduces inference time, while prompt-tuning enhances accuracy
across various Whisper model sizes, including Small, Medium, and Large.
Together, these techniques achieve a balance between optimal WER and inference
speed.",2024-12-27T18:32:24Z,http://arxiv.org/abs/2412.19785v1,"Kumud Tripathi, Raj Gothi, Pankaj Wasnik"
"Analysis of Premature Death Rates in Texas Counties: The Impact of Air
  Quality, Socioeconomic Factors, and COPD Prevalence","Understanding factors contributing to premature mortality is critical for
public health planning. This study examines the relationships between premature
death rates and multiple risk factors across several Texas counties, utilizing
EPA air quality data, Census information, and county health records from recent
years. We analyze the impact of air quality (PM2.5 levels), socioeconomic
factors (median household income), and health conditions (COPD prevalence)
through statistical analysis and modeling techniques. Results reveal COPD
prevalence as a strong predictor of premature death rates, with higher
prevalence associated with a substantial increase in years of potential life
lost. While socioeconomic factors show a significant negative correlation, air
quality demonstrates more complex indirect relationships. These findings
emphasize the need for integrated public health interventions that prioritize
key health conditions while addressing underlying socioeconomic disparities.",2024-12-27T18:12:04Z,http://arxiv.org/abs/2412.19774v1,"Richard Rich, Ernesto Diaz"
"On Universally Free First-Order Extensions of Belnap-Dunn's Four-Valued
  Logic and Nelson's Paraconsistent Logic N4","The aim of this paper is to introduce the logics FFDE and FN4, which are
universally free versions of Belnap-Dunn's four-valued logic, also known as the
logic of first-degree entailment (FDE), and Nelson's paraconsistent logic QN4
(N-). Both FDE and QN4 are suitable to be interpreted as information-based
logics, that is, logics that are capable of representing the deductive behavior
of possibly inconsistent and incomplete information in a database. Like QN4 and
some non-free first-order extensions of FDE, FFDE and FN4 are endowed with
Kripke-style variable domain semantics, which allows representing the dynamic
aspect of information processing, that is, how a database receives new
information over time, including information about new individuals. We argue,
however, that FFDE and FN4 can better represent the development of inconsistent
and incomplete information states (i.e., configurations of a database) over
time than their non-free versions. First, because they allow for empty domains,
which corresponds to the idea that a database may acknowledge no individual at
all at an early stage of its development. Second, because they allow for empty
names, which get interpreted as information about new individuals is inserted
into the database. Also, both systems include an identity predicate that is
interpreted along the same lines of the other logical operators, viz., in terms
of independent positive and negative rules.",2024-12-27T17:57:18Z,http://arxiv.org/abs/2412.19767v1,"Henrique Antunes, Abilio Rodrigues"
"UAV-Enabled Secure ISAC Against Dual Eavesdropping Threats: Joint
  Beamforming and Trajectory Design","In this work, we study an unmanned aerial vehicle (UAV)-enabled secure
integrated sensing and communication (ISAC) system, where a UAV serves as an
aerial base station (BS) to simultaneously perform communication with a user
and detect a target on the ground, while a dual-functional eavesdropper
attempts to intercept the signals for both sensing and communication. Facing
the dual eavesdropping threats, we aim to enhance the average achievable
secrecy rate for the communication user by jointly designing the UAV trajectory
together with the transmit information and sensing beamforming, while
satisfying the requirements on sensing performance and sensing security, as
well as the UAV power and flight constraints. To address the non-convex nature
of the optimization problem, we employ the alternating optimization (AO)
strategy, jointly with the successive convex approximation (SCA) and
semidefinite relaxation (SDR) methods. Numerical results validate the proposed
approach, demonstrating its ability to achieve a high secrecy rate while
meeting the required sensing and security constraints.",2024-12-27T17:15:20Z,http://arxiv.org/abs/2412.19748v1,"Jianping Yao, Zeyu Yang, Zai Yang, Jie Xu, Tony Q. S. Quek"
"A General Framework of Brain Region Detection And Genetic Variants
  Selection in Imaging Genetics","Imaging genetics is a growing field that employs structural or functional
neuroimaging techniques to study individuals with genetic risk variants
potentially linked to specific illnesses. This area presents considerable
challenges to statisticians due to the heterogeneous information and different
data forms it involves. In addition, both imaging and genetic data are
typically high-dimensional, creating a ""big data squared"" problem. Moreover,
brain imaging data contains extensive spatial information. Simply vectorizing
tensor images and treating voxels as independent features can lead to
computational issues and disregard spatial structure. This paper presents a
novel statistical method for imaging genetics modeling while addressing all
these challenges. We explore a Canonical Correlation Analysis based linear
model for the joint modeling of brain imaging, genetic information, and
clinical phenotype, enabling the simultaneous detection of significant brain
regions and selection of important genetic variants associated with the
phenotype outcome. Scalable algorithms are developed to tackle the ""big data
squared"" issue. We apply the proposed method to explore the reaction speed, an
indicator of cognitive functions, and its associations with brain MRI and
genetic factors using the UK Biobank database. Our study reveals a notable
connection between the caudate nucleus region of brain and specific significant
SNPs, along with their respective regulated genes, and the reaction speed.",2024-12-27T16:54:11Z,http://arxiv.org/abs/2412.19735v1,"Siqiang Su, Zhenghao Li, Long Feng, Ting Li"
"Fully-relativistic evolution of vacuum tensor inhomogeneities during
  inflation","We present a complete method for the initialisation and extraction of
first-order inflationary tensor perturbations for fully relativistic
simulations which incorporate gravitational back-reaction. We outline a
correspondence between the Cosmological Perturbation Theory (CPT) framework and
the numerical relativity BSSN variables in the appropriate limit. We describe a
generation method for stochastic tensoral initial conditions, inspired by the
standard scalar initial condition used from inflation and implemented in
lattice cosmology. We discuss the implementation of this procedure in the
GRChombo/GRTeclyn code, and demonstrate the detailed quantitative
correspondence between the linearised and fully-nonlinear solutions in the
perturbative limit, through the evolution of the background and the tensor
power spectrum. We also validate the methodology by showing that energy and
momentum constraints are introduced and preserved to second-order or better. We
provide some preliminary indicative results probing tensoral non-Gaussianity
using the skewness and kurtosis. The computational pipeline presented here will
be used to study the emergence of a primordial tensor bispectra and
cross-spectra that incorporate the effect of nonlinear gravitational couplings
with the metric, which has potential applications for the analysis of
next-generation CMB surveys.",2024-12-27T16:42:59Z,http://arxiv.org/abs/2412.19731v1,"Ericka Florio, E. Paul S. Shellard"
"Learning to Forget: Bayesian Time Series Forecasting using Recurrent
  Sparse Spectrum Signature Gaussian Processes","The signature kernel is a kernel between time series of arbitrary length and
comes with strong theoretical guarantees from stochastic analysis. It has found
applications in machine learning such as covariance functions for Gaussian
processes. A strength of the underlying signature features is that they provide
a structured global description of a time series. However, this property can
quickly become a curse when local information is essential and forgetting is
required; so far this has only been addressed with ad-hoc methods such as
slicing the time series into subsegments. To overcome this, we propose a
principled, data-driven approach by introducing a novel forgetting mechanism
for signatures. This allows the model to dynamically adapt its context length
to focus on more recent information. To achieve this, we revisit the recently
introduced Random Fourier Signature Features, and develop Random Fourier
Decayed Signature Features (RFDSF) with Gaussian processes (GPs). This results
in a Bayesian time series forecasting algorithm with variational inference,
that offers a scalable probabilistic algorithm that processes and transforms a
time series into a joint predictive distribution over time steps in one pass
using recurrence. For example, processing a sequence of length $10^4$ steps in
$\approx 10^{-2}$ seconds and in $&lt; 1\text{GB}$ of GPU memory. We demonstrate
that it outperforms other GP-based alternatives and competes with
state-of-the-art probabilistic time series forecasting algorithms.",2024-12-27T16:31:09Z,http://arxiv.org/abs/2412.19727v1,"Csaba Tóth, Masaki Adachi, Michael A. Osborne, Harald Oberhauser"
"Text2Insight: Transform natural language text into insights seamlessly
  using multi-model architecture","The growing demand for dynamic, user-centric data analysis and visualization
is evident across domains like healthcare, finance, and research. Traditional
visualization tools often fail to meet individual user needs due to their
static and predefined nature. To address this gap, Text2Insight is introduced
as an innovative solution that delivers customized data analysis and
visualizations based on user-defined natural language requirements. Leveraging
a multi-model architecture, Text2Insight transforms user inputs into actionable
insights and dynamic visualizations.
  The methodology begins with analyzing the input dataset to extract structural
details such as columns and values. A pre-trained Llama3 model converts the
user's natural language query into an SQL query, which is further refined using
a Named Entity Recognition (NER) model for accuracy. A chart predictor
determines the most suitable visualization type, while the Llama3 model
generates insights based on the SQL query's results. The output is a
user-friendly and visually informative chart. To enhance analysis capabilities,
the system integrates a question-answering model and a predictive model using
the BERT framework. These models provide insights into historical data and
predict future trends.
  Performance evaluation of Text2Insight demonstrates its effectiveness,
achieving high accuracy (99%), precision (100%), recall (99%), and F1-score
(99%), with a BLEU score of 0.5. The question-answering model attained an
accuracy of 89% and the predictive model achieved 70% accuracy. These results
validate Text2Insight as a robust and viable solution for transforming natural
language text into dynamic, user-specific data analysis and visualizations.",2024-12-27T16:17:22Z,http://arxiv.org/abs/2412.19718v1,Pradeep Sain
"From Elements to Design: A Layered Approach for Automatic Graphic Design
  Composition","In this work, we investigate automatic design composition from multimodal
graphic elements. Although recent studies have developed various generative
models for graphic design, they usually face the following limitations: they
only focus on certain subtasks and are far from achieving the design
composition task; they do not consider the hierarchical information of graphic
designs during the generation process. To tackle these issues, we introduce the
layered design principle into Large Multimodal Models (LMMs) and propose a
novel approach, called LaDeCo, to accomplish this challenging task.
Specifically, LaDeCo first performs layer planning for a given element set,
dividing the input elements into different semantic layers according to their
contents. Based on the planning results, it subsequently predicts element
attributes that control the design composition in a layer-wise manner, and
includes the rendered image of previously generated layers into the context.
With this insightful design, LaDeCo decomposes the difficult task into smaller
manageable steps, making the generation process smoother and clearer. The
experimental results demonstrate the effectiveness of LaDeCo in design
composition. Furthermore, we show that LaDeCo enables some interesting
applications in graphic design, such as resolution adjustment, element filling,
design variation, etc. In addition, it even outperforms the specialized models
in some design subtasks without any task-specific training.",2024-12-27T16:13:08Z,http://arxiv.org/abs/2412.19712v1,"Jiawei Lin, Shizhao Sun, Danqing Huang, Ting Liu, Ji Li, Jiang Bian"
High precision spectroscopy of trilobite Rydberg molecules,"We perform three-photon photoassociation to obtain high resolution spectra of
$^{87}$Rb trilobite dimers for the principal quantum numbers $n = 22,24,25,26$,
and 27. The large binding energy of the molecules in combination with a
relative spectroscopic resolution of $10^{-4}$ provides a rigorous benchmark
for existing theoretical models. A recently developed Green's function
framework, which circumvents the convergence issues that afflicted previous
studies,, is employed to theoretically reproduce the vibrational spectrum of
the molecule with high accuracy. The relatively large molecular binding energy
are primarily determined by the low energy $S$-wave electron-atom scattering
length, thereby allowing us to extract the $^3S_1$ scattering phase shift with
unprecedented accuracy, at low energy regimes inaccessible to free electrons.",2024-12-27T16:04:17Z,http://arxiv.org/abs/2412.19710v1,"Markus Exner, Rohan Srikumar, Richard Blättner, Matthew T. Eiles, Peter Schmelcher, Herwig Ott"
"A Review on the Integration of Artificial Intelligence and Medical
  Imaging in IVF Ovarian Stimulation","Artificial intelligence (AI) has emerged as a powerful tool to enhance
decision-making and optimize treatment protocols in in vitro fertilization
(IVF). In particular, AI shows significant promise in supporting
decision-making during the ovarian stimulation phase of the IVF process. This
review evaluates studies focused on the applications of AI combined with
medical imaging in ovarian stimulation, examining methodologies, outcomes, and
current limitations. Our analysis of 13 studies on this topic reveals that,
reveal that while AI algorithms demonstrated notable potential in predicting
optimal hormonal dosages, trigger timing, and oocyte retrieval outcomes, the
medical imaging data utilized predominantly came from two-dimensional (2D)
ultrasound which mainly involved basic quantifications, such as follicle size
and number, with limited use of direct feature extraction or advanced image
analysis techniques. This points to an underexplored opportunity where advanced
image analysis approaches, such as deep learning, and more diverse imaging
modalities, like three-dimensional (3D) ultrasound, could unlock deeper
insights. Additionally, the lack of explainable AI (XAI) in most studies raises
concerns about the transparency and traceability of AI-driven decisions - key
factors for clinical adoption and trust. Furthermore, many studies relied on
single-center designs and small datasets, which limit the generalizability of
their findings. This review highlights the need for integrating advanced
imaging analysis techniques with explainable AI methodologies, as well as the
importance of leveraging multicenter collaborations and larger datasets.
Addressing these gaps has the potential to enhance ovarian stimulation
management, paving the way for efficient, personalized, and data-driven
treatment pathways that improve IVF outcomes.",2024-12-27T15:29:08Z,http://arxiv.org/abs/2412.19688v1,"Jana Zakall, Birgit Pohn, Antonia Graf, Daniel Kovatchki, Arezoo Borji, Ragib Shahriar Islam, Hossam Haick, Heinz Strohmer, Sepideh Hatamikia"
Deep ReLU networks -- injectivity capacity upper bounds,"We study deep ReLU feed forward neural networks (NN) and their injectivity
abilities. The main focus is on \emph{precisely} determining the so-called
injectivity capacity. For any given hidden layers architecture, it is defined
as the minimal ratio between number of network's outputs and inputs which
ensures unique recoverability of the input from a realizable output. A strong
recent progress in precisely studying single ReLU layer injectivity properties
is here moved to a deep network level. In particular, we develop a program that
connects deep $l$-layer net injectivity to an $l$-extension of the $\ell_0$
spherical perceptrons, thereby massively generalizing an isomorphism between
studying single layer injectivity and the capacity of the so-called
(1-extension) $\ell_0$ spherical perceptrons discussed in [82]. \emph{Random
duality theory} (RDT) based machinery is then created and utilized to
statistically handle properties of the extended $\ell_0$ spherical perceptrons
and implicitly of the deep ReLU NNs. A sizeable set of numerical evaluations is
conducted as well to put the entire RDT machinery in practical use. From these
we observe a rapidly decreasing tendency in needed layers' expansions, i.e., we
observe a rapid \emph{expansion saturation effect}. Only $4$ layers of depth
are sufficient to closely approach level of no needed expansion -- a result
that fairly closely resembles observations made in practical experiments and
that has so far remained completely untouchable by any of the existing
mathematical methodologies.",2024-12-27T14:57:40Z,http://arxiv.org/abs/2412.19677v1,Mihailo Stojnic
"Optimizing Local-Global Dependencies for Accurate 3D Human Pose
  Estimation","Transformer-based methods have recently achieved significant success in 3D
human pose estimation, owing to their strong ability to model long-range
dependencies. However, relying solely on the global attention mechanism is
insufficient for capturing the fine-grained local details, which are crucial
for accurate pose estimation. To address this, we propose SSR-STF, a
dual-stream model that effectively integrates local features with global
dependencies to enhance 3D human pose estimation. Specifically, we introduce
SSRFormer, a simple yet effective module that employs the skeleton selective
refine attention (SSRA) mechanism to capture fine-grained local dependencies in
human pose sequences, complementing the global dependencies modeled by the
Transformer. By adaptively fusing these two feature streams, SSR-STF can better
learn the underlying structure of human poses, overcoming the limitations of
traditional methods in local feature extraction. Extensive experiments on the
Human3.6M and MPI-INF-3DHP datasets demonstrate that SSR-STF achieves
state-of-the-art performance, with P1 errors of 37.4 mm and 13.2 mm
respectively, outperforming existing methods in both accuracy and
generalization. Furthermore, the motion representations learned by our model
prove effective in downstream tasks such as human mesh recovery. Codes are
available at https://github.com/poker-xu/SSR-STF.",2024-12-27T14:54:12Z,http://arxiv.org/abs/2412.19676v1,"Guangsheng Xu, Guoyi Zhang, Lejia Ye, Shuwei Gan, Xiaohu Zhang, Xia Yang"
"Innovation beyond intention: harnessing exaptation for technological
  breakthroughs","The frameworks that explore scientific and technological evolution suggest
that discoveries and inventions are intrinsic processes, while the wealth of
knowledge accumulated over time enables researchers to make further
advancements, echoing Newton's sentiment of ""standing on the shoulders of
giants."" Despite the exponential growth in new scientific and technical
knowledge, the consolidation-disruption (D) index suggests a concerning decline
in the disruptiveness of papers and patents. ""Exaptation"" a concept borrowed
from biological evolution, is now recognized as a pivotal yet often neglected
mechanism in technological evolution. Significant technologies often do not
emerge out of thin air but rather result from the application of existing
technologies in other domains. For instance, bird feathers initially served as
waterproofing and insulation before enabling flight, and microwave ovens
originated from radar magnetrons. Exaptation, acknowledged as the catalyst for
""innovation beyond intention"" signifies a cross-field evolutionary process that
is driven by functional shifts in pre-existing knowledge, technology, or
artifacts. In this study, we introduce the concept of exaptation value,
deliberately excluding serendipity. Our analysis reveals that, despite a
declining trend in the disruptiveness of innovation, there is an increasing
trend in the application of cross-domain knowledge within the innovation
process over time. We also explore the impact of technology exaptation on
innovation disruptiveness and discuss how leveraging technology adaptability
enhances innovation's disruptive potential.",2024-12-27T14:19:17Z,http://arxiv.org/abs/2412.19662v1,"Youwei He, Jeong-Dong Lee, Seungmin Lee"
The Value of Recall in Extensive-Form Games,"Imperfect-recall games, in which players may forget previously acquired
information, have found many practical applications, ranging from game
abstractions to team games and testing AI agents. In this paper, we quantify
the utility gain by endowing a player with perfect recall, which we call the
value of recall (VoR). While VoR can be unbounded in general, we parameterize
it in terms of various game properties, namely the structure of chance nodes
and the degree of absentmindedness (the number of successive times a player
enters the same information set). Further, we identify several pathologies that
arise with VoR, and show how to circumvent them. We also study the complexity
of computing VoR, and how to optimally apportion partial recall. Finally, we
connect VoR to other previously studied concepts in game theory, including the
price of anarchy. We use that connection in conjunction with the celebrated
smoothness framework to characterize VoR in a broad class of games.",2024-12-27T14:12:45Z,http://arxiv.org/abs/2412.19659v1,"Ratip Emin Berker, Emanuel Tewolde, Ioannis Anagnostides, Tuomas Sandholm, Vincent Conitzer"
Movable Antenna Aided Physical Layer Security with No Eavesdropper CSI,"A novel movable antenna (MA)-aided secure transmission framework is proposed
to enhance the secrecy transmission rate without relying on the eavesdropper's
channel state information. Within this framework, a joint beamforming and
jamming scheme is proposed, where the power of the confidential signal is
minimized by optimizing the positions of the MAs, and the residual power is
used to jam the eavesdropper. An efficient gradient-based method is employed to
solve this non-convex problem. Numerical results are provided to demonstrate
the superiority of the MA-based framework over systems using traditional
fixed-position antennas in secure transmission.",2024-12-27T14:05:08Z,http://arxiv.org/abs/2412.19656v1,"Zhenqiao Cheng, Chongjun Ouyang, Xingqi Zhang"
"FreStega: A Plug-and-Play Method for Boosting Imperceptibility and
  Capacity in Generative Linguistic Steganography for Real-World Scenarios","Linguistic steganography embeds secret information in seemingly innocent
texts, safeguarding privacy in surveillance environments. Generative linguistic
steganography leverages the probability distribution of language models (LMs)
and applies steganographic algorithms to generate stego tokens, gaining
attention with recent Large Language Model (LLM) advancements. To enhance
security, researchers develop distribution-preserving stego algorithms to
minimize the gap between stego sampling and LM sampling. However, the reliance
on language model distributions, coupled with deviations from real-world cover
texts, results in insufficient imperceptibility when facing steganalysis
detectors in real-world scenarios. Moreover, LLM distributions tend to be more
deterministic, resulting in reduced entropy and, consequently, lower embedding
capacity. In this paper, we propose FreStega, a plug-and-play method to
reconstruct the distribution of language models used for generative linguistic
steganography. FreStega dynamically adjusts token probabilities from the
language model at each step of stegotext auto-regressive generation, leveraging
both sequential and spatial dimensions. In sequential adjustment, the
temperature is dynamically adjusted based on instantaneous entropy, enhancing
the diversity of stego texts and boosting embedding capacity. In the spatial
dimension, the distribution is aligned with guidance from the target domain
corpus, closely mimicking real cover text in the target domain. By reforming
the distribution, FreStega enhances the imperceptibility of stego text in
practical scenarios and improves steganographic capacity by 15.41\%, all
without compromising the quality of the generated text. FreStega serves as a
plug-and-play remedy to enhance the imperceptibility and embedding capacity of
existing distribution-preserving steganography methods in real-world scenarios.",2024-12-27T13:56:51Z,http://arxiv.org/abs/2412.19652v1,Kaiyi Pang
"Branes and Representations of DAHA $C^\vee C_1$: affine braid group
  action on category","We study the representation theory of the spherical double affine Hecke
algebra (DAHA) of $C^\vee C_1$, using brane quantization. By showing a
one-to-one correspondence between Lagrangian $A$-branes with compact support
and finite-dimensional representations of the spherical DAHA, we provide
evidence of derived equivalence between the $A$-brane category of
$\mathrm{SL}(2,\mathbb{C})$-character variety of a four-punctured sphere and
the representation category of DAHA of $C^\vee C_1$. The $D_4$ root system
plays an essential role in understanding both the geometry and representation
theory. In particular, this $A$-model approach reveals the action of an affine
braid group of type $D_4$ on the category. As a by-product, our geometric
investigation offers detailed information about the low-energy dynamics of the
SU(2) $N_f=4$ Seiberg-Witten theory.",2024-12-27T13:54:31Z,http://arxiv.org/abs/2412.19647v1,"Junkang Huang, Satoshi Nawata, Yutai Zhang, Shutong Zhuang"
"VideoMaker: Zero-shot Customized Video Generation with the Inherent
  Force of Video Diffusion Models","Zero-shot customized video generation has gained significant attention due to
its substantial application potential. Existing methods rely on additional
models to extract and inject reference subject features, assuming that the
Video Diffusion Model (VDM) alone is insufficient for zero-shot customized
video generation. However, these methods often struggle to maintain consistent
subject appearance due to suboptimal feature extraction and injection
techniques. In this paper, we reveal that VDM inherently possesses the force to
extract and inject subject features. Departing from previous heuristic
approaches, we introduce a novel framework that leverages VDM's inherent force
to enable high-quality zero-shot customized video generation. Specifically, for
feature extraction, we directly input reference images into VDM and use its
intrinsic feature extraction process, which not only provides fine-grained
features but also significantly aligns with VDM's pre-trained knowledge. For
feature injection, we devise an innovative bidirectional interaction between
subject features and generated content through spatial self-attention within
VDM, ensuring that VDM has better subject fidelity while maintaining the
diversity of the generated video.Experiments on both customized human and
object video generation validate the effectiveness of our framework.",2024-12-27T13:49:25Z,http://arxiv.org/abs/2412.19645v1,"Tao Wu, Yong Zhang, Xiaodong Cun, Zhongang Qi, Junfu Pu, Huanzhang Dou, Guangcong Zheng, Ying Shan, Xi Li"
Signatures of prediction during natural listening in MEG data?,"The brain uses contextual information and prior knowledge to anticipate
upcoming content during language comprehension. Recent research has shown
predictive signals can be revealed in pre-onset ECoG activity during
naturalistic narrative listening, by building encoding models based on word
embeddings from Large Language Models (LLMs). Similarly, evidence for
long-range predictive encoding has been observed in fMRI data, where
incorporating embeddings for multiple upcoming words in a narrative improves
alignment with brain activity. This study examines whether similar predictive
information can be detected in MEG, a technique with higher temporal resolution
than fMRI but a lower signal-to-noise ratio than ECoG. Our findings indicate
that MEG captures pre-onset representations up to 1 second before word onset,
consistent with ECoG results. However, unlike fMRI findings, incorporating
future word embeddings did not enhance MEG encoding, even for one word into the
future, which suggests that the pre-onset encoding may not reflect predictive
processing. This work demonstrates that MEG combined with LLMs is a valuable
approach for studying language processing in naturalistic narratives and
highlights the need to study further what constitutes evidence for prediction
during natural listening.",2024-12-27T12:49:03Z,http://arxiv.org/abs/2412.19622v1,"Sahel Azizpour, Britta U. Westner, Jakub Szewczyk, Umut Güçlü, Linda Geerligs"
"Machine Generated Product Advertisements: Benchmarking LLMs Against
  Human Performance","This study compares the performance of AI-generated and human-written product
descriptions using a multifaceted evaluation model. We analyze descriptions for
100 products generated by four AI models (Gemma 2B, LLAMA, GPT2, and ChatGPT 4)
with and without sample descriptions, against human-written descriptions. Our
evaluation metrics include sentiment, readability, persuasiveness, Search
Engine Optimization(SEO), clarity, emotional appeal, and call-to-action
effectiveness. The results indicate that ChatGPT 4 performs the best. In
contrast, other models demonstrate significant shortcomings, producing
incoherent and illogical output that lacks logical structure and contextual
relevance. These models struggle to maintain focus on the product being
described, resulting in disjointed sentences that do not convey meaningful
information. This research provides insights into the current capabilities and
limitations of AI in the creation of content for e-Commerce.",2024-12-27T12:11:50Z,http://arxiv.org/abs/2412.19610v1,Sanjukta Ghosh
"Enhancing Fine-grained Image Classification through Attentive Batch
  Training","Fine-grained image classification, which is a challenging task in computer
vision, requires precise differentiation among visually similar object
categories. In this paper, we propose 1) a novel module called Residual
Relationship Attention (RRA) that leverages the relationships between images
within each training batch to effectively integrate visual feature vectors of
batch images and 2) a novel technique called Relationship Position Encoding
(RPE), which encodes the positions of relationships between original images in
a batch and effectively preserves the relationship information between images
within the batch. Additionally, we design a novel framework, namely
Relationship Batch Integration (RBI), which utilizes RRA in conjunction with
RPE, allowing the discernment of vital visual features that may remain elusive
when examining a singular image representative of a particular class. Through
extensive experiments, our proposed method demonstrates significant
improvements in the accuracy of different fine-grained classifiers, with an
average increase of $(+2.78\%)$ and $(+3.83\%)$ on the CUB200-2011 and Stanford
Dog datasets, respectively, while achieving a state-of-the-art results
$(95.79\%)$ on the Stanford Dog dataset. Despite not achieving the same level
of improvement as in fine-grained image classification, our method still
demonstrates its prowess in leveraging general image classification by
attaining a state-of-the-art result of $(93.71\%)$ on the Tiny-Imagenet
dataset. Furthermore, our method serves as a plug-in refinement module and can
be easily integrated into different networks.",2024-12-27T12:07:58Z,http://arxiv.org/abs/2412.19606v1,"Duy M. Le, Bao Q. Bui, Anh Tran, Cong Tran, Cuong Pham"
"Nonequilibrium Response Theory: From Precision Limits to Strong
  Perturbation","Fluctuation-response relations lie at the heart of statistical physics, yet
their formulation in nonequilibrium steady states remains challenging. This
Letter makes two key contributions to this field: First, we establish
fundamental limits on nonequilibrium steady-state responses by deriving upper
bounds on response precisions using steady-state Fisher information. Our
analysis reveals that the sensitivity of observables to perturbations is
governed by mean first passage times, steady-state currents, and activities,
though it cannot be enhanced indefinitely by increasing these quantities.
Notably, we demonstrate that the role of activity in response precision
parallels that of repeated measurements in metrology. Second, we develop novel
identities connecting the responses to arbitrarily strong perturbations with
those to small perturbations. These identities significantly extend previous
nonequilibrium response theories, which primarily focused on small
perturbations, to encompass arbitrarily strong perturbations.",2024-12-27T11:57:42Z,http://arxiv.org/abs/2412.19602v1,"Ruicheng Bao, Shiling Liang"
"ViDTA: Enhanced Drug-Target Affinity Prediction via Virtual Graph Nodes
  and Attention-based Feature Fusion","Drug-target interaction is fundamental in understanding how drugs affect
biological systems, and accurately predicting drug-target affinity (DTA) is
vital for drug discovery. Recently, deep learning methods have emerged as a
significant approach for estimating the binding strength between drugs and
target proteins. However, existing methods simply utilize the drug's local
information from molecular topology rather than global information.
Additionally, the features of drugs and proteins are usually fused with a
simple concatenation operation, limiting their effectiveness. To address these
challenges, we proposed ViDTA, an enhanced DTA prediction framework. We
introduce virtual nodes into the Graph Neural Network (GNN)-based drug feature
extraction network, which acts as a global memory to exchange messages more
efficiently. By incorporating virtual graph nodes, we seamlessly integrate
local and global features of drug molecular structures, expanding the GNN's
receptive field. Additionally, we propose an attention-based linear feature
fusion network for better capturing the interaction information between drugs
and proteins. Experimental results evaluated on various benchmarks including
Davis, Metz, and KIBA demonstrate that our proposed ViDTA outperforms the
state-of-the-art baselines.",2024-12-27T11:19:10Z,http://arxiv.org/abs/2412.19589v1,"Minghui Li, Zikang Guo, Yang Wu, Peijin Guo, Yao Shi, Shengshan Hu, Wei Wan, Shengqing Hu"
"An Actionable Hierarchical Scene Representation Enhancing Autonomous
  Inspection Missions in Unknown Environments","In this article, we present the Layered Semantic Graphs (LSG), a novel
actionable hierarchical scene graph, fully integrated with a multi-modal
mission planner, the FLIE: A First-Look based Inspection and Exploration
planner. The novelty of this work stems from aiming to address the task of
maintaining an intuitive and multi-resolution scene representation, while
simultaneously offering a tractable foundation for planning and scene
understanding during an ongoing inspection mission of apriori unknown
targets-of-interest in an unknown environment. The proposed LSG scheme is
composed of locally nested hierarchical graphs, at multiple layers of
abstraction, with the abstract concepts grounded on the functionality of the
integrated FLIE planner. Furthermore, LSG encapsulates real-time semantic
segmentation models that offer extraction and localization of desired semantic
elements within the hierarchical representation. This extends the capability of
the inspection planner, which can then leverage LSG to make an informed
decision to inspect a particular semantic of interest. We also emphasize the
hierarchical and semantic path-planning capabilities of LSG, which can extend
inspection missions by improving situational awareness for human operators in
an unknown environment. The validity of the proposed scheme is proven through
extensive evaluations of the proposed architecture in simulations, as well as
experimental field deployments on a Boston Dynamics Spot quadruped robot in
urban outdoor environment settings.",2024-12-27T10:57:17Z,http://arxiv.org/abs/2412.19582v1,"Vignesh Kottayam Viswanathan, Mario Alberto Valdes Saucedo, Sumeet Gajanan Satpute, Christoforos Kanellakis, George Nikolakopoulos"
"Graph-attention-based Casual Discovery with Trust Region-navigated
  Clipping Policy Optimization","In many domains of empirical sciences, discovering the causal structure
within variables remains an indispensable task. Recently, to tackle with
unoriented edges or latent assumptions violation suffered by conventional
methods, researchers formulated a reinforcement learning (RL) procedure for
causal discovery, and equipped REINFORCE algorithm to search for the
best-rewarded directed acyclic graph. The two keys to the overall performance
of the procedure are the robustness of RL methods and the efficient encoding of
variables. However, on the one hand, REINFORCE is prone to local convergence
and unstable performance during training. Neither trust region policy
optimization, being computationally-expensive, nor proximal policy optimization
(PPO), suffering from aggregate constraint deviation, is decent alternative for
combinatory optimization problems with considerable individual subactions. We
propose a trust region-navigated clipping policy optimization method for causal
discovery that guarantees both better search efficiency and steadiness in
policy optimization, in comparison with REINFORCE, PPO and our prioritized
sampling-guided REINFORCE implementation. On the other hand, to boost the
efficient encoding of variables, we propose a refined graph attention encoder
called SDGAT that can grasp more feature information without priori
neighbourhood information. With these improvements, the proposed method
outperforms former RL method in both synthetic and benchmark datasets in terms
of output results and optimization robustness.",2024-12-27T10:50:43Z,http://arxiv.org/abs/2412.19578v1,"Shixuan Liu, Yanghe Feng, Keyu Wu, Guangquan Cheng, Jincai Huang, Zhong Liu"
"Gauging or extending bulk and boundary conformal field theories:
  Application to bulk and domain wall problem in topological matter and their
  descriptions by (mock) modular covariant","We study gauging operations (or group extensions) in (smeared) boundary
conformal field theories (BCFTs) and bulk conformal field theories and their
applications to various phenomena in topologically ordered systems. We apply
the resultant theories to the correspondence between the renormalization group
(RG) flow of CFTs and the classification of topological quantum field theories
in the testable information of general classes of partition functions. One can
obtain the bulk topological properties of $2+1$ dimensional topological ordered
phase corresponding to the massive RG flow of $1+1$ dimensional systems, or
smeared BCFT. We present an obstruction of mass condensation for smeared BCFT
analogous to the Lieb-Shultz-Mattis theorem for noninvertible symmetry. Related
to the bulk topological degeneracies in $2+1$ dimensions and quantum phases in
$1+1$ dimensions we construct a new series of BCFT. We also investigate the
implications of the massless RG flow of $1+1$ dimensional CFT to $2+1$
dimensional topological order which corresponds to the earlier proposal by L.
Kong and H. Zheng in [Nucl. Phys. B 966 (2021), 115384], arXiv:1912.01760
closely related to the integer-spin simple current by Schellekens and
Gato-Rivera. We study the properties of the product of two CFTs connected by
the two kinds of massless flows. The (mock) modular covariants appearing in the
analysis seem to contain new ones. By applying the folding trick to the coupled
model, we provide a general method to solve the gapped and charged domain wall.
One can obtain the general phenomenology of the transportation of anyons
through the domain wall. Our work gives a unified direction for the future
theoretical and numerical studies of the topological phase based on the
established data of classifications of conformal field theories or modular
invariants.",2024-12-27T10:46:30Z,http://arxiv.org/abs/2412.19577v1,Yoshiki Fukusumi
"xFLIE: Leveraging Actionable Hierarchical Scene Representations for
  Autonomous Semantic-Aware Inspection Missions","This article presents xFLIE, a fully integrated 3D hierarchical scene graph
based autonomous inspection architecture. Specifically, we present a
tightly-coupled solution of incremental 3D Layered Semantic Graphs (LSG)
construction and real-time exploitation by a multi-modal autonomy, First-Look
based Inspection and Exploration (FLIE) planner, to address the task of
inspection of apriori unknown semantic targets of interest in unknown
environments. This work aims to address the challenge of maintaining, in
addition to or as an alternative to volumetric models, an intuitive scene
representation during large-scale inspection missions. Through its
contributions, the proposed architecture aims to provide a high-level
multi-tiered abstract environment representation whilst simultaneously
maintaining a tractable foundation for rapid and informed decision-making
capable of enhancing inspection planning through scene understanding, what
should it inspect ?, and reasoning, why should it inspect ?. The proposed LSG
framework is designed to leverage the concept of nesting lower local graphs, at
multiple layers of abstraction, with the abstract concepts grounded on the
functionality of the integrated FLIE planner. Through intuitive scene
representation, the proposed architecture offers an easily digestible
environment model for human operators which helps to improve situational
awareness and their understanding of the operating environment. We highlight
the use-case benefits of hierarchical and semantic path-planning capability
over LSG to address queries, by the integrated planner as well as the human
operator. The validity of the proposed architecture is evaluated in large-scale
simulated outdoor urban scenarios as well as being deployed onboard a Boston
Dynamics Spot quadruped robot for extensive outdoor field experiments.",2024-12-27T10:26:59Z,http://arxiv.org/abs/2412.19571v1,"Vignesh Kottayam Viswanathan, Mario A. V. Saucedo, Sumeet Gajanan Satpute, Christoforos Kanellakis, George Nikolakopoulos"
"Hindsight Planner: A Closed-Loop Few-Shot Planner for Embodied
  Instruction Following","This work focuses on building a task planner for Embodied Instruction
Following (EIF) using Large Language Models (LLMs). Previous works typically
train a planner to imitate expert trajectories, treating this as a supervised
task. While these methods achieve competitive performance, they often lack
sufficient robustness. When a suboptimal action is taken, the planner may
encounter an out-of-distribution state, which can lead to task failure. In
contrast, we frame the task as a Partially Observable Markov Decision Process
(POMDP) and aim to develop a robust planner under a few-shot assumption. Thus,
we propose a closed-loop planner with an adaptation module and a novel
hindsight method, aiming to use as much information as possible to assist the
planner. Our experiments on the ALFRED dataset indicate that our planner
achieves competitive performance under a few-shot assumption. For the first
time, our few-shot agent's performance approaches and even surpasses that of
the full-shot supervised agent.",2024-12-27T10:05:45Z,http://arxiv.org/abs/2412.19562v1,"Yuxiao Yang, Shenao Zhang, Zhihan Liu, Huaxiu Yao, Zhaoran Wang"
Single-qubit quantum gate at an arbitrary speed,"Quantum information processing comprises physical processes, which obey the
quantum speed limit (QSL): high speed requires strong driving. Single-qubit
gates using Rabi oscillation, which is based on the rotating wave approximation
(RWA), satisfy this bound in the form that the gate time $T$ is inversely
proportional to the Rabi frequency $\Omega$, characterizing the driving
strength. However, if the gate time is comparable or shorter than the qubit
period $T_{0} \equiv 2\pi / \omega_{0}$, the RWA actually breaks down since the
Rabi frequency has to be large compared to the qubit frequency $\omega_{0}$ due
to the QSL, which is given as $T \gtrsim \pi/\Omega$. We show that it is
possible to construct a universal set of single-qubit gates at this
strong-coupling and ultrafast regime, by adjusting the central frequency
$\omega$ and the Rabi frequency $\Omega$ of the driving pulse. We observe a
transition in the scaling behavior of the central frequency from the long-gate
time regime ($T \gg T_{0}$) to the short-gate time ($T \ll T_{0}$) regime. In
the former, the central frequency is nearly resonant to the qubit, i.e.,
$\omega \simeq \omega_{0}$, whereas in the latter, the central frequency is
inversely proportional to the gate time, i.e., $\omega \sim \pi/T$. We identify
the transition gate time at which the scaling exponent $n$ of the optimal
central frequency $\omega \sim T^{n}$ changes from $n=0$ to $n=-1$.",2024-12-27T10:05:27Z,http://arxiv.org/abs/2412.19561v1,"Seongjin Ahn, Kichan Park, Daehee Cho, Mikyoung Lim, Taeyoung Choi, Andrey S. Moskalenko"
Bolstering up the existence of $P_s(2080)$,"We present a detailed study of the partial decay widths of a spin-parity
resonance $J^P=3/2^-$ $N^*$ with a mass of $\simeq$ 2070 MeV obtained from the
coupled channel s wave vector-baryon $\rho N$, $\omega N$, $\phi N$,
$K^*\Lambda$ and $K^*\Sigma$ dynamics. This state, which couples strongly to
the $K^*\Sigma$ channel, corresponds to a nucleon with a hidden strange quark
content, in analogy to the $P_c$ states discovered by the LHCb collaboration,
and we denote it as $P_s(2080)$. A state with such a nature can decay to
vector-baryon, pseudoscalar-baryon, and pseudoscalar-baryon resonance channels,
involving triangular loops in the latter two cases. As we will show, the
partial decay widths to pseudoscalar-baryon resonance channels, like $\pi
N^*(1535)$, $\pi N^*(1650)$, $K\Lambda(1405)$, are comparable to those related
to ground state baryons in the final state, like $\pi N$, $\eta N$, $K\Lambda$.
In this way, reactions involving such lighter baryon resonances in the final
state can be used as an alternative source of information on the properties of
a $N^*$ with hidden strangeness.",2024-12-27T10:00:42Z,http://arxiv.org/abs/2412.19559v1,"Breno Agatão, A. Vertel Nieto, K. P. Khemchandani, A. Martinez Torres, Seung-il Nam"
"Learning states enhanced knowledge tracing: Simulating the diversity in
  real-world learning process","The Knowledge Tracing (KT) task focuses on predicting a learner's future
performance based on the historical interactions. The knowledge state plays a
key role in learning process. However, considering that the knowledge state is
influenced by various learning factors in the interaction process, such as the
exercises similarities, responses reliability and the learner's learning state.
Previous models still face two major limitations. First, due to the exercises
differences caused by various complex reasons and the unreliability of
responses caused by guessing behavior, it is hard to locate the historical
interaction which is most relevant to the current answered exercise. Second,
the learning state is also a key factor to influence the knowledge state, which
is always ignored by previous methods. To address these issues, we propose a
new method named Learning State Enhanced Knowledge Tracing (LSKT). Firstly, to
simulate the potential differences in interactions, inspired by Item Response
Theory~(IRT) paradigm, we designed three different embedding methods ranging
from coarse-grained to fine-grained views and conduct comparative analysis on
them. Secondly, we design a learning state extraction module to capture the
changing learning state during the learning process of the learner. In turn,
with the help of the extracted learning state, a more detailed knowledge state
could be captured. Experimental results on four real-world datasets show that
our LSKT method outperforms the current state-of-the-art methods.",2024-12-27T09:41:25Z,http://arxiv.org/abs/2412.19550v1,"Shanshan Wang, Xueying Zhang, Keyang Wang, Xun Yang, Xingyi Zhang"
"Unprejudiced Training Auxiliary Tasks Makes Primary Better: A Multi-Task
  Learning Perspective","Human beings can leverage knowledge from relative tasks to improve learning
on a primary task. Similarly, multi-task learning methods suggest using
auxiliary tasks to enhance a neural network's performance on a specific primary
task. However, previous methods often select auxiliary tasks carefully but
treat them as secondary during training. The weights assigned to auxiliary
losses are typically smaller than the primary loss weight, leading to
insufficient training on auxiliary tasks and ultimately failing to support the
main task effectively. To address this issue, we propose an uncertainty-based
impartial learning method that ensures balanced training across all tasks.
Additionally, we consider both gradients and uncertainty information during
backpropagation to further improve performance on the primary task. Extensive
experiments show that our method achieves performance comparable to or better
than state-of-the-art approaches. Moreover, our weighting strategy is effective
and robust in enhancing the performance of the primary task regardless the
noise auxiliary tasks' pseudo labels.",2024-12-27T09:27:18Z,http://arxiv.org/abs/2412.19547v1,"Yuanze Li, Chun-Mei Feng, Qilong Wang, Guanglei Yang, Wangmeng Zuo"
"GHZ-W Genuinely Entangled Subspace Verification with Adaptive Local
  Measurements","Genuinely entangled subspaces (GESs) are valuable resources in quantum
information science. Among these, the three-qubit GHZ-W GES, spanned by the
three-qubit Greenberger-Horne-Zeilinger (GHZ) and W states, is a universal and
crucial entangled subspace resource for three-qubit systems. In this work, we
develop two adaptive verification strategies, the XZ strategy and the rotation
strategy, for the three-qubit GHZ-W GES using local measurements and one-way
classical communication. These strategies are experimentally feasible,
efficient and possess a concise analytical expression for the sample complexity
of the rotation strategy, which scales approximately as
$2.248/\epsilon\ln(1/\delta)$, where $\epsilon$ is the infidelity and
$1-\delta$ is the confidence level. Furthermore, we comprehensively analyze the
two-dimensional two-qubit subspaces and classify them into three distinct
types, including unverifiable entangled subspaces, revealing intrinsic
limitations in local verification of entangled subspaces.",2024-12-27T09:07:44Z,http://arxiv.org/abs/2412.19540v1,"Congcong Zheng, Ping Xu, Kun Wang, Zaichen Zhang"
"Finger in Camera Speaks Everything: Unconstrained Air-Writing for
  Real-World","Air-writing is a challenging task that combines the fields of computer vision
and natural language processing, offering an intuitive and natural approach for
human-computer interaction. However, current air-writing solutions face two
primary challenges: (1) their dependency on complex sensors (e.g., Radar, EEGs
and others) for capturing precise handwritten trajectories, and (2) the absence
of a video-based air-writing dataset that covers a comprehensive vocabulary
range. These limitations impede their practicality in various real-world
scenarios, including the use on devices like iPhones and laptops. To tackle
these challenges, we present the groundbreaking air-writing Chinese character
video dataset (AWCV-100K-UCAS2024), serving as a pioneering benchmark for
video-based air-writing. This dataset captures handwritten trajectories in
various real-world scenarios using commonly accessible RGB cameras, eliminating
the need for complex sensors. AWCV-100K-UCAS2024 includes 8.8 million video
frames, encompassing the complete set of 3,755 characters from the GB2312-80
level-1 set (GB1). Furthermore, we introduce our baseline approach, the
video-based character recognizer (VCRec). VCRec adeptly extracts fingertip
features from sparse visual cues and employs a spatio-temporal sequence module
for analysis. Experimental results showcase the superior performance of VCRec
compared to existing models in recognizing air-written characters, both
quantitatively and qualitatively. This breakthrough paves the way for enhanced
human-computer interaction in real-world contexts. Moreover, our approach
leverages affordable RGB cameras, enabling its applicability in a diverse range
of scenarios. The code and data examples will be made public at
https://github.com/wmeiqi/AWCV.",2024-12-27T09:04:04Z,http://arxiv.org/abs/2412.19537v1,"Meiqi Wu, Kaiqi Huang, Yuanqiang Cai, Shiyu Hu, Yuzhong Zhao, Weiqiang Wang"
"Estimation of System Parameters Including Repeated Cross-Sectional Data
  through Emulator-Informed Deep Generative Model","Differential equations (DEs) are crucial for modeling the evolution of
natural or engineered systems. Traditionally, the parameters in DEs are
adjusted to fit data from system observations. However, in fields such as
politics, economics, and biology, available data are often independently
collected at distinct time points from different subjects (i.e., repeated
cross-sectional (RCS) data). Conventional optimization techniques struggle to
accurately estimate DE parameters when RCS data exhibit various
heterogeneities, leading to a significant loss of information. To address this
issue, we propose a new estimation method called the emulator-informed
deep-generative model (EIDGM), designed to handle RCS data. Specifically, EIDGM
integrates a physics-informed neural network-based emulator that immediately
generates DE solutions and a Wasserstein generative adversarial network-based
parameter generator that can effectively mimic the RCS data. We evaluated EIDGM
on exponential growth, logistic population models, and the Lorenz system,
demonstrating its superior ability to accurately capture parameter
distributions. Additionally, we applied EIDGM to an experimental dataset of
Amyloid beta 40 and beta 42, successfully capturing diverse parameter
distribution shapes. This shows that EIDGM can be applied to model a wide range
of systems and extended to uncover the operating principles of systems based on
limited data.",2024-12-27T08:19:23Z,http://arxiv.org/abs/2412.19517v1,"Hyunwoo Cho, Sung Woong Cho, Hyeontae Jo, Hyung Ju Hwang"
"Real-time classification of EEG signals using Machine Learning
  deployment","The prevailing educational methods predominantly rely on traditional
classroom instruction or online delivery, often limiting the teachers' ability
to engage effectively with all the students simultaneously. A more intrinsic
method of evaluating student attentiveness during lectures can enable the
educators to tailor the course materials and their teaching styles in order to
better meet the students' needs. The aim of this paper is to enhance teaching
quality in real time, thereby fostering a higher student engagement in the
classroom activities. By monitoring the students' electroencephalography (EEG)
signals and employing machine learning algorithms, this study proposes a
comprehensive solution for addressing this challenge. Machine learning has
emerged as a powerful tool for simplifying the analysis of complex variables,
enabling the effective assessment of the students' concentration levels based
on specific parameters. However, the real-time impact of machine learning
models necessitates a careful consideration as their deployment is concerned.
This study proposes a machine learning-based approach for predicting the level
of students' comprehension with regard to a certain topic. A browser interface
was introduced that accesses the values of the system's parameters to determine
a student's level of concentration on a chosen topic. The deployment of the
proposed system made it necessary to address the real-time challenges faced by
the students, consider the system's cost, and establish trust in its efficacy.
This paper presents the efforts made for approaching this pertinent issue
through the implementation of innovative technologies and provides a framework
for addressing key considerations for future research directions.",2024-12-27T08:14:28Z,http://arxiv.org/abs/2412.19515v1,"Swati Chowdhuri, Satadip Saha, Samadrita Karmakar, Ankur Chanda"
"Uncertainty quantification for improving radiomic-based models in
  radiation pneumonitis prediction","Background and Objective: Radiation pneumonitis (RP) is a side effect of
thoracic radiation therapy. Recently, Machine learning (ML) models enhanced
with radiomic and dosiomic features provide better predictions by incorporating
spatial information beyond DVHs. However, to improve the clinical decision
process, we propose to use uncertainty quantification (UQ) to improve the
confidence in model prediction. This study evaluates the impact of post hoc UQ
methods on the discriminative performance and calibration of ML models for RP
prediction. Methods: This study evaluated four ML models: logistic regression
(LR), support vector machines (SVM), extreme gradient boosting (XGB), and
random forest (RF), using radiomic, dosiomic, and dosimetric features to
predict RP. We applied UQ methods, including Patt scaling, isotonic regression,
Venn-ABERS predictor, and Conformal Prediction, to quantify uncertainty. Model
performance was assessed through Area Under the Receiver Operating
Characteristic curve (AUROC), Area Under the Precision-Recall Curve (AUPRC),
and Adaptive Calibration Error (ACE) using Leave-One-Out Cross-Validation
(LOO-CV). Results: UQ methods enhanced predictive performance, particularly for
high-certainty predictions, while also improving calibration. Radiomic and
dosiomic features increased model accuracy but introduced calibration
challenges, especially for non-linear models like XGB and RF. Performance gains
from UQ methods were most noticeable at higher certainty thresholds.
Conclusion: Integrating UQ into ML models with radiomic and dosiomic features
improves both predictive accuracy and calibration, supporting more reliable
clinical decision-making. The findings emphasize the value of UQ methods in
enhancing applicability of predictive models for RP in healthcare settings.",2024-12-27T08:01:42Z,http://arxiv.org/abs/2412.19511v1,"Chanon Puttanawarut, Romen Samuel Wabina, Nat Sirirutbunkajorn"
"DrivingWorld: ConstructingWorld Model for Autonomous Driving via Video
  GPT","Recent successes in autoregressive (AR) generation models, such as the GPT
series in natural language processing, have motivated efforts to replicate this
success in visual tasks. Some works attempt to extend this approach to
autonomous driving by building video-based world models capable of generating
realistic future video sequences and predicting ego states. However, prior
works tend to produce unsatisfactory results, as the classic GPT framework is
designed to handle 1D contextual information, such as text, and lacks the
inherent ability to model the spatial and temporal dynamics essential for video
generation. In this paper, we present DrivingWorld, a GPT-style world model for
autonomous driving, featuring several spatial-temporal fusion mechanisms. This
design enables effective modeling of both spatial and temporal dynamics,
facilitating high-fidelity, long-duration video generation. Specifically, we
propose a next-state prediction strategy to model temporal coherence between
consecutive frames and apply a next-token prediction strategy to capture
spatial information within each frame. To further enhance generalization
ability, we propose a novel masking strategy and reweighting strategy for token
prediction to mitigate long-term drifting issues and enable precise control.
Our work demonstrates the ability to produce high-fidelity and consistent video
clips of over 40 seconds in duration, which is over 2 times longer than
state-of-the-art driving world models. Experiments show that, in contrast to
prior works, our method achieves superior visual quality and significantly more
accurate controllable future video generation. Our code is available at
https://github.com/YvanYin/DrivingWorld.",2024-12-27T07:44:07Z,http://arxiv.org/abs/2412.19505v1,"Xiaotao Hu, Wei Yin, Mingkai Jia, Junyuan Deng, Xiaoyang Guo, Qian Zhang, Xiaoxiao Long, Ping Tan"
Casevo: A Cognitive Agents and Social Evolution Simulator,"In this paper, we introduce a multi-agent simulation framework Casevo
(Cognitive Agents and Social Evolution Simulator), that integrates large
language models (LLMs) to simulate complex social phenomena and decision-making
processes. Casevo is designed as a discrete-event simulator driven by agents
with features such as Chain of Thoughts (CoT), Retrieval-Augmented Generation
(RAG), and Customizable Memory Mechanism. Casevo enables dynamic social
modeling, which can support various scenarios such as social network analysis,
public opinion dynamics, and behavior prediction in complex social systems. To
demonstrate the effectiveness of Casevo, we utilize one of the U.S. 2020
midterm election TV debates as a simulation example. Our results show that
Casevo facilitates more realistic and flexible agent interactions, improving
the quality of dynamic social phenomena simulation. This work contributes to
the field by providing a robust system for studying large-scale, high-fidelity
social behaviors with advanced LLM-driven agents, expanding the capabilities of
traditional agent-based modeling (ABM). The open-source code repository address
of casevo is https://github.com/rgCASS/casevo.",2024-12-27T07:33:49Z,http://arxiv.org/abs/2412.19498v1,"Zexun Jiang, Yafang Shi, Maoxu Li, Hongjiang Xiao, Yunxiao Qin, Qinglan Wei, Ye Wang, Yuan Zhang"
"Multi-P$^2$A: A Multi-perspective Benchmark on Privacy Assessment for
  Large Vision-Language Models","Large Vision-Language Models (LVLMs) exhibit impressive potential across
various tasks but also face significant privacy risks, limiting their practical
applications. Current researches on privacy assessment for LVLMs is limited in
scope, with gaps in both assessment dimensions and privacy categories. To
bridge this gap, we propose Multi-P$^2$A, a comprehensive benchmark for
evaluating the privacy preservation capabilities of LVLMs in terms of privacy
awareness and leakage. Privacy awareness measures the model's ability to
recognize the privacy sensitivity of input data, while privacy leakage assesses
the risk of the model unintentionally disclosing privacy information in its
output. We design a range of sub-tasks to thoroughly evaluate the model's
privacy protection offered by LVLMs. Multi-P$^2$A covers 26 categories of
personal privacy, 15 categories of trade secrets, and 18 categories of state
secrets, totaling 31,962 samples. Based on Multi-P$^2$A, we evaluate the
privacy preservation capabilities of 21 open-source and 2 closed-source LVLMs.
Our results reveal that current LVLMs generally pose a high risk of
facilitating privacy breaches, with vulnerabilities varying across personal
privacy, trade secret, and state secret.",2024-12-27T07:33:39Z,http://arxiv.org/abs/2412.19496v1,"Jie Zhang, Xiangkui Cao, Zhouyu Han, Shiguang Shan, Xilin Chen"
Retrieval-augmented Generation for GenAI-enabled Semantic Communications,"Semantic communication (SemCom) is an emerging paradigm aiming at
transmitting only task-relevant semantic information to the receiver, which can
significantly improve communication efficiency. Recent advancements in
generative artificial intelligence (GenAI) have empowered GenAI-enabled SemCom
(GenSemCom) to further expand its potential in various applications. However,
current GenSemCom systems still face challenges such as semantic inconsistency,
limited adaptability to diverse tasks and dynamic environments, and the
inability to leverage insights from past transmission. Motivated by the success
of retrieval-augmented generation (RAG) in the domain of GenAI, this paper
explores the integration of RAG in GenSemCom systems. Specifically, we first
provide a comprehensive review of existing GenSemCom systems and the
fundamentals of RAG techniques. We then discuss how RAG can be integrated into
GenSemCom. Following this, we conduct a case study on semantic image
transmission using an RAG-enabled diffusion-based SemCom system, demonstrating
the effectiveness of the proposed integration. Finally, we outline future
directions for advancing RAG-enabled GenSemCom systems.",2024-12-27T07:30:01Z,http://arxiv.org/abs/2412.19494v1,"Shunpu Tang, Ruichen Zhang, Yuxuan Yan, Qianqian Yang, Dusit Niyato, Xianbin Wang, Shiwen Mao"
"Two superconducting thin films systems with potential integration of
  different quantum functionalities","Quantum computation based on superconducting circuits utilizes
superconducting qubits with Josephson tunnel junctions. Engineering
high-coherence qubits requires materials optimization. In this work, we present
two superconducting thin film systems, grown on silicon (Si), and one obtained
from the other via annealing. Cobalt (Co) thin films grown on Si were found to
be superconducting [EPL 131 (2020) 47001]. These films also happen to be a
self-organised hybrid superconductor/ferromagnet/superconductor (S/F/S)
structure. The S/F/S hybrids are important for superconducting $\pi$-qubits
[PRL 95 (2005) 097001] and in quantum information processing. Here we present
our results on the superconductivity of a hybrid Co film followed by the
superconductivity of a CoSi$_2$ film, which was prepared by annealing the Co
film. CoSi$_2$, with its $1/f$ noise about three orders of magnitude smaller
compared to the most commonly used superconductor aluminium (Al), is a
promising material for high-coherence qubits. The hybrid Co film revealed
superconducting transition temperature $T_c$ = 5 K and anisotropy in the upper
critical field between the in-plane and out-of-plane directions. The anisotropy
was of the order of ratio of lateral dimensions to thickness of the
superconducting Co grains, suggesting a quasi-2D nature of superconductivity.
On the other hand, CoSi$_2$ film showed a $T_c$ of 900 mK. In the resistivity
vs. temperature curve, we observe a peak near $T_c$. Magnetic field scan as a
function of $T$ shows a monotonic increase in intensity of this peak with
temperature. The origin of the peak has been explained in terms of parallel
resistive model for the particular measurement configuration. Although our
CoSi$_2$ film contains grain boundaries, we observed a perpendicular critical
field of 15 mT and a critical current density of 3.8x10$^7$ A/m$^2$, comparable
with epitaxial CoSi$_2$ films.",2024-12-27T07:23:20Z,http://arxiv.org/abs/2412.19493v1,"Snehal Mandal, Biplab Biswas, Suvankar Purakait, Anupam Roy, Biswarup Satpati, Indranil Das, B. N. Dev"
Towards Open-Vocabulary Remote Sensing Image Semantic Segmentation,"Recently, deep learning based methods have revolutionized remote sensing
image segmentation. However, these methods usually rely on a pre-defined
semantic class set, thus needing additional image annotation and model training
when adapting to new classes. More importantly, they are unable to segment
arbitrary semantic classes. In this work, we introduce Open-Vocabulary Remote
Sensing Image Semantic Segmentation (OVRSISS), which aims to segment arbitrary
semantic classes in remote sensing images. To address the lack of OVRSISS
datasets, we develop LandDiscover50K, a comprehensive dataset of 51,846 images
covering 40 diverse semantic classes. In addition, we propose a novel framework
named GSNet that integrates domain priors from special remote sensing models
and versatile capabilities of general vision-language models. Technically,
GSNet consists of a Dual-Stream Image Encoder (DSIE), a Query-Guided Feature
Fusion (QGFF), and a Residual Information Preservation Decoder (RIPD). DSIE
first captures comprehensive features from both special models and general
models in dual streams. Then, with the guidance of variable vocabularies, QGFF
integrates specialist and generalist features, enabling them to complement each
other. Finally, RIPD is proposed to aggregate multi-source features for more
accurate mask predictions. Experiments show that our method outperforms other
methods by a large margin, and our proposed LandDiscover50K improves the
performance of OVRSISS methods. The proposed dataset and method will be made
publicly available at https://github.com/yecy749/GSNet.",2024-12-27T07:20:30Z,http://arxiv.org/abs/2412.19492v1,"Chengyang Ye, Yunzhi Zhuge, Pingping Zhang"
"Multi-label Classification using Deep Multi-order Context-aware Kernel
  Networks","Multi-label classification is a challenging task in pattern recognition. Many
deep learning methods have been proposed and largely enhanced classification
performance. However, most of the existing sophisticated methods ignore context
in the models' learning process. Since context may provide additional cues to
the learned models, it may significantly boost classification performances. In
this work, we make full use of context information (namely geometrical
structure of images) in order to learn better context-aware similarities
(a.k.a. kernels) between images. We reformulate context-aware kernel design as
a feed-forward network that outputs explicit kernel mapping features. Our
obtained context-aware kernel network further leverages multiple orders of
patch neighbors within different distances, resulting into a more
discriminating Deep Multi-order Context-aware Kernel Network (DMCKN) for
multi-label classification. We evaluate the proposed method on the challenging
Corel5K and NUS-WIDE benchmarks, and empirical results show that our method
obtains competitive performances against the related state-of-the-art, and both
quantitative and qualitative performances corroborate its effectiveness and
superiority for multi-label image classification.",2024-12-27T07:16:11Z,http://arxiv.org/abs/2412.19491v1,"Mingyuan Jiu, Hailong Zhu, Hichem Sahbi"
UniBrain: A Unified Model for Cross-Subject Brain Decoding,"Brain decoding aims to reconstruct original stimuli from fMRI signals,
providing insights into interpreting mental content. Current approaches rely
heavily on subject-specific models due to the complex brain processing
mechanisms and the variations in fMRI signals across individuals. Therefore,
these methods greatly limit the generalization of models and fail to capture
cross-subject commonalities. To address this, we present UniBrain, a unified
brain decoding model that requires no subject-specific parameters. Our approach
includes a group-based extractor to handle variable fMRI signal lengths, a
mutual assistance embedder to capture cross-subject commonalities, and a
bilevel feature alignment scheme for extracting subject-invariant features. We
validate our UniBrain on the brain decoding benchmark, achieving comparable
performance to current state-of-the-art subject-specific models with extremely
fewer parameters. We also propose a generalization benchmark to encourage the
community to emphasize cross-subject commonalities for more general brain
decoding. Our code is available at https://github.com/xiaoyao3302/UniBrain.",2024-12-27T07:03:47Z,http://arxiv.org/abs/2412.19487v1,"Zicheng Wang, Zhen Zhao, Luping Zhou, Parashkev Nachev"
Learning Radiance Fields from a Single Snapshot Compressive Image,"In this paper, we explore the potential of Snapshot Compressive Imaging (SCI)
technique for recovering the underlying 3D scene structure from a single
temporal compressed image. SCI is a cost-effective method that enables the
recording of high-dimensional data, such as hyperspectral or temporal
information, into a single image using low-cost 2D imaging sensors. To achieve
this, a series of specially designed 2D masks are usually employed, reducing
storage and transmission requirements and offering potential privacy
protection. Inspired by this, we take one step further to recover the encoded
3D scene information leveraging powerful 3D scene representation capabilities
of neural radiance fields (NeRF). Specifically, we propose SCINeRF, in which we
formulate the physical imaging process of SCI as part of the training of NeRF,
allowing us to exploit its impressive performance in capturing complex scene
structures. In addition, we further integrate the popular 3D Gaussian Splatting
(3DGS) framework and propose SCISplat to improve 3D scene reconstruction
quality and training/rendering speed by explicitly optimizing point clouds into
3D Gaussian representations. To assess the effectiveness of our method, we
conduct extensive evaluations using both synthetic data and real data captured
by our SCI system. Experimental results demonstrate that our proposed approach
surpasses the state-of-the-art methods in terms of image reconstruction and
novel view synthesis. Moreover, our method also exhibits the ability to render
high frame-rate multi-view consistent images in real time by leveraging SCI and
the rendering capabilities of 3DGS. Codes will be available at:
https://github.com/WU- CVGL/SCISplat.",2024-12-27T06:40:44Z,http://arxiv.org/abs/2412.19483v1,"Yunhao Li, Xiang Liu, Xiaodong Wang, Xin Yuan, Peidong Liu"
"Exploiting Dynamic Sparsity for Near-Field Spatial Non-Stationary
  XL-MIMO Channel Tracking","This work considers a spatial non-stationary channel tracking problem in
broadband extremely large-scale multiple-input-multiple-output (XL-MIMO)
systems. In the case of spatial non-stationary, each scatterer has a certain
visibility region (VR) over antennas and power change may occur among visible
antennas. Concentrating on the temporal correlation of XL-MIMO channels, we
design a three-layer Markov prior model and hierarchical two-dimensional (2D)
Markov model to exploit the dynamic sparsity of sparse channel vectors and VRs,
respectively. Then, we formulate the channel tracking problem as a bilinear
measurement process, and a novel dynamic alternating maximum a posteriori
(DA-MAP) framework is developed to solve the problem. The DA-MAP contains four
basic modules: channel estimation module, VR detection module, grid update
module, and temporal correlated module. Specifically, the first module is an
inverse-free variational Bayesian inference (IF-VBI) estimator that avoids
computational intensive matrix inverse each iteration; the second module is a
turbo compressive sensing (Turbo-CS) algorithm that only needs small-scale
matrix operations in a parallel fashion; the third module refines the
polar-delay domain grid; and the fourth module can process the temporal prior
information to ensure high-efficiency channel tracking. Simulations show that
the proposed method can achieve a significant channel tracking performance
while achieving low computational overhead.",2024-12-27T06:00:30Z,http://arxiv.org/abs/2412.19475v1,"Wenkang Xu amd An Liu, Min-jian Zhao, Giuseppe Caire, Yik-Chung Wu"
Movable Antenna-Aided Near-Field Integrated Sensing and Communication,"Integrated sensing and communication (ISAC) is emerging as a pivotal
technology for next-generation wireless networks. However, existing ISAC
systems are based on fixed-position antennas (FPAs), which inevitably incur a
loss in performance when balancing the trade-off between sensing and
communication. Movable antenna (MA) technology offers promising potential to
enhance ISAC performance by enabling flexible antenna movement. Nevertheless,
exploiting more spatial channel variations requires larger antenna moving
regions, which may invalidate the conventional far-field assumption for
channels between transceivers. Therefore, this paper utilizes the MA to enhance
sensing and communication capabilities in near-field ISAC systems, where a
full-duplex base station (BS) is equipped with multiple transmit and receive
MAs movable in large-size regions to simultaneously sense multiple targets and
serve multiple uplink (UL) and downlink (DL) users for communication. We aim to
maximize the weighted sum of sensing and communication rates (WSR) by jointly
designing the transmit beamformers, sensing signal covariance matrices, receive
beamformers, and MA positions at the BS, as well as the UL power allocation.
The resulting optimization problem is challenging to solve, while we propose an
efficient two-layer random position (RP) algorithm to tackle it. In addition,
to reduce movement delay and cost, we design an antenna position matching (APM)
algorithm based on the greedy strategy to minimize the total MA movement
distance. Extensive simulation results demonstrate the substantial performance
improvement achieved by deploying MAs in near-field ISAC systems. Moreover, the
results show the effectiveness of the proposed APM algorithm in reducing the
antenna movement distance, which is helpful for energy saving and time overhead
reduction for MA-aided near-field ISAC systems with large moving regions.",2024-12-27T05:45:35Z,http://arxiv.org/abs/2412.19470v1,"Jingze Ding, Zijian Zhou, Xiaodan Shao, Bingli Jiao, Rui Zhang"
"MNet-SAt: A Multiscale Network with Spatial-enhanced Attention for
  Segmentation of Polyps in Colonoscopy","Objective: To develop a novel deep learning framework for the automated
segmentation of colonic polyps in colonoscopy images, overcoming the
limitations of current approaches in preserving precise polyp boundaries,
incorporating multi-scale features, and modeling spatial dependencies that
accurately reflect the intricate and diverse morphology of polyps. Methods: To
address these limitations, we propose a novel Multiscale Network with
Spatial-enhanced Attention (MNet-SAt) for polyp segmentation in colonoscopy
images. This framework incorporates four key modules: Edge-Guided Feature
Enrichment (EGFE) preserves edge information for improved boundary quality;
Multi-Scale Feature Aggregator (MSFA) extracts and aggregates multi-scale
features across channel spatial dimensions, focusing on salient regions;
Spatial-Enhanced Attention (SEAt) captures spatial-aware global dependencies
within the multi-scale aggregated features, emphasizing the region of interest;
and Channel-Enhanced Atrous Spatial Pyramid Pooling (CE-ASPP) resamples and
recalibrates attentive features across scales. Results: We evaluated MNet-SAt
on the Kvasir-SEG and CVC-ClinicDB datasets, achieving Dice Similarity
Coefficients of 96.61% and 98.60%, respectively. Conclusion: Both quantitative
(DSC) and qualitative assessments highlight MNet-SAt's superior performance and
generalization capabilities compared to existing methods. Significance:
MNet-SAt's high accuracy in polyp segmentation holds promise for improving
clinical workflows in early polyp detection and more effective treatment,
contributing to reduced colorectal cancer mortality rates.",2024-12-27T05:17:29Z,http://arxiv.org/abs/2412.19464v1,"Chandravardhan Singh Raghaw, Aryan Yadav, Jasmer Singh Sanjotra, Shalini Dangi, Nagendra Kumar"
A Prototype Unit for Image De-raining using Time-Lapse Data,"We address the challenge of single-image de-raining, a task that involves
recovering rain-free background information from a single rain image. While
recent advancements have utilized real-world time-lapse data for training,
enabling the estimation of consistent backgrounds and realistic rain streaks,
these methods often suffer from computational and memory consumption, limiting
their applicability in real-world scenarios. In this paper, we introduce a
novel solution: the Rain Streak Prototype Unit (RsPU). The RsPU efficiently
encodes rain streak-relevant features as real-time prototypes derived from
time-lapse data, eliminating the need for excessive memory resources. Our
de-raining network combines encoder-decoder networks with the RsPU, allowing us
to learn and encapsulate diverse rain streak-relevant features as concise
prototypes, employing an attention-based approach. To ensure the effectiveness
of our approach, we propose a feature prototype loss encompassing cohesion and
divergence components. This loss function captures both the compactness and
diversity aspects of the prototypical rain streak features within the RsPU. Our
method evaluates various de-raining benchmarks, accompanied by comprehensive
ablation studies. We show that it can achieve competitive results in various
rain images compared to state-of-the-art methods.",2024-12-27T05:04:56Z,http://arxiv.org/abs/2412.19459v1,"Jaehoon Cho, Minjung Yoo, Jini Yang, Sunok Kim"
"DriveEditor: A Unified 3D Information-Guided Framework for Controllable
  Object Editing in Driving Scenes","Vision-centric autonomous driving systems require diverse data for robust
training and evaluation, which can be augmented by manipulating object
positions and appearances within existing scene captures. While recent
advancements in diffusion models have shown promise in video editing, their
application to object manipulation in driving scenarios remains challenging due
to imprecise positional control and difficulties in preserving high-fidelity
object appearances. To address these challenges in position and appearance
control, we introduce DriveEditor, a diffusion-based framework for object
editing in driving videos. DriveEditor offers a unified framework for
comprehensive object editing operations, including repositioning, replacement,
deletion, and insertion. These diverse manipulations are all achieved through a
shared set of varying inputs, processed by identical position control and
appearance maintenance modules. The position control module projects the given
3D bounding box while preserving depth information and hierarchically injects
it into the diffusion process, enabling precise control over object position
and orientation. The appearance maintenance module preserves consistent
attributes with a single reference image by employing a three-tiered approach:
low-level detail preservation, high-level semantic maintenance, and the
integration of 3D priors from a novel view synthesis model. Extensive
qualitative and quantitative evaluations on the nuScenes dataset demonstrate
DriveEditor's exceptional fidelity and controllability in generating diverse
driving scene edits, as well as its remarkable ability to facilitate downstream
tasks.",2024-12-27T04:49:36Z,http://arxiv.org/abs/2412.19458v1,"Yiyuan Liang, Zhiying Yan, Liqun Chen, Jiahuan Zhou, Luxin Yan, Sheng Zhong, Xu Zou"
"Feature Alignment-Based Knowledge Distillation for Efficient Compression
  of Large Language Models","This study proposes a knowledge distillation algorithm based on large
language models and feature alignment, aiming to effectively transfer the
knowledge of large pre-trained models into lightweight student models, thereby
reducing computational costs while maintaining high model performance.
Different from the traditional soft label distillation method, this method
introduces a multi-layer feature alignment strategy to deeply align the
intermediate features and attention mechanisms of the teacher model and the
student model, maximally retaining the semantic expression ability and context
modeling ability of the teacher model. In terms of method design, a multi-task
loss function is constructed, including feature matching loss, attention
alignment loss, and output distribution matching loss, to ensure multi-level
information transfer through joint optimization. The experiments were
comprehensively evaluated on the GLUE data set and various natural language
processing tasks. The results show that the proposed model performs very close
to the state-of-the-art GPT-4 model in terms of evaluation indicators such as
perplexity, BLEU, ROUGE, and CER. At the same time, it far exceeds baseline
models such as DeBERTa, XLNet, and GPT-3, showing significant performance
improvements and computing efficiency advantages. Research results show that
the feature alignment distillation strategy is an effective model compression
method that can significantly reduce computational overhead and storage
requirements while maintaining model capabilities. Future research can be
further expanded in the directions of self-supervised learning, cross-modal
feature alignment, and multi-task transfer learning to provide more flexible
and efficient solutions for the deployment and optimization of deep learning
models.",2024-12-27T04:37:06Z,http://arxiv.org/abs/2412.19449v1,"Shuo Wang, Chihang Wang, Jia Gao, Zhen Qi, Hongye Zheng, Xiaoxuan Liao"
The Rendezvous Between Extreme Value Theory and Next-generation Networks,"Promising technologies such as massive multiple-input and multiple-output,
reconfigurable intelligent reflecting surfaces, non-terrestrial networks,
millimetre wave communication, ultra-reliable lowlatency communication are
envisioned as the enablers for next-generation (NG) networks. In contrast to
conventional communication systems meeting specific average performance
requirements, NG systems are expected to meet quality-of-service requirements
in extreme scenarios as well. In this regard, extreme value theory (EVT)
provides a powerful framework for the design of communication systems. In this
paper, we provide a comprehensive survey of advances in communication that
utilize EVT to characterize the extreme order statistics of interest. We first
give an overview of the history of EVT and then elaborate on the fundamental
theorems. Subsequently, we discuss different problems of interest in NG
communication systems and how EVT can be utilized for their analysis. We
finally point out the open challenges and future directions of EVT in NG
communication systems.",2024-12-27T04:06:32Z,http://arxiv.org/abs/2412.19438v1,"Srinivas Sagar, Athira Subhash, Chen-Feng Liu, Ahmed Elzanaty, Yazan H. Al-Badarneh, Sheetal Kalyani, Mohamed-Slim Alouini, Mehdi Bennis, Lajos Hanzo"
"Low-Rank Contextual Reinforcement Learning from Heterogeneous Human
  Feedback","Reinforcement learning from human feedback (RLHF) has become a cornerstone
for aligning large language models with human preferences. However, the
heterogeneity of human feedback, driven by diverse individual contexts and
preferences, poses significant challenges for reward learning. To address this,
we propose a Low-rank Contextual RLHF (LoCo-RLHF) framework that integrates
contextual information to better model heterogeneous feedback while maintaining
computational efficiency. Our approach builds on a contextual preference model,
leveraging the intrinsic low-rank structure of the interaction between user
contexts and query-answer pairs to mitigate the high dimensionality of feature
representations. Furthermore, we address the challenge of distributional shifts
in feedback through our Pessimism in Reduced Subspace (PRS) policy, inspired by
pessimistic offline reinforcement learning techniques. We theoretically
demonstrate that our policy achieves a tighter sub-optimality gap compared to
existing methods. Extensive experiments validate the effectiveness of
LoCo-RLHF, showcasing its superior performance in personalized RLHF settings
and its robustness to distribution shifts.",2024-12-27T04:02:46Z,http://arxiv.org/abs/2412.19436v1,"Seong Jin Lee, Will Wei Sun, Yufeng Liu"
"Residual Feature-Reutilization Inception Network for Image
  Classification","Capturing feature information effectively is of great importance in the field
of computer vision. With the development of convolutional neural networks
(CNNs), concepts like residual connection and multiple scales promote continual
performance gains in diverse deep learning vision tasks. In this paper, we
propose a novel CNN architecture that it consists of residual
feature-reutilization inceptions (ResFRI) or split-residual
feature-reutilization inceptions (Split-ResFRI). And it is composed of four
convolutional combinations of different structures connected by specially
designed information interaction passages, which are utilized to extract
multi-scale feature information and effectively increase the receptive field of
the model. Moreover, according to the network structure designed above,
Split-ResFRI can adjust the segmentation ratio of the input information,
thereby reducing the number of parameters and guaranteeing the model
performance. Specifically, in experiments based on popular vision datasets,
such as CIFAR10 ($97.94$\%), CIFAR100 ($85.91$\%) and Tiny Imagenet
($70.54$\%), we obtain state-of-the-art results compared with other modern
models under the premise that the model size is approximate and no additional
data is used.",2024-12-27T03:55:25Z,http://arxiv.org/abs/2412.19433v1,"Yuanpeng He, Wenjie Song, Lijian Li, Tianxiang Zhan, Wenpin Jiao"
Revisiting PCA for time series reduction in temporal dimension,"Revisiting PCA for Time Series Reduction in Temporal Dimension; Jiaxin Gao,
Wenbo Hu, Yuntian Chen; Deep learning has significantly advanced time series
analysis (TSA), enabling the extraction of complex patterns for tasks like
classification, forecasting, and regression. Although dimensionality reduction
has traditionally focused on the variable space-achieving notable success in
minimizing data redundancy and computational complexity-less attention has been
paid to reducing the temporal dimension. In this study, we revisit Principal
Component Analysis (PCA), a classical dimensionality reduction technique, to
explore its utility in temporal dimension reduction for time series data. It is
generally thought that applying PCA to the temporal dimension would disrupt
temporal dependencies, leading to limited exploration in this area. However,
our theoretical analysis and extensive experiments demonstrate that applying
PCA to sliding series windows not only maintains model performance, but also
enhances computational efficiency. In auto-regressive forecasting, the temporal
structure is partially preserved through windowing, and PCA is applied within
these windows to denoise the time series while retaining their statistical
information. By preprocessing time-series data with PCA, we reduce the temporal
dimensionality before feeding it into TSA models such as Linear, Transformer,
CNN, and RNN architectures. This approach accelerates training and inference
and reduces resource consumption. Notably, PCA improves Informer training and
inference speed by up to 40% and decreases GPU memory usage of TimesNet by 30%,
without sacrificing model accuracy. Comparative analysis against other
reduction methods further highlights the effectiveness of PCA in improving the
efficiency of TSA models.",2024-12-27T03:17:26Z,http://arxiv.org/abs/2412.19423v1,"Jiaxin Gao, Wenbo Hu, Yuntian Chen"
"Gx2Mol: De Novo Generation of Hit-like Molecules from Gene Expression
  Profiles via Deep Learning","De novo generation of hit-like molecules is a challenging task in the drug
discovery process. Most methods in previous studies learn the semantics and
syntax of molecular structures by analyzing molecular graphs or simplified
molecular input line entry system (SMILES) strings; however, they do not take
into account the drug responses of the biological systems consisting of genes
and proteins. In this study we propose a deep generative model, Gx2Mol, which
utilizes gene expression profiles to generate molecular structures with
desirable phenotypes for arbitrary target proteins. In the algorithm, a
variational autoencoder is employed as a feature extractor to learn the latent
feature distribution of the gene expression profiles. Then, a long short-term
memory is leveraged as the chemical generator to produce syntactically valid
SMILES strings that satisfy the feature conditions of the gene expression
profile extracted by the feature extractor. Experimental results and case
studies demonstrate that the proposed Gx2Mol model can produce new molecules
with potential bioactivities and drug-like properties.",2024-12-27T03:16:56Z,http://arxiv.org/abs/2412.19422v1,"Chen Li, Yuki Matsukiyo, Yoshihiro Yamanishi"
"Adiabatic topological passage based on coupling of giant atom with two
  Su-Schrieffer-Heeger chains","We study an adiabatic topological passage of two Su-Schrieffer-Heeger (SSH)
chains mediated by a giant atom. When two finite SSH chains are in the
topological phase and the frequency of the giant atom is equal to the center
frequency of the SSH chains, the system is reduced to a subsystem that
describes the coupling of a giant atom to the edge states of two SSH chains. In
this case, we can find dark states that act as adiabatic topological passages.
This allows us to adiabatically transfer excitations of the giant atom to
either one end of two SSH chains in a fully controllable way. In addition, we
show good robustness of the adiabatic topological passages to both giant atom
frequency mismatch and the coupling disorders in two SSH chains. Our study
provides a method to realize quantum information processing and fabricate
quantum optical devices based on the coupling of the giant atom to topological
matter.",2024-12-27T03:15:49Z,http://arxiv.org/abs/2412.19421v1,"Da-Wei Wang, Ling Zhou, Yu-xi Liu"
"Generalized Uncertainty-Based Evidential Fusion with Hybrid Multi-Head
  Attention for Weak-Supervised Temporal Action Localization","Weakly supervised temporal action localization (WS-TAL) is a task of
targeting at localizing complete action instances and categorizing them with
video-level labels. Action-background ambiguity, primarily caused by background
noise resulting from aggregation and intra-action variation, is a significant
challenge for existing WS-TAL methods. In this paper, we introduce a hybrid
multi-head attention (HMHA) module and generalized uncertainty-based evidential
fusion (GUEF) module to address the problem. The proposed HMHA effectively
enhances RGB and optical flow features by filtering redundant information and
adjusting their feature distribution to better align with the WS-TAL task.
Additionally, the proposed GUEF adaptively eliminates the interference of
background noise by fusing snippet-level evidences to refine uncertainty
measurement and select superior foreground feature information, which enables
the model to concentrate on integral action instances to achieve better action
localization and classification performance. Experimental results conducted on
the THUMOS14 dataset demonstrate that our method outperforms state-of-the-art
methods. Our code is available in
\url{https://github.com/heyuanpengpku/GUEF/tree/main}.",2024-12-27T03:04:57Z,http://arxiv.org/abs/2412.19418v1,"Yuanpeng He, Lijian Li, Tianxiang Zhan, Wenpin Jiao, Chi-Man Pun"
KALAHash: Knowledge-Anchored Low-Resource Adaptation for Deep Hashing,"Deep hashing has been widely used for large-scale approximate nearest
neighbor search due to its storage and search efficiency. However, existing
deep hashing methods predominantly rely on abundant training data, leaving the
more challenging scenario of low-resource adaptation for deep hashing
relatively underexplored. This setting involves adapting pre-trained models to
downstream tasks with only an extremely small number of training samples
available. Our preliminary benchmarks reveal that current methods suffer
significant performance degradation due to the distribution shift caused by
limited training samples. To address these challenges, we introduce
Class-Calibration LoRA (CLoRA), a novel plug-and-play approach that dynamically
constructs low-rank adaptation matrices by leveraging class-level textual
knowledge embeddings. CLoRA effectively incorporates prior class knowledge as
anchors, enabling parameter-efficient fine-tuning while maintaining the
original data distribution. Furthermore, we propose Knowledge-Guided Discrete
Optimization (KIDDO), a framework to utilize class knowledge to compensate for
the scarcity of visual information and enhance the discriminability of hash
codes. Extensive experiments demonstrate that our proposed method, Knowledge-
Anchored Low-Resource Adaptation Hashing (KALAHash), significantly boosts
retrieval performance and achieves a 4x data efficiency in low-resource
scenarios.",2024-12-27T03:04:54Z,http://arxiv.org/abs/2412.19417v1,"Shu Zhao, Tan Yu, Xiaoshuai Hao, Wenchao Ma, Vijaykrishnan Narayanan"
MINIMA: Modality Invariant Image Matching,"Image matching for both cross-view and cross-modality plays a critical role
in multimodal perception. In practice, the modality gap caused by different
imaging systems/styles poses great challenges to the matching task. Existing
works try to extract invariant features for specific modalities and train on
limited datasets, showing poor generalization. In this paper, we present
MINIMA, a unified image matching framework for multiple cross-modal cases.
Without pursuing fancy modules, our MINIMA aims to enhance universal
performance from the perspective of data scaling up. For such purpose, we
propose a simple yet effective data engine that can freely produce a large
dataset containing multiple modalities, rich scenarios, and accurate matching
labels. Specifically, we scale up the modalities from cheap but rich RGB-only
matching data, by means of generative models. Under this setting, the matching
labels and rich diversity of the RGB dataset are well inherited by the
generated multimodal data. Benefiting from this, we construct MD-syn, a new
comprehensive dataset that fills the data gap for general multimodal image
matching. With MD-syn, we can directly train any advanced matching pipeline on
randomly selected modality pairs to obtain cross-modal ability. Extensive
experiments on in-domain and zero-shot matching tasks, including $19$
cross-modal cases, demonstrate that our MINIMA can significantly outperform the
baselines and even surpass modality-specific methods. The dataset and code are
available at https://github.com/LSXI7/MINIMA .",2024-12-27T02:39:50Z,http://arxiv.org/abs/2412.19412v1,"Xingyu Jiang, Jiangwei Ren, Zizhuo Li, Xin Zhou, Dingkang Liang, Xiang Bai"
"MLLM-SUL: Multimodal Large Language Model for Semantic Scene
  Understanding and Localization in Traffic Scenarios","Multimodal large language models (MLLMs) have shown satisfactory effects in
many autonomous driving tasks. In this paper, MLLMs are utilized to solve joint
semantic scene understanding and risk localization tasks, while only relying on
front-view images. In the proposed MLLM-SUL framework, a dual-branch visual
encoder is first designed to extract features from two resolutions, and rich
visual information is conducive to the language model describing risk objects
of different sizes accurately. Then for the language generation, LLaMA model is
fine-tuned to predict scene descriptions, containing the type of driving
scenario, actions of risk objects, and driving intentions and suggestions of
ego-vehicle. Ultimately, a transformer-based network incorporating a regression
token is trained to locate the risk objects. Extensive experiments on the
existing DRAMA-ROLISP dataset and the extended DRAMA-SRIS dataset demonstrate
that our method is efficient, surpassing many state-of-the-art image-based and
video-based methods. Specifically, our method achieves 80.1% BLEU-1 score and
298.5% CIDEr score in the scene understanding task, and 59.6% accuracy in the
localization task. Codes and datasets are available at
https://github.com/fjq-tongji/MLLM-SUL.",2024-12-27T02:05:38Z,http://arxiv.org/abs/2412.19406v1,"Jiaqi Fan, Jianhua Wu, Jincheng Gao, Jianhao Yu, Yafei Wang, Hongqing Chu, Bingzhao Gao"
"Online distributed algorithms for mixed equilibrium problems in dynamic
  environments","In this paper, the mixed equilibrium problem with coupled inequality
constraints in dynamic environments is solved by employing a multi-agent
system, where each agent only has access to its own bifunction, its own
constraint function, and can only communicate with its immediate neighbors via
a time-varying digraph. At each time, the goal of agents is to cooperatively
find a point in the constraint set such that the sum of local bifunctions with
a free variable is non-negative. Different from existing works, here the
bifunctions and the constraint functions are time-varying and only available to
agents after decisions are made. To tackle this problem, first, an online
distributed algorithm involving accurate gradient information is proposed based
on mirror descent algorithms and primal-dual strategies. Of particular interest
is that dynamic regrets, whose offline benchmarks are to find the solution at
each time, are employed to measure the performance of the algorithm. Under mild
assumptions on the graph and the bifunctions, we prove that if the deviation in
the solution sequence grows within a certain rate, then both the dynamic regret
and the violation of coupled inequality constraints increase sublinearly.
Second, considering the case where each agent only has access to a noisy
estimate on the accurate gradient, we propose an online distributed algorithm
involving the stochastic gradients. The result shows that under the same
conditions as in the first case, if the noise distribution satisfies the
sub-Gaussian condition, then dynamic regrets, as well as constraint violations,
increase sublinearly with high probability. Finally, several simulation
examples are presented to corroborate the validity of our results.",2024-12-27T01:26:26Z,http://arxiv.org/abs/2412.19399v1,"Hang Xu, Kaihong Lu, Yu-Long Wang, Qixin Zhu"
"A Generalized Einstein Relation for Markovian Friction Coefficients from
  Molecular Trajectories","We present a generalized Einstein relation for the friction coefficients
associated with an underlying memory kernel in terms of observable time
correlation functions. There is considerable freedom in the correlations
involved, and this allows the expression to be tailored to the particular
system to achieve numerical stability. We demonstrate this by recovering the
site-specific friction coefficients from trajectories of a freely diffusing
model trimer, and we show that the accuracy is greatly improved over
established Volterra inversion methods for kernel extraction.",2024-12-27T01:20:37Z,http://arxiv.org/abs/2412.19398v1,"J. M. Hall, M. G. Guenza"
"Comparing Few to Rank Many: Active Human Preference Learning using
  Randomized Frank-Wolfe","We study learning of human preferences from a limited comparison feedback.
This task is ubiquitous in machine learning. Its applications such as
reinforcement learning from human feedback, have been transformational. We
formulate this problem as learning a Plackett-Luce model over a universe of $N$
choices from $K$-way comparison feedback, where typically $K \ll N$. Our
solution is the D-optimal design for the Plackett-Luce objective. The design
defines a data logging policy that elicits comparison feedback for a small
collection of optimally chosen points from all ${N \choose K}$ feasible
subsets. The main algorithmic challenge in this work is that even fast methods
for solving D-optimal designs would have $O({N \choose K})$ time complexity. To
address this issue, we propose a randomized Frank-Wolfe (FW) algorithm that
solves the linear maximization sub-problems in the FW method on randomly chosen
variables. We analyze the algorithm, and evaluate it empirically on synthetic
and open-source NLP datasets.",2024-12-27T01:10:17Z,http://arxiv.org/abs/2412.19396v1,"Kiran Koshy Thekumparampil, Gaurush Hiranandani, Kousha Kalantari, Shoham Sabach, Branislav Kveton"
"BeSplat -- Gaussian Splatting from a Single Blurry Image and Event
  Stream","Novel view synthesis has been greatly enhanced by the development of radiance
field methods. The introduction of 3D Gaussian Splatting (3DGS) has effectively
addressed key challenges, such as long training times and slow rendering
speeds, typically associated with Neural Radiance Fields (NeRF), while
maintaining high-quality reconstructions. In this work (BeSplat), we
demonstrate the recovery of sharp radiance field (Gaussian splats) from a
single motion-blurred image and its corresponding event stream. Our method
jointly learns the scene representation via Gaussian Splatting and recovers the
camera motion through Bezier SE(3) formulation effectively, minimizing
discrepancies between synthesized and real-world measurements of both blurry
image and corresponding event stream. We evaluate our approach on both
synthetic and real datasets, showcasing its ability to render view-consistent,
sharp images from the learned radiance field and the estimated camera
trajectory. To the best of our knowledge, ours is the first work to address
this highly challenging ill-posed problem in a Gaussian Splatting framework
with the effective incorporation of temporal information captured using the
event stream.",2024-12-26T22:35:29Z,http://arxiv.org/abs/2412.19370v1,"Gopi Raju Matta, Reddypalli Trisha, Kaushik Mitra"
Neuromorphic Dual-channel Encoding of Luminance and Contrast,"There is perceptual and physiological evidence that the retina registers and
signals luminance and luminance contrast using dual-channel mechanisms. This
process begins in the retina, wherein the luminance of a uniform zone and
differentials of luminance in neighboring zones determine the degree of
brightness or darkness of the zones. The neurons that process the information
can be classified as ""bright"" or ""dark"" channels. The present paper provides an
overview of these retinal mechanisms along with evidence that they provide
brightness judgments that are log-linear across roughly seven orders of
magnitude.",2024-12-26T22:09:30Z,http://arxiv.org/abs/2412.19365v1,Ernest Greene
"Evaluating Convolutional Neural Networks for COVID-19 classification in
  chest X-ray images","Coronavirus Disease 2019 (COVID-19) pandemic rapidly spread globally,
impacting the lives of billions of people. The effective screening of infected
patients is a critical step to struggle with COVID-19, and treating the
patients avoiding this quickly disease spread. The need for automated and
scalable methods has increased due to the unavailability of accurate automated
toolkits. Recent researches using chest X-ray images suggest they include
relevant information about the COVID-19 virus. Hence, applying machine learning
techniques combined with radiological imaging promises to identify this disease
accurately. It is straightforward to collect these images once it is spreadly
shared and analyzed in the world. This paper presents a method for automatic
COVID-19 detection using chest Xray images through four convolutional neural
networks, namely: AlexNet, VGG-11, SqueezeNet, and DenseNet-121. This method
had been providing accurate diagnostics for positive or negative COVID-19
classification. We validate our experiments using a ten-fold cross-validation
procedure over the training and test sets. Our findings include the shallow
fine-tuning and data augmentation strategies that can assist in dealing with
the low number of positive COVID-19 images publicly available. The accuracy for
all CNNs is higher than 97.00%, and the SqueezeNet model achieved the best
result with 99.20%.",2024-12-26T22:05:30Z,http://arxiv.org/abs/2412.19362v1,"Leonardo Gabriel Ferreira Rodrigues, Danilo Ferreira da Silva, Larissa Ferreira Rodrigues, João Fernando Mari"
Microscopic imprints of learned solutions in adaptive resistor networks,"In physical networks trained using supervised learning, physical parameters
are adjusted to produce desired responses to inputs. An example is electrical
contrastive local learning networks of nodes connected by edges that are
resistors that adjust their conductances during training. When an edge
conductance changes, it upsets the current balance of every node. In response,
physics adjusts the node voltages to minimize the dissipated power. Learning in
these systems is therefore a coupled double-optimization process, in which the
network descends both a cost landscape in the high-dimensional space of edge
conductances, and a physical landscape -- the power -- in the high-dimensional
space of node voltages. Because of this coupling, the physical landscape of a
trained network contains information about the learned task. Here we
demonstrate that all the physical information relevant to the trained
input-output relation can be captured by a susceptibility, an experimentally
measurable quantity. We supplement our theoretical results with simulations to
show that the susceptibility is positively correlated with functional
importance and that we can extract physical insight into how the system
performs the task from the conductances of highly susceptible edges.",2024-12-26T21:36:23Z,http://arxiv.org/abs/2412.19356v1,"Marcel Guzman, Felipe Martins, Menachem Stern, Andrea J. Liu"
"Experimental demonstration and modeling of near-infrared nonlinear
  third-order triple-photon generation stimulated over one mode","Triple Photon Generation (TPG) is a third-order nonlinear optical interaction
in which a photon, i.e. the pump, splits into three lower energy photons, i.e.
modes 1, 2 and 3. The triplets possess different quantum signatures from those
of photon pairs, with a strong interest in quantum information. In the present
study, we performed the first experimental demonstration of TPG stimulated over
one mode of the triplet, mode 1, the previous work on TPG concerning
stimulation over two modes,2 and 3. The nonlinear medium is a KTiOPO4 crystal
pumped in the picosecond regime (15 ps, 10 Hz) at a pump wavelength of 532 nm.
The stimulation beam is emitted by a tunable optical parametric generator: the
phase-matching was found at a stimulation wavelength of 1491 nm, the other two
modes of the triplet being both at 1654 nm in orthogonal polarizations. Using
superconducting nanowires single photon detectors, the measurement of the
polarizations and wavelength signatures of the two generated modes are in full
agreement with calculations. It has been possible to generate a total number of
photons per pulse on modes 2 and 3 up to 2x10-4, which corresponds to the
generation of 10-4 triplets per pulse, or 10-5 triplets per second since the
repetition rate is equal to 10 Hz. We interpreted these results in the
framework of a model we developed on the basis of the nonlinear momentum
operator in the Heisenberg representation under the undepleted pump and
stimulation approximation.",2024-12-26T20:42:17Z,http://arxiv.org/abs/2412.19348v1,"Julien Bertrand, Veronique Boutou, Corinne Felix, David Jegouso, Benoit Boulanger"
"Semi-Supervised Learning from Small Annotated Data and Large Unlabeled
  Data for Fine-grained PICO Entity Recognition","Objective: Extracting PICO elements -- Participants, Intervention,
Comparison, and Outcomes -- from clinical trial literature is essential for
clinical evidence retrieval, appraisal, and synthesis. Existing approaches do
not distinguish the attributes of PICO entities. This study aims to develop a
named entity recognition (NER) model to extract PICO entities with fine
granularities.
  Materials and Methods: Using a corpus of 2,511 abstracts with PICO mentions
from 4 public datasets, we developed a semi-supervised method to facilitate the
training of a NER model, FinePICO, by combining limited annotated data of PICO
entities and abundant unlabeled data. For evaluation, we divided the entire
dataset into two subsets: a smaller group with annotations and a larger group
without annotations. We then established the theoretical lower and upper
performance bounds based on the performance of supervised learning models
trained solely on the small, annotated subset and on the entire set with
complete annotations, respectively. Finally, we evaluated FinePICO on both the
smaller annotated subset and the larger, initially unannotated subset. We
measured the performance of FinePICO using precision, recall, and F1.
  Results: Our method achieved precision/recall/F1 of 0.567/0.636/0.60,
respectively, using a small set of annotated samples, outperforming the
baseline model (F1: 0.437) by more than 16\%. The model demonstrates
generalizability to a different PICO framework and to another corpus, which
consistently outperforms the benchmark in diverse experimental settings
(p-value \textless0.001).
  Conclusion: This study contributes a generalizable and effective
semi-supervised approach to named entity recognition leveraging large unlabeled
data together with small, annotated data. It also initially supports
fine-grained PICO extraction.",2024-12-26T20:24:35Z,http://arxiv.org/abs/2412.19346v1,"Fangyi Chen, Gongbo Zhang, Yilu Fang, Yifan Peng, Chunhua Weng"
"Sparse recovery from quadratic equations, part II: hardness and
  incoherence","We study the square root bottleneck in the recovery of sparse vectors from
quadratic equations. It is acknowledged that a sparse vector $ \mathbf x_0\in
\mathbb{R}^n$, $\| \mathbf x_0\|_0 = k$ can in theory be recovered from as few
as $O(k)$ generic quadratic equations but no polynomial time algorithm is known
for this task unless $m = \Omega(k^2)$. This bottleneck was in fact shown in
previous work to be essentially related to the initialization of descent
algorithms. Starting such algorithms sufficiently close to the planted signal
is known to imply convergence to this signal. In this paper, we show that as
soon as $m\gtrsim \mu_0^{-2}k \vee \mu_0^{-4}$ (up to log factors) where $\mu_0
= \| \mathbf x_0\|_\infty/\| \mathbf x_0\|_2$, it is possible to recover a
$k$-sparse vector $ \mathbf x_0\in \mathbb{R}^n$ from $m$ quadratic equations
of the form $\langle \mathbf A_i, \mathbf x \mathbf x^\intercal\rangle =
\langle \mathbf A_i, \mathbf x_0 \mathbf x_0^\intercal\rangle + \varepsilon_i $
by minimizing the classical empirical loss. The proof idea carries over to the
phase retrieval setting for which it provides an original initialization that
matches the current optimal sample complexity (see e.g. [Cai 2023]). In the
maximally incoherent regime $\mu_0^{-2}=k$, and for $m=o(k^2)$ we provide
evidence for topological hardness by showing that a property known as the
Overlap Gap Property (OGP), which originated in spin glass theory and is
conjectured to be indicative of algorithmic intractability when optimizing over
random structures, holds for a particular level of overparametrization. The key
ingredient of the proof is a lower bound on the tail of chi-squared random
variables which follows from the theory of moderate deviations.",2024-12-26T20:11:44Z,http://arxiv.org/abs/2412.19341v1,Augustin Cosse
"CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language
  Models","Recent advances in Large Vision-Language Models (LVLMs) have sparked
significant progress in general-purpose vision tasks through visual instruction
tuning. While some works have demonstrated the capability of LVLMs to generate
segmentation masks that align phrases with natural language descriptions in a
single image, they struggle with segmentation-grounded comparisons across
multiple images, particularly at finer granularities such as object parts. In
this paper, we introduce the new task of part-focused semantic co-segmentation,
which seeks to identify and segment common and unique objects and parts across
images. To address this task, we present CALICO, the first LVLM that can
segment and reason over multiple masks across images, enabling object
comparison based on their constituent parts. CALICO features two proposed
components, a novel Correspondence Extraction Module, which captures
semantic-rich information to identify part-level correspondences between
objects, and a Correspondence Adaptation Module, which embeds this information
into the LVLM to facilitate multi-image understanding in a parameter-efficient
manner. To support training and evaluation, we curate MixedParts, a
comprehensive multi-image segmentation dataset containing $\sim$2.4M samples
across $\sim$44K images with diverse object and part categories. Experimental
results show CALICO, finetuned on only 0.3% of its architecture, achieves
robust performance in part-focused semantic co-segmentation.",2024-12-26T18:59:37Z,http://arxiv.org/abs/2412.19331v1,"Kiet A. Nguyen, Adheesh Juvekar, Tianjiao Yu, Muntasir Wahed, Ismini Lourentzou"
"Resolving the Ambiguity of Complete-to-Partial Point Cloud Registration
  for Image-Guided Liver Surgery with Patches-to-Partial Matching","In image-guided liver surgery, the initial rigid alignment between
preoperative and intraoperative data, often represented as point clouds, is
crucial for providing sub-surface information from preoperative CT/MRI images
to the surgeon during the procedure. Currently, this alignment is typically
performed using semi-automatic methods, which, while effective to some extent,
are prone to errors that demand manual correction. Point cloud
correspondence-based registration methods are promising to serve as a fully
automatic solution. However, they may struggle in scenarios with limited
intraoperative surface visibility, a common challenge in liver surgery,
particularly in laparoscopic procedures, which we refer to as
complete-to-partial ambiguity. We first illustrate this ambiguity by evaluating
the performance of state-of-the-art learning-based point cloud registration
methods on our carefully constructed in silico and in vitro datasets. Then, we
propose a patches-to-partial matching strategy as a plug-and-play module to
resolve the ambiguity, which can be seamlessly integrated into learning-based
registration methods without disrupting their end-to-end structure. It has
proven effective and efficient in improving registration performance for cases
with limited intraoperative visibility. The constructed benchmark and the
proposed module establish a solid foundation for advancing applications of
point cloud correspondence-based registration methods in image-guided liver
surgery.",2024-12-26T18:58:29Z,http://arxiv.org/abs/2412.19328v1,"Zixin Yang, Jon S. Heiselman, Cheng Han, Kelly Merrell, Richard Simon, Cristian. A. Linte"
"A novel framework for MCDM based on Z numbers and soft likelihood
  function","The optimization on the structure of process of information management under
uncertain environment has attracted lots of attention from researchers around
the world. Nevertheless, how to obtain accurate and rational evaluation from
assessments produced by experts is still an open problem. Specially,
intuitionistic fuzzy set provides an effective solution in handling
indeterminate information. And Yager proposes a novel method for fusion of
probabilistic evidence to handle uncertain and conflicting information lately
which is called soft likelihood function. This paper devises a novel framework
of soft likelihood function based on information volume of fuzzy membership and
credibility measure for extracting truly useful and valuable information from
uncertainty. An application is provided to verify the validity and correctness
of the proposed framework. Besides, the comparisons with other existing methods
further demonstrate the superiority of the novel framework of soft likelihood
function.",2024-12-26T18:47:19Z,http://arxiv.org/abs/2412.19321v1,Yuanpeng He
Thermodynamic reduction of contact dynamics,"A universal algorithm to derive a macroscopic dynamics from the microscopic
dynamical system via the averaging process and symplecto-contact reduction was
introduced by Jin-wook Lim and the second-named author in [LO23]. They apply
the algorithm to derive non-equilibrium thermodynamics from the statistical
mechanics utilizing the relative information entropy as a generating function
of the associated thermodynamic equilibrium. In the present paper, we apply
this algorithm to the contact Hamiltonian dynamical systems. We describe a
procedure of obtaining a discrete set of dynamical invariants of the given
contact Hamiltonian system, or more generally of a contact multi-Hamiltonian
system in a canonical way by deriving a (finite-dimensional non-equilibrium)
thermodynamic system. We call this reduction the thermodynamic reduction of
contact dynamics.",2024-12-26T18:42:11Z,http://arxiv.org/abs/2412.19319v1,"Hyun-Seok Do, Yong-Geun Oh"
"From Interets to Insights: An LLM Approach to Course Recommendations
  Using Natural Language Queries","Most universities in the United States encourage their students to explore
academic areas before declaring a major and to acquire academic breadth by
satisfying a variety of requirements. Each term, students must choose among
many thousands of offerings, spanning dozens of subject areas, a handful of
courses to take. The curricular environment is also dynamic, and poor
communication and search functions on campus can limit a student's ability to
discover new courses of interest. To support both students and their advisers
in such a setting, we explore a novel Large Language Model (LLM) course
recommendation system that applies a Retrieval Augmented Generation (RAG)
method to the corpus of course descriptions. The system first generates an
'ideal' course description based on the user's query. This description is
converted into a search vector using embeddings, which is then used to find
actual courses with similar content by comparing embedding similarities. We
describe the method and assess the quality and fairness of some example
prompts. Steps to deploy a pilot system on campus are discussed.",2024-12-26T18:19:53Z,http://arxiv.org/abs/2412.19312v1,"Hugh Van Deventer, Mark Mills, August Evrard"
"Perceive, Query &amp; Reason: Enhancing Video QA with Question-Guided
  Temporal Queries","Video Question Answering (Video QA) is a challenging video understanding task
that requires models to comprehend entire videos, identify the most relevant
information based on contextual cues from a given question, and reason
accurately to provide answers. Recent advancements in Multimodal Large Language
Models (MLLMs) have transformed video QA by leveraging their exceptional
commonsense reasoning capabilities. This progress is largely driven by the
effective alignment between visual data and the language space of MLLMs.
However, for video QA, an additional space-time alignment poses a considerable
challenge for extracting question-relevant information across frames. In this
work, we investigate diverse temporal modeling techniques to integrate with
MLLMs, aiming to achieve question-guided temporal modeling that leverages
pre-trained visual and textual alignment in MLLMs. We propose T-Former, a novel
temporal modeling method that creates a question-guided temporal bridge between
frame-wise visual perception and the reasoning capabilities of LLMs. Our
evaluation across multiple video QA benchmarks demonstrates that T-Former
competes favorably with existing temporal modeling approaches and aligns with
recent advancements in video QA.",2024-12-26T17:53:14Z,http://arxiv.org/abs/2412.19304v1,"Roberto Amoroso, Gengyuan Zhang, Rajat Koner, Lorenzo Baraldi, Rita Cucchiara, Volker Tresp"
Manga Generation via Layout-controllable Diffusion,"Generating comics through text is widely studied. However, there are few
studies on generating multi-panel Manga (Japanese comics) solely based on plain
text. Japanese manga contains multiple panels on a single page, with
characteristics such as coherence in storytelling, reasonable and diverse page
layouts, consistency in characters, and semantic correspondence between panel
drawings and panel scripts. Therefore, generating manga poses a significant
challenge. This paper presents the manga generation task and constructs the
Manga109Story dataset for studying manga generation solely from plain text.
Additionally, we propose MangaDiffusion to facilitate the intra-panel and
inter-panel information interaction during the manga generation process. The
results show that our method particularly ensures the number of panels,
reasonable and diverse page layouts. Based on our approach, there is potential
to converting a large amount of textual stories into more engaging manga
readings, leading to significant application prospects.",2024-12-26T17:52:19Z,http://arxiv.org/abs/2412.19303v1,"Siyu Chen, Dengjie Li, Zenghao Bao, Yao Zhou, Lingfeng Tan, Yujie Zhong, Zheng Zhao"
RecLM: Recommendation Instruction Tuning,"Modern recommender systems aim to deeply understand users' complex
preferences through their past interactions. While deep collaborative filtering
approaches using Graph Neural Networks (GNNs) excel at capturing user-item
relationships, their effectiveness is limited when handling sparse data or
zero-shot scenarios, primarily due to constraints in ID-based embedding
functions. To address these challenges, we propose a model-agnostic
recommendation instruction-tuning paradigm that seamlessly integrates large
language models with collaborative filtering. Our proposed Recommendation
Language Model (RecLM) enhances the capture of user preference diversity
through a carefully designed reinforcement learning reward function that
facilitates self-augmentation of language models. Comprehensive evaluations
demonstrate significant advantages of our approach across various settings, and
its plug-and-play compatibility with state-of-the-art recommender systems
results in notable performance enhancements.",2024-12-26T17:51:54Z,http://arxiv.org/abs/2412.19302v1,"Yangqin Jiang, Yuhao Yang, Lianghao Xia, Da Luo, Kangyi Lin, Chao Huang"
RAG with Differential Privacy,"Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to
provide *Large Language Models* (LLM) with fresh and relevant context,
mitigating the risk of hallucinations and improving the overall quality of
responses in environments with large and fast moving knowledge bases. However,
the integration of external documents into the generation process raises
significant privacy concerns. Indeed, when added to a prompt, it is not
possible to guarantee a response will not inadvertently expose confidential
data, leading to potential breaches of privacy and ethical dilemmas. This paper
explores a practical solution to this problem suitable to general knowledge
extraction from personal data. It shows *differentially private token
generation* is a viable approach to private RAG.",2024-12-26T17:34:26Z,http://arxiv.org/abs/2412.19291v1,Nicolas Grislain
"ViPCap: Retrieval Text-Based Visual Prompts for Lightweight Image
  Captioning","Recent lightweight image captioning models using retrieved data mainly focus
on text prompts. However, previous works only utilize the retrieved text as
text prompts, and the visual information relies only on the CLIP visual
embedding. Because of this issue, there is a limitation that the image
descriptions inherent in the prompt are not sufficiently reflected in the
visual embedding space. To tackle this issue, we propose ViPCap, a novel
retrieval text-based visual prompt for lightweight image captioning. ViPCap
leverages the retrieved text with image information as visual prompts to
enhance the ability of the model to capture relevant visual information. By
mapping text prompts into the CLIP space and generating multiple randomized
Gaussian distributions, our method leverages sampling to explore randomly
augmented distributions and effectively retrieves the semantic features that
contain image information. These retrieved features are integrated into the
image and designated as the visual prompt, leading to performance improvements
on the datasets such as COCO, Flickr30k, and NoCaps. Experimental results
demonstrate that ViPCap significantly outperforms prior lightweight captioning
models in efficiency and effectiveness, demonstrating the potential for a
plug-and-play solution.",2024-12-26T17:29:38Z,http://arxiv.org/abs/2412.19289v1,"Taewhan Kim, Soeun Lee, Si-Woo Kim, Dong-Jin Kim"
"Thermal amplification and melting of phases in spin-orbit-coupled spin-1
  Bose-Einstein condensates","We implement Hartree-Fock-Bogoliubov theory with Popov approximation for a
homogeneous Raman-induced spin-orbit-coupled spin-1 Bose-Einstein condensate
and investigate the effects of finite temperature ($T$) on the ground-state
phase diagram. We calculate the roton gap as a function of Raman coupling
($\Omega$) or quadratic Zeeman field strength ($\epsilon$) to extract the
critical points separating the supersolid stripe phase from the plane wave or
zero-momentum phase at finite temperatures. We present a few representative
finite-temperature phase diagrams for the system in the $T-\Omega$ and
$T-\epsilon$ planes. Our observations indicate that the supersolid stripe phase
melts at finite temperatures. We also discuss the contrasting roles of quantum
and thermal fluctuations in shifting the phase boundary separating the
supersolid stripe from the plane-wave phase.",2024-12-26T17:03:05Z,http://arxiv.org/abs/2412.19285v1,"Ritu, Rajat, Arko Roy, Sandeep Gautam"
Improving Generalization for AI-Synthesized Voice Detection,"AI-synthesized voice technology has the potential to create realistic human
voices for beneficial applications, but it can also be misused for malicious
purposes. While existing AI-synthesized voice detection models excel in
intra-domain evaluation, they face challenges in generalizing across different
domains, potentially becoming obsolete as new voice generators emerge. Current
solutions use diverse data and advanced machine learning techniques (e.g.,
domain-invariant representation, self-supervised learning), but are limited by
predefined vocoders and sensitivity to factors like background noise and
speaker identity. In this work, we introduce an innovative disentanglement
framework aimed at extracting domain-agnostic artifact features related to
vocoders. Utilizing these features, we enhance model learning in a flat loss
landscape, enabling escape from suboptimal solutions and improving
generalization. Extensive experiments on benchmarks show our approach
outperforms state-of-the-art methods, achieving up to 5.12% improvement in the
equal error rate metric in intra-domain and 7.59% in cross-domain evaluations.",2024-12-26T16:45:20Z,http://arxiv.org/abs/2412.19279v1,"Hainan Ren, Lin Li, Chun-Hao Liu, Xin Wang, Shu Hu"
Optimizing Multi-Stage Language Models for Effective Text Retrieval,"Efficient text retrieval is critical for applications such as legal document
analysis, particularly in specialized contexts like Japanese legal systems.
Existing retrieval methods often underperform in such domain-specific
scenarios, necessitating tailored approaches. In this paper, we introduce a
novel two-phase text retrieval pipeline optimized for Japanese legal datasets.
Our method leverages advanced language models to achieve state-of-the-art
performance, significantly improving retrieval efficiency and accuracy. To
further enhance robustness and adaptability, we incorporate an ensemble model
that integrates multiple retrieval strategies, resulting in superior outcomes
across diverse tasks. Extensive experiments validate the effectiveness of our
approach, demonstrating strong performance on both Japanese legal datasets and
widely recognized benchmarks like MS-MARCO. Our work establishes new standards
for text retrieval in domain-specific and general contexts, providing a
comprehensive solution for addressing complex queries in legal and multilingual
environments.",2024-12-26T16:05:19Z,http://arxiv.org/abs/2412.19265v1,"Quang Hoang Trung, Le Trung Hoang, Nguyen Van Hoang Phuc"
"VoiceDiT: Dual-Condition Diffusion Transformer for Environment-Aware
  Speech Synthesis","We present VoiceDiT, a multi-modal generative model for producing
environment-aware speech and audio from text and visual prompts. While aligning
speech with text is crucial for intelligible speech, achieving this alignment
in noisy conditions remains a significant and underexplored challenge in the
field. To address this, we present a novel audio generation pipeline named
VoiceDiT. This pipeline includes three key components: (1) the creation of a
large-scale synthetic speech dataset for pre-training and a refined real-world
speech dataset for fine-tuning, (2) the Dual-DiT, a model designed to
efficiently preserve aligned speech information while accurately reflecting
environmental conditions, and (3) a diffusion-based Image-to-Audio Translator
that allows the model to bridge the gap between audio and image, facilitating
the generation of environmental sound that aligns with the multi-modal prompts.
Extensive experimental results demonstrate that VoiceDiT outperforms previous
models on real-world datasets, showcasing significant improvements in both
audio quality and modality integration.",2024-12-26T15:52:58Z,http://arxiv.org/abs/2412.19259v1,"Jaemin Jung, Junseok Ahn, Chaeyoung Jung, Tan Dat Nguyen, Youngjoon Jang, Joon Son Chung"
"Leveraging Self-Training and Variational Autoencoder for Agitation
  Detection in People with Dementia Using Wearable Sensors","Dementia is a neurodegenerative disorder that has been growing among elder
people over the past decades. This growth profoundly impacts the quality of
life for patients and caregivers due to the symptoms arising from it. Agitation
and aggression (AA) are some of the symptoms of people with severe dementia
(PwD) in long-term care or hospitals. AA not only causes discomfort but also
puts the patients or others at potential risk. Existing monitoring solutions
utilizing different wearable sensors integrated with Artificial Intelligence
(AI) offer a way to detect AA early enough for timely and adequate medical
intervention. However, most studies are limited by the availability of
accurately labeled datasets, which significantly affects the efficacy of such
solutions in real-world scenarios. This study presents a novel comprehensive
approach to detect AA in PwD using physiological data from the Empatica E4
wristbands. The research creates a diverse dataset, consisting of three
distinct datasets gathered from 14 participants across multiple hospitals in
Canada. These datasets have not been extensively explored due to their limited
labeling. We propose a novel approach employing self-training and a variational
autoencoder (VAE) to detect AA in PwD effectively. The proposed approach aims
to learn the representation of the features extracted using the VAE and then
uses a semi-supervised block to generate labels, classify events, and detect
AA. We demonstrate that combining Self-Training and Variational Autoencoder
mechanism significantly improves model performance in classifying AA in PwD.
Among the tested techniques, the XGBoost classifier achieved the highest
accuracy of 90.16\%. By effectively addressing the challenge of limited labeled
data, the proposed system not only learns new labels but also proves its
superiority in detecting AA.",2024-12-26T15:34:25Z,http://arxiv.org/abs/2412.19254v1,"Abeer Badawi, Somayya Elmoghazy, Samira Choudhury, Khalid Elgazzar, Amer Burhan"
Network double autoregression,"Modeling high-dimensional time series with simple structures is a challenging
problem. This paper proposes a network double autoregression (NDAR) model,
which combines the advantages of network structure and the double
autoregression (DAR) model, to handle high-dimensional, conditionally
heteroscedastic, and network-structured data within a simple framework. The
parameters of the model are estimated using quasi-maximum likelihood
estimation, and the asymptotic properties of the estimators are derived. The
selection of the model's lag order will be based on the Bayesian information
criterion. Finite-sample simulations show that the proposed model performs well
even with moderate time dimensions and network sizes. Finally, the model is
applied to analyze three different categories of stock data.",2024-12-26T15:28:41Z,http://arxiv.org/abs/2412.19251v1,"Tingting Li, Hao Wang"
"Causal Speech Enhancement with Predicting Semantics based on Quantized
  Self-supervised Learning Features","Real-time speech enhancement (SE) is essential to online speech
communication. Causal SE models use only the previous context while predicting
future information, such as phoneme continuation, may help performing causal
SE. The phonetic information is often represented by quantizing latent features
of self-supervised learning (SSL) models. This work is the first to incorporate
SSL features with causality into an SE model. The causal SSL features are
encoded and combined with spectrogram features using feature-wise linear
modulation to estimate a mask for enhancing the noisy input speech.
Simultaneously, we quantize the causal SSL features using vector quantization
to represent phonetic characteristics as semantic tokens. The model not only
encodes SSL features but also predicts the future semantic tokens in multi-task
learning (MTL). The experimental results using VoiceBank + DEMAND dataset show
that our proposed method achieves 2.88 in PESQ, especially with semantic
prediction MTL, in which we confirm that the semantic prediction played an
important role in causal SE.",2024-12-26T15:08:36Z,http://arxiv.org/abs/2412.19248v1,"Emiru Tsunoo, Yuki Saito, Wataru Nakata, Hiroshi Saruwatari"
Sentiment trading with large language models,"We investigate the efficacy of large language models (LLMs) in sentiment
analysis of U.S. financial news and their potential in predicting stock market
returns. We analyze a dataset comprising 965,375 news articles that span from
January 1, 2010, to June 30, 2023; we focus on the performance of various LLMs,
including BERT, OPT, FINBERT, and the traditional Loughran-McDonald dictionary
model, which has been a dominant methodology in the finance literature. The
study documents a significant association between LLM scores and subsequent
daily stock returns. Specifically, OPT, which is a GPT-3 based LLM, shows the
highest accuracy in sentiment prediction with an accuracy of 74.4%, slightly
ahead of BERT (72.5%) and FINBERT (72.2%). In contrast, the Loughran-McDonald
dictionary model demonstrates considerably lower effectiveness with only 50.1%
accuracy. Regression analyses highlight a robust positive impact of OPT model
scores on next-day stock returns, with coefficients of 0.274 and 0.254 in
different model specifications. BERT and FINBERT also exhibit predictive
relevance, though to a lesser extent. Notably, we do not observe a significant
relationship between the Loughran-McDonald dictionary model scores and stock
returns, challenging the efficacy of this traditional method in the current
financial context. In portfolio performance, the long-short OPT strategy excels
with a Sharpe ratio of 3.05, compared to 2.11 for BERT and 2.07 for FINBERT
long-short strategies. Strategies based on the Loughran-McDonald dictionary
yield the lowest Sharpe ratio of 1.23. Our findings emphasize the superior
performance of advanced LLMs, especially OPT, in financial market prediction
and portfolio management, marking a significant shift in the landscape of
financial analysis tools with implications to financial regulation and policy
analysis.",2024-12-26T15:01:24Z,http://arxiv.org/abs/2412.19245v1,"Kemal Kirtac, Guido Germano"
Functional structural equation modeling with latent variables,"Handling latent variables in Structural Equation Models (SEMs) in a case
where both the latent variables and their corresponding indicators in the
measurement error part of the model are random curves presents significant
challenges, especially with sparse data. In this paper, we develop a novel
family of Functional Structural Equation Models (FSEMs) that incorporate latent
variables modeled as Gaussian Processes (GPs). The introduced FSEMs are built
upon functional regression models having response variables modeled as
underlying GPs. The model flexibly adapts to cases when the random curves'
realizations are observed only over a sparse subset of the domain, and the
inferential framework is based on a restricted maximum likelihood approach. The
advantage of this framework lies in its ability and flexibility in handling
various data scenarios, including regularly and irregularly spaced points and
thus missing data. To extract smooth estimates for the functional parameters,
we employ a penalized likelihood approach that selects the smoothing parameters
using a cross-validation method. We evaluate the performance of the proposed
model using simulation studies and a real data example, which suggests that our
model performs well in practice. The uncertainty associated with the estimates
of the functional coefficients is also assessed by constructing confidence
regions for each estimate. The goodness of fit indices that are commonly used
to evaluate the fit of SEMs are developed for the FSEMs introduced in this
paper. Overall, the proposed method is a promising approach for modeling
functional data in SEMs with functional latent variables.",2024-12-26T14:57:14Z,http://arxiv.org/abs/2412.19242v1,"Fatemeh Asgari, Valeria Vitelli, Uta Sailer"
SeaMo: A Multi-Seasonal and Multimodal Remote Sensing Foundation Model,"Remote Sensing (RS) data contains a wealth of multi-dimensional information
crucial for Earth observation. Owing to its vast volume, diverse sources, and
temporal properties, RS data is highly suitable for the development of large
Visual Foundation Models (VFMs). VFMs act as robust feature extractors,
learning from extensive RS data, and are subsequently fine-tuned for deployment
in various geoscientific tasks. However, current VFMs in the RS domain are
predominantly pretrained and tailored exclusively for specific characteristics
of RS imagery, neglecting the potential of utilizing the multi-dimensional
properties of RS data. Therefore, in this work, we propose SeaMo, a pioneering
visual foundation model that integrates multi-seasonal and multimodal
information in the RS field. SeaMo is designed to harness multiple properties
of RS data. Within the masked image modeling framework, we employ non-aligned
cropping techniques to extract spatial properties, use multi-source inputs for
multimodal integration, and incorporate temporal-multimodal fusion blocks for
effective assimilation of multi-seasonal data. SeaMo explicitly models the
multi-dimensional properties of RS data, making the model more comprehensive,
robust, and versatile. We applied SeaMo to several downstream geoscience tasks,
which demonstrated exceptional performance. Extensive ablation studies were
conducted to validate the model's superiority.",2024-12-26T14:40:38Z,http://arxiv.org/abs/2412.19237v1,"Xuyang Li, Danfeng Hong, Chenyu Li, Jocelyn Chanussot"
"Are Two Hidden Layers Still Enough for the Physics-Informed Neural
  Networks?","The article discusses the development of various methods and techniques for
initializing and training neural networks with a single hidden layer, as well
as training a separable physics-informed neural network consisting of neural
networks with a single hidden layer to solve physical problems described by
ordinary differential equations (ODEs) and partial differential equations
(PDEs). A method for strictly deterministic initialization of a neural network
with one hidden layer for solving physical problems described by an ODE is
proposed. Modifications to existing methods for weighting the loss function are
given, as well as new methods developed for training strictly
deterministic-initialized neural networks to solve ODEs (detaching, additional
weighting based on the second derivative, predicted solution-based weighting,
relative residuals). An algorithm for physics-informed data-driven
initialization of a neural network with one hidden layer is proposed. A neural
network with pronounced generalizing properties is presented, whose
generalizing abilities of which can be precisely controlled by adjusting
network parameters. A metric for measuring the generalization of such neural
network has been introduced. A gradient-free neuron-by-neuron fitting method
has been developed for adjusting the parameters of a single-hidden-layer neural
network, which does not require the use of an optimizer or solver for its
implementation. The proposed methods have been extended to 2D problems using
the separable physics-informed neural networks approach. Numerous experiments
have been carried out to develop the above methods and approaches. Experiments
on physical problems, such as solving various ODEs and PDEs, have demonstrated
that these methods for initializing and training neural networks with one or
two hidden layers (SPINN) achieve competitive accuracy and, in some cases,
state-of-the-art results.",2024-12-26T14:30:54Z,http://arxiv.org/abs/2412.19235v1,"Vasiliy A. Es'kin, Alexey O. Malkhanov, Mikhail E. Smorkalov"
Multi-view Fake News Detection Model Based on Dynamic Hypergraph,"With the rapid development of online social networks and the inadequacies in
content moderation mechanisms, the detection of fake news has emerged as a
pressing concern for the public. Various methods have been proposed for fake
news detection, including text-based approaches as well as a series of
graph-based approaches. However, the deceptive nature of fake news renders
text-based approaches less effective. Propagation tree-based methods focus on
the propagation process of individual news, capturing pairwise relationships
but lacking the capability to capture high-order complex relationships. Large
heterogeneous graph-based approaches necessitate the incorporation of
substantial additional information beyond news text and user data, while
hypergraph-based approaches rely on predefined hypergraph structures. To tackle
these issues, we propose a novel dynamic hypergraph-based multi-view fake news
detection model (DHy-MFND) that learns news embeddings across three distinct
views: text-level, propagation tree-level, and hypergraph-level. By employing
hypergraph structures to model complex high-order relationships among multiple
news pieces and introducing dynamic hypergraph structure learning, we optimize
predefined hypergraph structures while learning news embeddings. Additionally,
we introduce contrastive learning to capture authenticity-relevant embeddings
across different views. Extensive experiments on two benchmark datasets
demonstrate the effectiveness of our proposed DHy-MFND compared with a broad
range of competing baselines.",2024-12-26T14:05:51Z,http://arxiv.org/abs/2412.19227v1,"Rongping Ye, Xiaobing Pei"
"VINEVI: A Virtualized Network Vision Architecture for Smart Monitoring
  of Heterogeneous Applications and Infrastructures","Monitoring heterogeneous infrastructures and applications is essential to
cope with user requirements properly, but it still lacks enhancements. The
well-known state-of-the-art methods and tools do not support seamless
monitoring of bare-metal, low-cost infrastructures, neither hosted nor
virtualized services with fine-grained details. This work proposes VIrtualized
NEtwork VIsion architecture (VINEVI), an intelligent method for seamless
monitoring heterogeneous infrastructures and applications. The VINEVI
architecture advances state of the art with a node-embedded traffic
classification agent placing physical and virtualized infrastructures enabling
real-time traffic classification. VINEVI combines this real-time traffic
classification with well-known tools such as Prometheus and Victoria Metrics to
monitor the entire stack from the hardware to the virtualized applications.
Experimental results showcased that VINEVI architecture allowed seamless
heterogeneous infrastructure monitoring with a higher level of detail beyond
literature. Also, our node-embedded real-time Internet traffic classifier
evolved with flexibility the methods with monitoring heterogeneous
infrastructures seamlessly.",2024-12-26T14:05:14Z,http://arxiv.org/abs/2412.19226v1,"Rodrigo Moreira, Hugo G. V. O. da Cunha, Larissa F. Rodrigues Moreira, Flávio de Oliveira Silva"
"Completion as Enhancement: A Degradation-Aware Selective Image Guided
  Network for Depth Completion","In this paper, we introduce the Selective Image Guided Network (SigNet), a
novel degradation-aware framework that transforms depth completion into depth
enhancement for the first time. Moving beyond direct completion using
convolutional neural networks (CNNs), SigNet initially densifies sparse depth
data through non-CNN densification tools to obtain coarse yet dense depth. This
approach eliminates the mismatch and ambiguity caused by direct convolution
over irregularly sampled sparse data. Subsequently, SigNet redefines completion
as enhancement, establishing a self-supervised degradation bridge between the
coarse depth and the targeted dense depth for effective RGB-D fusion. To
achieve this, SigNet leverages the implicit degradation to adaptively select
high-frequency components (e.g., edges) of RGB data to compensate for the
coarse depth. This degradation is further integrated into a multi-modal
conditional Mamba, dynamically generating the state parameters to enable
efficient global high-frequency information interaction. We conduct extensive
experiments on the NYUv2, DIML, SUN RGBD, and TOFDC datasets, demonstrating the
state-of-the-art (SOTA) performance of SigNet.",2024-12-26T14:05:01Z,http://arxiv.org/abs/2412.19225v1,"Zhiqiang Yan, Zhengxue Wang, Kun Wang, Jun Li, Jian Yang"
"Transformer-Based Wireless Capsule Endoscopy Bleeding Tissue Detection
  and Classification","Informed by the success of the transformer model in various computer vision
tasks, we design an end-to-end trainable model for the automatic detection and
classification of bleeding and non-bleeding frames extracted from Wireless
Capsule Endoscopy (WCE) videos. Based on the DETR model, our model uses the
Resnet50 for feature extraction, the transformer encoder-decoder for bleeding
and non-bleeding region detection, and a feedforward neural network for
classification. Trained in an end-to-end approach on the Auto-WCEBleedGen
Version 1 challenge training set, our model performs both detection and
classification tasks as a single unit. Our model achieves an accuracy, recall,
and F1-score classification percentage score of 98.28, 96.79, and 98.37
respectively, on the Auto-WCEBleedGen version 1 validation set. Further, we
record an average precision (AP @ 0.5), mean-average precision (mAP) of 0.7447
and 0.7328 detection results. This earned us a 3rd place position in the
challenge. Our code is publicly available via
https://github.com/BasitAlawode/WCEBleedGen.",2024-12-26T13:49:39Z,http://arxiv.org/abs/2412.19218v1,"Basit Alawode, Shibani Hamza, Adarsh Ghimire, Divya Velayudhan"
"Applying the maximum entropy principle to multi-species neural networks
  improves species distribution models","The rapid expansion of citizen science initiatives has led to a significant
growth of biodiversity databases, and particularly presence-only (PO)
observations. PO data are invaluable for understanding species distributions
and their dynamics, but their use in Species Distribution Models (SDM) is
curtailed by sampling biases and the lack of information on absences. Poisson
point processes are widely used for SDMs, with Maxent being one of the most
popular methods. Maxent maximises the entropy of a probability distribution
across sites as a function of predefined transformations of environmental
variables, called features. In contrast, neural networks and deep learning have
emerged as a promising technique for automatic feature extraction from complex
input variables. In this paper, we propose DeepMaxent, which harnesses neural
networks to automatically learn shared features among species, using the
maximum entropy principle. To do so, it employs a normalised Poisson loss where
for each species, presence probabilities across sites are modelled by a neural
network. We evaluate DeepMaxent on a benchmark dataset known for its spatial
sampling biases, using PO data for calibration and presence-absence (PA) data
for validation across six regions with different biological groups and
environmental covariates. Our results indicate that DeepMaxent improves model
performance over Maxent and other state-of-the-art SDMs across regions and
taxonomic groups. The method performs particularly well in regions of uneven
sampling, demonstrating substantial potential to improve species distribution
modelling. The method opens the possibility to learn more robust environmental
features predicting jointly many species and scales to arbitrary large numbers
of sites without an increased memory demand.",2024-12-26T13:47:04Z,http://arxiv.org/abs/2412.19217v1,"Maxime Ryckewaert, Diego Marcos, Christophe Botella, Maximilien Servajean, Pierre Bonnet, Alexis Joly"
"Large Language Models Meet Graph Neural Networks: A Perspective of Graph
  Mining","Graph mining is an important area in data mining and machine learning that
involves extracting valuable information from graph-structured data. In recent
years, significant progress has been made in this field through the development
of graph neural networks (GNNs). However, GNNs are still deficient in
generalizing to diverse graph data. Aiming to this issue, Large Language Models
(LLMs) could provide new solutions for graph mining tasks with their superior
semantic understanding. In this review, we systematically review the
combination and application techniques of LLMs and GNNs and present a novel
taxonomy for research in this interdisciplinary field, which involves three
main categories: GNN-driving-LLM, LLM-driving-GNN, and GNN-LLM-co-driving.
Within this framework, we reveal the capabilities of LLMs in enhancing graph
feature extraction as well as improving the effectiveness of downstream tasks
such as node classification, link prediction, and community detection. Although
LLMs have demonstrated their great potential in handling graph-structured data,
their high computational requirements and complexity remain challenges. Future
research needs to continue to explore how to efficiently fuse LLMs and GNNs to
achieve more powerful graph learning and reasoning capabilities and provide new
impetus for the development of graph mining techniques.",2024-12-26T13:21:09Z,http://arxiv.org/abs/2412.19211v1,"Yuxin You, Zhen Liu, Xiangchao Wen, Yongtao Zhang, Wei Ai"
"GAIS: A Novel Approach to Instance Selection with Graph Attention
  Networks","Instance selection (IS) is a crucial technique in machine learning that aims
to reduce dataset size while maintaining model performance. This paper
introduces a novel method called Graph Attention-based Instance Selection
(GAIS), which leverages Graph Attention Networks (GATs) to identify the most
informative instances in a dataset. GAIS represents the data as a graph and
uses GATs to learn node representations, enabling it to capture complex
relationships between instances. The method processes data in chunks, applies
random masking and similarity thresholding during graph construction, and
selects instances based on confidence scores from the trained GAT model.
Experiments on 13 diverse datasets demonstrate that GAIS consistently
outperforms traditional IS methods in terms of effectiveness, achieving high
reduction rates (average 96\%) while maintaining or improving model
performance. Although GAIS exhibits slightly higher computational costs, its
superior performance in maintaining accuracy with significantly reduced
training data makes it a promising approach for graph-based data selection.",2024-12-26T12:51:14Z,http://arxiv.org/abs/2412.19201v1,"Zahiriddin Rustamov, Ayham Zaitouny, Rafat Damseh, Nazar Zaki"
"Personalized Dynamic Music Emotion Recognition with Dual-Scale
  Attention-Based Meta-Learning","Dynamic Music Emotion Recognition (DMER) aims to predict the emotion of
different moments in music, playing a crucial role in music information
retrieval. The existing DMER methods struggle to capture long-term dependencies
when dealing with sequence data, which limits their performance. Furthermore,
these methods often overlook the influence of individual differences on emotion
perception, even though everyone has their own personalized emotional
perception in the real world. Motivated by these issues, we explore more
effective sequence processing methods and introduce the Personalized DMER
(PDMER) problem, which requires models to predict emotions that align with
personalized perception. Specifically, we propose a Dual-Scale Attention-Based
Meta-Learning (DSAML) method. This method fuses features from a dual-scale
feature extractor and captures both short and long-term dependencies using a
dual-scale attention transformer, improving the performance in traditional
DMER. To achieve PDMER, we design a novel task construction strategy that
divides tasks by annotators. Samples in a task are annotated by the same
annotator, ensuring consistent perception. Leveraging this strategy alongside
meta-learning, DSAML can predict personalized perception of emotions with just
one personalized annotation sample. Our objective and subjective experiments
demonstrate that our method can achieve state-of-the-art performance in both
traditional DMER and PDMER.",2024-12-26T12:47:35Z,http://arxiv.org/abs/2412.19200v1,"Dengming Zhang, Weitao You, Ziheng Liu, Lingyun Sun, Pei Chen"
An End-to-End Depth-Based Pipeline for Selfie Image Rectification,"Portraits or selfie images taken from a close distance typically suffer from
perspective distortion. In this paper, we propose an end-to-end deep
learning-based rectification pipeline to mitigate the effects of perspective
distortion. We learn to predict the facial depth by training a deep CNN. The
estimated depth is utilized to adjust the camera-to-subject distance by moving
the camera farther, increasing the camera focal length, and reprojecting the 3D
image features to the new perspective. The reprojected features are then fed to
an inpainting module to fill in the missing pixels. We leverage a
differentiable renderer to enable end-to-end training of our depth estimation
and feature extraction nets to improve the rectified outputs. To boost the
results of the inpainting module, we incorporate an auxiliary module to predict
the horizontal movement of the camera which decreases the area that requires
hallucination of challenging face parts such as ears. Unlike previous works, we
process the full-frame input image at once without cropping the subject's face
and processing it separately from the rest of the body, eliminating the need
for complex post-processing steps to attach the face back to the subject's
body. To train our network, we utilize the popular game engine Unreal Engine to
generate a large synthetic face dataset containing various subjects, head
poses, expressions, eyewear, clothes, and lighting. Quantitative and
qualitative results show that our rectification pipeline outperforms previous
methods, and produces comparable results with a time-consuming 3D GAN-based
method while being more than 260 times faster.",2024-12-26T11:57:54Z,http://arxiv.org/abs/2412.19189v1,"Ahmed Alhawwary, Phong Nguyen-Ha, Janne Mustaniemi, Janne Heikkilä"
"Multi-Head Attention Driven Dynamic Visual-Semantic Embedding for
  Enhanced Image-Text Matching","With the rapid development of multimodal learning, the image-text matching
task, as a bridge connecting vision and language, has become increasingly
important. Based on existing research, this study proposes an innovative visual
semantic embedding model, Multi-Headed Consensus-Aware Visual-Semantic
Embedding (MH-CVSE). This model introduces a multi-head self-attention
mechanism based on the consensus-aware visual semantic embedding model (CVSE)
to capture information in multiple subspaces in parallel, significantly
enhancing the model's ability to understand and represent the complex
relationship between images and texts. In addition, we adopt a parameterized
feature fusion strategy to flexibly integrate feature information at different
levels, further improving the model's expressive power. In terms of loss
function design, the MH-CVSE model adopts a dynamic weight adjustment strategy
to dynamically adjust the weight according to the loss value itself, so that
the model can better balance the contribution of different loss terms during
training. At the same time, we introduce a cosine annealing learning rate
strategy to help the model converge more stably in the later stages of
training. Extensive experimental verification on the Flickr30k dataset shows
that the MH-CVSE model achieves better performance than previous methods in
both bidirectional image and text retrieval tasks, fully demonstrating its
effectiveness and superiority.",2024-12-26T11:46:22Z,http://arxiv.org/abs/2412.19184v1,Wenjing Chen
"Unraveling the magnetic and electronic complexity of intermetallic
  ErPd$_2$Si$_2$: Anisotropic thermal expansion, phase transitions, and twofold
  magnetotransport behavior","We present a comprehensive investigation into the physical properties of
intermetallic ErPd$_2$Si$_2$, a compound renowned for its intriguing magnetic
and electronic characteristics. We confirm the tetragonal crystal structure of
ErPd$_2$Si$_2$ within the $I4/mmm$ space group. Notably, we observed
anisotropic thermal expansion, with the lattice constant $a$ expanding and $c$
contracting between 15 K and 300 K. This behavior is attributed to lattice
vibrations and electronic contributions. Heat capacity measurements revealed
three distinct temperature regimes: $T_1 \sim 3.0$ K, $T_\textrm{N} \sim 4.20$
K, and $T_2 \sim 15.31$ K. These correspond to the disappearance of
spin-density waves, the onset of an incommensurate antiferromagnetic (AFM)
structure, and the crystal-field splitting and/or the presence of short-range
spin fluctuations, respectively. Remarkably, the AFM phase transition anomaly
was observed exclusively in low-field magnetization data (120 Oe) at
$T_\textrm{N}$. A high magnetic field ($B =$ 3 T) effectively suppressed this
anomaly, likely due to spin-flop and spin-flip transitions. Furthermore, the
extracted effective PM moments closely matched the expected theoretical value,
suggesting a dominant magnetic contribution from localized 4$f$ spins of Er.
Additionally, significant differences in resistance ($R$) values at low
temperatures under applied $B$ indicated a magnetoresistance (MR) effect with a
minimum value of -4.36\%. Notably, the measured MR effect exhibited anisotropic
behavior, where changes in the strength or direction of the applied $B$ induced
variations in the MR effect. A twofold symmetry of $R$ was discerned at 3 T and
9 T, originating from the orientation of spin moments relative to the applied
$B$. Intriguingly, above $T_\textrm{N}$, short-range spin fluctuations also
displayed a preferred orientation along the $c$-axis due to single-ion
anisotropy.",2024-12-26T11:39:24Z,http://arxiv.org/abs/2412.19181v1,"Kaitong Sun, Si Wu, Guanping Xu, Lingwei Li, Hongyu Chen, Qian Zhao, Muqing Su, Wolfgang Schmidt, Chongde Cao, Hai-Feng Li"
"Mask Approximation Net: Merging Feature Extraction and Distribution
  Learning for Remote Sensing Change Captioning","Remote sensing image change description, as a novel multimodal task in the
field of remote sensing processing, not only enables the detection of changes
in surface conditions but also provides detailed descriptions of these changes,
thereby enhancing human interpretability and interactivity. However, previous
methods mainly employed Convolutional Neural Network (CNN) architectures to
extract bitemporal image features. This approach often leads to an overemphasis
on designing specific network architectures and limits the captured feature
distributions to the current dataset, resulting in poor generalizability and
robustness when applied to other datasets or real-world scenarios. To address
these limitations, this paper proposes a novel approach for remote sensing
image change detection and description that integrates diffusion models, aiming
to shift the focus from conventional feature learning paradigms to data
distribution learning. The proposed method primarily includes a simple
multi-scale change detection module, whose output features are subsequently
refined using a diffusion model. Additionally, we introduce a frequency-guided
complex filter module to handle high-frequency noise during the diffusion
process, which helps to maintain model performance. Finally, we validate the
effectiveness of our proposed method on several remote sensing change detection
description datasets, demonstrating its superior performance. The code
available at MaskApproxNet.",2024-12-26T11:35:57Z,http://arxiv.org/abs/2412.19179v1,"Dongwei Sun, Xiangyong Cao"
"Reversed in Time: A Novel Temporal-Emphasized Benchmark for Cross-Modal
  Video-Text Retrieval","Cross-modal (e.g. image-text, video-text) retrieval is an important task in
information retrieval and multimodal vision-language understanding field.
Temporal understanding makes video-text retrieval more challenging than
image-text retrieval. However, we find that the widely used video-text
benchmarks have shortcomings in comprehensively assessing abilities of models,
especially in temporal understanding, causing large-scale image-text
pre-trained models can already achieve comparable zero-shot performance with
video-text pre-trained models. In this paper, we introduce RTime, a novel
temporal-emphasized video-text retrieval dataset. We first obtain videos of
actions or events with significant temporality, and then reverse these videos
to create harder negative samples. We then recruit annotators to judge the
significance and reversibility of candidate videos, and write captions for
qualified videos. We further adopt GPT-4 to extend more captions based on
human-written captions. Our RTime dataset currently consists of 21k videos with
10 captions per video, totalling about 122 hours. Based on RTime, we propose
three retrieval benchmark tasks: RTime-Origin, RTime-Hard, and RTime-Binary. We
further enhance the use of harder-negatives in model training, and benchmark a
variety of video-text models on RTime. Extensive experiment analysis proves
that RTime indeed poses new and higher challenges to video-text retrieval. We
release our RTime
dataset\footnote{\url{https://github.com/qyr0403/Reversed-in-Time}} to further
advance video-text retrieval and multimodal understanding research.",2024-12-26T11:32:00Z,http://arxiv.org/abs/2412.19178v1,"Yang Du, Yuqi Liu, Qin Jin"
"Towards Popularity-Aware Recommendation: A Multi-Behavior Enhanced
  Framework with Orthogonality Constraint","Top-$K$ recommendation involves inferring latent user preferences and
generating personalized recommendations accordingly, which is now ubiquitous in
various decision systems. Nonetheless, recommender systems usually suffer from
severe \textit{popularity bias}, leading to the over-recommendation of popular
items. Such a bias deviates from the central aim of reflecting user preference
faithfully, compromising both customer satisfaction and retailer profits.
Despite the prevalence, existing methods tackling popularity bias still have
limitations due to the considerable accuracy-debias tradeoff and the
sensitivity to extensive parameter selection, further exacerbated by the
extreme sparsity in positive user-item interactions.
  In this paper, we present a \textbf{Pop}ularity-aware top-$K$ recommendation
algorithm integrating multi-behavior \textbf{S}ide \textbf{I}nformation
(PopSI), aiming to enhance recommendation accuracy and debias performance
simultaneously. Specifically, by leveraging multiple user feedback that mirrors
similar user preferences and formulating it as a three-dimensional tensor,
PopSI can utilize all slices to capture the desiring user preferences
effectively. Subsequently, we introduced a novel orthogonality constraint to
refine the estimated item feature space, enforcing it to be invariant to item
popularity features thereby addressing our model's sensitivity to popularity
bias. Comprehensive experiments on real-world e-commerce datasets demonstrate
the general improvements of PopSI over state-of-the-art debias methods with a
marginal accuracy-debias tradeoff and scalability to practical applications.
The source code for our algorithm and experiments is available at
\url{https://github.com/Eason-sys/PopSI}.",2024-12-26T11:06:49Z,http://arxiv.org/abs/2412.19172v1,"Yishan Han, Biao Xu, Yao Wang, Shanxing Gao"
"High-Precision Schottky Diagnostics for Low-SNR Betatron Tune
  Measurement in Ramping Synchrotrons","This paper presents a novel Schottky diagnostics-based method for real-time
betatron tune measurement in ramping synchrotrons, exemplified by the Shanghai
Advanced Proton Therapy (SAPT) facility. The proposed approach achieves high
precision under challenging conditions, including low frequency resolution and
signal-to-noise ratios (SNR) as low as -15 dB within the bandwidth of a
narrowband detector. By employing Short-Time Fourier Transform (STFT) analysis
with automatically optimized time windows, the method effectively addresses the
rapid increase in revolution frequency from 4 MHz to 7.5 MHz over 0.35 seconds,
assuming constant beam properties within each window. Monte Carlo
macro-particle simulations are employed to generate Schottky signals, which are
subsequently combined with real noise collected from an analog-to-digital
converter to emulate practical conditions. The betatron tune measurement
procedure integrates longitudinal signal exclusion, spectrum smoothing, and
spectral multiplication to reliably extract transverse Schottky spectra buried
in noise, to enable precise betatron tune determination. Experimental results
demonstrate that the proposed method surpasses existing approaches in
precision, accuracy, and robustness, while meeting stringent design
requirements. This innovative approach addresses key limitations of Schottky
diagnostics for betatron tune measurement in ramping synchrotrons, providing a
foundation for applications such as proton therapy.",2024-12-26T11:05:21Z,http://arxiv.org/abs/2412.19171v1,"Peihan Sun, Manzhou Zhang, Renxian Yuan, Deming Li, Jian Dong, Ying Shi"
GFG -- Gender-Fair Generation: A CALAMITA Challenge,"Gender-fair language aims at promoting gender equality by using terms and
expressions that include all identities and avoid reinforcing gender
stereotypes. Implementing gender-fair strategies is particularly challenging in
heavily gender-marked languages, such as Italian. To address this, the
Gender-Fair Generation challenge intends to help shift toward gender-fair
language in written communication. The challenge, designed to assess and
monitor the recognition and generation of gender-fair language in both mono-
and cross-lingual scenarios, includes three tasks: (1) the detection of
gendered expressions in Italian sentences, (2) the reformulation of gendered
expressions into gender-fair alternatives, and (3) the generation of
gender-fair language in automatic translation from English to Italian. The
challenge relies on three different annotated datasets: the GFL-it corpus,
which contains Italian texts extracted from administrative documents provided
by the University of Brescia; GeNTE, a bilingual test set for gender-neutral
rewriting and translation built upon a subset of the Europarl dataset; and
Neo-GATE, a bilingual test set designed to assess the use of non-binary
neomorphemes in Italian for both fair formulation and translation tasks.
Finally, each task is evaluated with specific metrics: average of F1-score
obtained by means of BERTScore computed on each entry of the datasets for task
1, an accuracy measured with a gender-neutral classifier, and a
coverage-weighted accuracy for tasks 2 and 3.",2024-12-26T10:58:40Z,http://arxiv.org/abs/2412.19168v1,"Simona Frenda, Andrea Piergentili, Beatrice Savoldi, Marco Madeddu, Martina Rosola, Silvia Casola, Chiara Ferrando, Viviana Patti, Matteo Negri, Luisa Bentivogli"
"Dual Channel Multi-Attention in ViT for Biometric Authentication using
  Forehead Subcutaneous Vein Pattern and Periocular Pattern","Traditional biometric systems, like face and fingerprint recognition, have
encountered significant setbacks due to wearing face masks and hygiene
concerns. To meet the challenges of the partially covered face due to face
masks and hygiene concerns of fingerprint recognition, this paper proposes a
novel dual-channel multi-attention Vision Transformer (ViT) framework for
biometric authentication using forehead subcutaneous vein patterns and
periocular patterns, offering a promising alternative to traditional methods,
capable of performing well even with face masks and without any physical touch.
The proposed framework leverages a dual-channel ViT architecture, designed to
handle two distinct biometric traits. It can capture long-range dependencies of
independent features from the vein and periocular patterns. A custom classifier
is then designed to integrate the independently extracted features, producing a
final class prediction. The performance of the proposed algorithm was
rigorously evaluated using the Forehead Subcutaneous Vein Pattern and
Periocular Biometric Pattern (FSVP-PBP) database. The results demonstrated the
superiority of the algorithm over state-of-the-art methods, achieving
remarkable classification accuracy of $99.3 \pm 0.02\%$ with the combined vein
and periocular patterns.",2024-12-26T10:40:15Z,http://arxiv.org/abs/2412.19160v1,"Arun K. Sharma, Shubhobrata Bhattacharya, Motahar Reza"
"Referencing Where to Focus: Improving VisualGrounding with Referential
  Query","Visual Grounding aims to localize the referring object in an image given a
natural language expression. Recent advancements in DETR-based visual grounding
methods have attracted considerable attention, as they directly predict the
coordinates of the target object without relying on additional efforts, such as
pre-generated proposal candidates or pre-defined anchor boxes. However,
existing research primarily focuses on designing stronger multi-modal decoder,
which typically generates learnable queries by random initialization or by
using linguistic embeddings. This vanilla query generation approach inevitably
increases the learning difficulty for the model, as it does not involve any
target-related information at the beginning of decoding. Furthermore, they only
use the deepest image feature during the query learning process, overlooking
the importance of features from other levels. To address these issues, we
propose a novel approach, called RefFormer. It consists of the query adaption
module that can be seamlessly integrated into CLIP and generate the referential
query to provide the prior context for decoder, along with a task-specific
decoder. By incorporating the referential query into the decoder, we can
effectively mitigate the learning difficulty of the decoder, and accurately
concentrate on the target object. Additionally, our proposed query adaption
module can also act as an adapter, preserving the rich knowledge within CLIP
without the need to tune the parameters of the backbone network. Extensive
experiments demonstrate the effectiveness and efficiency of our proposed
method, outperforming state-of-the-art approaches on five visual grounding
benchmarks.",2024-12-26T10:19:20Z,http://arxiv.org/abs/2412.19155v1,"Yabing Wang, Zhuotao Tian, Qingpei Guo, Zheng Qin, Sanping Zhou, Ming Yang, Le Wang"
AskChart: Universal Chart Understanding through Textual Enhancement,"Chart understanding tasks such as ChartQA and Chart-to-Text involve
automatically extracting and interpreting key information from charts, enabling
users to query or convert visual data into structured formats. State-of-the-art
approaches primarily focus on visual cues from chart images, failing to
explicitly incorporate rich textual information (e.g., data labels and axis
labels) embedded within the charts. This textual information is vital for
intuitive human comprehension and interpretation of charts. Moreover, existing
models are often large and computationally intensive, limiting their practical
applicability. In this paper, we introduce AskChart, a universal model that
explicitly integrates both textual and visual cues from charts using a Mixture
of Experts (MoE) architecture. AskChart facilitates the learning of enhanced
visual-textual representations of charts for effectively handling multiple
chart understanding tasks, while maintaining a smaller model size. To capture
the synergy between visual and textual modalities, we curate a large-scale
dataset named ChartBank with about 7.5M data samples, which helps align textual
and visual information and facilitates the extraction of visual entities and
text. To effectively train AskChart, we design a three-stage training strategy
to align visual and textual modalities for learning robust visual-textual
representations and optimizing the learning of the MoE layer. Extensive
experiments across five datasets demonstrate the significant performance gains
of AskChart in four chart understanding tasks. Remarkably, AskChart with 4.6B
parameters outperforms state-of-the-art models with 13B parameters by 68.3% in
Open-ended ChartQA and 49.2% in Chart-to-Text tasks, while achieving comparable
performance in ChartQA and Chart-to-Table tasks.",2024-12-26T09:59:43Z,http://arxiv.org/abs/2412.19146v1,"Xudong Yang, Yifan Wu, Yizhang Zhu, Nan Tang, Yuyu Luo"
"CLIP-GS: Unifying Vision-Language Representation with 3D Gaussian
  Splatting","Recent works in 3D multimodal learning have made remarkable progress.
However, typically 3D multimodal models are only capable of handling point
clouds. Compared to the emerging 3D representation technique, 3D Gaussian
Splatting (3DGS), the spatially sparse point cloud cannot depict the texture
information of 3D objects, resulting in inferior reconstruction capabilities.
This limitation constrains the potential of point cloud-based 3D multimodal
representation learning. In this paper, we present CLIP-GS, a novel multimodal
representation learning framework grounded in 3DGS. We introduce the GS
Tokenizer to generate serialized gaussian tokens, which are then processed
through transformer layers pre-initialized with weights from point cloud
models, resulting in the 3DGS embeddings. CLIP-GS leverages contrastive loss
between 3DGS and the visual-text embeddings of CLIP, and we introduce an image
voting loss to guide the directionality and convergence of gradient
optimization. Furthermore, we develop an efficient way to generate triplets of
3DGS, images, and text, facilitating CLIP-GS in learning unified multimodal
representations. Leveraging the well-aligned multimodal representations,
CLIP-GS demonstrates versatility and outperforms point cloud-based models on
various 3D tasks, including multimodal retrieval, zero-shot, and few-shot
classification.",2024-12-26T09:54:25Z,http://arxiv.org/abs/2412.19142v1,"Siyu Jiao, Haoye Dong, Yuyang Yin, Zequn Jie, Yinlong Qian, Yao Zhao, Humphrey Shi, Yunchao Wei"
"How Panel Layouts Define Manga: Insights from Visual Ablation
  Experiments","Today, manga has gained worldwide popularity. However, the question of how
various elements of manga, such as characters, text, and panel layouts, reflect
the uniqueness of a particular work, or even define it, remains an unexplored
area. In this paper, we aim to quantitatively and qualitatively analyze the
visual characteristics of manga works, with a particular focus on panel layout
features. As a research method, we used facing page images of manga as input to
train a deep learning model for predicting manga titles, examining
classification accuracy to quantitatively analyze these features. Specifically,
we conducted ablation studies by limiting page image information to panel
frames to analyze the characteristics of panel layouts. Through a series of
quantitative experiments using all 104 works, 12 genres, and 10,122 facing page
images from the Manga109 dataset, as well as qualitative analysis using
Grad-CAM, our study demonstrates that the uniqueness of manga works is strongly
reflected in their panel layouts.",2024-12-26T09:53:37Z,http://arxiv.org/abs/2412.19141v1,"Siyuan Feng, Teruya Yoshinaga, Katsuhiko Hayashi, Koki Washio, Hidetaka Kamigaito"
"SILC-EFSA: Self-aware In-context Learning Correction for Entity-level
  Financial Sentiment Analysis","In recent years, fine-grained sentiment analysis in finance has gained
significant attention, but the scarcity of entity-level datasets remains a key
challenge. To address this, we have constructed the largest English and Chinese
financial entity-level sentiment analysis datasets to date. Building on this
foundation, we propose a novel two-stage sentiment analysis approach called
Self-aware In-context Learning Correction (SILC). The first stage involves
fine-tuning a base large language model to generate pseudo-labeled data
specific to our task. In the second stage, we train a correction model using a
GNN-based example retriever, which is informed by the pseudo-labeled data. This
two-stage strategy has allowed us to achieve state-of-the-art performance on
the newly constructed datasets, advancing the field of financial sentiment
analysis. In a case study, we demonstrate the enhanced practical utility of our
data and methods in monitoring the cryptocurrency market. Our datasets and code
are available at https://github.com/NLP-Bin/SILC-EFSA.",2024-12-26T09:53:01Z,http://arxiv.org/abs/2412.19140v1,"Senbin Zhu, Chenyuan He, Hongde Liu, Pengcheng Dong, Hanjie Zhao, Yuchen Yan, Yuxiang Jia, Hongying Zan, Min Peng"
PlanLLM: Video Procedure Planning with Refinable Large Language Models,"Video procedure planning, i.e., planning a sequence of action steps given the
video frames of start and goal states, is an essential ability for embodied AI.
Recent works utilize Large Language Models (LLMs) to generate enriched action
step description texts to guide action step decoding. Although LLMs are
introduced, these methods decode the action steps into a closed-set of one-hot
vectors, limiting the model's capability of generalizing to new steps or tasks.
Additionally, fixed action step descriptions based on world-level commonsense
may contain noise in specific instances of visual states. In this paper, we
propose PlanLLM, a cross-modal joint learning framework with LLMs for video
procedure planning. We propose an LLM-Enhanced Planning module which fully uses
the generalization ability of LLMs to produce free-form planning output and to
enhance action step decoding. We also propose Mutual Information Maximization
module to connect world-level commonsense of step descriptions and
sample-specific information of visual states, enabling LLMs to employ the
reasoning ability to generate step sequences. With the assistance of LLMs, our
method can both closed-set and open vocabulary procedure planning tasks. Our
PlanLLM achieves superior performance on three benchmarks, demonstrating the
effectiveness of our designs.",2024-12-26T09:51:05Z,http://arxiv.org/abs/2412.19139v1,"Dejie Yang, Zijing Zhao, YangLiu"
"A Rhetorical Relations-Based Framework for Tailored Multimedia Document
  Summarization","In the rapidly evolving landscape of digital content, the task of summarizing
multimedia documents, which encompass textual, visual, and auditory elements,
presents intricate challenges. These challenges include extracting pertinent
information from diverse formats, maintaining the structural integrity and
semantic coherence of the original content, and generating concise yet
informative summaries. This paper introduces a novel framework for multimedia
document summarization that capitalizes on the inherent structure of the
document to craft coherent and succinct summaries. Central to this framework is
the incorporation of a rhetorical structure for structural analysis, augmented
by a graph-based representation to facilitate the extraction of pivotal
information. Weighting algorithms are employed to assign significance values to
document units, thereby enabling effective ranking and selection of relevant
content. Furthermore, the framework is designed to accommodate user preferences
and time constraints, ensuring the production of personalized and contextually
relevant summaries. The summarization process is elaborately delineated,
encompassing document specification, graph construction, unit weighting, and
summary extraction, supported by illustrative examples and algorithmic
elucidation. This proposed framework represents a significant advancement in
automatic summarization, with broad potential applications across multimedia
document processing, promising transformative impacts in the field.",2024-12-26T09:29:59Z,http://arxiv.org/abs/2412.19133v1,"Azze-Eddine Maredj, Madjid Sadallah"
Semantic Residual for Multimodal Unified Discrete Representation,"Recent research in the domain of multimodal unified representations
predominantly employs codebook as representation forms, utilizing Vector
Quantization(VQ) for quantization, yet there has been insufficient exploration
of other quantization representation forms. Our work explores more precise
quantization methods and introduces a new framework, Semantic Residual
Cross-modal Information Disentanglement (SRCID), inspired by the numerical
residual concept inherent to Residual Vector Quantization (RVQ). SRCID employs
semantic residual-based information disentanglement for multimodal data to
better handle the inherent discrepancies between different modalities. Our
method enhances the capabilities of unified multimodal representations and
demonstrates exceptional performance in cross-modal generalization and
cross-modal zero-shot retrieval. Its average results significantly surpass
existing state-of-the-art models, as well as previous attempts with RVQ and
Finite Scalar Quantization (FSQ) based on these modals.",2024-12-26T09:08:52Z,http://arxiv.org/abs/2412.19128v1,"Hai Huang, Shulei Wang, Yan Xia"
SDRS: Shape-Differentiable Robot Simulator,"Robot simulators are indispensable tools across many fields, and recent
research has significantly improved their functionality by incorporating
additional gradient information. However, existing differentiable robot
simulators suffer from non-differentiable singularities, when robots undergo
substantial shape changes. To address this, we present the Shape-Differentiable
Robot Simulator (SDRS), designed to be differentiable under significant robot
shape changes. The core innovation of SDRS lies in its representation of robot
shapes using a set of convex polyhedrons. This approach allows us to generalize
smooth, penalty-based contact mechanics for interactions between any pair of
convex polyhedrons. Using the separating hyperplane theorem, SDRS introduces a
separating plane for each pair of contacting convex polyhedrons. This
separating plane functions as a zero-mass auxiliary entity, with its state
determined by the principle of least action. This setup ensures global
differentiability, even as robot shapes undergo significant geometric and
topological changes. To demonstrate the practical value of SDRS, we provide
examples of robot co-design scenarios, where both robot shapes and control
movements are optimized simultaneously.",2024-12-26T09:05:22Z,http://arxiv.org/abs/2412.19127v1,"Xiaohan Ye, Xifeng Gao, Kui Wu, Zherong Pan, Taku Komura"
Generalizations of Cyclic Codes over Product Rings,"In this article, for the finite field $\mathbb{F}_q$, we show that the
$\mathbb{F}_q$-algebra $\mathbb{F}_q[x]/\langle f(x) \rangle$ is isomorphic to
the product ring $\mathbb{F}_q^{\deg f(x)}$ if and only if $f(x)$ splits over
$\mathbb{F}_q$ into distinct factors. We generalize this result to the quotient
of the polynomial algebra $\mathbb{F}_q[x_1, x_2,\dots, x_k]$ by the ideal
$\langle f_1(x_1), f_2(x_2),\dots, f_k(x_k)\rangle.$ On the other hand, every
finite dimensional $\mathbb{F}_q$-algebra $\mathcal{A}$ has an orthogonal basis
of idempotents with their sum equal to $1_{\mathcal{A}}$ if and only if
$\mathcal{A}\cong\mathbb{F}_q^l$ as $\mathbb{F}_q$-algebras, where
$l=\dim_{\mathbb{F}_q} \mathcal{A}$. We utilize this characterization to study
polycyclic codes over $\mathcal{A}$ and get a unique decomposition of
polycyclic codes over $\mathcal{A}$ into polycyclic codes over $\mathbb{F}_q$
for every such orthogonal basis of $\mathcal{A}$, which is referred to as an
$\mathbb{F}_q$-decomposition. An $\mathbb{F}_q$-decomposition enables us to use
results of polycyclic codes over $\mathbb{F}_q$ to study polycyclic codes over
$\mathcal{A}$; for instance, we show that the annihilator dual of a polycyclic
code over $\mathcal{A}$ is a polycyclic code over $\mathcal{A}$. Furthermore,
we consider the obvious Gray map (which is obtained by restricting scalars from
$\mathcal{A}$ to $\mathbb{F}_q$) to find and study codes over $\mathbb{F}_q$
from codes over $\mathcal{A}$. Finally, with the help of different Gray maps,
we produce a good number of examples of MDS or almost-MDS or/and optimal codes;
some of them are LCD over $\mathbb{F}_q$.",2024-12-26T08:58:48Z,http://arxiv.org/abs/2412.19126v1,"Akanksha, Ritumoni Sarma"
"Advanced Knowledge Transfer: Refined Feature Distillation for Zero-Shot
  Quantization in Edge Computing","We introduce AKT (Advanced Knowledge Transfer), a novel method to enhance the
training ability of low-bit quantized (Q) models in the field of zero-shot
quantization (ZSQ). Existing research in ZSQ has focused on generating
high-quality data from full-precision (FP) models. However, these approaches
struggle with reduced learning ability in low-bit quantization due to its
limited information capacity. To overcome this limitation, we propose effective
training strategy compared to data generation. Particularly, we analyzed that
refining feature maps in the feature distillation process is an effective way
to transfer knowledge to the Q model. Based on this analysis, AKT efficiently
transfer core information from the FP model to the Q model. AKT is the first
approach to utilize both spatial and channel attention information in feature
distillation in ZSQ. Our method addresses the fundamental gradient exploding
problem in low-bit Q models. Experiments on CIFAR-10 and CIFAR-100 datasets
demonstrated the effectiveness of the AKT. Our method led to significant
performance enhancement in existing generative models. Notably, AKT achieved
significant accuracy improvements in low-bit Q models, achieving
state-of-the-art in the 3,5bit scenarios on CIFAR-10. The code is available at
https://github.com/Inpyo-Hong/AKT-Advanced-knowledge-Transfer.",2024-12-26T08:52:27Z,http://arxiv.org/abs/2412.19125v1,"Inpyo Hong, Youngwan Jo, Hyojeong Lee, Sunghyun Ahn, Sanghyun Park"
"Evaluating Self-Supervised Learning in Medical Imaging: A Benchmark for
  Robustness, Generalizability, and Multi-Domain Impact","Self-supervised learning (SSL) has emerged as a promising paradigm in medical
imaging, addressing the chronic challenge of limited labeled data in healthcare
settings. While SSL has shown impressive results, existing studies in the
medical domain are often limited in scope, focusing on specific datasets or
modalities, or evaluating only isolated aspects of model performance. This
fragmented evaluation approach poses a significant challenge, as models
deployed in critical medical settings must not only achieve high accuracy but
also demonstrate robust performance and generalizability across diverse
datasets and varying conditions. To address this gap, we present a
comprehensive evaluation of SSL methods within the medical domain, with a
particular focus on robustness and generalizability. Using the MedMNIST dataset
collection as a standardized benchmark, we evaluate 8 major SSL methods across
11 different medical datasets. Our study provides an in-depth analysis of model
performance in both in-domain scenarios and the detection of
out-of-distribution (OOD) samples, while exploring the effect of various
initialization strategies, model architectures, and multi-domain pre-training.
We further assess the generalizability of SSL methods through cross-dataset
evaluations and the in-domain performance with varying label proportions (1%,
10%, and 100%) to simulate real-world scenarios with limited supervision. We
hope this comprehensive benchmark helps practitioners and researchers make more
informed decisions when applying SSL methods to medical applications.",2024-12-26T08:51:56Z,http://arxiv.org/abs/2412.19124v1,"Valay Bundele, Oğuz Ata Çal, Bora Kargi, Karahan Sarıtaş, Kıvanç Tezören, Zohreh Ghaderi, Hendrik Lensch"
"Characterizing resources for multiparameter estimation of SU(2) and
  SU(1,1) unitaries","We investigate the estimation of multiple parameters generated by a unitary
evolution with non-commuting Hamiltonians that form a closed algebra. In
particular, we consider the three-parameter estimation of SU(2) and SU(1,1)
unitaries and analyze the ideal scaling of precision in terms of typical
resources such as the total particle number, identifying novel probe states
that can achieve Heisenberg scaling for all the three parameters. On top of
that, we also consider a more pragmatic framework where the estimation is
performed via the so-called method of moments, i.e., via measurements of
signal-to-noise ratios of time-evolved observables, which we restrict to be the
first two moments of the Hamiltonian generators. We consider the ideal classes
of states that we have identified by maximizing the quantum Fisher information
matrix, and analyze the maximal precision achievable from measuring only the
first two moments of the generators. As a result, we find that in this context
with limited resources accessible, the twin-Fock state emerges as the only
probe state that allows the estimation of two out of the three parameters with
Heisenberg precision scaling. We also analyze further states, including
Gaussian states, as well as Schr{\""o}dinger-cat-like states, this time
restricting to measurements linear in the su(2) and su(1,1) operators. In this
case, we find that while the former can indeed achieve Heisenberg scaling for
one or two parameters, the latter cannot, which confirms the fact that more
complicated measurements would be needed in that case.",2024-12-26T08:36:38Z,http://arxiv.org/abs/2412.19119v1,"Shaowei Du, Shuheng Liu, Frank E. S. Steinhoff, Giuseppe Vitagliano"
Discrete vs. Continuous Trade-offs for Generative Models,"This work explores the theoretical and practical foundations of denoising
diffusion probabilistic models (DDPMs) and score-based generative models, which
leverage stochastic processes and Brownian motion to model complex data
distributions. These models employ forward and reverse diffusion processes
defined through stochastic differential equations (SDEs) to iteratively add and
remove noise, enabling high-quality data generation. By analyzing the
performance bounds of these models, we demonstrate how score estimation errors
propagate through the reverse process and bound the total variation distance
using discrete Girsanov transformations, Pinsker's inequality, and the data
processing inequality (DPI) for an information theoretic lens.",2024-12-26T08:14:27Z,http://arxiv.org/abs/2412.19114v1,"Jathin Korrapati, Tanish Baranwal, Rahul Shah"
"Spectral Enhancement and Pseudo-Anchor Guidance for Infrared-Visible
  Person Re-Identification","The development of deep learning has facilitated the application of person
re-identification (ReID) technology in intelligent security. Visible-infrared
person re-identification (VI-ReID) aims to match pedestrians across infrared
and visible modality images enabling 24-hour surveillance. Current studies
relying on unsupervised modality transformations as well as inefficient
embedding constraints to bridge the spectral differences between infrared and
visible images, however, limit their potential performance. To tackle the
limitations of the above approaches, this paper introduces a simple yet
effective Spectral Enhancement and Pseudo-anchor Guidance Network, named
SEPG-Net. Specifically, we propose a more homogeneous spectral enhancement
scheme based on frequency domain information and greyscale space, which avoids
the information loss typically caused by inefficient modality transformations.
Further, a Pseudo Anchor-guided Bidirectional Aggregation (PABA) loss is
introduced to bridge local modality discrepancies while better preserving
discriminative identity embeddings. Experimental results on two public
benchmark datasets demonstrate the superior performance of SEPG-Net against
other state-of-the-art methods. The code is available at
https://github.com/1024AILab/ReID-SEPG.",2024-12-26T08:03:53Z,http://arxiv.org/abs/2412.19111v1,"Yiyuan Ge, Zhihao Chen, Ziyang Wang, Jiaju Kang, Mingya Zhang"
"A Selective Secure Precoding Framework for MU-MIMO Rate-Splitting
  Multiple Access Networks Under Limited CSIT","In this paper, we propose a robust and adaptable secure precoding framework
designed to encapsulate a intricate scenario where legitimate users have
different information security: secure private or normal public information.
Leveraging rate-splitting multiple access (RSMA), we formulate the sum secrecy
spectral efficiency (SE) maximization problem in downlink multi-user
multiple-input multiple-output (MIMO) systems with multi-eavesdropper. To
resolve the challenges including the heterogeneity of security, non-convexity,
and non-smoothness of the problem, we initially approximate the problem using a
LogSumExp technique. Subsequently, we derive the first-order optimality
condition in the form of a generalized eigenvalue problem. We utilize a power
iteration-based method to solve the condition, thereby achieving a superior
local optimal solution. The proposed algorithm is further extended to a more
realistic scenario involving limited channel state information at the
transmitter (CSIT). To effectively utilize the limited channel information, we
employ a conditional average rate approach. Handling the conditional average by
deriving useful bounds, we establish a lower bound for the objective function
under the conditional average. Then we apply the similar optimization method as
for the perfect CSIT case. In simulations, we validate the proposed algorithm
in terms of the sum secrecy SE.",2024-12-26T08:00:02Z,http://arxiv.org/abs/2412.19110v1,"Sangmin Lee, Seokjun Park, Jeonghun Park, Jinseok Choi"
"Graph Mixture of Experts and Memory-augmented Routers for Multivariate
  Time Series Anomaly Detection","Multivariate time series (MTS) anomaly detection is a critical task that
involves identifying abnormal patterns or events in data that consist of
multiple interrelated time series. In order to better model the complex
interdependence between entities and the various inherent characteristics of
each entity, the GNN based methods are widely adopted by existing methods. In
each layer of GNN, node features aggregate information from their neighboring
nodes to update their information. In doing so, from shallow layer to deep
layer in GNN, original individual node features continue to be weakened and
more structural information,i.e., from short-distance neighborhood to
long-distance neighborhood, continues to be enhanced. However, research to date
has largely ignored the understanding of how hierarchical graph information is
represented and their characteristics that can benefit anomaly detection.
Existing methods simply leverage the output from the last layer of GNN for
anomaly estimation while neglecting the essential information contained in the
intermediate GNN layers. To address such limitations, in this paper, we propose
a Graph Mixture of Experts (Graph-MoE) network for multivariate time series
anomaly detection, which incorporates the mixture of experts (MoE) module to
adaptively represent and integrate hierarchical multi-layer graph information
into entity representations. It is worth noting that our Graph-MoE can be
integrated into any GNN-based MTS anomaly detection method in a plug-and-play
manner. In addition, the memory-augmented routers are proposed in this paper to
capture the correlation temporal information in terms of the global historical
features of MTS to adaptively weigh the obtained entity representations to
achieve successful anomaly estimation. Extensive experiments on five
challenging datasets prove the superiority of our approach and each proposed
module.",2024-12-26T07:49:51Z,http://arxiv.org/abs/2412.19108v1,"Xiaoyu Huang, Weidong Chen, Bo Hu, Zhendong Mao"
"How Can Haptic Feedback Assist People with Blind and Low Vision (BLV): A
  Systematic Literature Review","People who are blind or have low vision (BLV) encounter numerous challenges
in their daily lives and work. To support them, various haptic assistive tools
have been developed. Despite these advancements, the effective utilization of
these tools -- including the optimal haptic feedback and on-body stimulation
positions for different tasks along with their limitations -- remains poorly
understood. Recognizing these gaps, we conducted a systematic literature review
spanning two decades (2004-2024) to evaluate the development of haptic
assistive tools within the HCI community. Our findings reveal that these tools
are primarily used for understanding graphical information, providing guidance
and navigation, and facilitating education and training, among other life and
work tasks. We identified three main limitations: hardware limitations,
functionality limitations, and UX and evaluation methods limitations. Based on
these insights, we discuss potential research avenues and offer suggestions for
enhancing the effectiveness of future haptic assistive technologies.",2024-12-26T07:47:26Z,http://arxiv.org/abs/2412.19105v1,"Chutian Jiang, Emily Kuang, Mingming Fan"
"Improving Generative Pre-Training: An In-depth Study of Masked Image
  Modeling and Denoising Models","In this work, we dive deep into the impact of additive noise in pre-training
deep networks. While various methods have attempted to use additive noise
inspired by the success of latent denoising diffusion models, when used in
combination with masked image modeling, their gains have been marginal when it
comes to recognition tasks. We thus investigate why this would be the case, in
an attempt to find effective ways to combine the two ideas. Specifically, we
find three critical conditions: corruption and restoration must be applied
within the encoder, noise must be introduced in the feature space, and an
explicit disentanglement between noised and masked tokens is necessary. By
implementing these findings, we demonstrate improved pre-training performance
for a wide range of recognition tasks, including those that require
fine-grained, high-frequency information to solve.",2024-12-26T07:47:20Z,http://arxiv.org/abs/2412.19104v1,"Hyesong Choi, Daeun Kim, Sungmin Cha, Kwang Moo Yi, Dongbo Min"
"The role of potential energy landscape research in the development of
  new electrolyte solutions","The development of new electrolyte solutions with improved characteristics is
a key challenge for creating high-performance batteries, fuel cells,
supercapacitors, and other electrochemical devices. The study of the potential
energy landscape (PEL) plays an important role in this process, providing
information about the interactions between solution components at the molecular
level. In this work, we review the practice of applying PEL research methods
based on classical and quantum-chemical algorithms to analyze the structure,
dynamics, and thermodynamic properties of electrolyte solutions. Intermolecular
and ion-molecular interactions at the microscopic level, which determine the
macroscopic properties of the electrolyte solution, are considered in detail.
The importance of identifying stable configurations of ions and their solvates
is emphasized. PEL analysis allows for the systematic determination of the most
probable structures and complexes formed in solution, which is important for
understanding ion transport mechanisms. The study of the PEL allows for the
determination of the energy barriers that must be overcome for ion migration,
which is related to the conductivity of the electrolyte. The application of PEL
research methods in combination with experimental data opens up new
possibilities for the rational design of electrolyte solutions with desired
physicochemical properties.",2024-12-26T07:45:29Z,http://arxiv.org/abs/2412.19103v1,Vitaly V. Chaban
"Reconstruction Target Matters in Masked Image Modeling for Cross-Domain
  Few-Shot Learning","Cross-Domain Few-Shot Learning (CDFSL) requires the model to transfer
knowledge from the data-abundant source domain to data-scarce target domains
for fast adaptation, where the large domain gap makes CDFSL a challenging
problem. Masked Autoencoder (MAE) excels in effectively using unlabeled data
and learning image's global structures, enhancing model generalization and
robustness. However, in the CDFSL task with significant domain shifts, we find
MAE even shows lower performance than the baseline supervised models. In this
paper, we first delve into this phenomenon for an interpretation. We find that
MAE tends to focus on low-level domain information during reconstructing pixels
while changing the reconstruction target to token features could mitigate this
problem. However, not all features are beneficial, as we then find
reconstructing high-level features can hardly improve the model's
transferability, indicating a trade-off between filtering domain information
and preserving the image's global structure. In all, the reconstruction target
matters for the CDFSL task. Based on the above findings and interpretations, we
further propose Domain-Agnostic Masked Image Modeling (DAMIM) for the CDFSL
task. DAMIM includes an Aggregated Feature Reconstruction module to
automatically aggregate features for reconstruction, with balanced learning of
domain-agnostic information and images' global structure, and a Lightweight
Decoder module to further benefit the encoder's generalizability. Experiments
on four CDFSL datasets demonstrate that our method achieves state-of-the-art
performance.",2024-12-26T07:43:01Z,http://arxiv.org/abs/2412.19101v1,"Ran Ma, Yixiong Zou, Yuhua Li, Ruixuan Li"
"BSDB-Net: Band-Split Dual-Branch Network with Selective State Spaces
  Mechanism for Monaural Speech Enhancement","Although the complex spectrum-based speech enhancement(SE) methods have
achieved significant performance, coupling amplitude and phase can lead to a
compensation effect, where amplitude information is sacrificed to compensate
for the phase that is harmful to SE. In addition, to further improve the
performance of SE, many modules are stacked onto SE, resulting in increased
model complexity that limits the application of SE. To address these problems,
we proposed a dual-path network based on compressed frequency using Mamba.
First, we extract amplitude and phase information through parallel dual
branches. This approach leverages structured complex spectra to implicitly
capture phase information and solves the compensation effect by decoupling
amplitude and phase, and the network incorporates an interaction module to
suppress unnecessary parts and recover missing components from the other
branch. Second, to reduce network complexity, the network introduces a
band-split strategy to compress the frequency dimension. To further reduce
complexity while maintaining good performance, we designed a Mamba-based module
that models the time and frequency dimensions under linear complexity. Finally,
compared to baselines, our model achieves an average 8.3 times reduction in
computational complexity while maintaining superior performance. Furthermore,
it achieves a 25 times reduction in complexity compared to transformer-based
models.",2024-12-26T07:42:07Z,http://arxiv.org/abs/2412.19099v1,"Cunhang Fan, Enrui Liu, Andong Li, Jianhua Tao, Jian Zhou, Jiahao Li, Chengshi Zheng, Zhao Lv"
"Towards structural softness and enhanced electromechanical responses in
  HfO2 ferroelectrics","Structural softness - often characterized by unstable phonon modes and large
electromechanical responses - is a hallmark of ferroelectric perovskites like
BaTiO3 or Pb(Ti,Zr)O3. Whether HfO2 ferroelectrics present any such structural
softness is still a matter of debate. Here, using first principles
calculations, we predict that it is possible to induce structural instabilities
in hafnia. More specifically, our calculations show that in-plane epitaxial
tensile strain causes a mechanical instability of the ferroelectric phase,
which transforms discontinuously into an antipolar polymorph. Then, upon
release of the tensile strain, the antipolar polymorph transforms back to the
ferroelectric state by a soft phonon instability. We show that the softening is
accompanied by enhancements in the dielectric and piezoelectric responses.
While these transitions occur at high epitaxial strains for pure ferroelectric
HfO2, we show that the required deformations are considerably lowered in
superlattices with other simple oxides, which may facilitate realizing these
effects experimentally.",2024-12-26T07:23:16Z,http://arxiv.org/abs/2412.19093v1,"Binayak Mukherjee, Natalya S. Fedorova, Jorge Íñiguez-González"
"Quantum Algorithm for Vector Set Orthogonal Normalization and Matrix QR
  Decomposition with Polynomial Speedup","Vector set orthogonal normalization and matrix QR decomposition are
fundamental problems in matrix analysis with important applications in many
fields. We know that Gram-Schmidt process is a widely used method to solve
these two problems. However, the existing methods, including Gram-Schmidt
process have problems of high complexity, scaling $O(N^3)$ in the system
dimension $N$, which leads to difficulties when calculating large-scale or
ill-conditioned problems. With the development of quantum information
processing, a series of quantum algorithms have been proposed, providing
advantages and speedups over classical algorithms in many fields. In this
paper, we propose quantum algorithms to solve these two problems based on the
idea of Gram-Schmidt process and quantum phase estimation. The complexity of
proposed quantum algorithms is also theoretically and numerically analyzed. We
find that our algorithms provide polynomial acceleration over the best-known
classical and quantum algorithms on these two problems, scaling
$O(N^2\mathrm{poly}(\log N))$ in the dimension $N$ of the system.",2024-12-26T07:04:34Z,http://arxiv.org/abs/2412.19090v1,"Zi-Ming Li, Yu-xi Liu"
"Assessing Pre-trained Models for Transfer Learning through Distribution
  of Spectral Components","Pre-trained model assessment for transfer learning aims to identify the
optimal candidate for the downstream tasks from a model hub, without the need
of time-consuming fine-tuning. Existing advanced works mainly focus on
analyzing the intrinsic characteristics of the entire features extracted by
each pre-trained model or how well such features fit the target labels. This
paper proposes a novel perspective for pre-trained model assessment through the
Distribution of Spectral Components (DISCO). Through singular value
decomposition of features extracted from pre-trained models, we investigate
different spectral components and observe that they possess distinct
transferability, contributing diversely to the fine-tuning performance.
Inspired by this, we propose an assessment method based on the distribution of
spectral components which measures the proportions of their corresponding
singular values. Pre-trained models with features concentrating on more
transferable components are regarded as better choices for transfer learning.
We further leverage the labels of downstream data to better estimate the
transferability of each spectral component and derive the final assessment
criterion. Our proposed method is flexible and can be applied to both
classification and regression tasks. We conducted comprehensive experiments
across three benchmarks and two tasks including image classification and object
detection, demonstrating that our method achieves state-of-the-art performance
in choosing proper pre-trained models from the model hub for transfer learning.",2024-12-26T06:54:22Z,http://arxiv.org/abs/2412.19085v1,"Tengxue Zhang, Yang Shu, Xinyang Chen, Yifei Long, Chenjuan Guo, Bin Yang"
"Social Optima in Linear Quadratic Graphon Field Control: Analysis via
  Infinite Dimensional Approach","This paper is concerned with linear quadratic graphon field social control
problem where the noises of individual agents are correlated. Compared with the
well-studied mean field system, the graphon field system consists of a large
number of agents coupled weakly via a weighted undirected graph where each node
represents an individual agent. Another notable feature of this paper is that
the dynamics of states of agents are driven by Brownian motions with a
correlation matrix. The infinite dimensional approach is adopted to design the
centralized and decentralized controls for our large population system. By
graphon theory, we prove that the linear quadratic (LQ) social optimum control
problem under the centralized information pattern is equivalent to an LQ
optimal control problem concerned with a stochastic evolution equation, and the
feedback-type optimal centralized control is obtained. Then, by designing an
auxiliary infinite dimensional optimal control problem through agent number
$N\rightarrow\infty$, a set of decentralized strategies are constructed, which
are further shown to be asymptotically social optimal.",2024-12-26T06:46:19Z,http://arxiv.org/abs/2412.19082v1,"De-xuan Xu, Zhun Gou, Nan-jing Huang"
"Graph-Enhanced Dual-Stream Feature Fusion with Pre-Trained Model for
  Acoustic Traffic Monitoring","Microphone array techniques are widely used in sound source localization and
smart city acoustic-based traffic monitoring, but these applications face
significant challenges due to the scarcity of labeled real-world traffic audio
data and the complexity and diversity of application scenarios. The DCASE
Challenge's Task 10 focuses on using multi-channel audio signals to count
vehicles (cars or commercial vehicles) and identify their directions
(left-to-right or vice versa). In this paper, we propose a graph-enhanced
dual-stream feature fusion network (GEDF-Net) for acoustic traffic monitoring,
which simultaneously considers vehicle type and direction to improve detection.
We propose a graph-enhanced dual-stream feature fusion strategy which consists
of a vehicle type feature extraction (VTFE) branch, a vehicle direction feature
extraction (VDFE) branch, and a frame-level feature fusion module to combine
the type and direction feature for enhanced performance. A pre-trained model
(PANNs) is used in the VTFE branch to mitigate data scarcity and enhance the
type features, followed by a graph attention mechanism to exploit temporal
relationships and highlight important audio events within these features. The
frame-level fusion of direction and type features enables fine-grained feature
representation, resulting in better detection performance. Experiments
demonstrate the effectiveness of our proposed method. GEDF-Net is our
submission that achieved 1st place in the DCASE 2024 Challenge Task 10.",2024-12-26T06:28:42Z,http://arxiv.org/abs/2412.19078v1,"Shitong Fan, Feiyang Xiao, Wenbo Wang, Shuhan Qi, Qiaoxi Zhu, Wenwu Wang, Jian Guan"
Effective and secure federated online learning to rank,"Online Learning to Rank (OLTR) optimises ranking models using implicit user
feedback, such as clicks. Unlike traditional Learning to Rank (LTR) methods
that rely on a static set of training data with relevance judgements to learn a
ranking model, OLTR methods update the model continually as new data arrives.
Thus, it addresses several drawbacks such as the high cost of human
annotations, potential misalignment between user preferences and human
judgments, and the rapid changes in user query intents. However, OLTR methods
typically require the collection of searchable data, user queries, and clicks,
which poses privacy concerns for users.
  Federated Online Learning to Rank (FOLTR) integrates OLTR within a Federated
Learning (FL) framework to enhance privacy by not sharing raw data. While
promising, FOLTR methods currently lag behind traditional centralised OLTR due
to challenges in ranking effectiveness, robustness with respect to data
distribution across clients, susceptibility to attacks, and the ability to
unlearn client interactions and data. This thesis presents a comprehensive
study on Federated Online Learning to Rank, addressing its effectiveness,
robustness, security, and unlearning capabilities, thereby expanding the
landscape of FOLTR.",2024-12-26T05:53:10Z,http://arxiv.org/abs/2412.19069v1,Shuyi Wang
Learning Monocular Depth from Events via Egomotion Compensation,"Event cameras are neuromorphically inspired sensors that sparsely and
asynchronously report brightness changes. Their unique characteristics of high
temporal resolution, high dynamic range, and low power consumption make them
well-suited for addressing challenges in monocular depth estimation (e.g.,
high-speed or low-lighting conditions). However, current existing methods
primarily treat event streams as black-box learning systems without
incorporating prior physical principles, thus becoming over-parameterized and
failing to fully exploit the rich temporal information inherent in event camera
data. To address this limitation, we incorporate physical motion principles to
propose an interpretable monocular depth estimation framework, where the
likelihood of various depth hypotheses is explicitly determined by the effect
of motion compensation. To achieve this, we propose a Focus Cost Discrimination
(FCD) module that measures the clarity of edges as an essential indicator of
focus level and integrates spatial surroundings to facilitate cost estimation.
Furthermore, we analyze the noise patterns within our framework and improve it
with the newly introduced Inter-Hypotheses Cost Aggregation (IHCA) module,
where the cost volume is refined through cost trend prediction and multi-scale
cost consistency constraints. Extensive experiments on real-world and synthetic
datasets demonstrate that our proposed framework outperforms cutting-edge
methods by up to 10\% in terms of the absolute relative error metric, revealing
superior performance in predicting accuracy.",2024-12-26T05:41:18Z,http://arxiv.org/abs/2412.19067v1,"Haitao Meng, Chonghao Zhong, Sheng Tang, Lian JunJia, Wenwei Lin, Zhenshan Bing, Yi Chang, Gang Chen, Alois Knoll"
Coarse-grained binning in Drell-Yan transverse momentum spectra,"We report a study of the determination of the intrinsic transverse momentum
of partons, the intrinsic $k_T$, from the dilepton transverse momentum $p_T$ in
Drell-Yan (DY) production at hadron colliders. The result shows that a good
sensitivity to the intrinsic $k_T$ distribution is achieved by measuring
relative ratios between the cross sections of suitably defined low-$p_T$ and
high-$p_T$ regions. The study is performed through both a pseudo-data test and
an extraction from measurements of the DY process by the CMS collaboration.
Since the methodology does not rely on any dedicated partition of bins, this
$p_T$-ratio observable requires less special treatment in very low $p_T$
regions, and propagates lower systematic uncertainties induced from unfolding
or momentum migration, in contrast with previous proposals of using a
fine-binning measurement of the differential cross section.",2024-12-26T05:13:39Z,http://arxiv.org/abs/2412.19060v1,"Wenxiao Zhan, Siqi Yang, Minghui Liu, Francesco Hautmann, Liang Han"
"SpectralKD: Understanding and Optimizing Vision Transformer Distillation
  through Spectral Analysis","Knowledge distillation effectively reduces model complexity while improving
performance, yet the underlying knowledge transfer mechanisms remain poorly
understood. We propose novel spectral analysis methods and guidelines to
optimize distillation, making the knowledge transfer process more
interpretable. Our analysis reveals that CaiT models concentrate information in
their first and last few layers, informing optimal layer selection for feature
map distillation. Surprisingly, we discover that Swin Transformer and CaiT
exhibit similar spectral encoding patterns despite their architectural
differences, enhancing our understanding of transformer architectures and
leading to improved feature map alignment strategies. Based on these insights,
we introduce a simple yet effective spectral alignment method named SpectralKD.
Experimental results demonstrate that following our guidelines enables
SpectralKD to achieve state-of-the-art performance (DeiT-Tiny: $+5.2\%$,
Swin-Tiny: $+1.4\%$ in ImageNet-1k Top-1 accuracy). Furthermore, through
spectral analysis of student models trained with and without distillation, we
show that distilled models mirror spectral patterns of their teachers,
providing a new lens for interpreting knowledge distillation dynamics. Our
code, pre-trained models, and experimental logs will be made publicly
available.",2024-12-26T04:45:05Z,http://arxiv.org/abs/2412.19055v1,"Huiyuan Tian, Bonan Xu, Shijian Li, Gang Pan"
Jasper and Stella: distillation of SOTA embedding models,"A crucial component of many deep learning applications (such as FAQ and RAG)
is dense retrieval, in which embedding models are used to convert raw text to
numerical vectors and then get the most similar text by MIPS (Maximum Inner
Product Search). Some text embedding benchmarks (e.g. MTEB, BEIR, and
AIR-Bench) have been established to evaluate embedding models accurately.
Thanks to these benchmarks, we can use SOTA models; however, the deployment and
application of these models in industry were hampered by their large vector
dimensions and numerous parameters. To alleviate this problem, 1) we present a
distillation technique that can enable a smaller student model to achieve good
performance. 2) Inspired by MRL we present a training approach of reducing the
vector dimensions based on its own vectors or its teacher vectors. 3) We do
simple yet effective alignment training between images and text to make our
model a multimodal encoder. We trained Stella and Jasper models using the
technologies above and achieved high scores on the MTEB leaderboard. We release
the model and data at Hugging Face Hub
(https://huggingface.co/infgrad/jasper_en_vision_language_v1) and the training
logs are at https://api.wandb.ai/links/dunnzhang0/z8jqoqpb.",2024-12-26T04:05:28Z,http://arxiv.org/abs/2412.19048v1,"Dun Zhang, FulongWang"
"Indonesian-English Code-Switching Speech Synthesizer Utilizing
  Multilingual STEN-TTS and Bert LID","Multilingual text-to-speech systems convert text into speech across multiple
languages. In many cases, text sentences may contain segments in different
languages, a phenomenon known as code-switching. This is particularly common in
Indonesia, especially between Indonesian and English. Despite its significance,
no research has yet developed a multilingual TTS system capable of handling
code-switching between these two languages. This study addresses
Indonesian-English code-switching in STEN-TTS. Key modifications include adding
a language identification component to the text-to-phoneme conversion using
finetuned BERT for per-word language identification, as well as removing
language embedding from the base model. Experimental results demonstrate that
the code-switching model achieves superior naturalness and improved speech
intelligibility compared to the Indonesian and English baseline STEN-TTS
models.",2024-12-26T03:37:40Z,http://arxiv.org/abs/2412.19043v1,"Ahmad Alfani Handoyo, Chung Tran, Dessi Puji Lestari, Sakriani Sakti"
"An experimental proposal certification for any three-qubit generalized
  Greenberger-Horne-Zeilinger states based on the fine-grained steering
  inequality","Multi-party quantum steering is an important concept in quantum information
theory and quantum mechanics, typically related to quantum entanglement and
quantum nonlocality. It enables precise manipulation of large quantum systems,
which is essential for large-scale quantum computing, simulations, and quantum
communication. Recently, a quantum steering certification for any three-qubit
generalized Greenberger-Horne-Zeilinger (GGHZ) states based on the fine-grained
steering inequality was proved [Quantum Studies: Mathematics and Foundations,
2022, 9(2): 175-198]. Here we provide an experimental proposal to prepare the
GGHZ states in photon system. The measurement observalbes in each party can be
realized by different polarization optical elements. By choosing the angles of
the waveplates, our experiment proposal can observe the maximum quantum
violation for any three-qubit GGHZ states. Our proposal can be easily extended
to high-dimensional qubits and multi-photon GHZ states, which provides a method
to study the complex multi-party quantum protocols.",2024-12-26T02:36:05Z,http://arxiv.org/abs/2412.19028v1,"Zhi-Hao Bian, Jia-Qi Sun, Yi Shen"
"Channel-Aware Optimal Transport: A Theoretical Framework for Generative
  Communication","Optimal transport has numerous applications, particularly in machine learning
tasks involving generative models. In practice, the transportation process
often encounters an information bottleneck, typically arising from the
conversion of a communication channel into a rate-limited bit pipeline using
error correction codes. While this conversion enables a channel-oblivious
approach to optimal transport, it fails to fully exploit the available degrees
of freedom. Motivated by the emerging paradigm of generative communication,
this paper examines the problem of channel-aware optimal transport, where a
block of i.i.d. random variables is transmitted through a memoryless channel to
generate another block of i.i.d. random variables with a prescribed marginal
distribution such that the end-to-end distortion is minimized. With unlimited
common randomness available to the encoder and decoder, the source-channel
separation architecture is shown to be asymptotically optimal as the
blocklength approaches infinity. On the other hand, in the absence of common
randomness, the source-channel separation architecture is generally suboptimal.
For this scenario, a hybrid coding scheme is proposed, which partially retains
the generative capabilities of the given channel while enabling reliable
transmission of digital information. It is demonstrated that the proposed
hybrid coding scheme can outperform both separation-based and uncoded schemes.",2024-12-26T02:23:08Z,http://arxiv.org/abs/2412.19025v1,"Xiqiang Qu, Ruibin Li, Jun Chen, Lei Yu, Xinbing Wang"
Adaptivity can help exponentially for shadow tomography,"In recent years there has been significant interest in understanding the
statistical complexity of learning from quantum data under the constraint that
one can only make unentangled measurements. While a key challenge in
establishing tight lower bounds in this setting is to deal with the fact that
the measurements can be chosen in an adaptive fashion, a recurring theme has
been that adaptivity offers little advantage over more straightforward,
nonadaptive protocols.
  In this note, we offer a counterpoint to this. We show that for the basic
task of shadow tomography, protocols that use adaptively chosen two-copy
measurements can be exponentially more sample-efficient than any protocol that
uses nonadaptive two-copy measurements.",2024-12-26T02:13:04Z,http://arxiv.org/abs/2412.19022v1,"Sitan Chen, Weiyuan Gong, Zhihan Zhang"
"Relation-aware Hierarchical Prompt for Open-vocabulary Scene Graph
  Generation","Open-vocabulary Scene Graph Generation (OV-SGG) overcomes the limitations of
the closed-set assumption by aligning visual relationship representations with
open-vocabulary textual representations. This enables the identification of
novel visual relationships, making it applicable to real-world scenarios with
diverse relationships. However, existing OV-SGG methods are constrained by
fixed text representations, limiting diversity and accuracy in image-text
alignment. To address these challenges, we propose the Relation-Aware
Hierarchical Prompting (RAHP) framework, which enhances text representation by
integrating subject-object and region-specific relation information. Our
approach utilizes entity clustering to address the complexity of relation
triplet categories, enabling the effective integration of subject-object
information. Additionally, we utilize a large language model (LLM) to generate
detailed region-aware prompts, capturing fine-grained visual interactions and
improving alignment between visual and textual modalities. RAHP also introduces
a dynamic selection mechanism within Vision-Language Models (VLMs), which
adaptively selects relevant text prompts based on the visual content, reducing
noise from irrelevant prompts. Extensive experiments on the Visual Genome and
Open Images v6 datasets demonstrate that our framework consistently achieves
state-of-the-art performance, demonstrating its effectiveness in addressing the
challenges of open-vocabulary scene graph generation.",2024-12-26T02:12:37Z,http://arxiv.org/abs/2412.19021v1,"Tao Liu, Rongjie Li, Chongyu Wang, Xuming He"
"Harnessing high-dimensional symmetric and anti-symmetric Bell states
  through quantum interference","High-dimensional quantum entanglement is an essential resource in quantum
technology since it provides benefits in increasing the information capacity
and processing speed. Thus, the controlled harnessing of high-dimensional
entanglement has long been hailed as a necessary prerequisite towards practical
quantum applications. By using a deterministic quantum state filter that
implemented through quantum interference, we present a generalised formulation
for the complete high-dimensional symmetric and anti-symmetric Bell basis, and
experimentally prepare four-dimensional orbital angular momentum Bell states
that provide the well-behaved symmetric or anti-symmetric properties.
Additionally, we use a concise yet efficient scan of temporal delay to directly
observe high-dimensional two-photon interference effects in spatial modes.
These results provide an alternative way for harnessing high-dimensional
entanglement, and may facilitate the use of quantum interference for more
complex quantum information processing tasks that beyond qubits.",2024-12-26T02:05:01Z,http://arxiv.org/abs/2412.19019v1,"Ling Hong, Yuning Zhang, Yuanyuan Chen, Lixiang Chen"
"Enhancing Audiovisual Speech Recognition through Bifocal Preference
  Optimization","Audiovisual Automatic Speech Recognition (AV-ASR) aims to improve speech
recognition accuracy by leveraging visual signals. It is particularly
challenging in unconstrained real-world scenarios across various domains due to
noisy acoustic environments, spontaneous speech, and the uncertain use of
visual information. Most previous works fine-tune audio-only ASR models on
audiovisual datasets, optimizing them for conventional ASR objectives. However,
they often neglect visual features and common errors in unconstrained video
scenarios. In this paper, we propose using a preference optimization strategy
to improve speech recognition accuracy for real-world videos. First, we create
preference data via simulating common errors that occurred in AV-ASR from two
focals: manipulating the audio or vision input and rewriting the output
transcript. Second, we propose BPO-AVASR, a Bifocal Preference Optimization
method to improve AV-ASR models by leveraging both input-side and output-side
preference. Extensive experiments demonstrate that our approach significantly
improves speech recognition accuracy across various domains, outperforming
previous state-of-the-art models on real-world video speech recognition.",2024-12-26T00:26:45Z,http://arxiv.org/abs/2412.19005v1,"Yihan Wu, Yichen Lu, Yifan Peng, Xihua Wang, Ruihua Song, Shinji Watanabe"
"Impact of resummation on the production and experimental bounds of
  scalar high-electric-charge objects","A one-loop Dyson-Schwinger-like resummation scheme is applied to scalar
High-Electric-Charge compact Objects (HECOs), extending previous work on
spin-1/2 case. The electromagnetic interactions of HECOs are considered within
the framework of strongly coupled scalar Quantun Electrodynamics. The
resummation amounts to determining non-trivial ultraviolet (UV) fixed points,
at which the effective Lagrangian, which will lead to the pertinent predictions
on the cross sections, is computed. In contrast to the fermionic HECO case, in
which the fixed point structure was determined solely by the interactions of
the HECOs with the photon field, in the scalar case the existence of
non-trivial UV fixed points requires the presence of additional strong self
interactions among the HECOs. Our resummation scheme, which is notably
different from a lattice strong-coupling approach, makes the computation of the
pertinent scalar-HECO-production cross sections reliable, thus allowing
revisiting the mass bounds obtained from searches for such objects in current
or future colliders. Our MadGraph implementation of the results leads to
enhanced (up to ~30%) lower bounds on the mass of scalar HECOs, as compared to
those extracted from the tree-level processes typically used in LHC collider
searches by ATLAS and MoEDAL experiments.",2024-12-25T23:13:36Z,http://arxiv.org/abs/2412.19001v1,"Jean Alexandre, Nick E. Mavromatos, Vasiliki A. Mitsou, Emanuela Musumeci"
"GeoMatch++: Morphology Conditioned Geometry Matching for
  Multi-Embodiment Grasping","Despite recent progress on multi-finger dexterous grasping, current methods
focus on single grippers and unseen objects, and even the ones that explore
cross-embodiment, often fail to generalize well to unseen end-effectors. This
work addresses the problem of dexterous grasping generalization to unseen
end-effectors via a unified policy that learns correlation between gripper
morphology and object geometry. Robot morphology contains rich information
representing how joints and links connect and move with respect to each other
and thus, we leverage it through attention to learn better end-effector
geometry features. Our experiments show an average of 9.64% increase in grasp
success rate across 3 out-of-domain end-effectors compared to previous methods.",2024-12-25T22:36:57Z,http://arxiv.org/abs/2412.18998v1,"Yunze Wei, Maria Attarian, Igor Gilitschenski"
"MiTREE: Multi-input Transformer Ecoregion Encoder for Species
  Distribution Modelling","Climate change poses an extreme threat to biodiversity, making it imperative
to efficiently model the geographical range of different species. The
availability of large-scale remote sensing images and environmental data has
facilitated the use of machine learning in Species Distribution Models (SDMs),
which aim to predict the presence of a species at any given location.
Traditional SDMs, reliant on expert observation, are labor-intensive, but
advancements in remote sensing and citizen science data have facilitated
machine learning approaches to SDM development. However, these models often
struggle with leveraging spatial relationships between different inputs -- for
instance, learning how climate data should inform the data present in satellite
imagery -- without upsampling or distorting the original inputs. Additionally,
location information and ecological characteristics at a location play a
crucial role in predicting species distribution models, but these aspects have
not yet been incorporated into state-of-the-art approaches. In this work, we
introduce MiTREE: a multi-input Vision-Transformer-based model with an
ecoregion encoder. MiTREE computes spatial cross-modal relationships without
upsampling as well as integrates location and ecological context. We evaluate
our model on the SatBird Summer and Winter datasets, the goal of which is to
predict bird species encounter rates, and we find that our approach improves
upon state-of-the-art baselines.",2024-12-25T22:20:47Z,http://arxiv.org/abs/2412.18995v1,"Theresa Chen, Yao-Yi Chiang"
"Geospatial Data Fusion: Combining Lidar, SAR, and Optical Imagery with
  AI for Enhanced Urban Mapping","This study explores the integration of Lidar, Synthetic Aperture Radar (SAR),
and optical imagery through advanced artificial intelligence techniques for
enhanced urban mapping. By fusing these diverse geospatial datasets, we aim to
overcome the limitations associated with single-sensor data, achieving a more
comprehensive representation of urban environments. The research employs Fully
Convolutional Networks (FCNs) as the primary deep learning model for urban
feature extraction, enabling precise pixel-wise classification of essential
urban elements, including buildings, roads, and vegetation. To optimize the
performance of the FCN model, we utilize Particle Swarm Optimization (PSO) for
hyperparameter tuning, significantly enhancing model accuracy. Key findings
indicate that the FCN-PSO model achieved a pixel accuracy of 92.3% and a mean
Intersection over Union (IoU) of 87.6%, surpassing traditional single-sensor
approaches. These results underscore the potential of fused geospatial data and
AI-driven methodologies in urban mapping, providing valuable insights for urban
planning and management. The implications of this research pave the way for
future developments in real-time mapping and adaptive urban infrastructure
planning.",2024-12-25T22:17:31Z,http://arxiv.org/abs/2412.18994v1,"Sajjad Afroosheh, Mohammadreza Askari"
"On the architecture of the Symplectic $(A_\infty,2)$-Category","This note relates to the author's construction of the Symplectic
$(A_\infty,2)$-Category, $\mathsf{Symp}$. Here we explain two ways of encoding
the information in $\mathsf{Symp}$, one topological, one algebraic. The
topological encoding is as an $(A_\infty,2)$-flow category, which we define
here. The algebraic encoding is as a linear $(A_\infty,2)$-category, which we
extract from the topological encoding. In upcoming work, the author and
Wehrheim plan to use the adiabatic Fredholm theory recently developed by
Bottman-Wehrheim to construct $\mathsf{Symp}$ as an $(A_\infty,2)$-flow
category.
  The definition of linear $(A_\infty,2)$-category that we give in this note is
different than the one proposed by Bottman-Carmeli. The recursive structure of
the 2-associahedra identifies faces with fiber products of 2-associahedra over
associahedra, and these fiber products led Bottman-Carmeli to associate
operations to singular chains on 2-associahedra. The innovation in our new
definition of linear $(A_\infty,2)$-category is to extend the family of
2-associahedra to include all fiber products of 2-associahedra over
associahedra. This allows us to associate operations to cellular chains, which
in particular enables us to produce a definition that involves only one
operation in each arity, governed by a collection of $(A_\infty,2)$-equations.",2024-12-25T22:11:18Z,http://arxiv.org/abs/2412.18993v1,Nathaniel Bottman
"Detection and classification of DDoS flooding attacks by machine
  learning method","This study focuses on a method for detecting and classifying distributed
denial of service (DDoS) attacks, such as SYN Flooding, ACK Flooding, HTTP
Flooding, and UDP Flooding, using neural networks. Machine learning,
particularly neural networks, is highly effective in detecting malicious
traffic. A dataset containing normal traffic and various DDoS attacks was used
to train a neural network model with a 24-106-5 architecture. The model
achieved high Accuracy (99.35%), Precision (99.32%), Recall (99.54%), and
F-score (0.99) in the classification task. All major attack types were
correctly identified. The model was also further tested in the lab using
virtual infrastructures to generate normal and DDoS traffic. The results showed
that the model can accurately classify attacks under near-real-world
conditions, demonstrating 95.05% accuracy and balanced F-score scores for all
attack types. This confirms that neural networks are an effective tool for
detecting DDoS attacks in modern information security systems.",2024-12-25T21:58:52Z,http://arxiv.org/abs/2412.18990v1,"Dmytro Tymoshchuk, Oleh Yasniy, Mykola Mytnyk, Nataliya Zagorodna, Vitaliy Tymoshchuk"
"HAND: Hierarchical Attention Network for Multi-Scale Handwritten
  Document Recognition and Layout Analysis","Handwritten document recognition (HDR) is one of the most challenging tasks
in the field of computer vision, due to the various writing styles and complex
layouts inherent in handwritten texts. Traditionally, this problem has been
approached as two separate tasks, handwritten text recognition and layout
analysis, and struggled to integrate the two processes effectively. This paper
introduces HAND (Hierarchical Attention Network for Multi-Scale Document), a
novel end-to-end and segmentation-free architecture for simultaneous text
recognition and layout analysis tasks. Our model's key components include an
advanced convolutional encoder integrating Gated Depth-wise Separable and
Octave Convolutions for robust feature extraction, a Multi-Scale Adaptive
Processing (MSAP) framework that dynamically adjusts to document complexity and
a hierarchical attention decoder with memory-augmented and sparse attention
mechanisms. These components enable our model to scale effectively from
single-line to triple-column pages while maintaining computational efficiency.
Additionally, HAND adopts curriculum learning across five complexity levels. To
improve the recognition accuracy of complex ancient manuscripts, we fine-tune
and integrate a Domain-Adaptive Pre-trained mT5 model for post-processing
refinement. Extensive evaluations on the READ 2016 dataset demonstrate the
superior performance of HAND, achieving up to 59.8% reduction in CER for
line-level recognition and 31.2% for page-level recognition compared to
state-of-the-art methods. The model also maintains a compact size of 5.60M
parameters while establishing new benchmarks in both text recognition and
layout analysis. Source code and pre-trained models are available at :
https://github.com/MHHamdan/HAND.",2024-12-25T20:36:29Z,http://arxiv.org/abs/2412.18981v1,"Mohammed Hamdan, Abderrahmane Rahiche, Mohamed Cheriet"
CGCOD: Class-Guided Camouflaged Object Detection,"Camouflaged Object Detection (COD) is designed to identify objects that blend
seamlessly with their surroundings. Due to the complexity of camouflaged
objects (such as shape, color, and texture), their semantic cues are often
blurred or completely lost, posing a significant challenge for COD. Existing
COD methods often rely on visual features, which are not stable enough in
changeable camouflage environments. This instability leads to false positives
and false negatives, resulting in incomplete or inaccurate segmentation
results. In this paper, to solve this problem, we propose a new task,
Class-Guided Camouflaged Object Detection (CG-COD), which extends the
traditional COD task by introducing object class knowledge, significantly
improving the robustness and segmentation accuracy of the model in complex
environments. Toward this end, we construct a dataset, CamoClass, containing
the camouflaged objects in the real scenes and their corresponding class
annotation. Based on this, we propose a multi-stage framework CGNet which
consists of a plug-and-play class prompt generator and a class-guided detector.
Under the guidance of textual information, CGNet enables efficient
segmentation. It is worth emphasizing that for the first time, we extend the
object class annotations on existing COD benchmark datasets, and introduce a
flexible framework to improve the performance of the existing COD model under
text guidance.",2024-12-25T19:38:32Z,http://arxiv.org/abs/2412.18977v1,"Chenxi Zhang, Qing Zhang, Jiayun Wu, Youwei Pang"
Injecting Bias into Text Classification Models using Backdoor Attacks,"The rapid growth of natural language processing (NLP) and pre-trained
language models have enabled accurate text classification in a variety of
settings. However, text classification models are susceptible to backdoor
attacks, where an attacker embeds a trigger into the victim model to make the
model predict attacker-desired labels in targeted scenarios. In this paper, we
propose to utilize backdoor attacks for a new purpose: bias injection. We
develop a backdoor attack in which a subset of the training dataset is poisoned
to associate strong male actors with negative sentiment. We execute our attack
on two popular text classification datasets (IMDb and SST) and seven different
models ranging from traditional Doc2Vec-based models to LSTM networks and
modern transformer-based BERT and RoBERTa models. Our results show that the
reduction in backdoored models' benign classification accuracy is limited,
implying that our attacks remain stealthy, whereas the models successfully
learn to associate strong male actors with negative sentiment (100% attack
success rate with &gt;= 3% poison rate). Attacks on BERT and RoBERTa are
particularly more stealthy and effective, demonstrating an increased risk of
using modern and larger models. We also measure the generalizability of our
bias injection by proposing two metrics: (i) U-BBSR which uses previously
unseen words when measuring attack success, and (ii) P-BBSR which measures
attack success using paraphrased test samples. U-BBSR and P-BBSR results show
that the bias injected by our attack can go beyond memorizing a trigger phrase.",2024-12-25T19:32:02Z,http://arxiv.org/abs/2412.18975v1,"A. Dilara Yavuz, M. Emre Gursoy"
"Don't Lose Yourself: Boosting Multimodal Recommendation via Reducing
  Node-neighbor Discrepancy in Graph Convolutional Network","The rapid expansion of multimedia contents has led to the emergence of
multimodal recommendation systems. It has attracted increasing attention in
recommendation systems because its full utilization of data from different
modalities alleviates the persistent data sparsity problem. As such, multimodal
recommendation models can learn personalized information about nodes in terms
of visual and textual. To further alleviate the data sparsity problem, some
previous works have introduced graph convolutional networks (GCNs) for
multimodal recommendation systems, to enhance the semantic representation of
users and items by capturing the potential relationships between them. However,
adopting GCNs inevitably introduces the over-smoothing problem, which make
nodes to be too similar. Unfortunately, incorporating multimodal information
will exacerbate this challenge because nodes that are too similar will lose the
personalized information learned through multimodal information. To address
this problem, we propose a novel model that retains the personalized
information of ego nodes during feature aggregation by Reducing Node-neighbor
Discrepancy (RedN^nD). Extensive experiments on three public datasets show that
RedN^nD achieves state-of-the-art performance on accuracy and robustness, with
significant improvements over existing GCN-based multimodal frameworks.",2024-12-25T18:41:36Z,http://arxiv.org/abs/2412.18962v1,"Zheyu Chen, Jinfeng Xu, Haibo Hu"
Musings About the Future of Search: A Return to the Past?,"When you have a question, the most effective way to have the question
answered is to directly connect with experts on the topic and have a
conversation with them. Prior to the invention of writing, this was the only
way. Although effective, this solution exhibits scalability challenges. Writing
allowed knowledge to be materialized, preserved, and replicated, enabling the
development of different technologies over the centuries to connect information
seekers with relevant information. This progression ultimately culminated in
the ten-blue-links web search paradigm we're familiar with, just before the
recent emergence of generative AI. However, we often forget that consuming
static content is an imperfect solution. With the advent of large language
models, it has become possible to develop a superior experience by allowing
users to directly engage with experts. These interactions can of course satisfy
information needs, but expert models can do so much more. This coming future
requires reimagining search.",2024-12-25T18:09:34Z,http://arxiv.org/abs/2412.18956v1,"Jimmy Lin, Pankaj Gupta, Will Horn, Gilad Mishne"
"Leave-One-EquiVariant: Alleviating invariance-related information loss
  in contrastive music representations","Contrastive learning has proven effective in self-supervised musical
representation learning, particularly for Music Information Retrieval (MIR)
tasks. However, reliance on augmentation chains for contrastive view generation
and the resulting learnt invariances pose challenges when different downstream
tasks require sensitivity to certain musical attributes. To address this, we
propose the Leave One EquiVariant (LOEV) framework, which introduces a
flexible, task-adaptive approach compared to previous work by selectively
preserving information about specific augmentations, allowing the model to
maintain task-relevant equivariances. We demonstrate that LOEV alleviates
information loss related to learned invariances, improving performance on
augmentation related tasks and retrieval without sacrificing general
representation quality. Furthermore, we introduce a variant of LOEV, LOEV++,
which builds a disentangled latent space by design in a self-supervised manner,
and enables targeted retrieval based on augmentation related attributes.",2024-12-25T18:06:44Z,http://arxiv.org/abs/2412.18955v1,"Julien Guinot, Elio Quinton, György Fazekas"
"MedHallBench: A New Benchmark for Assessing Hallucination in Medical
  Large Language Models","Medical Large Language Models (MLLMs) have demonstrated potential in
healthcare applications, yet their propensity for hallucinations -- generating
medically implausible or inaccurate information -- presents substantial risks
to patient care. This paper introduces MedHallBench, a comprehensive benchmark
framework for evaluating and mitigating hallucinations in MLLMs. Our
methodology integrates expert-validated medical case scenarios with established
medical databases to create a robust evaluation dataset. The framework employs
a sophisticated measurement system that combines automated ACHMI (Automatic
Caption Hallucination Measurement in Medical Imaging) scoring with rigorous
clinical expert evaluations and utilizes reinforcement learning methods to
achieve automatic annotation. Through an optimized reinforcement learning from
human feedback (RLHF) training pipeline specifically designed for medical
applications, MedHallBench enables thorough evaluation of MLLMs across diverse
clinical contexts while maintaining stringent accuracy standards. We conducted
comparative experiments involving various models, utilizing the benchmark to
establish a baseline for widely adopted large language models (LLMs). Our
findings indicate that ACHMI provides a more nuanced understanding of the
effects of hallucinations compared to traditional metrics, thereby highlighting
its advantages in hallucination assessment. This research establishes a
foundational framework for enhancing MLLMs' reliability in healthcare settings
and presents actionable strategies for addressing the critical challenge of AI
hallucinations in medical applications.",2024-12-25T16:51:29Z,http://arxiv.org/abs/2412.18947v1,"Kaiwen Zuo, Yirui Jiang"
"Label-free SERS Discrimination of Proline from Hydroxylated Proline at
  Single-molecule Level Assisted by a Deep Learning Model","Discriminating the low-abundance hydroxylated proline from hydroxylated
proline is crucial for monitoring diseases and eval-uating therapeutic outcomes
that require single-molecule sensors. While the plasmonic nanopore sensor can
detect the hydrox-ylation with single-molecule sensitivity by surface enhanced
Raman spectroscopy (SERS), it suffers from intrinsic fluctuations of
single-molecule signals as well as strong interference from citrates. Here, we
used the occurrence frequency histogram of the single-molecule SERS peaks to
extract overall dataset spectral features, overcome the signal fluctuations and
investigate the citrate-replaced plasmonic nanopore sensors for clean and
distinguishable signals of proline and hydroxylated proline. By ligand exchange
of the citrates by analyte molecules, the representative peaks of citrates
decreased with incubation time, prov-ing occupation of the plasmonic hot spot
by the analytes. As a result, the discrimination of the single-molecule SERS
signals of proline and hydroxylated proline was possible with the convolutional
neural network model with 96.6% accuracy.",2024-12-25T15:46:52Z,http://arxiv.org/abs/2412.18935v1,"Yingqi Zhao, Kuo Zhan, Pei-Lin Xin, Zuyan Chen, Shuai Li, Francesco De Angelis, Jianan Huang"
TINQ: Temporal Inconsistency Guided Blind Video Quality Assessment,"Blind video quality assessment (BVQA) has been actively researched for
user-generated content (UGC) videos. Recently, super-resolution (SR) techniques
have been widely applied in UGC. Therefore, an effective BVQA method for both
UGC and SR scenarios is essential. Temporal inconsistency, referring to
irregularities between consecutive frames, is relevant to video quality.
Current BVQA approaches typically model temporal relationships in UGC videos
using statistics of motion information, but inconsistencies remain unexplored.
Additionally, different from temporal inconsistency in UGC videos, such
inconsistency in SR videos is amplified due to upscaling algorithms. In this
paper, we introduce the Temporal Inconsistency Guided Blind Video Quality
Assessment (TINQ) metric, demonstrating that exploring temporal inconsistency
is crucial for effective BVQA. Since temporal inconsistencies vary between UGC
and SR videos, they are calculated in different ways. Based on this, a spatial
module highlights inconsistent areas across consecutive frames at coarse and
fine granularities. In addition, a temporal module aggregates features over
time in two stages. The first stage employs a visual memory capacity block to
adaptively segment the time dimension based on estimated complexity, while the
second stage focuses on selecting key features. The stages work together
through Consistency-aware Fusion Units to regress cross-time-scale video
quality. Extensive experiments on UGC and SR video quality datasets show that
our method outperforms existing state-of-the-art BVQA methods. Code is
available at https://github.com/Lighting-YXLI/TINQ.",2024-12-25T15:43:41Z,http://arxiv.org/abs/2412.18933v1,"Yixiao Li, Xiaoyuan Yang, Weide Liu, Xin Jin, Xu Jia, Yukun Lai, Haotao Liu, Paul L Rosin, Wei Zhou"
"Malware Classification using a Hybrid Hidden Markov Model-Convolutional
  Neural Network","The proliferation of malware variants poses a significant challenges to
traditional malware detection approaches, such as signature-based methods,
necessitating the development of advanced machine learning techniques. In this
research, we present a novel approach based on a hybrid architecture combining
features extracted using a Hidden Markov Model (HMM), with a Convolutional
Neural Network (CNN) then used for malware classification. Inspired by the
strong results in previous work using an HMM-Random Forest model, we propose
integrating HMMs, which serve to capture sequential patterns in opcode
sequences, with CNNs, which are adept at extracting hierarchical features. We
demonstrate the effectiveness of our approach on the popular Malicia dataset,
and we obtain superior performance, as compared to other machine learning
methods -- our results surpass the aforementioned HMM-Random Forest model. Our
findings underscore the potential of hybrid HMM-CNN architectures in bolstering
malware classification capabilities, offering several promising avenues for
further research in the field of cybersecurity.",2024-12-25T15:34:57Z,http://arxiv.org/abs/2412.18932v1,"Ritik Mehta, Olha Jureckova, Mark Stamp"
"Graph Cut-guided Maximal Coding Rate Reduction for Learning Image
  Embedding and Clustering","In the era of pre-trained models, image clustering task is usually addressed
by two relevant stages: a) to produce features from pre-trained vision models;
and b) to find clusters from the pre-trained features. However, these two
stages are often considered separately or learned by different paradigms,
leading to suboptimal clustering performance. In this paper, we propose a
unified framework, termed graph Cut-guided Maximal Coding Rate Reduction
(CgMCR$^2$), for jointly learning the structured embeddings and the clustering.
To be specific, we attempt to integrate an efficient clustering module into the
principled framework for learning structured representation, in which the
clustering module is used to provide partition information to guide the
cluster-wise compression and the learned embeddings is aligned to desired
geometric structures in turn to help for yielding more accurate partitions. We
conduct extensive experiments on both standard and out-of-domain image datasets
and experimental results validate the effectiveness of our approach.",2024-12-25T15:20:54Z,http://arxiv.org/abs/2412.18930v1,"W. He, Z. Huang, X. Meng, X. Qi, R. Xiao, C. -G. Li"
"UNIC-Adapter: Unified Image-instruction Adapter with Multi-modal
  Transformer for Image Generation","Recently, text-to-image generation models have achieved remarkable
advancements, particularly with diffusion models facilitating high-quality
image synthesis from textual descriptions. However, these models often struggle
with achieving precise control over pixel-level layouts, object appearances,
and global styles when using text prompts alone. To mitigate this issue,
previous works introduce conditional images as auxiliary inputs for image
generation, enhancing control but typically necessitating specialized models
tailored to different types of reference inputs. In this paper, we explore a
new approach to unify controllable generation within a single framework.
Specifically, we propose the unified image-instruction adapter (UNIC-Adapter)
built on the Multi-Modal-Diffusion Transformer architecture, to enable flexible
and controllable generation across diverse conditions without the need for
multiple specialized models. Our UNIC-Adapter effectively extracts multi-modal
instruction information by incorporating both conditional images and task
instructions, injecting this information into the image generation process
through a cross-attention mechanism enhanced by Rotary Position Embedding.
Experimental results across a variety of tasks, including pixel-level spatial
control, subject-driven image generation, and style-image-based image
synthesis, demonstrate the effectiveness of our UNIC-Adapter in unified
controllable image generation.",2024-12-25T15:19:02Z,http://arxiv.org/abs/2412.18928v1,"Lunhao Duan, Shanshan Zhao, Wenjun Yan, Yinglun Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Mingming Gong, Gui-Song Xia"
Exemplar-condensed Federated Class-incremental Learning,"We propose Exemplar-Condensed federated class-incremental learning (ECoral)
to distil the training characteristics of real images from streaming data into
informative rehearsal exemplars. The proposed method eliminates the limitations
of exemplar selection in replay-based approaches for mitigating catastrophic
forgetting in federated continual learning (FCL). The limitations particularly
related to the heterogeneity of information density of each summarized data.
Our approach maintains the consistency of training gradients and the
relationship to past tasks for the summarized exemplars to represent the
streaming data compared to the original images effectively. Additionally, our
approach reduces the information-level heterogeneity of the summarized data by
inter-client sharing of the disentanglement generative model. Extensive
experiments show that our ECoral outperforms several state-of-the-art methods
and can be seamlessly integrated with many existing approaches to enhance
performance.",2024-12-25T15:13:40Z,http://arxiv.org/abs/2412.18926v1,"Rui Sun, Yumin Zhang, Varun Ojha, Tejal Shah, Haoran Duan, Bo Wei, Rajiv Ranjan"
"An Attentive Dual-Encoder Framework Leveraging Multimodal Visual and
  Semantic Information for Automatic OSAHS Diagnosis","Obstructive sleep apnea-hypopnea syndrome (OSAHS) is a common sleep disorder
caused by upper airway blockage, leading to oxygen deprivation and disrupted
sleep. Traditional diagnosis using polysomnography (PSG) is expensive,
time-consuming, and uncomfortable. Existing deep learning methods using facial
image analysis lack accuracy due to poor facial feature capture and limited
sample sizes. To address this, we propose a multimodal dual encoder model that
integrates visual and language inputs for automated OSAHS diagnosis. The model
balances data using randomOverSampler, extracts key facial features with
attention grids, and converts physiological data into meaningful text.
Cross-attention combines image and text data for better feature extraction, and
ordered regression loss ensures stable learning. Our approach improves
diagnostic efficiency and accuracy, achieving 91.3% top-1 accuracy in a
four-class severity classification task, demonstrating state-of-the-art
performance. Code will be released upon acceptance.",2024-12-25T14:42:17Z,http://arxiv.org/abs/2412.18919v1,"Yingchen Wei, Xihe Qiu, Xiaoyu Tan, Jingjing Huang, Wei Chu, Yinghui Xu, Yuan Qi"
"Open-Vocabulary Panoptic Segmentation Using BERT Pre-Training of
  Vision-Language Multiway Transformer Model","Open-vocabulary panoptic segmentation remains a challenging problem. One of
the biggest difficulties lies in training models to generalize to an unlimited
number of classes using limited categorized training data. Recent popular
methods involve large-scale vision-language pre-trained foundation models, such
as CLIP. In this paper, we propose OMTSeg for open-vocabulary segmentation
using another large-scale vision-language pre-trained model called BEiT-3 and
leveraging the cross-modal attention between visual and linguistic features in
BEiT-3 to achieve better performance. Experiments result demonstrates that
OMTSeg performs favorably against state-of-the-art models.",2024-12-25T14:31:00Z,http://arxiv.org/abs/2412.18917v1,"Yi-Chia Chen, Wei-Hua Li, Chu-Song Chen"
"Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With
  Structured Memories","Long-range tasks require reasoning over long inputs. Existing solutions
either need large compute budgets, training data, access to model weights, or
use complex, task-specific approaches. We present PRISM, which alleviates these
concerns by processing information as a stream of chunks, maintaining a
structured in-context memory specified by a typed hierarchy schema. This
approach demonstrates superior performance to baselines on diverse tasks while
using at least 4x smaller contexts than long-context models. Moreover, PRISM is
token-efficient. By producing short outputs and efficiently leveraging
key-value (KV) caches, it achieves up to 54% cost reduction when compared to
alternative short-context approaches. The method also scales down to tiny
information chunks (e.g., 500 tokens) without increasing the number of tokens
encoded or sacrificing quality. Furthermore, we show that it is possible to
generate schemas to generalize our approach to new tasks with minimal effort.",2024-12-25T14:14:31Z,http://arxiv.org/abs/2412.18914v1,"Dulhan Jayalath, James Bradley Wendt, Nicholas Monath, Sandeep Tata, Beliz Gunel"
Robust Target Speaker Direction of Arrival Estimation,"In multi-speaker environments the direction of arrival (DOA) of a target
speaker is key for improving speech clarity and extracting target speaker's
voice. However, traditional DOA estimation methods often struggle in the
presence of noise, reverberation, and particularly when competing speakers are
present. To address these challenges, we propose RTS-DOA, a robust real-time
DOA estimation system. This system innovatively uses the registered speech of
the target speaker as a reference and leverages full-band and sub-band spectral
information from a microphone array to estimate the DOA of the target speaker's
voice. Specifically, the system comprises a speech enhancement module for
initially improving speech quality, a spatial module for learning spatial
information, and a speaker module for extracting voiceprint features.
Experimental results on the LibriSpeech dataset demonstrate that our RTS-DOA
system effectively tackles multi-speaker scenarios and established new optimal
benchmarks.",2024-12-25T14:04:21Z,http://arxiv.org/abs/2412.18913v1,"Zixuan Li, Shulin He, Xueliang Zhang"
"Research Experiment on Multi-Model Comparison for Chinese Text
  Classification Tasks","With the explosive growth of Chinese text data and advancements in natural
language processing technologies, Chinese text classification has become one of
the key techniques in fields such as information retrieval and sentiment
analysis, attracting increasing attention. This paper conducts a comparative
study on three deep learning models:TextCNN, TextRNN, and FastText.specifically
for Chinese text classification tasks. By conducting experiments on the
THUCNews dataset, the performance of these models is evaluated, and their
applicability in different scenarios is discussed.",2024-12-25T13:54:40Z,http://arxiv.org/abs/2412.18908v1,JiaCheng Li
"FedCFA: Alleviating Simpson's Paradox in Model Aggregation with
  Counterfactual Federated Learning","Federated learning (FL) is a promising technology for data privacy and
distributed optimization, but it suffers from data imbalance and heterogeneity
among clients. Existing FL methods try to solve the problems by aligning client
with server model or by correcting client model with control variables. These
methods excel on IID and general Non-IID data but perform mediocrely in
Simpson's Paradox scenarios. Simpson's Paradox refers to the phenomenon that
the trend observed on the global dataset disappears or reverses on a subset,
which may lead to the fact that global model obtained through aggregation in FL
does not accurately reflect the distribution of global data. Thus, we propose
FedCFA, a novel FL framework employing counterfactual learning to generate
counterfactual samples by replacing local data critical factors with global
average data, aligning local data distributions with the global and mitigating
Simpson's Paradox effects. In addition, to improve the quality of
counterfactual samples, we introduce factor decorrelation (FDC) loss to reduce
the correlation among features and thus improve the independence of extracted
factors. We conduct extensive experiments on six datasets and verify that our
method outperforms other FL methods in terms of efficiency and global model
accuracy under limited communication rounds.",2024-12-25T13:35:54Z,http://arxiv.org/abs/2412.18904v1,"Zhonghua Jiang, Jimin Xu, Shengyu Zhang, Tao Shen, Jiwei Li, Kun Kuang, Haibin Cai, Fei Wu"
"Effects of chiral symmetry restoration on dilepton production in heavy
  ion collisions","Because of their weak interactions with the strongly interacting matter
produced in relativistic heavy-ion collisions, dileptons provide an ideal probe
of the early dynamics of these collisions. Here, we study dilepton production
using a partonic transport model that is based on an extended
Nambu-Jona-Lasinio (NJL) model. In this model, the in-medium quark masses
decrease with increasing temperature as a result of the restoration of chiral
symmetry. We find that the extracted temperature from dileptons of intermediate
masses agrees well with the temperature of the partonic matter, suggesting that
dilepton production can be used as a thermometer for the produced partonic
matter. Our results also indicate that the extracted in-medium quark masses
decrease with increasing dilepton temperature, implying that dilepton
production can further serve as a probe of chiral symmetry restoration in high
energy heavy-ion collisions.",2024-12-25T12:57:25Z,http://arxiv.org/abs/2412.18895v1,"Wen-Hao Zhou, Che Ming Ko, Kai-Jia Sun"
"CoEvo: Continual Evolution of Symbolic Solutions Using Large Language
  Models","Large Language Models (LLMs) have emerged as transformative tools in
artificial intelligence, capable of processing and understanding extensive
human knowledge to enhance problem-solving across various domains. This paper
explores the potential of LLMs to drive the discovery of symbolic solutions
within scientific and engineering disciplines, where such solutions are crucial
for advancing theoretical and practical applications. We propose a novel
framework that utilizes LLMs in an evolutionary search methodology, augmented
by a dynamic knowledge library that integrates and refines insights in an
\textit{open-ended manner}. This approach aims to tackle the dual challenges of
efficiently navigating complex symbolic representation spaces and leveraging
both existing and newly generated knowledge to foster open-ended innovation. By
enabling LLMs to interact with and expand upon a knowledge library, we
facilitate the continuous generation of novel solutions in diverse forms such
as language, code, and mathematical expressions. Our experimental results
demonstrate that this method not only enhances the efficiency of searching for
symbolic solutions but also supports the ongoing discovery process, akin to
human scientific endeavors. This study represents a first effort in
conceptualizing the search for symbolic solutions as a lifelong, iterative
process, marking a significant step towards harnessing AI in the perpetual
pursuit of scientific and engineering breakthroughs. We have open-sourced our
code and data, please visit \url{https://github.com/pgg3/CoEvo} for more
information.",2024-12-25T12:27:27Z,http://arxiv.org/abs/2412.18890v1,"Ping Guo, Qingfu Zhang, Xi Lin"
"HV-BEV: Decoupling Horizontal and Vertical Feature Sampling for
  Multi-View 3D Object Detection","The application of vision-based multi-view environmental perception system
has been increasingly recognized in autonomous driving technology, especially
the BEV-based models. Current state-of-the-art solutions primarily encode image
features from each camera view into the BEV space through explicit or implicit
depth prediction. However, these methods often focus on improving the accuracy
of projecting 2D features into corresponding depth regions, while overlooking
the highly structured information of real-world objects and the varying height
distributions of objects across different scenes. In this work, we propose
HV-BEV, a novel approach that decouples feature sampling in the BEV grid
queries paradigm into horizontal feature aggregation and vertical adaptive
height-aware reference point sampling, aiming to improve both the aggregation
of objects' complete information and generalization to diverse road
environments. Specifically, we construct a learnable graph structure in the
horizontal plane aligned with the ground for 3D reference points, reinforcing
the association of the same instance across different BEV grids, especially
when the instance spans multiple image views around the vehicle. Additionally,
instead of relying on uniform sampling within a fixed height range, we
introduce a height-aware module that incorporates historical information,
enabling the reference points to adaptively focus on the varying heights at
which objects appear in different scenes. Extensive experiments validate the
effectiveness of our proposed method, demonstrating its superior performance
over the baseline across the nuScenes dataset. Moreover, our best-performing
model achieves a remarkable 50.5% mAP and 59.8% NDS on the nuScenes testing
set.",2024-12-25T11:49:14Z,http://arxiv.org/abs/2412.18884v1,"Di Wu, Feng Yang, Benlian Xu, Pan Liao, Wenhui Zhao, Dingwen Zhang"
"Towards Compatible Semantic Communication: A Perspective on Digital
  Coding and Modulation","Semantic communication (SC) is emerging as a pivotal innovation within the 6G
framework, aimed at enabling more intelligent transmission. This development
has led to numerous studies focused on designing advanced systems through
powerful deep learning techniques. Nevertheless, many of these approaches
envision an analog transmission manner by formulating the transmitted signals
as continuous-valued semantic representation vectors, limiting their
compatibility with existing digital systems. To enhance compatibility, it is
essential to explore digitized SC systems. This article systematically
identifies two promising paradigms for designing digital SC: probabilistic and
deterministic approaches, according to the modulation strategies. For both, we
first provide a comprehensive analysis of the methodologies. Then, we put
forward the principles of designing digital SC systems with a specific focus on
informativeness and robustness of semantic representations to enhance
performance, along with constellation design. Additionally, we present a case
study to demonstrate the effectiveness of these methods. Moreover, this article
also explores the intrinsic advantages and opportunities provided by digital SC
systems, and then outlines several potential research directions for future
investigation.",2024-12-25T11:19:40Z,http://arxiv.org/abs/2412.18876v1,"Guangyi Zhang, Kequan Zhou, Yunlong Cai, Qiyu Hu, Guanding Yu"
Cross-PCR: A Robust Cross-Source Point Cloud Registration Framework,"Due to the density inconsistency and distribution difference between
cross-source point clouds, previous methods fail in cross-source point cloud
registration. We propose a density-robust feature extraction and matching
scheme to achieve robust and accurate cross-source registration. To address the
density inconsistency between cross-source data, we introduce a density-robust
encoder for extracting density-robust features. To tackle the issue of
challenging feature matching and few correct correspondences, we adopt a
loose-to-strict matching pipeline with a ``loose generation, strict selection''
idea. Under it, we employ a one-to-many strategy to loosely generate initial
correspondences. Subsequently, high-quality correspondences are strictly
selected to achieve robust registration through sparse matching and dense
matching. On the challenging Kinect-LiDAR scene in the cross-source 3DCSR
dataset, our method improves feature matching recall by 63.5 percentage points
(pp) and registration recall by 57.6 pp. It also achieves the best performance
on 3DMatch, while maintaining robustness under diverse downsampling densities.",2024-12-25T11:14:59Z,http://arxiv.org/abs/2412.18873v1,"Guiyu Zhao, Zhentao Guo, Zewen Du, Hongbin Ma"
"Enhancing Robustness in Manipulability Assessment: The Pseudo-Ellipsoid
  Approach","Manipulability analysis is a methodology employed to assess the capacity of
an articulated system, at a specific configuration, to produce motion or exert
force in diverse directions. The conventional method entails generating a
virtual ellipsoid using the system's configuration and model. Yet, this
approach poses challenges when applied to systems such as the human body, where
direct access to such information is limited, necessitating reliance on
estimations. Any inaccuracies in these estimations can distort the ellipsoid's
configuration, potentially compromising the accuracy of the manipulability
assessment. To address this issue, this article extends the standard approach
by introducing the concept of the manipulability pseudo-ellipsoid. Through a
series of theoretical analyses, simulations, and experiments, the article
demonstrates that the proposed method exhibits reduced sensitivity to noise in
sensory information, consequently enhancing the robustness of the approach.",2024-12-25T11:05:11Z,http://arxiv.org/abs/2412.18869v1,"Erfan Shahriari, Kim Kirstin Peper, Matej Hoffmann, Sami Haddadin"
"WeatherGS: 3D Scene Reconstruction in Adverse Weather Conditions via
  Gaussian Splatting","3D Gaussian Splatting (3DGS) has gained significant attention for 3D scene
reconstruction, but still suffers from complex outdoor environments, especially
under adverse weather. This is because 3DGS treats the artifacts caused by
adverse weather as part of the scene and will directly reconstruct them,
largely reducing the clarity of the reconstructed scene. To address this
challenge, we propose WeatherGS, a 3DGS-based framework for reconstructing
clear scenes from multi-view images under different weather conditions.
Specifically, we explicitly categorize the multi-weather artifacts into the
dense particles and lens occlusions that have very different characters, in
which the former are caused by snowflakes and raindrops in the air, and the
latter are raised by the precipitation on the camera lens. In light of this, we
propose a dense-to-sparse preprocess strategy, which sequentially removes the
dense particles by an Atmospheric Effect Filter (AEF) and then extracts the
relatively sparse occlusion masks with a Lens Effect Detector (LED). Finally,
we train a set of 3D Gaussians by the processed images and generated masks for
excluding occluded areas, and accurately recover the underlying clear scene by
Gaussian splatting. We conduct a diverse and challenging benchmark to
facilitate the evaluation of 3D reconstruction under complex weather scenarios.
Extensive experiments on this benchmark demonstrate that our WeatherGS
consistently produces high-quality, clean scenes across various weather
scenarios, outperforming existing state-of-the-art methods. See project
page:https://jumponthemoon.github.io/weather-gs.",2024-12-25T10:16:57Z,http://arxiv.org/abs/2412.18862v1,"Chenghao Qian, Yuhu Guo, Wenjing Li, Gustav Markkula"
Bootstrap Your Own Context Length,"We introduce a bootstrapping approach to train long-context language models
by exploiting their short-context capabilities only. Our method utilizes a
simple agent workflow to synthesize diverse long-context instruction tuning
data, thereby eliminating the necessity for manual data collection and
annotation. The proposed data synthesis workflow requires only a short-context
language model, a text retriever, and a document collection, all of which are
readily accessible within the open-source ecosystem. Subsequently, language
models are fine-tuned using the synthesized data to extend their context
lengths. In this manner, we effectively transfer the short-context capabilities
of language models to long-context scenarios through a bootstrapping process.
We conduct experiments with the open-source Llama-3 family of models and
demonstrate that our method can successfully extend the context length to up to
1M tokens, achieving superior performance across various benchmarks.",2024-12-25T10:08:54Z,http://arxiv.org/abs/2412.18860v1,"Liang Wang, Nan Yang, Xingxing Zhang, Xiaolong Huang, Furu Wei"
Identifiability of the spatial SEIR-HCD model of COVID-19 propagation,"This paper investigates the identifiability of a spatial mathematical model
of the spread of fast-moving epidemics based on the law of acting masses and
diffusion processes. The research algorithm is based on global methods of Sobol
sensitivity analysis and Bayesian approach, which together allowed to reduce
the variation boundaries of unknown parameters for further solving the problem
of parameter identification by measurements of the number of detected cases,
critical and dead. It is shown that for identification of diffusion
coefficients responsible for the rate of movement of individuals in space, it
is necessary to use additional information about the process.",2024-12-25T09:58:22Z,http://arxiv.org/abs/2412.18858v1,"Olga Krivorotko, Tatiana Zvonareva, Andrei Neverov"
"Digital Twin Enhanced Deep Reinforcement Learning for Intelligent
  Omni-Surface Configurations in MU-MIMO Systems","Intelligent omni-surface (IOS) is a promising technique to enhance the
capacity of wireless networks, by reflecting and refracting the incident signal
simultaneously. Traditional IOS configuration schemes, relying on all
sub-channels' channel state information and user equipments' mobility, are
difficult to implement in complex realistic systems. Existing works attempt to
address this issue employing deep reinforcement learning (DRL), but this method
requires a lot of trial-and-error interactions with the external environment
for efficient results and thus cannot satisfy the real-time decision-making. To
enable model-free and real-time IOS control, this paper puts forth a new
framework that integrates DRL and digital twins. DeepIOS, a DRL based IOS
configuration scheme with the goal of maximizing the sum data rate, is first
developed to jointly optimize the phase-shift and amplitude of IOS in
multi-user multiple-input-multiple-output systems. Thereafter, to further
reduce the computational complexity, DeepIOS introduces an action branch
architecture, which separately decides two optimization variables in parallel.
Finally, a digital twin module is constructed through supervised learning as a
pre-verification platform for DeepIOS, such that the decision-making's
real-time can be guaranteed. The formulated framework is a closed-loop system,
in which the physical space provides data to establish and calibrate the
digital space, while the digital space generates experience samples for DeepIOS
training and sends the trained parameters to the IOS controller for
configurations. Numerical results show that compared with random and MAB
schemes, the proposed framework attains a higher data rate and is more robust
to different settings. Furthermore, the action branch architecture reduces
DeepIOS's computational complexity, and the digital twin module improves the
convergence speed and run-time.",2024-12-25T09:53:07Z,http://arxiv.org/abs/2412.18856v1,"Xiaowen Ye, Xianghao Yu, Liqun Fu"
"Gravitational waves from equatorially eccentric extreme mass ratio
  inspirals around swirling Kerr black holes","We have studied the gravitational wave generated by extreme mass ratio
inspirals (EMRIs) along eccentric orbits on equatorial plane within the frame
of the swirling-Kerr black hole spacetime. The swirling-Kerr black hole has an
extra swirling parameter, which characterizes the rotation of universe
background. Our findings indicate that this swirling parameter leads to a delay
phase shift in the gravitational waveforms. The impact of the swirling
parameter on EMRI gravitational waves is suppressed by the black hole's spin
parameter. As a result, extracting information about the swirling parameter
from gravitational waves in a static black hole spacetime is much easier than
in the case of a rapidly rotating black hole. Our analysis also shows that a
high black hole spin leads to a greater overlap of gravitational waveforms for
different swirling parameters. We further investigate the potential issue of
waveform confusion caused by the orbital eccentricity and semi-latus rectum
parameters. As the swirling parameter increases, the relative variation in
eccentricity also increases, while the variation in the semi-latus rectum
decreases rapidly. The trends in these changes with the swirling parameter
resemble those observed with the MOG (Modified Gravity) parameter, though with
different rates of change. These results provide deeper insights into the
properties of EMRI gravitational waves and the swirling of the universe
background.",2024-12-25T09:51:58Z,http://arxiv.org/abs/2412.18854v1,"Yuhang Gu, Songbai Chen, Jiliang Jing"
Machine Learning-Based Detection of Pump-and-Dump Schemes in Real-Time,"Cryptocurrency markets often face manipulation through prevalent
pump-and-dump (P&amp;D) schemes, where self-organized Telegram groups, some
exceeding two million members, artificially inflate target cryptocurrency
prices. These groups sell premium access to inside information, worsening
information asymmetry and financial risks for subscribers and all investors.
This paper presents a real-time prediction pipeline to forecast target coins
and alert investors to possible P&amp;D schemes. In a Poloniex case study, the
model accurately identified the target coin among the top five from 50 random
coins in 24 out of 43 (55.81%) P&amp;D events. The pipeline uses advanced natural
language processing (NLP) to classify Telegram messages, identifying 2,079 past
pump events and detecting new ones in real-time. Our analysis also evaluates
the susceptibility of token standards - ERC-20, ERC-721, BRC-20, Inscriptions,
and Runes - to manipulation and identifies exchanges commonly involved in P&amp;D
schemes.",2024-12-25T09:23:36Z,http://arxiv.org/abs/2412.18848v1,"Manuel Bolz, Kevin Bründler, Liam Kane, Panagiotis Patsias, Liam Tessendorf, Krzysztof Gogol, Taehoon Kim, Claudio Tessone"
"TPCH: Tensor-interacted Projection and Cooperative Hashing for
  Multi-view Clustering","In recent years, anchor and hash-based multi-view clustering methods have
gained attention for their efficiency and simplicity in handling large-scale
data. However, existing methods often overlook the interactions among
multi-view data and higher-order cooperative relationships during projection,
negatively impacting the quality of hash representation in low-dimensional
spaces, clustering performance, and sensitivity to noise. To address this
issue, we propose a novel approach named Tensor-Interacted Projection and
Cooperative Hashing for Multi-View Clustering(TPCH). TPCH stacks multiple
projection matrices into a tensor, taking into account the synergies and
communications during the projection process. By capturing higher-order
multi-view information through dual projection and Hamming space, TPCH employs
an enhanced tensor nuclear norm to learn more compact and distinguishable hash
representations, promoting communication within and between views. Experimental
results demonstrate that this refined method significantly outperforms
state-of-the-art methods in clustering on five large-scale multi-view datasets.
Moreover, in terms of CPU time, TPCH achieves substantial acceleration compared
to the most advanced current methods. The code is available at
\textcolor{red}{\url{https://github.com/jankin-wang/TPCH}}.",2024-12-25T09:22:11Z,http://arxiv.org/abs/2412.18847v1,"Zhongwen Wang, Xingfeng Li, Yinghui Sun, Quansen Sun, Yuan Sun, Han Ling, Jian Dai, Zhenwen Ren"
"Enhancing Federated Graph Learning via Adaptive Fusion of Structural and
  Node Characteristics","Federated Graph Learning (FGL) has demonstrated the advantage of training a
global Graph Neural Network (GNN) model across distributed clients using their
local graph data. Unlike Euclidean data (\eg, images), graph data is composed
of nodes and edges, where the overall node-edge connections determine the
topological structure, and individual nodes along with their neighbors capture
local node features. However, existing studies tend to prioritize one aspect
over the other, leading to an incomplete understanding of the data and the
potential misidentification of key characteristics across varying graph
scenarios. Additionally, the non-independent and identically distributed
(non-IID) nature of graph data makes the extraction of these two data
characteristics even more challenging. To address the above issues, we propose
a novel FGL framework, named FedGCF, which aims to simultaneously extract and
fuse structural properties and node features to effectively handle diverse
graph scenarios. FedGCF first clusters clients by structural similarity,
performing model aggregation within each cluster to form the shared structural
model. Next, FedGCF selects the clients with common node features and
aggregates their models to generate a common node model. This model is then
propagated to all clients, allowing common node features to be shared. By
combining these two models with a proper ratio, FedGCF can achieve a
comprehensive understanding of the graph data and deliver better performance,
even under non-IID distributions. Experimental results show that FedGCF
improves accuracy by 4.94%-7.24% under different data distributions and reduces
communication cost by 64.18%-81.25% to reach the same accuracy compared to
baselines.",2024-12-25T09:20:06Z,http://arxiv.org/abs/2412.18845v1,"Xianjun Gao, Jianchun Liu, Hongli Xu, Shilong Wang, Liusheng Huang"
"Context-Based Semantic-Aware Alignment for Semi-Supervised Multi-Label
  Learning","Due to the lack of extensive precisely-annotated multi-label data in real
word, semi-supervised multi-label learning (SSMLL) has gradually gained
attention. Abundant knowledge embedded in vision-language models (VLMs)
pre-trained on large-scale image-text pairs could alleviate the challenge of
limited labeled data under SSMLL setting.Despite existing methods based on
fine-tuning VLMs have achieved advances in weakly-supervised multi-label
learning, they failed to fully leverage the information from labeled data to
enhance the learning of unlabeled data. In this paper, we propose a
context-based semantic-aware alignment method to solve the SSMLL problem by
leveraging the knowledge of VLMs. To address the challenge of handling multiple
semantics within an image, we introduce a novel framework design to extract
label-specific image features. This design allows us to achieve a more compact
alignment between text features and label-specific image features, leading the
model to generate high-quality pseudo-labels. To incorporate the model with
comprehensive understanding of image, we design a semi-supervised context
identification auxiliary task to enhance the feature representation by
capturing co-occurrence information. Extensive experiments on multiple
benchmark datasets demonstrate the effectiveness of our proposed method.",2024-12-25T09:06:54Z,http://arxiv.org/abs/2412.18842v1,"Heng-Bo Fan, Ming-Kun Xie, Jia-Hao Xiao, Sheng-Jun Huang"
DiFiC: Your Diffusion Model Holds the Secret to Fine-Grained Clustering,"Fine-grained clustering is a practical yet challenging task, whose essence
lies in capturing the subtle differences between instances of different
classes. Such subtle differences can be easily disrupted by data augmentation
or be overwhelmed by redundant information in data, leading to significant
performance degradation for existing clustering methods. In this work, we
introduce DiFiC a fine-grained clustering method building upon the conditional
diffusion model. Distinct from existing works that focus on extracting
discriminative features from images, DiFiC resorts to deducing the textual
conditions used for image generation. To distill more precise and
clustering-favorable object semantics, DiFiC further regularizes the diffusion
target and guides the distillation process utilizing neighborhood similarity.
Extensive experiments demonstrate that DiFiC outperforms both state-of-the-art
discriminative and generative clustering methods on four fine-grained image
clustering benchmarks. We hope the success of DiFiC will inspire future
research to unlock the potential of diffusion models in tasks beyond
generation. The code will be released.",2024-12-25T08:55:48Z,http://arxiv.org/abs/2412.18838v1,"Ruohong Yang, Peng Hu, Xi Peng, Xiting Liu, Yunfan Li"
"Experimental secure entanglement-free quantum remote sensing over 50 km
  of optical fiber","Secure quantum remote sensing (SQRS) uses quantum states to gather
information about distant objects or environments while ensuring secure data
transmission against eavesdropping. It has potential applications in various
fields, including environmental monitoring, military surveillance, and disaster
response, where both data accuracy and transmission security are critical.
Recent experiments have demonstrated the feasibility of SQRS using entanglement
states. Here, we experimentally demonstrate an SQRS that can estimate a phase
without requiring entanglement, offering the practical advantage that
single-qubit states are easier to prepare. We successfully estimate the preset
phase information at a remote site over a fiber distance of 50 km, which serves
as a key step toward long-distance applications.",2024-12-25T08:52:06Z,http://arxiv.org/abs/2412.18837v1,"Wenjie He, Chunfeng Huang, Rui Guan, Ye Chen, Zhenrong Zhang, Kejin Wei"
"Adaptive Rate Control for Deep Video Compression with Rate-Distortion
  Prediction","Deep video compression has made significant progress in recent years,
achieving rate-distortion performance that surpasses that of traditional video
compression methods. However, rate control schemes tailored for deep video
compression have not been well studied. In this paper, we propose a neural
network-based $\lambda$-domain rate control scheme for deep video compression,
which determines the coding parameter $\lambda$ for each to-be-coded frame
based on the rate-distortion-$\lambda$ (R-D-$\lambda$) relationships directly
learned from uncompressed frames, achieving high rate control accuracy
efficiently without the need for pre-encoding. Moreover, this content-aware
scheme is able to mitigate inter-frame quality fluctuations and adapt to abrupt
changes in video content. Specifically, we introduce two neural network-based
predictors to estimate the relationship between bitrate and $\lambda$, as well
as the relationship between distortion and $\lambda$ for each frame. Then we
determine the coding parameter $\lambda$ for each frame to achieve the target
bitrate. Experimental results demonstrate that our approach achieves high rate
control accuracy at the mini-GOP level with low time overhead and mitigates
inter-frame quality fluctuations across video content of varying resolutions.",2024-12-25T08:42:23Z,http://arxiv.org/abs/2412.18834v1,"Bowen Gu, Hao Chen, Ming Lu, Jie Yao, Zhan Ma"
"PhyloGen: Language Model-Enhanced Phylogenetic Inference via Graph
  Structure Generation","Phylogenetic trees elucidate evolutionary relationships among species, but
phylogenetic inference remains challenging due to the complexity of combining
continuous (branch lengths) and discrete parameters (tree topology).
Traditional Markov Chain Monte Carlo methods face slow convergence and
computational burdens. Existing Variational Inference methods, which require
pre-generated topologies and typically treat tree structures and branch lengths
independently, may overlook critical sequence features, limiting their accuracy
and flexibility. We propose PhyloGen, a novel method leveraging a pre-trained
genomic language model to generate and optimize phylogenetic trees without
dependence on evolutionary models or aligned sequence constraints. PhyloGen
views phylogenetic inference as a conditionally constrained tree structure
generation problem, jointly optimizing tree topology and branch lengths through
three core modules: (i) Feature Extraction, (ii) PhyloTree Construction, and
(iii) PhyloTree Structure Modeling. Meanwhile, we introduce a Scoring Function
to guide the model towards a more stable gradient descent. We demonstrate the
effectiveness and robustness of PhyloGen on eight real-world benchmark
datasets. Visualization results confirm PhyloGen provides deeper insights into
phylogenetic relationships.",2024-12-25T08:33:05Z,http://arxiv.org/abs/2412.18827v1,"ChenRui Duan, Zelin Zang, Siyuan Li, Yongjie Xu, Stan Z. Li"
"Conductance-Photoacoustic Spectroscopy for OneSignal Measurement of
  Multi-components","Ensuring safety and efficiency in emerging hydrogen-hydrocarbon fuel systems
requires accurate measurement of multiple gas components in real time. However,
existing detection techniques generally lack the capability to quantitatively
measure hydrogen and natural gas constituents simultaneously. Here, we present
a novel conductance-photoacoustic spectroscopy (CPAS) method that integrates
platinum-modified conductance measurements with beat-frequency photoacoustic
detection. By bridging a quartz tuning fork with a platinum microwire, our
approach enables direct monitoring of hydrogen concentration via frequency
modulation, while simultaneously capturing propane's photoacoustic signal with
a single detection channel. Experimental results confirm that the platinum
microwire effectively fine-tunes the tuning fork's mechanical properties for
high-sensitivity hydrogen measurement, and the beat-frequency photoacoustic
signals from propane absorption reveal complementary hydrocarbon concentration
information. This unified sensor design is inherently compact, rapid, and
calibration-free, making it particularly suitable for applications that demand
real-time multiparameter gas analysis, including industrial process control and
environmental monitoring. Taken together, these findings demonstrate that
combining conductance and photoacoustic spectroscopies into a single integrated
platform significantly advances the state of multi-component gas detection and
holds promise for further enhancements in sensitivity and adaptability.",2024-12-25T08:23:07Z,http://arxiv.org/abs/2412.18822v1,"Ruobin Zhuang, Jianfeng He, Huadan Zheng"
Quantifying the memory and dynamical stability of magnetar bursts,"The time series of energy and waiting time of magnetar bursts carry important
information about the source activity. In this paper, we investigate the memory
and dynamical stability of magnetar bursts from four soft gamma repeater (SGR)
sources: SGR 1806$-$20, SGR 1900+14, SGR J1935+2154 and SGR J1550$-$5418. Based
on the rescaled range analysis, we quantify the memory in magnetar bursts for
the first time and find that there exists long-term memory in the time series
of both waiting time and energy. We investigate the dynamical stability in the
context of randomness and chaos. For all the four SGR samples, we find that the
waiting time is not completely random, but the energy of two SGRs is consistent
with a total random organization. Furthermore, both waiting time and energy
exhibits weak chaos. We also find no significant difference between SGRs and
repeating fast radio bursts (FRBs) in the randomness-chaos phase space. The
statistical similarity between SGRs and repeating FRBs hints that there may be
potential physical connection between these two phenomena.",2024-12-25T08:22:36Z,http://arxiv.org/abs/2412.18821v1,"Yu Sang, Hai-Nan Lin"
LLM-assisted vector similarity search,"As data retrieval demands become increasingly complex, traditional search
methods often fall short in addressing nuanced and conceptual queries. Vector
similarity search has emerged as a promising technique for finding semantically
similar information efficiently. However, its effectiveness diminishes when
handling intricate queries with contextual nuances. This paper explores a
hybrid approach combining vector similarity search with Large Language Models
(LLMs) to enhance search accuracy and relevance. The proposed two-step solution
first employs vector similarity search to shortlist potential matches, followed
by an LLM for context-aware ranking of the results. Experiments on structured
datasets demonstrate that while vector similarity search alone performs well
for straightforward queries, the LLM-assisted approach excels in processing
complex queries involving constraints, negations, or conceptual requirements.
By leveraging the natural language understanding capabilities of LLMs, this
method improves the accuracy of search results for complex tasks without
sacrificing efficiency. We also discuss real-world applications and propose
directions for future research to refine and scale this technique for diverse
datasets and use cases.
  Original article:
https://engineering.grab.com/llm-assisted-vector-similarity-search",2024-12-25T08:17:37Z,http://arxiv.org/abs/2412.18819v1,"Md Riyadh, Muqi Li, Felix Haryanto Lie, Jia Long Loh, Haotian Mi, Sayam Bohra"
"Sequential Interpretability: Methods, Applications, and Future Direction
  for Understanding Deep Learning Models in the Context of Sequential Data","  Deep learning continues to revolutionize an ever-growing number of critical
application areas including healthcare, transportation, finance, and basic
sciences. Despite their increased predictive power, model transparency and
human explainability remain a significant challenge due to the ""black box""
nature of modern deep learning models. In many cases the desired balance
between interpretability and performance is predominately task specific.
Human-centric domains such as healthcare necessitate a renewed focus on
understanding how and why these frameworks are arriving at critical and
potentially life-or-death decisions. Given the quantity of research and
empirical successes of deep learning for computer vision, most of the existing
interpretability research has focused on image processing techniques.
Comparatively, less attention has been paid to interpreting deep learning
frameworks using sequential data. Given recent deep learning advancements in
highly sequential domains such as natural language processing and physiological
signal processing, the need for deep sequential explanations is at an all-time
high. In this paper, we review current techniques for interpreting deep
learning techniques involving sequential data, identify similarities to
non-sequential methods, and discuss current limitations and future avenues of
sequential interpretability research.
",2020-04-27T00:58:42Z,http://arxiv.org/abs/2004.12524v1,"Benjamin Shickel, Parisa Rashidi"
Putting Natural in Natural Language Processing,"  Human language is firstly spoken and only secondarily written. Text, however,
is a very convenient and efficient representation of language, and modern
civilization has made it ubiquitous. Thus the field of NLP has overwhelmingly
focused on processing written rather than spoken language. Work on spoken
language, on the other hand, has been siloed off within the largely separate
speech processing community which has been inordinately preoccupied with
transcribing speech into text. Recent advances in deep learning have led to a
fortuitous convergence in methods between speech processing and mainstream NLP.
Arguably, the time is ripe for a unification of these two fields, and for
starting to take spoken language seriously as the primary mode of human
communication. Truly natural language processing could lead to better
integration with the rest of language science and could lead to systems which
are more data-efficient and more human-like, and which can communicate beyond
the textual modality.
",2023-05-08T09:29:31Z,http://arxiv.org/abs/2305.04572v2,Grzegorz Chrupała
Deep Learning for Political Science,"  Political science, and social science in general, have traditionally been
using computational methods to study areas such as voting behavior, policy
making, international conflict, and international development. More recently,
increasingly available quantities of data are being combined with improved
algorithms and affordable computational resources to predict, learn, and
discover new insights from data that is large in volume and variety. New
developments in the areas of machine learning, deep learning, natural language
processing (NLP), and, more generally, artificial intelligence (AI) are opening
up new opportunities for testing theories and evaluating the impact of
interventions and programs in a more dynamic and effective way. Applications
using large volumes of structured and unstructured data are becoming common in
government and industry, and increasingly also in social science research. This
chapter offers an introduction to such methods drawing examples from political
science. Focusing on the areas where the strengths of the methods coincide with
challenges in these fields, the chapter first presents an introduction to AI
and its core technology - machine learning, with its rapidly developing
subfield of deep learning. The discussion of deep neural networks is
illustrated with the NLP tasks that are relevant to political science. The
latest advances in deep learning methods for NLP are also reviewed, together
with their potential for improving information extraction and pattern
recognition from political science texts.
",2020-05-13T19:14:37Z,http://arxiv.org/abs/2005.06540v1,"Kakia Chatsiou, Slava Jankin Mikhaylov"
"Identifying Semantically Duplicate Questions Using Data Science
  Approach: A Quora Case Study","  Identifying semantically identical questions on, Question and Answering
social media platforms like Quora is exceptionally significant to ensure that
the quality and the quantity of content are presented to users, based on the
intent of the question and thus enriching overall user experience. Detecting
duplicate questions is a challenging problem because natural language is very
expressive, and a unique intent can be conveyed using different words, phrases,
and sentence structuring. Machine learning and deep learning methods are known
to have accomplished superior results over traditional natural language
processing techniques in identifying similar texts. In this paper, taking Quora
for our case study, we explored and applied different machine learning and deep
learning techniques on the task of identifying duplicate questions on Quora's
dataset. By using feature engineering, feature importance techniques, and
experimenting with seven selected machine learning classifiers, we demonstrated
that our models outperformed previous studies on this task. Xgboost model with
character level term frequency and inverse term frequency is our best machine
learning model that has also outperformed a few of the Deep learning baseline
models. We applied deep learning techniques to model four different deep neural
networks of multiple layers consisting of Glove embeddings, Long Short Term
Memory, Convolution, Max pooling, Dense, Batch Normalization, Activation
functions, and model merge. Our deep learning models achieved better accuracy
than machine learning models. Three out of four proposed architectures
outperformed the accuracy from previous machine learning and deep learning
research work, two out of four models outperformed accuracy from previous deep
learning study on Quora's question pair dataset, and our best model achieved
accuracy of 85.82% which is close to Quora state of the art accuracy.
",2020-04-18T19:39:58Z,http://arxiv.org/abs/2004.11694v1,"Navedanjum Ansari, Rajesh Sharma"
Vulgar Remarks Detection in Chittagonian Dialect of Bangla,"  The negative effects of online bullying and harassment are increasing with
Internet popularity, especially in social media. One solution is using natural
language processing (NLP) and machine learning (ML) methods for the automatic
detection of harmful remarks, but these methods are limited in low-resource
languages like the Chittagonian dialect of Bangla.This study focuses on
detecting vulgar remarks in social media using supervised ML and deep learning
algorithms.Logistic Regression achieved promising accuracy (0.91) while simple
RNN with Word2vec and fastTex had lower accuracy (0.84-0.90), highlighting the
issue that NN algorithms require more data.
",2023-08-29T17:19:32Z,http://arxiv.org/abs/2308.15448v1,"Tanjim Mahmud, Michal Ptaszynski, Fumito Masui"
"Development of Deep Learning Based Natural Language Processing Model for
  Turkish","  Natural language is one of the most fundamental features that distinguish
people from other living things and enable people to communicate each other.
Language is a tool that enables people to express their feelings and thoughts
and to transfers cultures through generations. Texts and audio are examples of
natural language in daily life. In the natural language, many words disappear
in time, on the other hand new words are derived. Therefore, while the process
of natural language processing (NLP) is complex even for human, it is difficult
to process in computer system. The area of linguistics examines how people use
language. NLP, which requires the collaboration of linguists and computer
scientists, plays an important role in human computer interaction. Studies in
NLP have increased with the use of artificial intelligence technologies in the
field of linguistics. With the deep learning methods which are one of the
artificial intelligence study areas, platforms close to natural language are
being developed. Developed platforms for language comprehension, machine
translation and part of speech (POS) tagging benefit from deep learning
methods. Recurrent Neural Network (RNN), one of the deep learning
architectures, is preferred for processing sequential data such as text or
audio data. In this study, Turkish POS tagging model has been proposed by using
Bidirectional Long-Short Term Memory (BLSTM) which is an RNN type. The proposed
POS tagging model is provided to natural language researchers with a platform
that allows them to perform and use their own analysis. In the development
phase of the platform developed by using BLSTM, the error rate of the POS
tagger has been reduced by taking feedback with expert opinion.
",2019-05-07T21:09:49Z,http://arxiv.org/abs/1905.05699v1,"Baris Baburoglu, Adem Tekerek, Mehmet Tekerek"
"Deep Learning Based Natural Language Processing for End to End Speech
  Translation","  Deep Learning methods employ multiple processing layers to learn hierarchial
representations of data. They have already been deployed in a humongous number
of applications and have produced state-of-the-art results. Recently with the
growth in processing power of computers to be able to do high dimensional
tensor calculations, Natural Language Processing (NLP) applications have been
given a significant boost in terms of efficiency as well as accuracy. In this
paper, we will take a look at various signal processing techniques and then
application of them to produce a speech-to-text system using Deep Recurrent
Neural Networks.
",2018-08-09T14:21:35Z,http://arxiv.org/abs/1808.04459v1,Sarvesh Patil
"Scientific Language Modeling: A Quantitative Review of Large Language
  Models in Molecular Science","  Efficient molecular modeling and design are crucial for the discovery and
exploration of novel molecules, and the incorporation of deep learning methods
has revolutionized this field. In particular, large language models (LLMs)
offer a fresh approach to tackle scientific problems from a natural language
processing (NLP) perspective, introducing a research paradigm called scientific
language modeling (SLM). However, two key issues remain: how to quantify the
match between model and data modalities and how to identify the
knowledge-learning preferences of models. To address these challenges, we
propose a multi-modal benchmark, named ChEBI-20-MM, and perform 1263
experiments to assess the model's compatibility with data modalities and
knowledge acquisition. Through the modal transition probability matrix, we
provide insights into the most suitable modalities for tasks. Furthermore, we
introduce a statistically interpretable approach to discover context-specific
knowledge mapping by localized feature filtering. Our pioneering analysis
offers an exploration of the learning mechanism and paves the way for advancing
SLM in molecular science.
",2024-02-06T16:12:36Z,http://arxiv.org/abs/2402.04119v1,"Pengfei Liu, Jun Tao, Zhixiang Ren"
Natural Language Understanding with Distributed Representation,"  This is a lecture note for the course DS-GA 3001 <Natural Language
Understanding with Distributed Representation> at the Center for Data Science ,
New York University in Fall, 2015. As the name of the course suggests, this
lecture note introduces readers to a neural network based approach to natural
language understanding/processing. In order to make it as self-contained as
possible, I spend much time on describing basics of machine learning and neural
networks, only after which how they are used for natural languages is
introduced. On the language front, I almost solely focus on language modelling
and machine translation, two of which I personally find most fascinating and
most fundamental to natural language understanding.
",2015-11-24T23:23:13Z,http://arxiv.org/abs/1511.07916v1,Kyunghyun Cho
A Review of Deep Learning Techniques for Protein Function Prediction,"  Deep Learning and big data have shown tremendous success in bioinformatics
and computational biology in recent years; artificial intelligence methods have
also significantly contributed in the task of protein function classification.
This review paper analyzes the recent developments in approaches for the task
of predicting protein function using deep learning. We explain the importance
of determining the protein function and why automating the following task is
crucial. Then, after reviewing the widely used deep learning techniques for
this task, we continue our review and highlight the emergence of the modern
State of The Art (SOTA) deep learning models which have achieved groundbreaking
results in the field of computer vision, natural language processing and
multi-modal learning in the last few years. We hope that this review will
provide a broad view of the current role and advances of deep learning in
biological sciences, especially in predicting protein function tasks and
encourage new researchers to contribute to this area.
",2022-10-27T20:30:25Z,http://arxiv.org/abs/2211.09705v1,"Divyanshu Aggarwal, Yasha Hasija"
"Text Analysis Using Deep Neural Networks in Digital Humanities and
  Information Science","  Combining computational technologies and humanities is an ongoing effort
aimed at making resources such as texts, images, audio, video, and other
artifacts digitally available, searchable, and analyzable. In recent years,
deep neural networks (DNN) dominate the field of automatic text analysis and
natural language processing (NLP), in some cases presenting a super-human
performance. DNNs are the state-of-the-art machine learning algorithms solving
many NLP tasks that are relevant for Digital Humanities (DH) research, such as
spell checking, language detection, entity extraction, author detection,
question answering, and other tasks. These supervised algorithms learn patterns
from a large number of ""right"" and ""wrong"" examples and apply them to new
examples. However, using DNNs for analyzing the text resources in DH research
presents two main challenges: (un)availability of training data and a need for
domain adaptation. This paper explores these challenges by analyzing multiple
use-cases of DH studies in recent literature and their possible solutions and
lays out a practical decision model for DH experts for when and how to choose
the appropriate deep learning approaches for their research. Moreover, in this
paper, we aim to raise awareness of the benefits of utilizing deep learning
models in the DH community.
",2023-07-30T12:54:39Z,http://arxiv.org/abs/2307.16217v1,"Omri Suissa, Avshalom Elmalech, Maayan Zhitomirsky-Geffet"
"Modular Mechanistic Networks: On Bridging Mechanistic and
  Phenomenological Models with Deep Neural Networks in Natural Language
  Processing","  Natural language processing (NLP) can be done using either top-down (theory
driven) and bottom-up (data driven) approaches, which we call mechanistic and
phenomenological respectively. The approaches are frequently considered to
stand in opposition to each other. Examining some recent approaches in deep
learning we argue that deep neural networks incorporate both perspectives and,
furthermore, that leveraging this aspect of deep learning may help in solving
complex problems within language technology, such as modelling language and
perception in the domain of spatial cognition.
",2018-07-21T11:37:15Z,http://arxiv.org/abs/1807.09844v2,"Simon Dobnik, John D. Kelleher"
Application Specific Compression of Deep Learning Models,"  Large Deep Learning models are compressed and deployed for specific
applications. However, current Deep Learning model compression methods do not
utilize the information about the target application. As a result, the
compressed models are application agnostic. Our goal is to customize the model
compression process to create a compressed model that will perform better for
the target application. Our method, Application Specific Compression (ASC),
identifies and prunes components of the large Deep Learning model that are
redundant specifically for the given target application. The intuition of our
work is to prune the parts of the network that do not contribute significantly
to updating the data representation for the given application. We have
experimented with the BERT family of models for three applications: Extractive
QA, Natural Language Inference, and Paraphrase Identification. We observe that
customized compressed models created using ASC method perform better than
existing model compression methods and off-the-shelf compressed models.
",2024-09-09T06:55:38Z,http://arxiv.org/abs/2409.05368v1,"Rohit Raj Rai, Angana Borah, Amit Awekar"
"Deep learning for nano-photonic materials -- The solution to
  everything!?","  Deep learning is currently being hyped as an almost magical tool for solving
all kinds of difficult problems that computers have not been able to solve in
the past. Particularly in the fields of computer vision and natural language
processing, spectacular results have been achieved. The hype has now
infiltrated several scientific communities. In (nano-)photonics, researchers
are trying to apply deep learning to all kinds of forward and inverse problems.
A particularly challenging problem is for instance the rational design of
nanophotonic materials and devices. In this opinion article, I will first
discuss the public expectations of deep learning and give an overview of the
quite different scales at which actors from industry and research are operating
their deep learning models. I then examine the weaknesses and dangers
associated with deep learning. Finally, I'll discuss the key strengths that
make this new set of statistical methods so attractive, and review a personal
selection of opportunities that shouldn't be missed in the current
developments.
",2023-10-12T16:13:25Z,http://arxiv.org/abs/2310.08618v2,Peter R. Wiecha
"Integrating AI Planning with Natural Language Processing: A Combination
  of Explicit and Tacit Knowledge","  Natural language processing (NLP) aims at investigating the interactions
between agents and humans, processing and analyzing large amounts of natural
language data. Large-scale language models play an important role in current
natural language processing. However, the challenges of explainability and
complexity come along with the developments of language models. One way is to
introduce logical relations and rules into natural language processing models,
such as making use of Automated Planning. Automated planning (AI planning)
focuses on building symbolic domain models and synthesizing plans to transit
initial states to goals based on domain models. Recently, there have been
plenty of works related to these two fields, which have the abilities to
generate explicit knowledge, e.g., preconditions and effects of action models,
and learn from tacit knowledge, e.g., neural models, respectively. Integrating
AI planning and natural language processing effectively improves the
communication between human and intelligent agents. This paper outlines the
commons and relations between AI planning and natural language processing,
argues that each of them can effectively impact on the other one by five areas:
(1) planning-based text understanding, (2) planning-based natural language
processing, (3) planning-based explainability, (4) text-based human-robot
interaction, and (5) applications. We also explore some potential future issues
between AI planning and natural language processing. To the best of our
knowledge, this survey is the first work that addresses the deep connections
between AI planning and Natural language processing.
",2022-02-15T02:19:09Z,http://arxiv.org/abs/2202.07138v2,"Kebing Jin, Hankz Hankui Zhuo"
Changes from Classical Statistics to Modern Statistics and Data Science,"  A coordinate system is a foundation for every quantitative science,
engineering, and medicine. Classical physics and statistics are based on the
Cartesian coordinate system. The classical probability and hypothesis testing
theory can only be applied to Euclidean data. However, modern data in the real
world are from natural language processing, mathematical formulas, social
networks, transportation and sensor networks, computer visions, automations,
and biomedical measurements. The Euclidean assumption is not appropriate for
non Euclidean data. This perspective addresses the urgent need to overcome
those fundamental limitations and encourages extensions of classical
probability theory and hypothesis testing , diffusion models and stochastic
differential equations from Euclidean space to non Euclidean space. Artificial
intelligence such as natural language processing, computer vision, graphical
neural networks, manifold regression and inference theory, manifold learning,
graph neural networks, compositional diffusion models for automatically
compositional generations of concepts and demystifying machine learning
systems, has been rapidly developed. Differential manifold theory is the
mathematic foundations of deep learning and data science as well. We urgently
need to shift the paradigm for data analysis from the classical Euclidean data
analysis to both Euclidean and non Euclidean data analysis and develop more and
more innovative methods for describing, estimating and inferring non Euclidean
geometries of modern real datasets. A general framework for integrated analysis
of both Euclidean and non Euclidean data, composite AI, decision intelligence
and edge AI provide powerful innovative ideas and strategies for fundamentally
advancing AI. We are expected to marry statistics with AI, develop a unified
theory of modern statistics and drive next generation of AI and data science.
",2022-10-30T21:35:53Z,http://arxiv.org/abs/2211.03756v1,"Kai Zhang, Shan Liu, Momiao Xiong"
"Recent Advances and Applications of Deep Learning Methods in Materials
  Science","  Deep learning (DL) is one of the fastest growing topics in materials data
science, with rapidly emerging applications spanning atomistic, image-based,
spectral, and textual data modalities. DL allows analysis of unstructured data
and automated identification of features. Recent development of large materials
databases has fueled the application of DL methods in atomistic prediction in
particular. In contrast, advances in image and spectral data have largely
leveraged synthetic data enabled by high quality forward models as well as by
generative unsupervised DL methods. In this article, we present a high-level
overview of deep-learning methods followed by a detailed discussion of recent
developments of deep learning in atomistic simulation, materials imaging,
spectral analysis, and natural language processing. For each modality we
discuss applications involving both theoretical and experimental data, typical
modeling approaches with their strengths and limitations, and relevant publicly
available software and datasets. We conclude the review with a discussion of
recent cross-cutting work related to uncertainty quantification in this field
and a brief perspective on limitations, challenges, and potential growth areas
for DL methods in materials science. The application of DL methods in materials
science presents an exciting avenue for future materials discovery and design.
",2021-10-28T00:09:04Z,http://arxiv.org/abs/2110.14820v1,"Kamal Choudhary, Brian DeCost, Chi Chen, Anubhav Jain, Francesca Tavazza, Ryan Cohn, Cheol WooPark, Alok Choudhary, Ankit Agrawal, Simon J. L. Billinge, Elizabeth Holm, Shyue Ping Ong, Chris Wolverton"
Data Augmentation Approaches in Natural Language Processing: A Survey,"  As an effective strategy, data augmentation (DA) alleviates data scarcity
scenarios where deep learning techniques may fail. It is widely applied in
computer vision then introduced to natural language processing and achieves
improvements in many tasks. One of the main focuses of the DA methods is to
improve the diversity of training data, thereby helping the model to better
generalize to unseen testing data. In this survey, we frame DA methods into
three categories based on the diversity of augmented data, including
paraphrasing, noising, and sampling. Our paper sets out to analyze DA methods
in detail according to the above categories. Further, we also introduce their
applications in NLP tasks as well as the challenges. Some helpful resources are
provided in the appendix.
",2021-10-05T07:35:32Z,http://arxiv.org/abs/2110.01852v3,"Bohan Li, Yutai Hou, Wanxiang Che"
"Few-Shot Learning for Clinical Natural Language Processing Using Siamese
  Neural Networks","  Clinical Natural Language Processing (NLP) has become an emerging technology
in healthcare that leverages a large amount of free-text data in electronic
health records (EHRs) to improve patient care, support clinical decisions, and
facilitate clinical and translational science research. Recently, deep learning
has achieved state-of-the-art performance in many clinical NLP tasks. However,
training deep learning models usually requires large annotated datasets, which
are normally not publicly available and can be time-consuming to build in
clinical domains. Working with smaller annotated datasets is typical in
clinical NLP and therefore, ensuring that deep learning models perform well is
crucial for the models to be used in real-world applications. A widely adopted
approach is fine-tuning existing Pre-trained Language Models (PLMs), but these
attempts fall short when the training dataset contains only a few annotated
samples. Few-Shot Learning (FSL) has recently been investigated to tackle this
problem. Siamese Neural Network (SNN) has been widely utilized as an FSL
approach in computer vision, but has not been studied well in NLP. Furthermore,
the literature on its applications in clinical domains is scarce. In this
paper, we propose two SNN-based FSL approaches for clinical NLP, including
Pre-Trained SNN (PT-SNN) and SNN with Second-Order Embeddings (SOE-SNN). We
evaluated the proposed approaches on two clinical tasks, namely clinical text
classification and clinical named entity recognition. We tested three few-shot
settings including 4-shot, 8-shot, and 16-shot learning. Both clinical NLP
tasks were benchmarked using three PLMs, including BERT,BioBERT, and
BioClinicalBERT. The experimental results verified the effectiveness of the
proposed SNN-based FSL approaches in both NLP tasks.
",2022-08-31T15:36:27Z,http://arxiv.org/abs/2208.14923v2,"David Oniani, Sonish Sivarajkumar, Yanshan Wang"
Federated Learning Meets Natural Language Processing: A Survey,"  Federated Learning aims to learn machine learning models from multiple
decentralized edge devices (e.g. mobiles) or servers without sacrificing local
data privacy. Recent Natural Language Processing techniques rely on deep
learning and large pre-trained language models. However, both big deep neural
and language models are trained with huge amounts of data which often lies on
the server side. Since text data is widely originated from end users, in this
work, we look into recent NLP models and techniques which use federated
learning as the learning framework. Our survey discusses major challenges in
federated natural language processing, including the algorithm challenges,
system challenges as well as the privacy issues. We also provide a critical
review of the existing Federated NLP evaluation methods and tools. Finally, we
highlight the current research gaps and future directions.
",2021-07-27T05:07:48Z,http://arxiv.org/abs/2107.12603v1,"Ming Liu, Stella Ho, Mengqi Wang, Longxiang Gao, Yuan Jin, He Zhang"
AllenNLP: A Deep Semantic Natural Language Processing Platform,"  This paper describes AllenNLP, a platform for research on deep learning
methods in natural language understanding. AllenNLP is designed to support
researchers who want to build novel language understanding models quickly and
easily. It is built on top of PyTorch, allowing for dynamic computation graphs,
and provides (1) a flexible data API that handles intelligent batching and
padding, (2) high-level abstractions for common operations in working with
text, and (3) a modular and extensible experiment framework that makes doing
good science easy. It also includes reference implementations of high quality
approaches for both core semantic problems (e.g. semantic role labeling (Palmer
et al., 2005)) and language understanding applications (e.g. machine
comprehension (Rajpurkar et al., 2016)). AllenNLP is an ongoing open-source
effort maintained by engineers and researchers at the Allen Institute for
Artificial Intelligence.
",2018-03-20T20:32:07Z,http://arxiv.org/abs/1803.07640v2,"Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson Liu, Matthew Peters, Michael Schmitz, Luke Zettlemoyer"
DeepZensols: Deep Natural Language Processing Framework,"  Reproducing results in publications by distributing publicly available source
code is becoming ever more popular. Given the difficulty of reproducing machine
learning (ML) experiments, there have been significant efforts in reducing the
variance of these results. As in any science, the ability to consistently
reproduce results effectively strengthens the underlying hypothesis of the
work, and thus, should be regarded as important as the novel aspect of the
research itself. The contribution of this work is a framework that is able to
reproduce consistent results and provides a means of easily creating, training,
and evaluating natural language processing (NLP) deep learning (DL) models.
",2021-09-08T01:16:05Z,http://arxiv.org/abs/2109.03383v1,"Paul Landes, Barbara Di Eugenio, Cornelia Caragea"
"Semi-Supervised Neural Text Generation by Joint Learning of Natural
  Language Generation and Natural Language Understanding Models","  In Natural Language Generation (NLG), End-to-End (E2E) systems trained
through deep learning have recently gained a strong interest. Such deep models
need a large amount of carefully annotated data to reach satisfactory
performance. However, acquiring such datasets for every new NLG application is
a tedious and time-consuming task. In this paper, we propose a semi-supervised
deep learning scheme that can learn from non-annotated data and annotated data
when available. It uses an NLG and a Natural Language Understanding (NLU)
sequence-to-sequence models which are learned jointly to compensate for the
lack of annotation. Experiments on two benchmark datasets show that, with
limited amount of annotated data, the method can achieve very competitive
results while not using any pre-processing or re-scoring tricks. These findings
open the way to the exploitation of non-annotated datasets which is the current
bottleneck for the E2E NLG system development to new applications.
",2019-09-29T11:37:18Z,http://arxiv.org/abs/1910.03484v1,"Raheel Qader, François Portet, Cyril Labbé"
Opening the black box of language acquisition,"  Recent advances in large language models using deep learning techniques have
renewed interest on how languages can be learned from data. However, it is
unclear whether or how these models represent grammatical information from the
learned languages. In addition, the models must be pre-trained on large corpora
before they can be used. In this work, we propose an alternative, more
transparent and cognitively plausible architecture for learning language.
Instead of using deep learning, our approach uses a minimal cognitive
architecture based on sequence memory and chunking. The learning mechanism is
based on the principles of reinforcement learning. We test our architecture on
a number of natural-like toy languages. Results show that the model can learn
these artificial languages from scratch and extract grammatical information
that supports learning. Our study demonstrates the power of this simple
architecture and stresses the importance of sequence memory as a key component
of the language learning process. Since other animals do not seem to have a
faithful sequence memory, this may explain why only humans have developed
complex languages.
",2024-02-18T19:11:58Z,http://arxiv.org/abs/2402.11681v1,"Jérôme Michaud, Anna Jon-and"
Can Transformers Reason in Fragments of Natural Language?,"  State-of-the-art deep-learning-based approaches to Natural Language
Processing (NLP) are credited with various capabilities that involve reasoning
with natural language texts. In this paper we carry out a large-scale empirical
study investigating the detection of formally valid inferences in controlled
fragments of natural language for which the satisfiability problem becomes
increasingly complex. We find that, while transformer-based language models
perform surprisingly well in these scenarios, a deeper analysis re-veals that
they appear to overfit to superficial patterns in the data rather than
acquiring the logical principles governing the reasoning in these fragments.
",2022-11-10T08:46:53Z,http://arxiv.org/abs/2211.05417v1,"Viktor Schlegel, Kamen V. Pavlov, Ian Pratt-Hartmann"
"Evolution of Natural Language Processing Technology: Not Just Language
  Processing Towards General Purpose AI","  Since the invention of computers, communication through natural language
(actual human language) has been a dream technology. However, natural language
is extremely difficult to mathematically formulate, making it difficult to
realize as an algorithm without considering programming. While there have been
numerous technological developments, one cannot say that any results allowing
free utilization have been achieved thus far. In the case of language learning
in humans, for instance when learning one's mother tongue or foreign language,
one must admit that this process is similar to the adage ""practice makes
perfect"" in principle, even though the learning method is significant up to a
point. Deep learning has played a central role in contemporary AI technology in
recent years. When applied to natural language processing (NLP), this produced
unprecedented results. Achievements exceeding the initial predictions have been
reported from the results of learning vast amounts of textual data using deep
learning. For instance, four arithmetic operations could be performed without
explicit learning, thereby enabling the explanation of complex images and the
generation of images from corresponding explanatory texts. It is an accurate
example of the learner embodying the concept of ""practice makes perfect"" by
using vast amounts of textual data. This report provides a technological
explanation of how cutting-edge NLP has made it possible to realize the
""practice makes perfect"" principle. Additionally, examples of how this can be
applied to business are provided. We reported in June 2022 in Japanese on the
NLP movement from late 2021 to early 2022. We would like to summarize this as a
memorandum since this is just the initial movement leading to the current large
language models (LLMs).
",2023-10-10T00:41:38Z,http://arxiv.org/abs/2310.06228v1,Masahiro Yamamoto
"FinLangNet: A Novel Deep Learning Framework for Credit Risk Prediction
  Using Linguistic Analogy in Financial Data","  Recent industrial applications in risk prediction still heavily rely on
extensively manually-tuned, statistical learning methods. Real-world financial
data, characterized by its high dimensionality, sparsity, high noise levels,
and significant imbalance, poses unique challenges for the effective
application of deep neural network models. In this work, we introduce a novel
deep learning risk prediction framework, FinLangNet, which conceptualizes
credit loan trajectories in a structure that mirrors linguistic constructs.
This framework is tailored for credit risk prediction using real-world
financial data, drawing on structural similarities to language by adapting
natural language processing techniques. It particularly emphasizes analyzing
the development and forecastability of mid-term credit histories through
multi-head and sequences of detailed financial events. Our research
demonstrates that FinLangNet surpasses traditional statistical methods in
predicting credit risk and that its integration with these methods enhances
credit overdue prediction models, achieving a significant improvement of over
4.24\% in the Kolmogorov-Smirnov metric.
",2024-04-19T17:01:46Z,http://arxiv.org/abs/2404.13004v2,"Yu Lei, Zixuan Wang, Chu Liu, Tongyao Wang, Dongyang Lee"
Deep Learning for Hindi Text Classification: A Comparison,"  Natural Language Processing (NLP) and especially natural language text
analysis have seen great advances in recent times. Usage of deep learning in
text processing has revolutionized the techniques for text processing and
achieved remarkable results. Different deep learning architectures like CNN,
LSTM, and very recent Transformer have been used to achieve state of the art
results variety on NLP tasks. In this work, we survey a host of deep learning
architectures for text classification tasks. The work is specifically concerned
with the classification of Hindi text. The research in the classification of
morphologically rich and low resource Hindi language written in Devanagari
script has been limited due to the absence of large labeled corpus. In this
work, we used translated versions of English data-sets to evaluate models based
on CNN, LSTM and Attention. Multilingual pre-trained sentence embeddings based
on BERT and LASER are also compared to evaluate their effectiveness for the
Hindi language. The paper also serves as a tutorial for popular text
classification techniques.
",2020-01-19T09:29:12Z,http://arxiv.org/abs/2001.10340v1,"Ramchandra Joshi, Purvi Goel, Raviraj Joshi"
"Prediction Algorithm for Heat Demand of Science and Technology Topics
  Based on Time Convolution Network","  Thanks to the rapid development of deep learning, big data analysis
technology is not only widely used in the field of natural language processing,
but also more mature in the field of numerical prediction. It is of great
significance for the subject heat prediction and analysis of science and
technology demand data. How to apply theme features to accurately predict the
theme heat of science and technology demand is the core to solve this problem.
In this paper, a prediction method of subject heat of science and technology
demand based on time convolution network (TCN) is proposed to obtain the
subject feature representation of science and technology demand. Time series
prediction is carried out based on TCN network and self attention mechanism,
which increases the accuracy of subject heat prediction of science and
technology demand data Experiments show that the prediction accuracy of this
algorithm is better than other time series prediction methods on the real
science and technology demand datasets.
",2022-03-21T03:30:05Z,http://arxiv.org/abs/2203.10718v1,"Cui Haiyan, Li Yawen, Xu Xin"
"A Data-Driven Study of Commonsense Knowledge using the ConceptNet
  Knowledge Base","  Acquiring commonsense knowledge and reasoning is recognized as an important
frontier in achieving general Artificial Intelligence (AI). Recent research in
the Natural Language Processing (NLP) community has demonstrated significant
progress in this problem setting. Despite this progress, which is mainly on
multiple-choice question answering tasks in limited settings, there is still a
lack of understanding (especially at scale) of the nature of commonsense
knowledge itself. In this paper, we propose and conduct a systematic study to
enable a deeper understanding of commonsense knowledge by doing an empirical
and structural analysis of the ConceptNet knowledge base. ConceptNet is a
freely available knowledge base containing millions of commonsense assertions
presented in natural language. Detailed experimental results on three carefully
designed research questions, using state-of-the-art unsupervised graph
representation learning ('embedding') and clustering techniques, reveal deep
substructures in ConceptNet relations, allowing us to make data-driven and
computational claims about the meaning of phenomena such as 'context' that are
traditionally discussed only in qualitative terms. Furthermore, our methodology
provides a case study in how to use data-science and computational
methodologies for understanding the nature of an everyday (yet complex)
psychological phenomenon that is an essential feature of human intelligence.
",2020-11-28T08:08:25Z,http://arxiv.org/abs/2011.14084v2,"Ke Shen, Mayank Kejriwal"
"Deep Learning, Natural Language Processing, and Explainable Artificial
  Intelligence in the Biomedical Domain","  In this article, we first give an introduction to artificial intelligence and
its applications in biology and medicine in Section 1. Deep learning methods
are then described in Section 2. We narrow down the focus of the study on
textual data in Section 3, where natural language processing and its
applications in the biomedical domain are described. In Section 4, we give an
introduction to explainable artificial intelligence and discuss the importance
of explainability of artificial intelligence systems, especially in the
biomedical domain.
",2022-02-25T13:30:51Z,http://arxiv.org/abs/2202.12678v2,"Milad Moradi, Matthias Samwald"
"Surveying the Landscape of Text Summarization with Deep Learning: A
  Comprehensive Review","  In recent years, deep learning has revolutionized natural language processing
(NLP) by enabling the development of models that can learn complex
representations of language data, leading to significant improvements in
performance across a wide range of NLP tasks. Deep learning models for NLP
typically use large amounts of data to train deep neural networks, allowing
them to learn the patterns and relationships in language data. This is in
contrast to traditional NLP approaches, which rely on hand-engineered features
and rules to perform NLP tasks. The ability of deep neural networks to learn
hierarchical representations of language data, handle variable-length input
sequences, and perform well on large datasets makes them well-suited for NLP
applications. Driven by the exponential growth of textual data and the
increasing demand for condensed, coherent, and informative summaries, text
summarization has been a critical research area in the field of NLP. Applying
deep learning to text summarization refers to the use of deep neural networks
to perform text summarization tasks. In this survey, we begin with a review of
fashionable text summarization tasks in recent years, including extractive,
abstractive, multi-document, and so on. Next, we discuss most deep
learning-based models and their experimental results on these tasks. The paper
also covers datasets and data representation for summarization tasks. Finally,
we delve into the opportunities and challenges associated with summarization
tasks and their corresponding methodologies, aiming to inspire future research
efforts to advance the field further. A goal of our survey is to explain how
these methods differ in their requirements as understanding them is essential
for choosing a technique suited for a specific setting.
",2023-10-13T21:24:37Z,http://arxiv.org/abs/2310.09411v1,"Guanghua Wang, Weili Wu"
"Neural translation and automated recognition of ICD10 medical entities
  from natural language","  The recognition of medical entities from natural language is an ubiquitous
problem in the medical field, with applications ranging from medical act coding
to the analysis of electronic health data for public health. It is however a
complex task usually requiring human expert intervention, thus making it
expansive and time consuming. The recent advances in artificial intelligence,
specifically the raise of deep learning methods, has enabled computers to make
efficient decisions on a number of complex problems, with the notable example
of neural sequence models and their powerful applications in natural language
processing. They however require a considerable amount of data to learn from,
which is typically their main limiting factor. However, the C\'epiDc stores an
exhaustive database of death certificates at the French national scale,
amounting to several millions of natural language examples provided with their
associated human coded medical entities available to the machine learning
practitioner. This article investigates the applications of deep neural
sequence models to the medical entity recognition from natural language
problem.
",2020-03-27T18:17:53Z,http://arxiv.org/abs/2004.13839v2,"Louis Falissard, Claire Morgand, Sylvie Roussel, Claire Imbaud, Walid Ghosn, Karim Bounebache, Grégoire Rey"
Do learned speech symbols follow Zipf's law?,"  In this study, we investigate whether speech symbols, learned through deep
learning, follow Zipf's law, akin to natural language symbols. Zipf's law is an
empirical law that delineates the frequency distribution of words, forming
fundamentals for statistical analysis in natural language processing. Natural
language symbols, which are invented by humans to symbolize speech content, are
recognized to comply with this law. On the other hand, recent breakthroughs in
spoken language processing have given rise to the development of learned speech
symbols; these are data-driven symbolizations of speech content. Our objective
is to ascertain whether these data-driven speech symbols follow Zipf's law, as
the same as natural language symbols. Through our investigation, we aim to
forge new ways for the statistical analysis of spoken language processing.
",2023-09-18T11:56:10Z,http://arxiv.org/abs/2309.09690v1,"Shinnosuke Takamichi, Hiroki Maeda, Joonyong Park, Daisuke Saito, Hiroshi Saruwatari"
"A Comprehensive Python Library for Deep Learning-Based Event Detection
  in Multivariate Time Series Data and Information Retrieval in NLP","  Event detection in time series data is crucial in various domains, including
finance, healthcare, cybersecurity, and science. Accurately identifying events
in time series data is vital for making informed decisions, detecting
anomalies, and predicting future trends. Despite extensive research exploring
diverse methods for event detection in time series, with deep learning
approaches being among the most advanced, there is still room for improvement
and innovation in this field. In this paper, we present a new deep learning
supervised method for detecting events in multivariate time series data. Our
method combines four distinct novelties compared to existing deep-learning
supervised methods. Firstly, it is based on regression instead of binary
classification. Secondly, it does not require labeled datasets where each point
is labeled; instead, it only requires reference events defined as time points
or intervals of time. Thirdly, it is designed to be robust by using a stacked
ensemble learning meta-model that combines deep learning models, ranging from
classic feed-forward neural networks (FFNs) to state-of-the-art architectures
like transformers. This ensemble approach can mitigate individual model
weaknesses and biases, resulting in more robust predictions. Finally, to
facilitate practical implementation, we have developed a Python package to
accompany our proposed method. The package, called eventdetector-ts, can be
installed through the Python Package Index (PyPI). In this paper, we present
our method and provide a comprehensive guide on the usage of the package. We
showcase its versatility and effectiveness through different real-world use
cases from natural language processing (NLP) to financial security domains.
",2023-10-25T09:13:19Z,http://arxiv.org/abs/2310.16485v2,"Menouar Azib, Benjamin Renard, Philippe Garnier, Vincent Génot, Nicolas André"
Deep Learning Works in Practice. But Does it Work in Theory?,"  Deep learning relies on a very specific kind of neural networks: those
superposing several neural layers. In the last few years, deep learning
achieved major breakthroughs in many tasks such as image analysis, speech
recognition, natural language processing, and so on. Yet, there is no
theoretical explanation of this success. In particular, it is not clear why the
deeper the network, the better it actually performs.
  We argue that the explanation is intimately connected to a key feature of the
data collected from our surrounding universe to feed the machine learning
algorithms: large non-parallelizable logical depth. Roughly speaking, we
conjecture that the shortest computational descriptions of the universe are
algorithms with inherently large computation times, even when a large number of
computers are available for parallelization. Interestingly, this conjecture,
combined with the folklore conjecture in theoretical computer science that $ P
\neq NC$, explains the success of deep learning.
",2018-01-31T13:12:30Z,http://arxiv.org/abs/1801.10437v1,"Lê Nguyên Hoang, Rachid Guerraoui"
Identifying and Exploiting Structures for Reliable Deep Learning,"  Deep learning research has recently witnessed an impressively fast-paced
progress in a wide range of tasks including computer vision, natural language
processing, and reinforcement learning. The extraordinary performance of these
systems often gives the impression that they can be used to revolutionise our
lives for the better. However, as recent works point out, these systems suffer
from several issues that make them unreliable for use in the real world,
including vulnerability to adversarial attacks (Szegedy et al. [248]), tendency
to memorise noise (Zhang et al. [292]), being over-confident on incorrect
predictions (miscalibration) (Guo et al. [99]), and unsuitability for handling
private data (Gilad-Bachrach et al. [88]). In this thesis, we look at each of
these issues in detail, investigate their causes, and propose computationally
cheap algorithms for mitigating them in practice. To do this, we identify
structures in deep neural networks that can be exploited to mitigate the above
causes of unreliability of deep learning algorithms.
",2021-08-16T13:40:01Z,http://arxiv.org/abs/2108.07083v1,Amartya Sanyal
Text Classification: A Perspective of Deep Learning Methods,"  In recent years, with the rapid development of information on the Internet,
the number of complex texts and documents has increased exponentially, which
requires a deeper understanding of deep learning methods in order to accurately
classify texts using deep learning techniques, and thus deep learning methods
have become increasingly important in text classification. Text classification
is a class of tasks that automatically classifies a set of documents into
multiple predefined categories based on their content and subject matter. Thus,
the main goal of text classification is to enable users to extract information
from textual resources and process processes such as retrieval, classification,
and machine learning techniques together in order to classify different
categories. Many new techniques of deep learning have already achieved
excellent results in natural language processing. The success of these learning
algorithms relies on their ability to understand complex models and non-linear
relationships in data. However, finding the right structure, architecture, and
techniques for text classification is a challenge for researchers. This paper
introduces deep learning-based text classification algorithms, including
important steps required for text classification tasks such as feature
extraction, feature reduction, and evaluation strategies and methods. At the
end of the article, different deep learning text classification methods are
compared and summarized.
",2023-09-24T21:49:51Z,http://arxiv.org/abs/2309.13761v1,Zhongwei Wan
Data Augmentation for Neural NLP,"  Data scarcity is a problem that occurs in languages and tasks where we do not
have large amounts of labeled data but want to use state-of-the-art models.
Such models are often deep learning models that require a significant amount of
data to train. Acquiring data for various machine learning problems is
accompanied by high labeling costs. Data augmentation is a low-cost approach
for tackling data scarcity. This paper gives an overview of current
state-of-the-art data augmentation methods used for natural language
processing, with an emphasis on methods for neural and transformer-based
models. Furthermore, it discusses the practical challenges of data
augmentation, possible mitigations, and directions for future research.
",2023-02-22T14:47:15Z,http://arxiv.org/abs/2302.11412v1,"Domagoj Pluščec, Jan Šnajder"
Natural Language Processing Advancements By Deep Learning: A Survey,"  Natural Language Processing (NLP) helps empower intelligent machines by
enhancing a better understanding of the human language for linguistic-based
human-computer communication. Recent developments in computational power and
the advent of large amounts of linguistic data have heightened the need and
demand for automating semantic analysis using data-driven approaches. The
utilization of data-driven strategies is pervasive now due to the significant
improvements demonstrated through the usage of deep learning methods in areas
such as Computer Vision, Automatic Speech Recognition, and in particular, NLP.
This survey categorizes and addresses the different aspects and applications of
NLP that have benefited from deep learning. It covers core NLP tasks and
applications and describes how deep learning methods and models advance these
areas. We further analyze and compare different approaches and state-of-the-art
models.
",2020-03-02T21:32:05Z,http://arxiv.org/abs/2003.01200v4,"Amirsina Torfi, Rouzbeh A. Shirvani, Yaser Keneshloo, Nader Tavaf, Edward A. Fox"
"Application of Transfer Learning to Sign Language Recognition using an
  Inflated 3D Deep Convolutional Neural Network","  Sign language is the primary language for people with a hearing loss. Sign
language recognition (SLR) is the automatic recognition of sign language, which
represents a challenging problem for computers, though some progress has been
made recently using deep learning. Huge amounts of data are generally required
to train deep learning models. However, corresponding datasets are missing for
the majority of sign languages. Transfer learning is a technique to utilize a
related task with an abundance of data available to help solve a target task
lacking sufficient data. Transfer learning has been applied highly successfully
in computer vision and natural language processing. However, much less research
has been conducted in the field of SLR. This paper investigates how effectively
transfer learning can be applied to isolated SLR using an inflated 3D
convolutional neural network as the deep learning architecture. Transfer
learning is implemented by pre-training a network on the American Sign Language
dataset MS-ASL and subsequently fine-tuning it separately on three different
sizes of the German Sign Language dataset SIGNUM. The results of the
experiments give clear empirical evidence that transfer learning can be
effectively applied to isolated SLR. The accuracy performances of the networks
applying transfer learning increased substantially by up to 21% as compared to
the baseline models that were not pre-trained on the MS-ASL dataset.
",2021-02-25T13:37:39Z,http://arxiv.org/abs/2103.05111v1,Roman Töngi
"Deep Learning Algorithms with Applications to Video Analytics for A
  Smart City: A Survey","  Deep learning has recently achieved very promising results in a wide range of
areas such as computer vision, speech recognition and natural language
processing. It aims to learn hierarchical representations of data by using deep
architecture models. In a smart city, a lot of data (e.g. videos captured from
many distributed sensors) need to be automatically processed and analyzed. In
this paper, we review the deep learning algorithms applied to video analytics
of smart city in terms of different research topics: object detection, object
tracking, face recognition, image classification and scene labeling.
",2015-12-10T03:23:54Z,http://arxiv.org/abs/1512.03131v1,"Li Wang, Dennis Sng"
Three-Class Text Sentiment Analysis Based on LSTM,"  Sentiment analysis is a crucial task in natural language processing (NLP)
with applications in public opinion monitoring, market research, and beyond.
This paper introduces a three-class sentiment classification method for Weibo
comments using Long Short-Term Memory (LSTM) networks to discern positive,
neutral, and negative sentiments. LSTM, as a deep learning model, excels at
capturing long-distance dependencies in text data, providing significant
advantages over traditional machine learning approaches. Through preprocessing
and feature extraction from Weibo comment texts, our LSTM model achieves
precise sentiment prediction. Experimental results demonstrate superior
performance, achieving an accuracy of 98.31% and an F1 score of 98.28%, notably
outperforming conventional models and other deep learning methods. This
underscores the effectiveness of LSTM in capturing nuanced sentiment
information within text, thereby enhancing classification accuracy. Despite its
strengths, the LSTM model faces challenges such as high computational
complexity and slower processing times for lengthy texts. Moreover, complex
emotional expressions like sarcasm and humor pose additional difficulties.
Future work could explore combining pre-trained models or advancing feature
engineering techniques to further improve both accuracy and practicality.
Overall, this study provides an effective solution for sentiment analysis on
Weibo comments.
",2024-12-23T07:21:07Z,http://arxiv.org/abs/2412.17347v1,Yin Qixuan
What Really is Deep Learning Doing?,"  Deep learning has achieved a great success in many areas, from computer
vision to natural language processing, to game playing, and much more. Yet,
what deep learning is really doing is still an open question. There are a lot
of works in this direction. For example, [5] tried to explain deep learning by
group renormalization, and [6] tried to explain deep learning from the view of
functional approximation. In order to address this very crucial question, here
we see deep learning from perspective of mechanical learning and learning
machine (see [1], [2]). From this particular angle, we can see deep learning
much better and answer with confidence: What deep learning is really doing? why
it works well, how it works, and how much data is necessary for learning. We
also will discuss advantages and disadvantages of deep learning at the end of
this work.
",2017-11-06T23:00:13Z,http://arxiv.org/abs/1711.03577v1,Chuyu Xiong
Geometric deep learning: going beyond Euclidean data,"  Many scientific fields study data with an underlying structure that is a
non-Euclidean space. Some examples include social networks in computational
social sciences, sensor networks in communications, functional networks in
brain imaging, regulatory networks in genetics, and meshed surfaces in computer
graphics. In many applications, such geometric data are large and complex (in
the case of social networks, on the scale of billions), and are natural targets
for machine learning techniques. In particular, we would like to use deep
neural networks, which have recently proven to be powerful tools for a broad
range of problems from computer vision, natural language processing, and audio
analysis. However, these tools have been most successful on data with an
underlying Euclidean or grid-like structure, and in cases where the invariances
of these structures are built into networks used to model them. Geometric deep
learning is an umbrella term for emerging techniques attempting to generalize
(structured) deep neural models to non-Euclidean domains such as graphs and
manifolds. The purpose of this paper is to overview different examples of
geometric deep learning problems and present available solutions, key
difficulties, applications, and future research directions in this nascent
field.
",2016-11-24T08:45:01Z,http://arxiv.org/abs/1611.08097v2,"Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, Pierre Vandergheynst"
"Large Language Models are not Models of Natural Language: they are
  Corpus Models","  Natural Language Processing (NLP) has become one of the leading application
areas in the current Artificial Intelligence boom. Transfer learning has
enabled large deep learning neural networks trained on the language modeling
task to vastly improve performance in almost all downstream language tasks.
Interestingly, when the language models are trained with data that includes
software code, they demonstrate remarkable abilities in generating functioning
computer code from natural language specifications. We argue that this creates
a conundrum for the claim that eliminative neural models are a radical
restructuring in our understanding of cognition in that they eliminate the need
for symbolic abstractions like generative phrase structure grammars. Because
the syntax of programming languages is by design determined by phrase structure
grammars, neural models that produce syntactic code are apparently
uninformative about the theoretical foundations of programming languages. The
demonstration that neural models perform well on tasks that involve clearly
symbolic systems, proves that they cannot be used as an argument that language
and other cognitive systems are not symbolic. Finally, we argue as a corollary
that the term language model is misleading and propose the adoption of the
working term corpus model instead, which better reflects the genesis and
contents of the model.
",2021-12-13T22:39:46Z,http://arxiv.org/abs/2112.07055v2,Csaba Veres
"Semantic and Relational Spaces in Science of Science: Deep Learning
  Models for Article Vectorisation","  Over the last century, we observe a steady and exponentially growth of
scientific publications globally. The overwhelming amount of available
literature makes a holistic analysis of the research within a field and between
fields based on manual inspection impossible. Automatic techniques to support
the process of literature review are required to find the epistemic and social
patterns that are embedded in scientific publications. In computer sciences,
new tools have been developed to deal with large volumes of data. In
particular, deep learning techniques open the possibility of automated
end-to-end models to project observations to a new, low-dimensional space where
the most relevant information of each observation is highlighted. Using deep
learning to build new representations of scientific publications is a growing
but still emerging field of research. The aim of this paper is to discuss the
potential and limits of deep learning for gathering insights about scientific
research articles. We focus on document-level embeddings based on the semantic
and relational aspects of articles, using Natural Language Processing (NLP) and
Graph Neural Networks (GNNs). We explore the different outcomes generated by
those techniques. Our results show that using NLP we can encode a semantic
space of articles, while with GNN we are able to build a relational space where
the social practices of a research community are also encoded.
",2020-11-05T14:57:41Z,http://arxiv.org/abs/2011.02887v1,"Diego Kozlowski, Jennifer Dusdal, Jun Pang, Andreas Zilian"
A Short Review on Data Modelling for Vector Fields,"  Machine learning methods based on statistical principles have proven highly
successful in dealing with a wide variety of data analysis and analytics tasks.
Traditional data models are mostly concerned with independent identically
distributed data. The recent success of end-to-end modelling scheme using deep
neural networks equipped with effective structures such as convolutional layers
or skip connections allows the extension to more sophisticated and structured
practical data, such as natural language, images, videos, etc. On the
application side, vector fields are an extremely useful type of data in
empirical sciences, as well as signal processing, e.g. non-parametric
transformations of 3D point clouds using 3D vector fields, the modelling of the
fluid flow in earth science, and the modelling of physical fields.
  This review article is dedicated to recent computational tools of vector
fields, including vector data representations, predictive model of spatial
data, as well as applications in computer vision, signal processing, and
empirical sciences.
",2020-09-01T17:07:29Z,http://arxiv.org/abs/2009.00577v1,"Jun Li, Wanrong Hong, Yusheng Xiang"
Machine Learning and Big Scientific Data,"  This paper reviews some of the challenges posed by the huge growth of
experimental data generated by the new generation of large-scale experiments at
UK national facilities at the Rutherford Appleton Laboratory site at Harwell
near Oxford. Such ""Big Scientific Data"" comes from the Diamond Light Source and
Electron Microscopy Facilities, the ISIS Neutron and Muon Facility, and the
UK's Central Laser Facility. Increasingly, scientists are now needing to use
advanced machine learning and other AI technologies both to automate parts of
the data pipeline and also to help find new scientific discoveries in the
analysis of their data. For commercially important applications, such as object
recognition, natural language processing and automatic translation, deep
learning has made dramatic breakthroughs. Google's DeepMind has now also used
deep learning technology to develop their AlphaFold tool to make predictions
for protein folding. Remarkably, they have been able to achieve some
spectacular results for this specific scientific problem. Can deep learning be
similarly transformative for other scientific problems? After a brief review of
some initial applications of machine learning at the Rutherford Appleton
Laboratory, we focus on challenges and opportunities for AI in advancing
materials science. Finally, we discuss the importance of developing some
realistic machine learning benchmarks using Big Scientific Data coming from a
number of different scientific domains. We conclude with some initial examples
of our ""SciML"" benchmark suite and of the research challenges these benchmarks
will enable.
",2019-10-12T15:02:13Z,http://arxiv.org/abs/1910.07631v1,"Tony Hey, Keith Butler, Sam Jackson, Jeyarajan Thiyagalingam"
Sentiment Analysis Challenges in Persian Language,"  The rapid growth in data on the internet requires a data mining process to
reach a decision to support insight. The Persian language has strong potential
for deep research in any aspect of natural language processing, especially
sentimental analysis approach. Thousands of websites and blogs updates and
modifies by Persian users around the world that contains millions of Persian
context. This range of application requires a comprehensive structured
framework to extract beneficial information for helping enterprises to enhance
their business and initiate a customer-centric management process by producing
effective recommender systems. Sentimental analysis is an intelligent approach
for extracting useful information from huge amounts of data to help an
enterprise for smart management process. In this road, machine learning and
deep learning techniques will become very helpful but there is the number of
challenges which are face to them. This paper tried to present and assert the
most important challenges of sentimental analysis in the Persian language. This
language is an Indo-European language which spoken by over 110 million people
around the world and is an official language in Iran, Tajikistan, and
Afghanistan. Its also widely used in Uzbekistan, Pakistan and Turkish by order.
",2019-07-09T20:46:37Z,http://arxiv.org/abs/1907.04407v2,Mohammad Heydari
"SDW-ASL: A Dynamic System to Generate Large Scale Dataset for Continuous
  American Sign Language","  Despite tremendous progress in natural language processing using deep
learning techniques in recent years, sign language production and comprehension
has advanced very little. One critical barrier is the lack of largescale
datasets available to the public due to the unbearable cost of labeled data
generation. Efforts to provide public data for American Sign Language (ASL)
comprehension have yielded two datasets, comprising more than thousand video
clips. These datasets are large enough to enable a meaningful start to deep
learning research on sign languages but are far too small to lead to any
solution that can be practically deployed. So far, there is still no suitable
dataset for ASL production. We proposed a system that can generate large scale
ASL datasets for continuous ASL. It is suitable for general ASL processing and
is particularly useful for ASL production. The continuous ASL dataset contains
English labeled human articulations in condensed body pose data formats. To
better serve the research community, we are releasing the first version of our
ASL dataset, which contains 30k sentences, 416k words, a vocabulary of 18k
words, in a total of 104 hours. This is the largest continuous sign language
dataset published to date in terms of video duration. We also describe a system
that can evolve and expand the dataset to incorporate better data processing
techniques and more contents when available. It is our hope that the release of
this ASL dataset and the sustainable dataset generation system to the public
will propel better deep-learning research in ASL natural language processing.
",2022-10-13T07:08:00Z,http://arxiv.org/abs/2210.06791v1,Yehong Jiang
Engineering A Large Language Model From Scratch,"  The proliferation of deep learning in natural language processing (NLP) has
led to the development and release of innovative technologies capable of
understanding and generating human language with remarkable proficiency.
Atinuke, a Transformer-based neural network, optimises performance across
various language tasks by utilising a unique configuration. The architecture
interweaves layers for processing sequential data with attention mechanisms to
draw meaningful affinities between inputs and outputs. Due to the configuration
of its topology and hyperparameter tuning, it can emulate human-like language
by extracting features and learning complex mappings. Atinuke is modular,
extensible, and integrates seamlessly with existing machine learning pipelines.
Advanced matrix operations like softmax, embeddings, and multi-head attention
enable nuanced handling of textual, acoustic, and visual signals. By unifying
modern deep learning techniques with software design principles and
mathematical theory, the system achieves state-of-the-art results on natural
language tasks whilst remaining interpretable and robust.
",2024-01-30T04:29:48Z,http://arxiv.org/abs/2401.16736v3,Abiodun Finbarrs Oketunji
A Deep Learning Approach to Create DNS Amplification Attacks,"  In recent years, deep learning has shown itself to be an incredibly valuable
tool in cybersecurity as it helps network intrusion detection systems to
classify attacks and detect new ones. Adversarial learning is the process of
utilizing machine learning to generate a perturbed set of inputs to then feed
to the neural network to misclassify it. Much of the current work in the field
of adversarial learning has been conducted in image processing and natural
language processing with a wide variety of algorithms. Two algorithms of
interest are the Elastic-Net Attack on Deep Neural Networks and TextAttack. In
our experiment the EAD and TextAttack algorithms are applied to a Domain Name
System amplification classifier. The algorithms are used to generate malicious
Distributed Denial of Service adversarial examples to then feed as inputs to
the network intrusion detection systems neural network to classify as valid
traffic. We show in this work that both image processing and natural language
processing adversarial learning algorithms can be applied against a network
intrusion detection neural network.
",2022-06-29T01:11:48Z,http://arxiv.org/abs/2206.14346v1,"Jared Mathews, Prosenjit Chatterjee, Shankar Banik, Cory Nance"
"Comprehensive Implementation of TextCNN for Enhanced Collaboration
  between Natural Language Processing and System Recommendation","  Natural Language Processing (NLP) is an important branch of artificial
intelligence that studies how to enable computers to understand, process, and
generate human language. Text classification is a fundamental task in NLP,
which aims to classify text into different predefined categories. Text
classification is the most basic and classic task in natural language
processing, and most of the tasks in natural language processing can be
regarded as classification tasks. In recent years, deep learning has achieved
great success in many research fields, and today, it has also become a standard
technology in the field of NLP, which is widely integrated into text
classification tasks. Unlike numbers and images, text processing emphasizes
fine-grained processing ability. Traditional text classification methods
generally require preprocessing the input model's text data. Additionally, they
also need to obtain good sample features through manual annotation and then use
classical machine learning algorithms for classification. Therefore, this paper
analyzes the application status of deep learning in the three core tasks of NLP
(including text representation, word order modeling, and knowledge
representation). This content explores the improvement and synergy achieved
through natural language processing in the context of text classification,
while also taking into account the challenges posed by adversarial techniques
in text generation, text classification, and semantic parsing. An empirical
study on text classification tasks demonstrates the effectiveness of
interactive integration training, particularly in conjunction with TextCNN,
highlighting the significance of these advancements in text classification
augmentation and enhancement.
",2024-03-12T07:25:53Z,http://arxiv.org/abs/2403.09718v1,"Xiaonan Xu, Zheng Xu, Zhipeng Ling, Zhengyu Jin, ShuQian Du"
Quantifying Uncertainties in Natural Language Processing Tasks,"  Reliable uncertainty quantification is a first step towards building
explainable, transparent, and accountable artificial intelligent systems.
Recent progress in Bayesian deep learning has made such quantification
realizable. In this paper, we propose novel methods to study the benefits of
characterizing model and data uncertainties for natural language processing
(NLP) tasks. With empirical experiments on sentiment analysis, named entity
recognition, and language modeling using convolutional and recurrent neural
network models, we show that explicitly modeling uncertainties is not only
necessary to measure output confidence levels, but also useful at enhancing
model performances in various NLP tasks.
",2018-11-18T01:36:05Z,http://arxiv.org/abs/1811.07253v1,"Yijun Xiao, William Yang Wang"
Sentiment Analysis for Sinhala Language using Deep Learning Techniques,"  Due to the high impact of the fast-evolving fields of machine learning and
deep learning, Natural Language Processing (NLP) tasks have further obtained
comprehensive performances for highly resourced languages such as English and
Chinese. However Sinhala, which is an under-resourced language with a rich
morphology, has not experienced these advancements. For sentiment analysis,
there exists only two previous research with deep learning approaches, which
focused only on document-level sentiment analysis for the binary case. They
experimented with only three types of deep learning models. In contrast, this
paper presents a much comprehensive study on the use of standard sequence
models such as RNN, LSTM, Bi-LSTM, as well as more recent state-of-the-art
models such as hierarchical attention hybrid neural networks, and capsule
networks. Classification is done at document-level but with more granularity by
considering POSITIVE, NEGATIVE, NEUTRAL, and CONFLICT classes. A data set of
15059 Sinhala news comments, annotated with these four classes and a corpus
consists of 9.48 million tokens are publicly released. This is the largest
sentiment annotated data set for Sinhala so far.
",2020-11-14T12:02:30Z,http://arxiv.org/abs/2011.07280v1,"Lahiru Senevirathne, Piyumal Demotte, Binod Karunanayake, Udyogi Munasinghe, Surangika Ranathunga"
"Navigating the Kaleidoscope of COVID-19 Misinformation Using Deep
  Learning","  Irrespective of the success of the deep learning-based mixed-domain transfer
learning approach for solving various Natural Language Processing tasks, it
does not lend a generalizable solution for detecting misinformation from
COVID-19 social media data. Due to the inherent complexity of this type of
data, caused by its dynamic (context evolves rapidly), nuanced (misinformation
types are often ambiguous), and diverse (skewed, fine-grained, and overlapping
categories) nature, it is imperative for an effective model to capture both the
local and global context of the target domain. By conducting a systematic
investigation, we show that: (i) the deep Transformer-based pre-trained models,
utilized via the mixed-domain transfer learning, are only good at capturing the
local context, thus exhibits poor generalization, and (ii) a combination of
shallow network-based domain-specific models and convolutional neural networks
can efficiently extract local as well as global context directly from the
target data in a hierarchical fashion, enabling it to offer a more
generalizable solution.
",2021-09-19T15:49:25Z,http://arxiv.org/abs/2110.15703v1,"Yuanzhi Chen, Mohammad Rashedul Hasan"
Knowledge Efficient Deep Learning for Natural Language Processing,"  Deep learning has become the workhorse for a wide range of natural language
processing applications. But much of the success of deep learning relies on
annotated examples. Annotation is time-consuming and expensive to produce at
scale. Here we are interested in methods for reducing the required quantity of
annotated data -- by making the learning methods more knowledge efficient so as
to make them more applicable in low annotation (low resource) settings. There
are various classical approaches to making the models more knowledge efficient
such as multi-task learning, transfer learning, weakly supervised and
unsupervised learning etc. This thesis focuses on adapting such classical
methods to modern deep learning models and algorithms.
  This thesis describes four works aimed at making machine learning models more
knowledge efficient. First, we propose a knowledge rich deep learning model
(KRDL) as a unifying learning framework for incorporating prior knowledge into
deep models. In particular, we apply KRDL built on Markov logic networks to
denoise weak supervision. Second, we apply a KRDL model to assist the machine
reading models to find the correct evidence sentences that can support their
decision. Third, we investigate the knowledge transfer techniques in
multilingual setting, where we proposed a method that can improve pre-trained
multilingual BERT based on the bilingual dictionary. Fourth, we present an
episodic memory network for language modelling, in which we encode the large
external knowledge for the pre-trained GPT.
",2020-08-28T23:32:33Z,http://arxiv.org/abs/2008.12878v1,Hai Wang
"Hierarchical Representation in Neural Language Models: Suppression and
  Recovery of Expectations","  Deep learning sequence models have led to a marked increase in performance
for a range of Natural Language Processing tasks, but it remains an open
question whether they are able to induce proper hierarchical generalizations
for representing natural language from linear input alone. Work using
artificial languages as training input has shown that LSTMs are capable of
inducing the stack-like data structures required to represent context-free and
certain mildly context-sensitive languages---formal language classes which
correspond in theory to the hierarchical structures of natural language. Here
we present a suite of experiments probing whether neural language models
trained on linguistic data induce these stack-like data structures and deploy
them while incrementally predicting words. We study two natural language
phenomena: center embedding sentences and syntactic island constraints on the
filler--gap dependency. In order to properly predict words in these structures,
a model must be able to temporarily suppress certain expectations and then
recover those expectations later, essentially pushing and popping these
expectations on a stack. Our results provide evidence that models can
successfully suppress and recover expectations in many cases, but do not fully
recover their previous grammatical state.
",2019-06-10T15:20:32Z,http://arxiv.org/abs/1906.04068v1,"Ethan Wilcox, Roger Levy, Richard Futrell"
"Fast and Scalable Expansion of Natural Language Understanding
  Functionality for Intelligent Agents","  Fast expansion of natural language functionality of intelligent virtual
agents is critical for achieving engaging and informative interactions.
However, developing accurate models for new natural language domains is a time
and data intensive process. We propose efficient deep neural network
architectures that maximally re-use available resources through transfer
learning. Our methods are applied for expanding the understanding capabilities
of a popular commercial agent and are evaluated on hundreds of new domains,
designed by internal or external developers. We demonstrate that our proposed
methods significantly increase accuracy in low resource settings and enable
rapid development of accurate models with less data.
",2018-05-03T21:21:16Z,http://arxiv.org/abs/1805.01542v1,"Anuj Goyal, Angeliki Metallinou, Spyros Matsoukas"
"DeCLUTR: Deep Contrastive Learning for Unsupervised Textual
  Representations","  Sentence embeddings are an important component of many natural language
processing (NLP) systems. Like word embeddings, sentence embeddings are
typically learned on large text corpora and then transferred to various
downstream tasks, such as clustering and retrieval. Unlike word embeddings, the
highest performing solutions for learning sentence embeddings require labelled
data, limiting their usefulness to languages and domains where labelled data is
abundant. In this paper, we present DeCLUTR: Deep Contrastive Learning for
Unsupervised Textual Representations. Inspired by recent advances in deep
metric learning (DML), we carefully design a self-supervised objective for
learning universal sentence embeddings that does not require labelled training
data. When used to extend the pretraining of transformer-based language models,
our approach closes the performance gap between unsupervised and supervised
pretraining for universal sentence encoders. Importantly, our experiments
suggest that the quality of the learned embeddings scale with both the number
of trainable parameters and the amount of unlabelled training data. Our code
and pretrained models are publicly available and can be easily adapted to new
domains or used to embed unseen text.
",2020-06-05T20:00:28Z,http://arxiv.org/abs/2006.03659v4,"John Giorgi, Osvald Nitski, Bo Wang, Gary Bader"
Towards Explainable Fact Checking,"  The past decade has seen a substantial rise in the amount of mis- and
disinformation online, from targeted disinformation campaigns to influence
politics, to the unintentional spreading of misinformation about public health.
This development has spurred research in the area of automatic fact checking,
from approaches to detect check-worthy claims and determining the stance of
tweets towards claims, to methods to determine the veracity of claims given
evidence documents. These automatic methods are often content-based, using
natural language processing methods, which in turn utilise deep neural networks
to learn higher-order features from text in order to make predictions. As deep
neural networks are black-box models, their inner workings cannot be easily
explained. At the same time, it is desirable to explain how they arrive at
certain decisions, especially if they are to be used for decision making. While
this has been known for some time, the issues this raises have been exacerbated
by models increasing in size, and by EU legislation requiring models to be used
for decision making to provide explanations, and, very recently, by legislation
requiring online platforms operating in the EU to provide transparent reporting
on their services. Despite this, current solutions for explainability are still
lacking in the area of fact checking. This thesis presents my research on
automatic fact checking, including claim check-worthiness detection, stance
detection and veracity prediction. Its contributions go beyond fact checking,
with the thesis proposing more general machine learning solutions for natural
language processing in the area of learning with limited labelled data.
Finally, the thesis presents some first solutions for explainable fact
checking.
",2021-08-23T16:22:50Z,http://arxiv.org/abs/2108.10274v2,Isabelle Augenstein
Geometric deep learning on graphs and manifolds using mixture model CNNs,"  Deep learning has achieved a remarkable performance breakthrough in several
fields, most notably in speech recognition, natural language processing, and
computer vision. In particular, convolutional neural network (CNN)
architectures currently produce state-of-the-art performance on a variety of
image analysis tasks such as object detection and recognition. Most of deep
learning research has so far focused on dealing with 1D, 2D, or 3D
Euclidean-structured data such as acoustic signals, images, or videos.
Recently, there has been an increasing interest in geometric deep learning,
attempting to generalize deep learning methods to non-Euclidean structured data
such as graphs and manifolds, with a variety of applications from the domains
of network analysis, computational social science, or computer graphics. In
this paper, we propose a unified framework allowing to generalize CNN
architectures to non-Euclidean domains (graphs and manifolds) and learn local,
stationary, and compositional task-specific features. We show that various
non-Euclidean CNN methods previously proposed in the literature can be
considered as particular instances of our framework. We test the proposed
method on standard tasks from the realms of image-, graph- and 3D shape
analysis and show that it consistently outperforms previous approaches.
",2016-11-25T10:05:03Z,http://arxiv.org/abs/1611.08402v3,"Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodolà, Jan Svoboda, Michael M. Bronstein"
"Unveiling the frontiers of deep learning: innovations shaping diverse
  domains","  Deep learning (DL) enables the development of computer models that are
capable of learning, visualizing, optimizing, refining, and predicting data. In
recent years, DL has been applied in a range of fields, including audio-visual
data processing, agriculture, transportation prediction, natural language,
biomedicine, disaster management, bioinformatics, drug design, genomics, face
recognition, and ecology. To explore the current state of deep learning, it is
necessary to investigate the latest developments and applications of deep
learning in these disciplines. However, the literature is lacking in exploring
the applications of deep learning in all potential sectors. This paper thus
extensively investigates the potential applications of deep learning across all
major fields of study as well as the associated benefits and challenges. As
evidenced in the literature, DL exhibits accuracy in prediction and analysis,
makes it a powerful computational tool, and has the ability to articulate
itself and optimize, making it effective in processing data with no prior
training. Given its independence from training data, deep learning necessitates
massive amounts of data for effective analysis and processing, much like data
volume. To handle the challenge of compiling huge amounts of medical,
scientific, healthcare, and environmental data for use in deep learning, gated
architectures like LSTMs and GRUs can be utilized. For multimodal learning,
shared neurons in the neural network for all activities and specialized neurons
for particular tasks are necessary.
",2023-09-06T04:50:39Z,http://arxiv.org/abs/2309.02712v1,"Shams Forruque Ahmed, Md. Sakib Bin Alam, Maliha Kabir, Shaila Afrin, Sabiha Jannat Rafa, Aanushka Mehjabin, Amir H. Gandomi"
Recent Trends in Deep Learning Based Natural Language Processing,"  Deep learning methods employ multiple processing layers to learn hierarchical
representations of data and have produced state-of-the-art results in many
domains. Recently, a variety of model designs and methods have blossomed in the
context of natural language processing (NLP). In this paper, we review
significant deep learning related models and methods that have been employed
for numerous NLP tasks and provide a walk-through of their evolution. We also
summarize, compare and contrast the various models and put forward a detailed
understanding of the past, present and future of deep learning in NLP.
",2017-08-09T04:02:17Z,http://arxiv.org/abs/1708.02709v8,"Tom Young, Devamanyu Hazarika, Soujanya Poria, Erik Cambria"
"Breaking the Curse of Dimensionality in Deep Neural Networks by Learning
  Invariant Representations","  Artificial intelligence, particularly the subfield of machine learning, has
seen a paradigm shift towards data-driven models that learn from and adapt to
data. This has resulted in unprecedented advancements in various domains such
as natural language processing and computer vision, largely attributed to deep
learning, a special class of machine learning models. Deep learning arguably
surpasses traditional approaches by learning the relevant features from raw
data through a series of computational layers.
  This thesis explores the theoretical foundations of deep learning by studying
the relationship between the architecture of these models and the inherent
structures found within the data they process. In particular, we ask What
drives the efficacy of deep learning algorithms and allows them to beat the
so-called curse of dimensionality-i.e. the difficulty of generally learning
functions in high dimensions due to the exponentially increasing need for data
points with increased dimensionality? Is it their ability to learn relevant
representations of the data by exploiting their structure? How do different
architectures exploit different data structures? In order to address these
questions, we push forward the idea that the structure of the data can be
effectively characterized by its invariances-i.e. aspects that are irrelevant
for the task at hand.
  Our methodology takes an empirical approach to deep learning, combining
experimental studies with physics-inspired toy models. These simplified models
allow us to investigate and interpret the complex behaviors we observe in deep
learning systems, offering insights into their inner workings, with the
far-reaching goal of bridging the gap between theory and practice.
",2023-10-24T19:50:41Z,http://arxiv.org/abs/2310.16154v1,Leonardo Petrini
Deep Learning-based Sentiment Analysis in Persian Language,"  Recently, there has been a growing interest in the use of deep learning
techniques for tasks in natural language processing (NLP), with sentiment
analysis being one of the most challenging areas, particularly in the Persian
language. The vast amounts of content generated by Persian users on thousands
of websites, blogs, and social networks such as Telegram, Instagram, and
Twitter present a rich resource of information. Deep learning techniques have
become increasingly favored for extracting insights from this extensive pool of
raw data, although they face several challenges. In this study, we introduced
and implemented a hybrid deep learning-based model for sentiment analysis,
using customer review data from the Digikala Online Retailer website. We
employed a variety of deep learning networks and regularization techniques as
classifiers. Ultimately, our hybrid approach yielded an impressive performance,
achieving an F1 score of 78.3 across three sentiment categories: positive,
negative, and neutral.
",2024-03-17T03:15:29Z,http://arxiv.org/abs/2403.11069v1,"Mohammad Heydari, Mohsen Khazeni, Mohammad Ali Soltanshahi"
"Optimized Three Deep Learning Models Based-PSO Hyperparameters for
  Beijing PM2.5 Prediction","  Deep learning is a machine learning approach that produces excellent
performance in various applications, including natural language processing,
image identification, and forecasting. Deep learning network performance
depends on the hyperparameter settings. This research attempts to optimize the
deep learning architecture of Long short term memory (LSTM), Convolutional
neural network (CNN), and Multilayer perceptron (MLP) for forecasting tasks
using Particle swarm optimization (PSO), a swarm intelligence-based
metaheuristic optimization methodology: Proposed M-1 (PSO-LSTM), M-2 (PSO-CNN),
and M-3 (PSO-MLP). Beijing PM2.5 datasets was analyzed to measure the
performance of the proposed models. PM2.5 as a target variable was affected by
dew point, pressure, temperature, cumulated wind speed, hours of snow, and
hours of rain. The deep learning network inputs consist of three different
scenarios: daily, weekly, and monthly. The results show that the proposed M-1
with three hidden layers produces the best results of RMSE and MAPE compared to
the proposed M-2, M-3, and all the baselines. A recommendation for air
pollution management could be generated by using these optimized models
",2023-06-10T16:06:44Z,http://arxiv.org/abs/2306.07296v1,"Andri Pranolo, Yingchi Mao, Aji Prasetya Wibawa, Agung Bella Putra Utama, Felix Andika Dwiyanto"
"Deep Learning in Proteomics Informatics: Applications, Challenges, and
  Future Directions","  Deep learning is an advanced technology that relies on large-scale data and
complex models for feature extraction and pattern recognition. It has been
widely applied across various fields, including computer vision, natural
language processing, and speech recognition. In recent years, deep learning has
demonstrated significant potential in the realm of proteomics informatics,
particularly in deciphering complex biological information. The introduction of
this technology not only accelerates the processing speed of protein data but
also enhances the accuracy of predictions regarding protein structure and
function. This provides robust support for both fundamental biology research
and applied biotechnological studies. Currently, deep learning is primarily
focused on applications such as protein sequence analysis, three-dimensional
structure prediction, functional annotation, and the construction of protein
interaction networks. These applications offer numerous advantages to proteomic
research. Despite its growing prevalence in this field, deep learning faces
several challenges including data scarcity, insufficient model
interpretability, and computational complexity; these factors hinder its
further advancement within proteomics. This paper comprehensively reviews the
applications of deep learning in proteomics along with the challenges it
encounters. The aim is to provide a systematic theoretical discussion and
practical basis for research in this domain to facilitate ongoing development
and innovation of deep learning technologies within proteomics.
",2024-12-23T07:21:27Z,http://arxiv.org/abs/2412.17349v1,"Yindan Luo, Jiaxin Cai"
Tamil Language Computing: the Present and the Future,"  This paper delves into the text processing aspects of Language Computing,
which enables computers to understand, interpret, and generate human language.
Focusing on tasks such as speech recognition, machine translation, sentiment
analysis, text summarization, and language modelling, language computing
integrates disciplines including linguistics, computer science, and cognitive
psychology to create meaningful human-computer interactions. Recent
advancements in deep learning have made computers more accessible and capable
of independent learning and adaptation. In examining the landscape of language
computing, the paper emphasises foundational work like encoding, where Tamil
transitioned from ASCII to Unicode, enhancing digital communication. It
discusses the development of computational resources, including raw data,
dictionaries, glossaries, annotated data, and computational grammars, necessary
for effective language processing. The challenges of linguistic annotation, the
creation of treebanks, and the training of large language models are also
covered, emphasising the need for high-quality, annotated data and advanced
language models. The paper underscores the importance of building practical
applications for languages like Tamil to address everyday communication needs,
highlighting gaps in current technology. It calls for increased research
collaboration, digitization of historical texts, and fostering digital usage to
ensure the comprehensive development of Tamil language processing, ultimately
enhancing global communication and access to digital services.
",2024-07-11T15:56:02Z,http://arxiv.org/abs/2407.08618v2,Kengatharaiyer Sarveswaran
A Survey on State-of-the-art Deep Learning Applications and Challenges,"  Deep learning, a branch of artificial intelligence, is a data-driven method
that uses multiple layers of interconnected units (neurons) to learn intricate
patterns and representations directly from raw input data. Empowered by this
learning capability, it has become a powerful tool for solving complex problems
and is the core driver of many groundbreaking technologies and innovations.
Building a deep learning model is challenging due to the algorithm's complexity
and the dynamic nature of real-world problems. Several studies have reviewed
deep learning concepts and applications. However, the studies mostly focused on
the types of deep learning models and convolutional neural network
architectures, offering limited coverage of the state-of-the-art deep learning
models and their applications in solving complex problems across different
domains. Therefore, motivated by the limitations, this study aims to
comprehensively review the state-of-the-art deep learning models in computer
vision, natural language processing, time series analysis and pervasive
computing. We highlight the key features of the models and their effectiveness
in solving the problems within each domain. Furthermore, this study presents
the fundamentals of deep learning, various deep learning model types and
prominent convolutional neural network architectures. Finally, challenges and
future directions in deep learning research are discussed to offer a broader
perspective for future researchers.
",2024-03-26T10:10:53Z,http://arxiv.org/abs/2403.17561v5,"Mohd Halim Mohd Noor, Ayokunle Olalekan Ige"
Semantic Labeling Using a Deep Contextualized Language Model,"  Generating schema labels automatically for column values of data tables has
many data science applications such as schema matching, and data discovery and
linking. For example, automatically extracted tables with missing headers can
be filled by the predicted schema labels which significantly minimizes human
effort. Furthermore, the predicted labels can reduce the impact of inconsistent
names across multiple data tables. Understanding the connection between column
values and contextual information is an important yet neglected aspect as
previously proposed methods treat each column independently. In this paper, we
propose a context-aware semantic labeling method using both the column values
and context. Our new method is based on a new setting for semantic labeling,
where we sequentially predict labels for an input table with missing headers.
We incorporate both the values and context of each data column using the
pre-trained contextualized language model, BERT, that has achieved significant
improvements in multiple natural language processing tasks. To our knowledge,
we are the first to successfully apply BERT to solve the semantic labeling
task. We evaluate our approach using two real-world datasets from different
domains, and we demonstrate substantial improvements in terms of evaluation
metrics over state-of-the-art feature-based methods.
",2020-10-30T03:04:22Z,http://arxiv.org/abs/2010.16037v1,"Mohamed Trabelsi, Jin Cao, Jeff Heflin"
"HealthPrompt: A Zero-shot Learning Paradigm for Clinical Natural
  Language Processing","  Deep learning algorithms are dependent on the availability of large-scale
annotated clinical text datasets. The lack of such publicly available datasets
is the biggest bottleneck for the development of clinical Natural Language
Processing(NLP) systems. Zero-Shot Learning(ZSL) refers to the use of deep
learning models to classify instances from new classes of which no training
data have been seen before. Prompt-based learning is an emerging ZSL technique
where we define task-based templates for NLP tasks. We developed a novel
prompt-based clinical NLP framework called HealthPrompt and applied the
paradigm of prompt-based learning on clinical texts. In this technique, rather
than fine-tuning a Pre-trained Language Model(PLM), the task definitions are
tuned by defining a prompt template. We performed an in-depth analysis of
HealthPrompt on six different PLMs in a no-data setting. Our experiments prove
that prompts effectively capture the context of clinical texts and perform
remarkably well without any training data.
",2022-03-09T21:44:28Z,http://arxiv.org/abs/2203.05061v1,"Sonish Sivarajkumar, Yanshan Wang"
Text classification dataset and analysis for Uzbek language,"  Text classification is an important task in Natural Language Processing
(NLP), where the goal is to categorize text data into predefined classes. In
this study, we analyse the dataset creation steps and evaluation techniques of
multi-label news categorisation task as part of text classification. We first
present a newly obtained dataset for Uzbek text classification, which was
collected from 10 different news and press websites and covers 15 categories of
news, press and law texts. We also present a comprehensive evaluation of
different models, ranging from traditional bag-of-words models to deep learning
architectures, on this newly created dataset. Our experiments show that the
Recurrent Neural Network (RNN) and Convolutional Neural Network (CNN) based
models outperform the rule-based models. The best performance is achieved by
the BERTbek model, which is a transformer-based BERT model trained on the Uzbek
corpus. Our findings provide a good baseline for further research in Uzbek text
classification.
",2023-02-28T11:21:24Z,http://arxiv.org/abs/2302.14494v1,"Elmurod Kuriyozov, Ulugbek Salaev, Sanatbek Matlatipov, Gayrat Matlatipov"
"ArNLI: Arabic Natural Language Inference for Entailment and
  Contradiction Detection","  Natural Language Inference (NLI) is a hot topic research in natural language
processing, contradiction detection between sentences is a special case of NLI.
This is considered a difficult NLP task which has a big influence when added as
a component in many NLP applications, such as Question Answering Systems, text
Summarization. Arabic Language is one of the most challenging low-resources
languages in detecting contradictions due to its rich lexical, semantics
ambiguity. We have created a data set of more than 12k sentences and named
ArNLI, that will be publicly available. Moreover, we have applied a new model
inspired by Stanford contradiction detection proposed solutions on English
language. We proposed an approach to detect contradictions between pairs of
sentences in Arabic language using contradiction vector combined with language
model vector as an input to machine learning model. We analyzed results of
different traditional machine learning classifiers and compared their results
on our created data set (ArNLI) and on an automatic translation of both PHEME,
SICK English data sets. Best results achieved using Random Forest classifier
with an accuracy of 99%, 60%, 75% on PHEME, SICK and ArNLI respectively.
",2022-09-28T09:37:16Z,http://arxiv.org/abs/2209.13953v1,"Khloud Al Jallad, Nada Ghneim"
"Data Science Kitchen at GermEval 2021: A Fine Selection of Hand-Picked
  Features, Delivered Fresh from the Oven","  This paper presents the contribution of the Data Science Kitchen at GermEval
2021 shared task on the identification of toxic, engaging, and fact-claiming
comments. The task aims at extending the identification of offensive language,
by including additional subtasks that identify comments which should be
prioritized for fact-checking by moderators and community managers. Our
contribution focuses on a feature-engineering approach with a conventional
classification backend. We combine semantic and writing style embeddings
derived from pre-trained deep neural networks with additional numerical
features, specifically designed for this task. Classifier ensembles are used to
derive predictions for each subtask via a majority voting scheme. Our best
submission achieved macro-averaged F1-scores of 66.8\%,\,69.9\% and 72.5\% for
the identification of toxic, engaging, and fact-claiming comments.
",2021-09-06T12:00:29Z,http://arxiv.org/abs/2109.02383v2,"Niclas Hildebrandt, Benedikt Boenninghoff, Dennis Orth, Christopher Schymura"
"Learning beyond datasets: Knowledge Graph Augmented Neural Networks for
  Natural language Processing","  Machine Learning has been the quintessential solution for many AI problems,
but learning is still heavily dependent on the specific training data. Some
learning models can be incorporated with a prior knowledge in the Bayesian set
up, but these learning models do not have the ability to access any organised
world knowledge on demand. In this work, we propose to enhance learning models
with world knowledge in the form of Knowledge Graph (KG) fact triples for
Natural Language Processing (NLP) tasks. Our aim is to develop a deep learning
model that can extract relevant prior support facts from knowledge graphs
depending on the task using attention mechanism. We introduce a
convolution-based model for learning representations of knowledge graph entity
and relation clusters in order to reduce the attention space. We show that the
proposed method is highly scalable to the amount of prior information that has
to be processed and can be applied to any generic NLP task. Using this method
we show significant improvement in performance for text classification with
News20, DBPedia datasets and natural language inference with Stanford Natural
Language Inference (SNLI) dataset. We also demonstrate that a deep learning
model can be trained well with substantially less amount of labeled training
data, when it has access to organised world knowledge in the form of knowledge
graph.
",2018-02-16T13:38:00Z,http://arxiv.org/abs/1802.05930v2,"K M Annervaz, Somnath Basu Roy Chowdhury, Ambedkar Dukkipati"
"Review and Prospect: Deep Learning in Nuclear Magnetic Resonance
  Spectroscopy","  Since the concept of Deep Learning (DL) was formally proposed in 2006, it had
a major impact on academic research and industry. Nowadays, DL provides an
unprecedented way to analyze and process data with demonstrated great results
in computer vision, medical imaging, natural language processing, etc. In this
Minireview, we summarize applications of DL in Nuclear Magnetic Resonance (NMR)
spectroscopy and outline a perspective for DL as entirely new approaches that
are likely to transform NMR spectroscopy into a much more efficient and
powerful technique in chemistry and life science.
",2020-01-13T05:13:20Z,http://arxiv.org/abs/2001.04813v2,"Dicheng Chen, Zi Wang, Di Guo, Vladislav Orekhov, Xiaobo Qu"
"Deep Bayesian Active Learning for Natural Language Processing: Results
  of a Large-Scale Empirical Study","  Several recent papers investigate Active Learning (AL) for mitigating the
data dependence of deep learning for natural language processing. However, the
applicability of AL to real-world problems remains an open question. While in
supervised learning, practitioners can try many different methods, evaluating
each against a validation set before selecting a model, AL affords no such
luxury. Over the course of one AL run, an agent annotates its dataset
exhausting its labeling budget. Thus, given a new task, an active learner has
no opportunity to compare models and acquisition functions. This paper provides
a large scale empirical study of deep active learning, addressing multiple
tasks and, for each, multiple datasets, multiple models, and a full suite of
acquisition functions. We find that across all settings, Bayesian active
learning by disagreement, using uncertainty estimates provided either by
Dropout or Bayes-by Backprop significantly improves over i.i.d. baselines and
usually outperforms classic uncertainty sampling.
",2018-08-16T22:46:40Z,http://arxiv.org/abs/1808.05697v3,"Aditya Siddhant, Zachary C. Lipton"
Generalization Error in Deep Learning,"  Deep learning models have lately shown great performance in various fields
such as computer vision, speech recognition, speech translation, and natural
language processing. However, alongside their state-of-the-art performance, it
is still generally unclear what is the source of their generalization ability.
Thus, an important question is what makes deep neural networks able to
generalize well from the training set to new data. In this article, we provide
an overview of the existing theory and bounds for the characterization of the
generalization error of deep neural networks, combining both classical and more
recent theoretical and empirical results.
",2018-08-03T12:57:12Z,http://arxiv.org/abs/1808.01174v3,"Daniel Jakubovitz, Raja Giryes, Miguel R. D. Rodrigues"
"An exact mapping between the Variational Renormalization Group and Deep
  Learning","  Deep learning is a broad set of techniques that uses multiple layers of
representation to automatically learn relevant features directly from
structured data. Recently, such techniques have yielded record-breaking results
on a diverse set of difficult machine learning tasks in computer vision, speech
recognition, and natural language processing. Despite the enormous success of
deep learning, relatively little is understood theoretically about why these
techniques are so successful at feature learning and compression. Here, we show
that deep learning is intimately related to one of the most important and
successful techniques in theoretical physics, the renormalization group (RG).
RG is an iterative coarse-graining scheme that allows for the extraction of
relevant features (i.e. operators) as a physical system is examined at
different length scales. We construct an exact mapping from the variational
renormalization group, first introduced by Kadanoff, and deep learning
architectures based on Restricted Boltzmann Machines (RBMs). We illustrate
these ideas using the nearest-neighbor Ising Model in one and two-dimensions.
Our results suggests that deep learning algorithms may be employing a
generalized RG-like scheme to learn relevant features from data.
",2014-10-14T20:00:09Z,http://arxiv.org/abs/1410.3831v1,"Pankaj Mehta, David J. Schwab"
"Attention versus Contrastive Learning of Tabular Data -- A Data-centric
  Benchmarking","  Despite groundbreaking success in image and text learning, deep learning has
not achieved significant improvements against traditional machine learning (ML)
when it comes to tabular data. This performance gap underscores the need for
data-centric treatment and benchmarking of learning algorithms. Recently,
attention and contrastive learning breakthroughs have shifted computer vision
and natural language processing paradigms. However, the effectiveness of these
advanced deep models on tabular data is sparsely studied using a few data sets
with very large sample sizes, reporting mixed findings after benchmarking
against a limited number of baselines. We argue that the heterogeneity of
tabular data sets and selective baselines in the literature can bias the
benchmarking outcomes. This article extensively evaluates state-of-the-art
attention and contrastive learning methods on a wide selection of 28 tabular
data sets (14 easy and 14 hard-to-classify) against traditional deep and
machine learning. Our data-centric benchmarking demonstrates when traditional
ML is preferred over deep learning and vice versa because no best learning
method exists for all tabular data sets. Combining between-sample and
between-feature attentions conquers the invincible traditional ML on tabular
data sets by a significant margin but fails on high dimensional data, where
contrastive learning takes a robust lead. While a hybrid attention-contrastive
learning strategy mostly wins on hard-to-classify data sets, traditional
methods are frequently superior on easy-to-classify data sets with presumably
simpler decision boundaries. To the best of our knowledge, this is the first
benchmarking paper with statistical analyses of attention and contrastive
learning performances on a diverse selection of tabular data sets against
traditional deep and machine learning baselines to facilitate further advances
in this field.
",2024-01-08T22:36:05Z,http://arxiv.org/abs/2401.04266v1,"Shourav B. Rabbani, Ivan V. Medri, Manar D. Samad"
Deep Natural Language Processing for LinkedIn Search Systems,"  Many search systems work with large amounts of natural language data, e.g.,
search queries, user profiles and documents, where deep learning based natural
language processing techniques (deep NLP) can be of great help. In this paper,
we introduce a comprehensive study of applying deep NLP techniques to five
representative tasks in search engines. Through the model design and
experiments of the five tasks, readers can find answers to three important
questions: (1) When is deep NLP helpful/not helpful in search systems? (2) How
to address latency challenges? (3) How to ensure model robustness? This work
builds on existing efforts of LinkedIn search, and is tested at scale on a
commercial search engine. We believe our experiences can provide useful
insights for the industry and research communities.
",2021-07-30T17:40:36Z,http://arxiv.org/abs/2108.08252v1,"Weiwei Guo, Xiaowei Liu, Sida Wang, Michaeel Kazi, Zhoutong Fu, Huiji Gao, Jun Jia, Liang Zhang, Bo Long"
Contextual Text Denoising with Masked Language Models,"  Recently, with the help of deep learning models, significant advances have
been made in different Natural Language Processing (NLP) tasks. Unfortunately,
state-of-the-art models are vulnerable to noisy texts. We propose a new
contextual text denoising algorithm based on the ready-to-use masked language
model. The proposed algorithm does not require retraining of the model and can
be integrated into any NLP system without additional training on paired
cleaning training data. We evaluate our method under synthetic noise and
natural noise and show that the proposed algorithm can use context information
to correct noise text and improve the performance of noisy inputs in several
downstream tasks.
",2019-10-30T18:47:37Z,http://arxiv.org/abs/1910.14080v2,"Yifu Sun, Haoming Jiang"
"BERT: A Review of Applications in Natural Language Processing and
  Understanding","  In this review, we describe the application of one of the most popular deep
learning-based language models - BERT. The paper describes the mechanism of
operation of this model, the main areas of its application to the tasks of text
analytics, comparisons with similar models in each task, as well as a
description of some proprietary models. In preparing this review, the data of
several dozen original scientific articles published over the past few years,
which attracted the most attention in the scientific community, were
systematized. This survey will be useful to all students and researchers who
want to get acquainted with the latest advances in the field of natural
language text analysis.
",2021-03-22T15:34:39Z,http://arxiv.org/abs/2103.11943v1,M. V. Koroteev
Deep learning systems as complex networks,"  Thanks to the availability of large scale digital datasets and massive
amounts of computational power, deep learning algorithms can learn
representations of data by exploiting multiple levels of abstraction. These
machine learning methods have greatly improved the state-of-the-art in many
challenging cognitive tasks, such as visual object recognition, speech
processing, natural language understanding and automatic translation. In
particular, one class of deep learning models, known as deep belief networks,
can discover intricate statistical structure in large data sets in a completely
unsupervised fashion, by learning a generative model of the data using
Hebbian-like learning mechanisms. Although these self-organizing systems can be
conveniently formalized within the framework of statistical mechanics, their
internal functioning remains opaque, because their emergent dynamics cannot be
solved analytically. In this article we propose to study deep belief networks
using techniques commonly employed in the study of complex networks, in order
to gain some insights into the structural and functional properties of the
computational graph resulting from the learning process.
",2018-09-28T10:06:36Z,http://arxiv.org/abs/1809.10941v1,"Alberto Testolin, Michele Piccolini, Samir Suweis"
"A Hybrid Approach to Dependency Parsing: Combining Rules and Morphology
  with Deep Learning","  Fully data-driven, deep learning-based models are usually designed as
language-independent and have been shown to be successful for many natural
language processing tasks. However, when the studied language is low-resourced
and the amount of training data is insufficient, these models can benefit from
the integration of natural language grammar-based information. We propose two
approaches to dependency parsing especially for languages with restricted
amount of training data. Our first approach combines a state-of-the-art deep
learning-based parser with a rule-based approach and the second one
incorporates morphological information into the parser. In the rule-based
approach, the parsing decisions made by the rules are encoded and concatenated
with the vector representations of the input words as additional information to
the deep network. The morphology-based approach proposes different methods to
include the morphological structure of words into the parser network.
Experiments are conducted on the IMST-UD Treebank and the results suggest that
integration of explicit knowledge about the target language to a neural parser
through a rule-based parsing system and morphological analysis leads to more
accurate annotations and hence, increases the parsing performance in terms of
attachment scores. The proposed methods are developed for Turkish, but can be
adapted to other languages as well.
",2020-02-24T08:34:33Z,http://arxiv.org/abs/2002.10116v1,"Şaziye Betül Özateş, Arzucan Özgür, Tunga Güngör, Balkız Öztürk"
"Exploring transfer learning for Deep NLP systems on rarely annotated
  languages","  Natural language processing (NLP) has experienced rapid advancements with the
rise of deep learning, significantly outperforming traditional rule-based
methods. By capturing hidden patterns and underlying structures within data,
deep learning has improved performance across various NLP tasks, overcoming the
limitations of rule-based systems. However, most research and development in
NLP has been concentrated on a select few languages, primarily those with large
numbers of speakers or financial significance, leaving many others
underexplored. This lack of research is often attributed to the scarcity of
adequately annotated datasets essential for training deep learning models.
Despite this challenge, there is potential in leveraging the linguistic
similarities between unexplored and well-studied languages, particularly those
in close geographic and linguistic proximity. This thesis investigates the
application of transfer learning for Part-of-Speech (POS) tagging between Hindi
and Nepali, two highly similar languages belonging to the Indo-Aryan language
family. Specifically, the work explores whether joint training of a POS tagging
model for both languages enhances performance. Additionally, we assess whether
multitask learning in Hindi, with auxiliary tasks such as gender and
singular/plural tagging, can contribute to improved POS tagging accuracy. The
deep learning architecture employed is the BLSTM-CNN-CRF model, trained under
different conditions: monolingual word embeddings, vector-mapped embeddings,
and jointly trained Hindi-Nepali word embeddings. Varying dropout rates (0.25
to 0.5) and optimizers (ADAM and AdaDelta) are also evaluated. Results indicate
that jointly trained Hindi-Nepali word embeddings improve performance across
all models compared to monolingual and vector-mapped embeddings.
",2024-10-15T13:33:54Z,http://arxiv.org/abs/2410.12879v1,"Dipendra Yadav, Tobias Strauß, Kristina Yordanova"
"SBNet: Segmentation-based Network for Natural Language-based Vehicle
  Search","  Natural language-based vehicle retrieval is a task to find a target vehicle
within a given image based on a natural language description as a query. This
technology can be applied to various areas including police searching for a
suspect vehicle. However, it is challenging due to the ambiguity of language
descriptions and the difficulty of processing multi-modal data. To tackle this
problem, we propose a deep neural network called SBNet that performs natural
language-based segmentation for vehicle retrieval. We also propose two
task-specific modules to improve performance: a substitution module that helps
features from different domains to be embedded in the same space and a future
prediction module that learns temporal information. SBnet has been trained
using the CityFlow-NL dataset that contains 2,498 tracks of vehicles with three
unique natural language descriptions each and tested 530 unique vehicle tracks
and their corresponding query sets. SBNet achieved a significant improvement
over the baseline in the natural language-based vehicle tracking track in the
AI City Challenge 2021.
",2021-04-22T08:06:17Z,http://arxiv.org/abs/2104.11589v1,"Sangrok Lee, Taekang Woo, Sang Hun Lee"
"Lifelong Learning Natural Language Processing Approach for Multilingual
  Data Classification","  The abundance of information in digital media, which in today's world is the
main source of knowledge about current events for the masses, makes it possible
to spread disinformation on a larger scale than ever before. Consequently,
there is a need to develop novel fake news detection approaches capable of
adapting to changing factual contexts and generalizing previously or
concurrently acquired knowledge. To deal with this problem, we propose a
lifelong learning-inspired approach, which allows for fake news detection in
multiple languages and the mutual transfer of knowledge acquired in each of
them. Both classical feature extractors, such as Term frequency-inverse
document frequency or Latent Dirichlet Allocation, and integrated deep NLP
(Natural Language Processing) BERT (Bidirectional Encoder Representations from
Transformers) models paired with MLP (Multilayer Perceptron) classifier, were
employed. The results of experiments conducted on two datasets dedicated to the
fake news classification task (in English and Spanish, respectively), supported
by statistical analysis, confirmed that utilization of additional languages
could improve performance for traditional methods. Also, in some cases
supplementing the deep learning method with classical ones can positively
impact obtained results. The ability of models to generalize the knowledge
acquired between the analyzed languages was also observed.
",2022-05-25T10:34:04Z,http://arxiv.org/abs/2206.11867v1,"Jędrzej Kozal, Michał Leś, Paweł Zyblewski, Paweł Ksieniewicz, Michał Woźniak"
Review of Text Style Transfer Based on Deep Learning,"  Text style transfer is a hot issue in recent natural language
processing,which mainly studies the text to adapt to different specific
situations, audiences and purposes by making some changes. The style of the
text usually includes many aspects such as morphology, grammar, emotion,
complexity, fluency, tense, tone and so on. In the traditional text style
transfer model, the text style is generally relied on by experts knowledge and
hand-designed rules, but with the application of deep learning in the field of
natural language processing, the text style transfer method based on deep
learning Started to be heavily researched. In recent years, text style transfer
is becoming a hot issue in natural language processing research. This article
summarizes the research on the text style transfer model based on deep learning
in recent years, and summarizes, analyzes and compares the main research
directions and progress. In addition, the article also introduces public data
sets and evaluation indicators commonly used for text style transfer. Finally,
the existing characteristics of the text style transfer model are summarized,
and the future development trend of the text style transfer model based on deep
learning is analyzed and forecasted.
",2020-05-06T15:35:53Z,http://arxiv.org/abs/2005.02914v3,"Xiangyang Li, Guo Pu, Keyu Ming, Pu Li, Jie Wang, Yuxuan Wang"
"Explaining the Deep Natural Language Processing by Mining Textual
  Interpretable Features","  Despite the high accuracy offered by state-of-the-art deep natural-language
models (e.g. LSTM, BERT), their application in real-life settings is still
widely limited, as they behave like a black-box to the end-user. Hence,
explainability is rapidly becoming a fundamental requirement of
future-generation data-driven systems based on deep-learning approaches.
Several attempts to fulfill the existing gap between accuracy and
interpretability have been done. However, robust and specialized xAI
(Explainable Artificial Intelligence) solutions tailored to deep
natural-language models are still missing. We propose a new framework, named
T-EBAnO, which provides innovative prediction-local and class-based
model-global explanation strategies tailored to black-box deep natural-language
models. Given a deep NLP model and the textual input data, T-EBAnO provides an
objective, human-readable, domain-specific assessment of the reasons behind the
automatic decision-making process. Specifically, the framework extracts sets of
interpretable features mining the inner knowledge of the model. Then, it
quantifies the influence of each feature during the prediction process by
exploiting the novel normalized Perturbation Influence Relation index at the
local level and the novel Global Absolute Influence and Global Relative
Influence indexes at the global level. The effectiveness and the quality of the
local and global explanations obtained with T-EBAnO are proved on (i) a
sentiment analysis task performed by a fine-tuned BERT model, and (ii) a toxic
comment classification task performed by an LSTM model.
",2021-06-12T06:25:09Z,http://arxiv.org/abs/2106.06697v1,"Francesco Ventura, Salvatore Greco, Daniele Apiletti, Tania Cerquitelli"
A Combined CNN and LSTM Model for Arabic Sentiment Analysis,"  Deep neural networks have shown good data modelling capabilities when dealing
with challenging and large datasets from a wide range of application areas.
Convolutional Neural Networks (CNNs) offer advantages in selecting good
features and Long Short-Term Memory (LSTM) networks have proven good abilities
of learning sequential data. Both approaches have been reported to provide
improved results in areas such image processing, voice recognition, language
translation and other Natural Language Processing (NLP) tasks. Sentiment
classification for short text messages from Twitter is a challenging task, and
the complexity increases for Arabic language sentiment classification tasks
because Arabic is a rich language in morphology. In addition, the availability
of accurate pre-processing tools for Arabic is another current limitation,
along with limited research available in this area. In this paper, we
investigate the benefits of integrating CNNs and LSTMs and report obtained
improved accuracy for Arabic sentiment analysis on different datasets.
Additionally, we seek to consider the morphological diversity of particular
Arabic words by using different sentiment classification levels.
",2018-07-09T01:41:20Z,http://arxiv.org/abs/1807.02911v3,"Abdulaziz M. Alayba, Vasile Palade, Matthew England, Rahat Iqbal"
A Deep Learning Approach to Analyzing Continuous-Time Systems,"  Scientists often use observational time series data to study complex natural
processes, but regression analyses often assume simplistic dynamics. Recent
advances in deep learning have yielded startling improvements to the
performance of models of complex processes, but deep learning is generally not
used for scientific analysis. Here we show that deep learning can be used to
analyze complex processes, providing flexible function approximation while
preserving interpretability. Our approach relaxes standard simplifying
assumptions (e.g., linearity, stationarity, and homoscedasticity) that are
implausible for many natural systems and may critically affect the
interpretation of data. We evaluate our model on incremental human language
processing, a domain with complex continuous dynamics. We demonstrate
substantial improvements on behavioral and neuroimaging data, and we show that
our model enables discovery of novel patterns in exploratory analyses, controls
for diverse confounds in confirmatory analyses, and opens up research questions
that are otherwise hard to study.
",2022-09-25T03:02:31Z,http://arxiv.org/abs/2209.12128v2,"Cory Shain, William Schuler"
Urdu Poetry Generated by Using Deep Learning Techniques,"  This study provides Urdu poetry generated using different deep-learning
techniques and algorithms. The data was collected through the Rekhta website,
containing 1341 text files with several couplets. The data on poetry was not
from any specific genre or poet. Instead, it was a collection of mixed Urdu
poems and Ghazals. Different deep learning techniques, such as the model
applied Long Short-term Memory Networks (LSTM) and Gated Recurrent Unit (GRU),
have been used. Natural Language Processing (NLP) may be used in machine
learning to understand, analyze, and generate a language humans may use and
understand. Much work has been done on generating poetry for different
languages using different techniques. The collection and use of data were also
different for different researchers. The primary purpose of this project is to
provide a model that generates Urdu poems by using data completely, not by
sampling data. Also, this may generate poems in pure Urdu, not Roman Urdu, as
in the base paper. The results have shown good accuracy in the poems generated
by the model.
",2023-09-25T15:44:24Z,http://arxiv.org/abs/2309.14233v1,"Muhammad Shoaib Farooq, Ali Abbas"
"Research on color recipe recommendation based on unstructured data using
  TENN","  Recently, services and business models based on large language models, such
as OpenAI Chatgpt, Google BARD, and Microsoft copilot, have been introduced,
and the applications utilizing natural language processing with deep learning
are increasing, and it is one of the natural language preprocessing methods.
Conversion to machine language through tokenization and processing of
unstructured data are increasing. Although algorithms that can understand and
apply human language are becoming increasingly sophisticated, it is difficult
to apply them to processes that rely on human emotions and senses in industries
that still mainly deal with standardized data. In particular, in processes
where brightness, saturation, and color information are essential, such as
painting and injection molding, most small and medium-sized companies,
excluding large corporations, rely on the tacit knowledge and sensibility of
color mixers, and even customer companies often present non-standardized
requirements. . In this paper, we proposed TENN to infer color recipe based on
unstructured data with emotional natural language, and demonstrated it.
",2024-08-17T04:45:48Z,http://arxiv.org/abs/2408.09094v1,"Seongsu Jhang, Donghwi Yoo, Jaeyong Kown"
"A Survey on Recent Approaches for Natural Language Processing in
  Low-Resource Scenarios","  Deep neural networks and huge language models are becoming omnipresent in
natural language applications. As they are known for requiring large amounts of
training data, there is a growing body of work to improve the performance in
low-resource settings. Motivated by the recent fundamental changes towards
neural models and the popular pre-train and fine-tune paradigm, we survey
promising approaches for low-resource natural language processing. After a
discussion about the different dimensions of data availability, we give a
structured overview of methods that enable learning when training data is
sparse. This includes mechanisms to create additional labeled data like data
augmentation and distant supervision as well as transfer learning settings that
reduce the need for target supervision. A goal of our survey is to explain how
these methods differ in their requirements as understanding them is essential
for choosing a technique suited for a specific low-resource setting. Further
key aspects of this work are to highlight open issues and to outline promising
directions for future research.
",2020-10-23T11:22:01Z,http://arxiv.org/abs/2010.12309v3,"Michael A. Hedderich, Lukas Lange, Heike Adel, Jannik Strötgen, Dietrich Klakow"
A Survey of Deep Learning for Mathematical Reasoning,"  Mathematical reasoning is a fundamental aspect of human intelligence and is
applicable in various fields, including science, engineering, finance, and
everyday life. The development of artificial intelligence (AI) systems capable
of solving math problems and proving theorems has garnered significant interest
in the fields of machine learning and natural language processing. For example,
mathematics serves as a testbed for aspects of reasoning that are challenging
for powerful deep learning models, driving new algorithmic and modeling
advances. On the other hand, recent advances in large-scale neural language
models have opened up new benchmarks and opportunities to use deep learning for
mathematical reasoning. In this survey paper, we review the key tasks,
datasets, and methods at the intersection of mathematical reasoning and deep
learning over the past decade. We also evaluate existing benchmarks and
methods, and discuss future research directions in this domain.
",2022-12-20T18:46:16Z,http://arxiv.org/abs/2212.10535v2,"Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, Kai-Wei Chang"
Macromolecule Classification Based on the Amino-acid Sequence,"  Deep learning is playing a vital role in every field which involves data. It
has emerged as a strong and efficient framework that can be applied to a broad
spectrum of complex learning problems which were difficult to solve using
traditional machine learning techniques in the past. In this study we focused
on classification of protein sequences with deep learning techniques. The study
of amino acid sequence is vital in life sciences. We used different word
embedding techniques from Natural Language processing to represent the amino
acid sequence as vectors. Our main goal was to classify sequences to four group
of classes, that are DNA, RNA, Protein and hybrid. After several tests we have
achieved almost 99% of train and test accuracy. We have experimented on CNN,
LSTM, Bidirectional LSTM, and GRU.
",2020-01-06T08:33:50Z,http://arxiv.org/abs/2001.01717v2,"Faisal Ghaffar, Sarwar Khan, Gaddisa O., Chen Yu-jhen"
Sketch-based Creativity Support Tools using Deep Learning,"  Sketching is a natural and effective visual communication medium commonly
used in creative processes. Recent developments in deep-learning models
drastically improved machines' ability in understanding and generating visual
content. An exciting area of development explores deep-learning approaches used
to model human sketches, opening opportunities for creative applications. This
chapter describes three fundamental steps in developing deep-learning-driven
creativity support tools that consumes and generates sketches: 1) a data
collection effort that generated a new paired dataset between sketches and
mobile user interfaces; 2) a sketch-based user interface retrieval system
adapted from state-of-the-art computer vision techniques; and, 3) a
conversational sketching system that supports the novel interaction of a
natural-language-based sketch/critique authoring process. In this chapter, we
survey relevant prior work in both the deep-learning and
human-computer-interaction communities, document the data collection process
and the systems' architectures in detail, present qualitative and quantitative
results, and paint the landscape of several future research directions in this
exciting area.
",2021-11-19T00:57:43Z,http://arxiv.org/abs/2111.09991v1,"Forrest Huang, Eldon Schoop, David Ha, Jeffrey Nichols, John Canny"
"A Review of the Applications of Deep Learning-Based Emergent
  Communication","  Emergent communication, or emergent language, is the field of research which
studies how human language-like communication systems emerge de novo in deep
multi-agent reinforcement learning environments. The possibilities of
replicating the emergence of a complex behavior like language have strong
intuitive appeal, yet it is necessary to complement this with clear notions of
how such research can be applicable to other fields of science, technology, and
engineering. This paper comprehensively reviews the applications of emergent
communication research across machine learning, natural language processing,
linguistics, and cognitive science. Each application is illustrated with a
description of its scope, an explication of emergent communication's unique
role in addressing it, a summary of the extant literature working towards the
application, and brief recommendations for near-term research directions.
",2024-07-03T17:43:54Z,http://arxiv.org/abs/2407.03302v1,"Brendon Boldt, David Mortensen"
"Nonlinear functional regression by functional deep neural network with
  kernel embedding","  With the rapid development of deep learning in various fields of science and
technology, such as speech recognition, image classification, and natural
language processing, recently it is also widely applied in the functional data
analysis (FDA) with some empirical success. However, due to the infinite
dimensional input, we need a powerful dimension reduction method for functional
learning tasks, especially for the nonlinear functional regression. In this
paper, based on the idea of smooth kernel integral transformation, we propose a
functional deep neural network with an efficient and fully data-dependent
dimension reduction method. The architecture of our functional net consists of
a kernel embedding step: an integral transformation with a data-dependent
smooth kernel; a projection step: a dimension reduction by projection with
eigenfunction basis based on the embedding kernel; and finally an expressive
deep ReLU neural network for the prediction. The utilization of smooth kernel
embedding enables our functional net to be discretization invariant, efficient,
and robust to noisy observations, capable of utilizing information in both
input functions and responses data, and have a low requirement on the number of
discrete points for an unimpaired generalization performance. We conduct
theoretical analysis including approximation error and generalization error
analysis, and numerical simulations to verify these advantages of our
functional net.
",2024-01-05T16:43:39Z,http://arxiv.org/abs/2401.02890v1,"Zhongjie Shi, Jun Fan, Linhao Song, Ding-Xuan Zhou, Johan A. K. Suykens"
Deep Lake: a Lakehouse for Deep Learning,"  Traditional data lakes provide critical data infrastructure for analytical
workloads by enabling time travel, running SQL queries, ingesting data with
ACID transactions, and visualizing petabyte-scale datasets on cloud storage.
They allow organizations to break down data silos, unlock data-driven
decision-making, improve operational efficiency, and reduce costs. However, as
deep learning usage increases, traditional data lakes are not well-designed for
applications such as natural language processing (NLP), audio processing,
computer vision, and applications involving non-tabular datasets. This paper
presents Deep Lake, an open-source lakehouse for deep learning applications
developed at Activeloop. Deep Lake maintains the benefits of a vanilla data
lake with one key difference: it stores complex data, such as images, videos,
annotations, as well as tabular data, in the form of tensors and rapidly
streams the data over the network to (a) Tensor Query Language, (b) in-browser
visualization engine, or (c) deep learning frameworks without sacrificing GPU
utilization. Datasets stored in Deep Lake can be accessed from PyTorch,
TensorFlow, JAX, and integrate with numerous MLOps tools.
",2022-09-22T05:04:09Z,http://arxiv.org/abs/2209.10785v2,"Sasun Hambardzumyan, Abhinav Tuli, Levon Ghukasyan, Fariz Rahman, Hrant Topchyan, David Isayan, Mark McQuade, Mikayel Harutyunyan, Tatevik Hakobyan, Ivo Stranic, Davit Buniatyan"
"A Survey on Deep Reinforcement Learning for Data Processing and
  Analytics","  Data processing and analytics are fundamental and pervasive. Algorithms play
a vital role in data processing and analytics where many algorithm designs have
incorporated heuristics and general rules from human knowledge and experience
to improve their effectiveness. Recently, reinforcement learning, deep
reinforcement learning (DRL) in particular, is increasingly explored and
exploited in many areas because it can learn better strategies in complicated
environments it is interacting with than statically designed algorithms.
Motivated by this trend, we provide a comprehensive review of recent works
focusing on utilizing DRL to improve data processing and analytics. First, we
present an introduction to key concepts, theories, and methods in DRL. Next, we
discuss DRL deployment on database systems, facilitating data processing and
analytics in various aspects, including data organization, scheduling, tuning,
and indexing. Then, we survey the application of DRL in data processing and
analytics, ranging from data preparation, natural language processing to
healthcare, fintech, etc. Finally, we discuss important open challenges and
future research directions of using DRL in data processing and analytics.
",2021-08-10T09:14:03Z,http://arxiv.org/abs/2108.04526v3,"Qingpeng Cai, Can Cui, Yiyuan Xiong, Wei Wang, Zhongle Xie, Meihui Zhang"
"Case Studies on using Natural Language Processing Techniques in Customer
  Relationship Management Software","  How can a text corpus stored in a customer relationship management (CRM)
database be used for data mining and segmentation? In order to answer this
question we inherited the state of the art methods commonly used in natural
language processing (NLP) literature, such as word embeddings, and deep
learning literature, such as recurrent neural networks (RNN). We used the text
notes from a CRM system which are taken by customer representatives of an
internet ads consultancy agency between years 2009 and 2020. We trained word
embeddings by using the corresponding text corpus and showed that these word
embeddings can not only be used directly for data mining but also be used in
RNN architectures, which are deep learning frameworks built with long short
term memory (LSTM) units, for more comprehensive segmentation objectives. The
results prove that structured text data in a CRM can be used to mine out very
valuable information and any CRM can be equipped with useful NLP features once
the problem definitions are properly built and the solution methods are
conveniently implemented.
",2021-06-09T16:07:07Z,http://arxiv.org/abs/2106.05160v1,Şükrü Ozan
"From the digital data revolution to digital health and digital economy
  toward a digital society: Pervasiveness of Artificial Intelligence","  Technological progress has led to powerful computers and communication
technologies that penetrate nowadays all areas of science, industry and our
private lives. As a consequence, all these areas are generating digital traces
of data amounting to big data resources. This opens unprecedented opportunities
but also challenges toward the analysis, management, interpretation and
utilization of these data. Fortunately, recent breakthroughs in deep learning
algorithms complement now machine learning and statistics methods for an
efficient analysis of such data. Furthermore, advances in text mining and
natural language processing, e.g., word-embedding methods, enable also the
processing of large amounts of text data from diverse sources as governmental
reports, blog entries in social media or clinical health records of patients.
In this paper, we present a perspective on the role of artificial intelligence
in these developments and discuss also potential problems we are facing in a
digital society.
",2020-08-03T17:15:18Z,http://arxiv.org/abs/2008.12672v2,Frank Emmert-Streib
"What Averages Do Not Tell -- Predicting Real Life Processes with
  Sequential Deep Learning","  Deep Learning is proven to be an effective tool for modeling sequential data
as shown by the success in Natural Language, Computer Vision and Signal
Processing. Process Mining concerns discovering insights on business processes
from their execution data that are logged by supporting information systems.
The logged data (event log) is formed of event sequences (traces) that
correspond to executions of a process. Many Deep Learning techniques have been
successfully adapted for predictive Process Mining that aims to predict process
outcomes, remaining time, the next event, or even the suffix of running traces.
Traces in Process Mining are multimodal sequences and very differently
structured than natural language sentences or images. This may require a
different approach to processing. So far, there has been little focus on these
differences and the challenges introduced. Looking at suffix prediction as the
most challenging of these tasks, the performance of Deep Learning models was
evaluated only on average measures and for a small number of real-life event
logs. Comparing the results between papers is difficult due to different
pre-processing and evaluation strategies. Challenges that may be relevant are
the skewness of trace-length distribution and the skewness of the activity
distribution in real-life event logs. We provide an end-to-end framework which
enables to compare the performance of seven state-of-the-art sequential
architectures in common settings. Results show that sequence modeling still has
a lot of room for improvement for majority of the more complex datasets.
Further research and insights are required to get consistent performance not
just in average measures but additionally over all the prefixes.
",2021-10-19T19:45:05Z,http://arxiv.org/abs/2110.10225v2,"István Ketykó, Felix Mannhardt, Marwan Hassani, Boudewijn van Dongen"
Phonology Recognition in American Sign Language,"  Inspired by recent developments in natural language processing, we propose a
novel approach to sign language processing based on phonological properties
validated by American Sign Language users. By taking advantage of datasets
composed of phonological data and people speaking sign language, we use a
pretrained deep model based on mesh reconstruction to extract the 3D
coordinates of the signers keypoints. Then, we train standard statistical and
deep machine learning models in order to assign phonological classes to each
temporal sequence of coordinates.
  Our paper introduces the idea of exploiting the phonological properties
manually assigned by sign language users to classify videos of people
performing signs by regressing a 3D mesh. We establish a new baseline for this
problem based on the statistical distribution of 725 different signs. Our
best-performing models achieve a micro-averaged F1-score of 58% for the major
location class and 70% for the sign type using statistical and deep learning
algorithms, compared to their corresponding baselines of 35% and 39%.
",2021-10-01T14:38:47Z,http://arxiv.org/abs/2110.00453v1,"Federico Tavella, Aphrodite Galata, Angelo Cangelosi"
Transfer Learning Between Related Tasks Using Expected Label Proportions,"  Deep learning systems thrive on abundance of labeled training data but such
data is not always available, calling for alternative methods of supervision.
One such method is expectation regularization (XR) (Mann and McCallum, 2007),
where models are trained based on expected label proportions. We propose a
novel application of the XR framework for transfer learning between related
tasks, where knowing the labels of task A provides an estimation of the label
proportion of task B. We then use a model trained for A to label a large
corpus, and use this corpus with an XR loss to train a model for task B. To
make the XR framework applicable to large-scale deep-learning setups, we
propose a stochastic batched approximation procedure. We demonstrate the
approach on the task of Aspect-based Sentiment classification, where we
effectively use a sentence-level sentiment predictor to train accurate
aspect-based predictor. The method improves upon fully supervised neural system
trained on aspect-level data, and is also cumulative with LM-based pretraining,
as we demonstrate by improving a BERT-based Aspect-based Sentiment model.
",2019-09-01T17:11:35Z,http://arxiv.org/abs/1909.00430v1,"Matan Ben Noach, Yoav Goldberg"
DeText: A Deep Text Ranking Framework with BERT,"  Ranking is the most important component in a search system. Mostsearch
systems deal with large amounts of natural language data,hence an effective
ranking system requires a deep understandingof text semantics. Recently, deep
learning based natural languageprocessing (deep NLP) models have generated
promising results onranking systems. BERT is one of the most successful models
thatlearn contextual embedding, which has been applied to capturecomplex
query-document relations for search ranking. However,this is generally done by
exhaustively interacting each query wordwith each document word, which is
inefficient for online servingin search product systems. In this paper, we
investigate how tobuild an efficient BERT-based ranking model for industry use
cases.The solution is further extended to a general ranking framework,DeText,
that is open sourced and can be applied to various rankingproductions. Offline
and online experiments of DeText on threereal-world search systems present
significant improvement overstate-of-the-art approaches.
",2020-08-06T05:12:11Z,http://arxiv.org/abs/2008.02460v1,"Weiwei Guo, Xiaowei Liu, Sida Wang, Huiji Gao, Ananth Sankar, Zimeng Yang, Qi Guo, Liang Zhang, Bo Long, Bee-Chung Chen, Deepak Agarwal"
Meta-Reinforcement Learning via Language Instructions,"  Although deep reinforcement learning has recently been very successful at
learning complex behaviors, it requires a tremendous amount of data to learn a
task. One of the fundamental reasons causing this limitation lies in the nature
of the trial-and-error learning paradigm of reinforcement learning, where the
agent communicates with the environment and progresses in the learning only
relying on the reward signal. This is implicit and rather insufficient to learn
a task well. On the contrary, humans are usually taught new skills via natural
language instructions. Utilizing language instructions for robotic motion
control to improve the adaptability is a recently emerged topic and
challenging. In this paper, we present a meta-RL algorithm that addresses the
challenge of learning skills with language instructions in multiple
manipulation tasks. On the one hand, our algorithm utilizes the language
instructions to shape its interpretation of the task, on the other hand, it
still learns to solve task in a trial-and-error process. We evaluate our
algorithm on the robotic manipulation benchmark (Meta-World) and it
significantly outperforms state-of-the-art methods in terms of training and
testing task success rates. Codes are available at
\url{https://tumi6robot.wixsite.com/million}.
",2022-09-11T19:42:48Z,http://arxiv.org/abs/2209.04924v2,"Zhenshan Bing, Alexander Koch, Xiangtong Yao, Kai Huang, Alois Knoll"
"Privacy-Preserving Serverless Edge Learning with Decentralized Small
  Data","  In the last decade, data-driven algorithms outperformed traditional
optimization-based algorithms in many research areas, such as computer vision,
natural language processing, etc. However, extensive data usages bring a new
challenge or even threat to deep learning algorithms, i.e., privacy-preserving.
Distributed training strategies have recently become a promising approach to
ensure data privacy when training deep models. This paper extends conventional
serverless platforms with serverless edge learning architectures and provides
an efficient distributed training framework from the networking perspective.
This framework dynamically orchestrates available resources among heterogeneous
physical units to efficiently fulfill deep learning objectives. The design
jointly considers learning task requests and underlying infrastructure
heterogeneity, including last-mile transmissions, computation abilities of
mobile devices, edge and cloud computing centers, and devices battery status.
Furthermore, to significantly reduce distributed training overheads,
small-scale data training is proposed by integrating with a general, simple
data classifier. This low-load enhancement can seamlessly work with various
distributed deep models to improve communications and computation efficiencies
during the training phase. Finally, open challenges and future research
directions encourage the research community to develop efficient distributed
deep learning techniques.
",2021-11-29T21:04:49Z,http://arxiv.org/abs/2111.14955v2,"Shih-Chun Lin, Chia-Hung Lin"
"Lessons Learned from a Citizen Science Project for Natural Language
  Processing","  Many Natural Language Processing (NLP) systems use annotated corpora for
training and evaluation. However, labeled data is often costly to obtain and
scaling annotation projects is difficult, which is why annotation tasks are
often outsourced to paid crowdworkers. Citizen Science is an alternative to
crowdsourcing that is relatively unexplored in the context of NLP. To
investigate whether and how well Citizen Science can be applied in this
setting, we conduct an exploratory study into engaging different groups of
volunteers in Citizen Science for NLP by re-annotating parts of a pre-existing
crowdsourced dataset. Our results show that this can yield high-quality
annotations and attract motivated volunteers, but also requires considering
factors such as scalability, participation over time, and legal and ethical
issues. We summarize lessons learned in the form of guidelines and provide our
code and data to aid future work on Citizen Science.
",2023-04-25T14:08:53Z,http://arxiv.org/abs/2304.12836v1,"Jan-Christoph Klie, Ji-Ung Lee, Kevin Stowe, Gözde Gül Şahin, Nafise Sadat Moosavi, Luke Bates, Dominic Petrak, Richard Eckart de Castilho, Iryna Gurevych"
WikiBERT models: deep transfer learning for many languages,"  Deep neural language models such as BERT have enabled substantial recent
advances in many natural language processing tasks. Due to the effort and
computational cost involved in their pre-training, language-specific models are
typically introduced only for a small number of high-resource languages such as
English. While multilingual models covering large numbers of languages are
available, recent work suggests monolingual training can produce better models,
and our understanding of the tradeoffs between mono- and multilingual training
is incomplete. In this paper, we introduce a simple, fully automated pipeline
for creating language-specific BERT models from Wikipedia data and introduce 42
new such models, most for languages up to now lacking dedicated deep neural
language models. We assess the merits of these models using the
state-of-the-art UDify parser on Universal Dependencies data, contrasting
performance with results using the multilingual BERT model. We find that UDify
using WikiBERT models outperforms the parser using mBERT on average, with the
language-specific models showing substantially improved performance for some
languages, yet limited improvement or a decrease in performance for others. We
also present preliminary results as first steps toward an understanding of the
conditions under which language-specific models are most beneficial. All of the
methods and models introduced in this work are available under open licenses
from https://github.com/turkunlp/wikibert.
",2020-06-02T11:57:53Z,http://arxiv.org/abs/2006.01538v1,"Sampo Pyysalo, Jenna Kanerva, Antti Virtanen, Filip Ginter"
A Neural Approach for Detecting Morphological Analogies,"  Analogical proportions are statements of the form ""A is to B as C is to D""
that are used for several reasoning and classification tasks in artificial
intelligence and natural language processing (NLP). For instance, there are
analogy based approaches to semantics as well as to morphology. In fact,
symbolic approaches were developed to solve or to detect analogies between
character strings, e.g., the axiomatic approach as well as that based on
Kolmogorov complexity. In this paper, we propose a deep learning approach to
detect morphological analogies, for instance, with reinflexion or conjugation.
We present empirical results that show that our framework is competitive with
the above-mentioned state of the art symbolic approaches. We also explore
empirically its transferability capacity across languages, which highlights
interesting similarities between them.
",2021-08-09T11:21:55Z,http://arxiv.org/abs/2108.03945v1,"Safa Alsaidi, Amandine Decker, Puthineath Lay, Esteban Marquer, Pierre-Alexandre Murena, Miguel Couceiro"
Meta Learning for Natural Language Processing: A Survey,"  Deep learning has been the mainstream technique in natural language
processing (NLP) area. However, the techniques require many labeled data and
are less generalizable across domains. Meta-learning is an arising field in
machine learning studying approaches to learn better learning algorithms.
Approaches aim at improving algorithms in various aspects, including data
efficiency and generalizability. Efficacy of approaches has been shown in many
NLP tasks, but there is no systematic survey of these approaches in NLP, which
hinders more researchers from joining the field. Our goal with this survey
paper is to offer researchers pointers to relevant meta-learning works in NLP
and attract more attention from the NLP community to drive future innovation.
This paper first introduces the general concepts of meta-learning and the
common approaches. Then we summarize task construction settings and application
of meta-learning for various NLP problems and review the development of
meta-learning in NLP community.
",2022-05-03T13:58:38Z,http://arxiv.org/abs/2205.01500v2,"Hung-yi Lee, Shang-Wen Li, Ngoc Thang Vu"
Towards Fully Bilingual Deep Language Modeling,"  Language models based on deep neural networks have facilitated great advances
in natural language processing and understanding tasks in recent years. While
models covering a large number of languages have been introduced, their
multilinguality has come at a cost in terms of monolingual performance, and the
best-performing models at most tasks not involving cross-lingual transfer
remain monolingual. In this paper, we consider the question of whether it is
possible to pre-train a bilingual model for two remotely related languages
without compromising performance at either language. We collect pre-training
data, create a Finnish-English bilingual BERT model and evaluate its
performance on datasets used to evaluate the corresponding monolingual models.
Our bilingual model performs on par with Google's original English BERT on GLUE
and nearly matches the performance of monolingual Finnish BERT on a range of
Finnish NLP tasks, clearly outperforming multilingual BERT. We find that when
the model vocabulary size is increased, the BERT-Base architecture has
sufficient capacity to learn two remotely related languages to a level where it
achieves comparable performance with monolingual models, demonstrating the
feasibility of training fully bilingual deep language models. The model and all
tools involved in its creation are freely available at
https://github.com/TurkuNLP/biBERT
",2020-10-22T12:22:50Z,http://arxiv.org/abs/2010.11639v1,"Li-Hsin Chang, Sampo Pyysalo, Jenna Kanerva, Filip Ginter"
"Relational Weight Priors in Neural Networks for Abstract Pattern
  Learning and Language Modelling","  Deep neural networks have become the dominant approach in natural language
processing (NLP). However, in recent years, it has become apparent that there
are shortcomings in systematicity that limit the performance and data
efficiency of deep learning in NLP. These shortcomings can be clearly shown in
lower-level artificial tasks, mostly on synthetic data. Abstract patterns are
the best known examples of a hard problem for neural networks in terms of
generalisation to unseen data. They are defined by relations between items,
such as equality, rather than their values. It has been argued that these
low-level problems demonstrate the inability of neural networks to learn
systematically. In this study, we propose Embedded Relation Based Patterns
(ERBP) as a novel way to create a relational inductive bias that encourages
learning equality and distance-based relations for abstract patterns. ERBP is
based on Relation Based Patterns (RBP), but modelled as a Bayesian prior on
network weights and implemented as a regularisation term in otherwise standard
network learning. ERBP is is easy to integrate into standard neural networks
and does not affect their learning capacity. In our experiments, ERBP priors
lead to almost perfect generalisation when learning abstract patterns from
synthetic noise-free sequences. ERBP also improves natural language models on
the word and character level and pitch prediction in melodies with RNN, GRU and
LSTM networks. We also find improvements in in the more complex tasks of
learning of graph edit distance and compositional sentence entailment. ERBP
consistently improves over RBP and over standard networks, showing that it
enables abstract pattern learning which contributes to performance in natural
language tasks.
",2021-03-10T17:21:16Z,http://arxiv.org/abs/2103.06198v1,"Radha Kopparti, Tillman Weyde"
"Off-the-shelf deep learning is not enough: parsimony, Bayes and
  causality","  Deep neural networks (""deep learning"") have emerged as a technology of choice
to tackle problems in natural language processing, computer vision, speech
recognition and gameplay, and in just a few years has led to superhuman level
performance and ushered in a new wave of ""AI."" Buoyed by these successes,
researchers in the physical sciences have made steady progress in incorporating
deep learning into their respective domains. However, such adoption brings
substantial challenges that need to be recognized and confronted. Here, we
discuss both opportunities and roadblocks to implementation of deep learning
within materials science, focusing on the relationship between correlative
nature of machine learning and causal hypothesis driven nature of physical
sciences. We argue that deep learning and AI are now well positioned to
revolutionize fields where causal links are known, as is the case for
applications in theory. When confounding factors are frozen or change only
weakly, this leaves open the pathway for effective deep learning solutions in
experimental domains. Similarly, these methods offer a pathway towards
understanding the physics of real-world systems, either via deriving reduced
representations, deducing algorithmic complexity, or recovering generative
physical models. However, extending deep learning and ""AI"" for models with
unclear causal relationship can produce misleading and potentially incorrect
results. Here, we argue the broad adoption of Bayesian methods incorporating
prior knowledge, development of DL solutions with incorporated physical
constraints, and ultimately adoption of causal models, offers a path forward
for fundamental and applied research. Most notably, while these advances can
change the way science is carried out in ways we cannot imagine, machine
learning is not going to substitute science any time soon.
",2020-05-04T15:16:30Z,http://arxiv.org/abs/2005.01557v1,"Rama K. Vasudevan, Maxim Ziatdinov, Lukas Vlcek, Sergei V. Kalinin"
Improving Retrieval-Based Question Answering with Deep Inference Models,"  Question answering is one of the most important and difficult applications at
the border of information retrieval and natural language processing, especially
when we talk about complex science questions which require some form of
inference to determine the correct answer. In this paper, we present a two-step
method that combines information retrieval techniques optimized for question
answering with deep learning models for natural language inference in order to
tackle the multi-choice question answering in the science domain. For each
question-answer pair, we use standard retrieval-based models to find relevant
candidate contexts and decompose the main problem into two different
sub-problems. First, assign correctness scores for each candidate answer based
on the context using retrieval models from Lucene. Second, we use deep learning
architectures to compute if a candidate answer can be inferred from some
well-chosen context consisting of sentences retrieved from the knowledge base.
In the end, all these solvers are combined using a simple neural network to
predict the correct answer. This proposed two-step model outperforms the best
retrieval-based solver by over 3% in absolute accuracy.
",2018-12-07T10:44:14Z,http://arxiv.org/abs/1812.02971v2,"George-Sebastian Pirtoaca, Traian Rebedea, Stefan Ruseti"
"An inclusive review on deep learning techniques and their scope in
  handwriting recognition","  Deep learning expresses a category of machine learning algorithms that have
the capability to combine raw inputs into intermediate features layers. These
deep learning algorithms have demonstrated great results in different fields.
Deep learning has particularly witnessed for a great achievement of human level
performance across a number of domains in computer vision and pattern
recognition. For the achievement of state-of-the-art performances in diverse
domains, the deep learning used different architectures and these architectures
used activation functions to perform various computations between hidden and
output layers of any architecture. This paper presents a survey on the existing
studies of deep learning in handwriting recognition field. Even though the
recent progress indicates that the deep learning methods has provided valuable
means for speeding up or proving accurate results in handwriting recognition,
but following from the extensive literature survey, the present study finds
that the deep learning has yet to revolutionize more and has to resolve many of
the most pressing challenges in this field, but promising advances have been
made on the prior state of the art. Additionally, an inadequate availability of
labelled data to train presents problems in this domain. Nevertheless, the
present handwriting recognition survey foresees deep learning enabling changes
at both bench and bedside with the potential to transform several domains as
image processing, speech recognition, computer vision, machine translation,
robotics and control, medical imaging, medical information processing,
bio-informatics, natural language processing, cyber security, and many others.
",2024-04-10T06:30:33Z,http://arxiv.org/abs/2404.08011v1,"Sukhdeep Singh, Sudhir Rohilla, Anuj Sharma"
Tackling Vision Language Tasks Through Learning Inner Monologues,"  Visual language tasks require AI models to comprehend and reason with both
visual and textual content. Driven by the power of Large Language Models
(LLMs), two prominent methods have emerged: (1) the hybrid integration between
LLMs and Vision-Language Models (VLMs), where visual inputs are firstly
converted into language descriptions by VLMs, serving as inputs for LLMs to
generate final answer(s); (2) visual feature alignment in language space, where
visual inputs are encoded as embeddings and projected to LLMs' language space
via further supervised fine-tuning. The first approach provides light training
costs and interpretability but is hard to be optimized in an end-to-end
fashion. The second approach presents decent performance, but feature alignment
usually requires large amounts of training data and lacks interpretability. To
tackle this dilemma, we propose a novel approach, Inner Monologue Multi-Modal
Optimization (IMMO), to solve complex vision language problems by simulating
inner monologue processes, a cognitive process in which an individual engages
in silent verbal communication with themselves. We enable LLMs and VLMs to
interact through natural language conversation and propose to use a two-stage
training process to learn how to do the inner monologue (self-asking questions
and answering questions). IMMO is evaluated on two popular tasks and the
results suggest by emulating the cognitive phenomenon of internal dialogue, our
approach can enhance reasoning and explanation abilities, contributing to the
more effective fusion of vision and language models. More importantly, instead
of using predefined human-crafted monologues, IMMO learns this process within
the deep learning models, promising wider applicability to many different AI
problems beyond vision language tasks.
",2023-08-19T10:10:49Z,http://arxiv.org/abs/2308.09970v1,"Diji Yang, Kezhen Chen, Jinmeng Rao, Xiaoyuan Guo, Yawen Zhang, Jie Yang, Yi Zhang"
"Developing Linguistic Patterns to Mitigate Inherent Human Bias in
  Offensive Language Detection","  With the proliferation of social media, there has been a sharp increase in
offensive content, particularly targeting vulnerable groups, exacerbating
social problems such as hatred, racism, and sexism. Detecting offensive
language use is crucial to prevent offensive language from being widely shared
on social media. However, the accurate detection of irony, implication, and
various forms of hate speech on social media remains a challenge. Natural
language-based deep learning models require extensive training with large,
comprehensive, and labeled datasets. Unfortunately, manually creating such
datasets is both costly and error-prone. Additionally, the presence of
human-bias in offensive language datasets is a major concern for deep learning
models. In this paper, we propose a linguistic data augmentation approach to
reduce bias in labeling processes, which aims to mitigate the influence of
human bias by leveraging the power of machines to improve the accuracy and
fairness of labeling processes. This approach has the potential to improve
offensive language classification tasks across multiple languages and reduce
the prevalence of offensive content on social media.
",2023-12-04T10:20:36Z,http://arxiv.org/abs/2312.01787v1,"Toygar Tanyel, Besher Alkurdi, Serkan Ayvaz"
Learning to Automatically Generate Fill-In-The-Blank Quizzes,"  In this paper we formalize the problem automatic fill-in-the-blank question
generation using two standard NLP machine learning schemes, proposing concrete
deep learning models for each. We present an empirical study based on data
obtained from a language learning platform showing that both of our proposed
settings offer promising results.
",2018-06-12T13:53:22Z,http://arxiv.org/abs/1806.04524v1,"Edison Marrese-Taylor, Ai Nakajima, Yutaka Matsuo, Ono Yuichi"
Procedural Text Mining with Large Language Models,"  Recent advancements in the field of Natural Language Processing, particularly
the development of large-scale language models that are pretrained on vast
amounts of knowledge, are creating novel opportunities within the realm of
Knowledge Engineering. In this paper, we investigate the usage of large
language models (LLMs) in both zero-shot and in-context learning settings to
tackle the problem of extracting procedures from unstructured PDF text in an
incremental question-answering fashion. In particular, we leverage the current
state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model,
accompanied by two variations of in-context learning that involve an ontology
with definitions of procedures and steps and a limited number of samples of
few-shot learning. The findings highlight both the promise of this approach and
the value of the in-context learning customisations. These modifications have
the potential to significantly address the challenge of obtaining sufficient
training data, a hurdle often encountered in deep learning-based Natural
Language Processing techniques for procedure extraction.
",2023-10-05T08:27:33Z,http://arxiv.org/abs/2310.03376v1,"Anisa Rula, Jennifer D'Souza"
Data Curation with Deep Learning [Vision],"  Data curation - the process of discovering, integrating, and cleaning data -
is one of the oldest, hardest, yet inevitable data management problems. Despite
decades of efforts from both researchers and practitioners, it is still one of
the most time consuming and least enjoyable work of data scientists. In most
organizations, data curation plays an important role so as to fully unlock the
value of big data. Unfortunately, the current solutions are not keeping up with
the ever-changing data ecosystem, because they often require substantially high
human cost. Meanwhile, deep learning is making strides in achieving remarkable
successes in multiple areas, such as image recognition, natural language
processing, and speech recognition. In this vision paper, we explore how some
of the fundamental innovations in deep learning could be leveraged to improve
existing data curation solutions and to help build new ones. In particular, we
provide a thorough overview of the current deep learning landscape, and
identify interesting research opportunities and dispel common myths. We hope
that the synthesis of these important domains will unleash a series of research
activities that will lead to significantly improved solutions for many data
curation tasks.
",2018-03-04T17:08:45Z,http://arxiv.org/abs/1803.01384v2,"Saravanan Thirumuruganathan, Nan Tang, Mourad Ouzzani, AnHai Doan"
"Unsupervised pre-training of graph transformers on patient population
  graphs","  Pre-training has shown success in different areas of machine learning, such
as Computer Vision, Natural Language Processing (NLP), and medical imaging.
However, it has not been fully explored for clinical data analysis. An immense
amount of clinical records are recorded, but still, data and labels can be
scarce for data collected in small hospitals or dealing with rare diseases. In
such scenarios, pre-training on a larger set of unlabelled clinical data could
improve performance. In this paper, we propose novel unsupervised pre-training
techniques designed for heterogeneous, multi-modal clinical data for patient
outcome prediction inspired by masked language modeling (MLM), by leveraging
graph deep learning over population graphs. To this end, we further propose a
graph-transformer-based network, designed to handle heterogeneous clinical
data. By combining masking-based pre-training with a transformer-based network,
we translate the success of masking-based pre-training in other domains to
heterogeneous clinical data. We show the benefit of our pre-training method in
a self-supervised and a transfer learning setting, utilizing three medical
datasets TADPOLE, MIMIC-III, and a Sepsis Prediction Dataset. We find that our
proposed pre-training methods help in modeling the data at a patient and
population level and improve performance in different fine-tuning tasks on all
datasets.
",2022-07-21T16:59:09Z,http://arxiv.org/abs/2207.10603v2,"Chantal Pellegrini, Nassir Navab, Anees Kazi"
Deep Graph Matching and Searching for Semantic Code Retrieval,"  Code retrieval is to find the code snippet from a large corpus of source code
repositories that highly matches the query of natural language description.
Recent work mainly uses natural language processing techniques to process both
query texts (i.e., human natural language) and code snippets (i.e., machine
programming language), however neglecting the deep structured features of query
texts and source codes, both of which contain rich semantic information. In
this paper, we propose an end-to-end deep graph matching and searching (DGMS)
model based on graph neural networks for the task of semantic code retrieval.
To this end, we first represent both natural language query texts and
programming language code snippets with the unified graph-structured data, and
then use the proposed graph matching and searching model to retrieve the best
matching code snippet. In particular, DGMS not only captures more structural
information for individual query texts or code snippets but also learns the
fine-grained similarity between them by cross-attention based semantic matching
operations. We evaluate the proposed DGMS model on two public code retrieval
datasets with two representative programming languages (i.e., Java and Python).
Experiment results demonstrate that DGMS significantly outperforms
state-of-the-art baseline models by a large margin on both datasets. Moreover,
our extensive ablation studies systematically investigate and illustrate the
impact of each part of DGMS.
",2020-10-24T14:16:50Z,http://arxiv.org/abs/2010.12908v2,"Xiang Ling, Lingfei Wu, Saizhuo Wang, Gaoning Pan, Tengfei Ma, Fangli Xu, Alex X. Liu, Chunming Wu, Shouling Ji"
Automatic Summarization of Natural Language,"  Automatic summarization of natural language is a current topic in computer
science research and industry, studied for decades because of its usefulness
across multiple domains. For example, summarization is necessary to create
reviews such as this one. Research and applications have achieved some success
in extractive summarization (where key sentences are curated), however,
abstractive summarization (synthesis and re-stating) is a hard problem and
generally unsolved in computer science. This literature review contrasts
historical progress up through current state of the art, comparing dimensions
such as: extractive vs. abstractive, supervised vs. unsupervised, NLP (Natural
Language Processing) vs Knowledge-based, deep learning vs algorithms,
structured vs. unstructured sources, and measurement metrics such as Rouge and
BLEU. Multiple dimensions are contrasted since current research uses
combinations of approaches as seen in the review matrix. Throughout this
summary, synthesis and critique is provided. This review concludes with
insights for improved abstractive summarization measurement, with surprising
implications for detecting understanding and comprehension in general.
",2018-12-18T14:17:56Z,http://arxiv.org/abs/1812.10549v1,Marc Everett Johnson
Sparse Deep Learning for Time Series Data: Theory and Applications,"  Sparse deep learning has become a popular technique for improving the
performance of deep neural networks in areas such as uncertainty
quantification, variable selection, and large-scale network compression.
However, most existing research has focused on problems where the observations
are independent and identically distributed (i.i.d.), and there has been little
work on the problems where the observations are dependent, such as time series
data and sequential data in natural language processing. This paper aims to
address this gap by studying the theory for sparse deep learning with dependent
data. We show that sparse recurrent neural networks (RNNs) can be consistently
estimated, and their predictions are asymptotically normally distributed under
appropriate assumptions, enabling the prediction uncertainty to be correctly
quantified. Our numerical results show that sparse deep learning outperforms
state-of-the-art methods, such as conformal predictions, in prediction
uncertainty quantification for time series data. Furthermore, our results
indicate that the proposed method can consistently identify the autoregressive
order for time series data and outperform existing methods in large-scale model
compression. Our proposed method has important practical implications in fields
such as finance, healthcare, and energy, where both accurate point estimates
and prediction uncertainty quantification are of concern.
",2023-10-05T01:26:13Z,http://arxiv.org/abs/2310.03243v1,"Mingxuan Zhang, Yan Sun, Faming Liang"
"Differentially Private Natural Language Models: Recent Advances and
  Future Directions","  Recent developments in deep learning have led to great success in various
natural language processing (NLP) tasks. However, these applications may
involve data that contain sensitive information. Therefore, how to achieve good
performance while also protecting the privacy of sensitive data is a crucial
challenge in NLP. To preserve privacy, Differential Privacy (DP), which can
prevent reconstruction attacks and protect against potential side knowledge, is
becoming a de facto technique for private data analysis. In recent years, NLP
in DP models (DP-NLP) has been studied from different perspectives, which
deserves a comprehensive review. In this paper, we provide the first systematic
review of recent advances in DP deep learning models in NLP. In particular, we
first discuss some differences and additional challenges of DP-NLP compared
with the standard DP deep learning. Then, we investigate some existing work on
DP-NLP and present its recent developments from three aspects: gradient
perturbation based methods, embedding vector perturbation based methods, and
ensemble model based methods. We also discuss some challenges and future
directions.
",2023-01-22T12:29:03Z,http://arxiv.org/abs/2301.09112v2,"Lijie Hu, Ivan Habernal, Lei Shen, Di Wang"
Do Large Language Models Mirror Cognitive Language Processing?,"  Large Language Models (LLMs) have demonstrated remarkable abilities in text
comprehension and logical reasoning, indicating that the text representations
learned by LLMs can facilitate their language processing capabilities. In
cognitive science, brain cognitive processing signals are typically utilized to
study human language processing. Therefore, it is natural to ask how well the
text embeddings from LLMs align with the brain cognitive processing signals,
and how training strategies affect the LLM-brain alignment? In this paper, we
employ Representational Similarity Analysis (RSA) to measure the alignment
between 23 mainstream LLMs and fMRI signals of the brain to evaluate how
effectively LLMs simulate cognitive language processing. We empirically
investigate the impact of various factors (e.g., pre-training data size, model
scaling, alignment training, and prompts) on such LLM-brain alignment.
Experimental results indicate that pre-training data size and model scaling are
positively correlated with LLM-brain similarity, and alignment training can
significantly improve LLM-brain similarity. Explicit prompts contribute to the
consistency of LLMs with brain cognitive language processing, while nonsensical
noisy prompts may attenuate such alignment. Additionally, the performance of a
wide range of LLM evaluations (e.g., MMLU, Chatbot Arena) is highly correlated
with the LLM-brain similarity.
",2024-02-28T03:38:20Z,http://arxiv.org/abs/2402.18023v2,"Yuqi Ren, Renren Jin, Tongxuan Zhang, Deyi Xiong"
"Social and environmental impact of recent developments in machine
  learning on biology and chemistry research","  Potential societal and environmental effects such as the rapidly increasing
resource use and the associated environmental impact, reproducibility issues,
and exclusivity, the privatization of ML research leading to a public research
brain-drain, a narrowing of the research effort caused by a focus on deep
learning, and the introduction of biases through a lack of sociodemographic
diversity in data and personnel caused by recent developments in machine
learning are a current topic of discussion and scientific publications.
However, these discussions and publications focus mainly on computer
science-adjacent fields, including computer vision and natural language
processing or basic ML research. Using bibliometric analysis of the complete
and full-text analysis of the open-access literature, we show that the same
observations can be made for applied machine learning in chemistry and biology.
These developments can potentially affect basic and applied research, such as
drug discovery and development, beyond the known issue of biased data sets.
",2022-10-01T20:29:01Z,http://arxiv.org/abs/2210.00356v1,Daniel Probst
"This Reads Like That: Deep Learning for Interpretable Natural Language
  Processing","  Prototype learning, a popular machine learning method designed for inherently
interpretable decisions, leverages similarities to learned prototypes for
classifying new data. While it is mainly applied in computer vision, in this
work, we build upon prior research and further explore the extension of
prototypical networks to natural language processing. We introduce a learned
weighted similarity measure that enhances the similarity computation by
focusing on informative dimensions of pre-trained sentence embeddings.
Additionally, we propose a post-hoc explainability mechanism that extracts
prediction-relevant words from both the prototype and input sentences. Finally,
we empirically demonstrate that our proposed method not only improves
predictive performance on the AG News and RT Polarity datasets over a previous
prototype-based approach, but also improves the faithfulness of explanations
compared to rationale-based recurrent convolutions.
",2023-10-25T21:18:35Z,http://arxiv.org/abs/2310.17010v1,"Claudio Fanconi, Moritz Vandenhirtz, Severin Husmann, Julia E. Vogt"
"SPASS: Scientific Prominence Active Search System with Deep Image
  Captioning Network","  Planetary exploration missions with Mars rovers are complicated, which
generally require elaborated task planning by human experts, from the path to
take to the images to capture. NASA has been using this process to acquire over
22 million images from the planet Mars. In order to improve the degree of
automation and thus efficiency in this process, we propose a system for
planetary rovers to actively search for prominence of prespecified scientific
features in captured images. Scientists can prespecify such search tasks in
natural language and upload them to a rover, on which the deployed system
constantly captions captured images with a deep image captioning network and
compare the auto-generated captions to the prespecified search tasks by certain
metrics so as to prioritize those images for transmission. As a beneficial side
effect, the proposed system can also be deployed to ground-based planetary data
systems as a content-based search engine.
",2018-09-10T15:18:37Z,http://arxiv.org/abs/1809.03385v1,Dicong Qiu
Applications of Deep Neural Networks with Keras,"  Deep learning is a group of exciting new technologies for neural networks.
Through a combination of advanced training techniques and neural network
architectural components, it is now possible to create neural networks that can
handle tabular data, images, text, and audio as both input and output. Deep
learning allows a neural network to learn hierarchies of information in a way
that is like the function of the human brain. This course will introduce the
student to classic neural network structures, Convolution Neural Networks
(CNN), Long Short-Term Memory (LSTM), Gated Recurrent Neural Networks (GRU),
General Adversarial Networks (GAN), and reinforcement learning. Application of
these architectures to computer vision, time series, security, natural language
processing (NLP), and data generation will be covered. High-Performance
Computing (HPC) aspects will demonstrate how deep learning can be leveraged
both on graphical processing units (GPUs), as well as grids. Focus is primarily
upon the application of deep learning to problems, with some introduction to
mathematical foundations. Readers will use the Python programming language to
implement deep learning using Google TensorFlow and Keras. It is not necessary
to know Python prior to this book; however, familiarity with at least one
programming language is assumed.
",2020-09-11T22:09:10Z,http://arxiv.org/abs/2009.05673v5,Jeff Heaton
On the Efficiency of NLP-Inspired Methods for Tabular Deep Learning,"  Recent advancements in tabular deep learning (DL) have led to substantial
performance improvements, surpassing the capabilities of traditional models.
With the adoption of techniques from natural language processing (NLP), such as
language model-based approaches, DL models for tabular data have also grown in
complexity and size. Although tabular datasets do not typically pose
scalability issues, the escalating size of these models has raised efficiency
concerns. Despite its importance, efficiency has been relatively underexplored
in tabular DL research. This paper critically examines the latest innovations
in tabular DL, with a dual focus on performance and computational efficiency.
The source code is available at https://github.com/basf/mamba-tabular.
",2024-11-26T08:23:29Z,http://arxiv.org/abs/2411.17207v1,"Anton Frederik Thielmann, Soheila Samiee"
Deep learning and sub-word-unit approach in written art generation,"  Automatic poetry generation is novel and interesting application of natural
language processing research. It became more popular during the last few years
due to the rapid development of technology and neural computing power. This
line of research can be applied to the study of linguistics and literature, for
social science experiments, or simply for entertainment. The most effective
known method of artificial poem generation uses recurrent neural networks
(RNN). We also used RNNs to generate poems in the style of Adam Mickiewicz. Our
network was trained on the Sir Thaddeus poem. For data pre-processing, we used
a specialized stemming tool, which is one of the major innovations and
contributions of this work. Our experiment was conducted on the source text,
divided into sub-word units (at a level of resolution close to syllables). This
approach is novel and is not often employed in the published literature. The
subwords units seem to be a natural choice for analysis of the Polish language,
as the language is morphologically rich due to cases, gender forms and a large
vocabulary. Moreover, Sir Thaddeus contains rhymes, so the analysis of
syllables can be meaningful. We verified our model with different settings for
the temperature parameter, which controls the randomness of the generated text.
We also compared our results with similar models trained on the same text but
divided into characters (which is the most common approach alongside the use of
full word units). The differences were tremendous. Our solution generated much
better poems that were able to follow the metre and vocabulary of the source
data text.
",2019-01-22T15:42:51Z,http://arxiv.org/abs/1901.07426v1,"Krzysztof Wołk, Emilia Zawadzka-Gosk, Wojciech Czarnowski"
"Rare Words: A Major Problem for Contextualized Embeddings And How to Fix
  it by Attentive Mimicking","  Pretraining deep neural network architectures with a language modeling
objective has brought large improvements for many natural language processing
tasks. Exemplified by BERT, a recently proposed such architecture, we
demonstrate that despite being trained on huge amounts of data, deep language
models still struggle to understand rare words. To fix this problem, we adapt
Attentive Mimicking, a method that was designed to explicitly learn embeddings
for rare words, to deep language models. In order to make this possible, we
introduce one-token approximation, a procedure that enables us to use Attentive
Mimicking even when the underlying language model uses subword-based
tokenization, i.e., it does not assign embeddings to all words. To evaluate our
method, we create a novel dataset that tests the ability of language models to
capture semantic properties of words without any task-specific fine-tuning.
Using this dataset, we show that adding our adapted version of Attentive
Mimicking to BERT does indeed substantially improve its understanding of rare
words.
",2019-04-14T15:26:52Z,http://arxiv.org/abs/1904.06707v4,"Timo Schick, Hinrich Schütze"
"Improving historical spelling normalization with bi-directional LSTMs
  and multi-task learning","  Natural-language processing of historical documents is complicated by the
abundance of variant spellings and lack of annotated data. A common approach is
to normalize the spelling of historical words to modern forms. We explore the
suitability of a deep neural network architecture for this task, particularly a
deep bi-LSTM network applied on a character level. Our model compares well to
previously established normalization algorithms when evaluated on a diverse set
of texts from Early New High German. We show that multi-task learning with
additional normalization data can improve our model's performance further.
",2016-10-25T12:30:26Z,http://arxiv.org/abs/1610.07844v1,"Marcel Bollmann, Anders Søgaard"
"Protein sequence classification using natural language processing
  techniques","  Proteins are essential to numerous biological functions, with their sequences
determining their roles within organisms. Traditional methods for determining
protein function are time-consuming and labor-intensive. This study addresses
the increasing demand for precise, effective, and automated protein sequence
classification methods by employing natural language processing (NLP)
techniques on a dataset comprising 75 target protein classes. We explored
various machine learning and deep learning models, including K-Nearest
Neighbors (KNN), Multinomial Na\""ive Bayes, Logistic Regression, Multi-Layer
Perceptron (MLP), Decision Tree, Random Forest, XGBoost, Voting and Stacking
classifiers, Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM),
and transformer models (BertForSequenceClassification, DistilBERT, and
ProtBert). Experiments were conducted using amino acid ranges of 1-4 grams for
machine learning models and different sequence lengths for CNN and LSTM models.
The KNN algorithm performed best on tri-gram data with 70.0% accuracy and a
macro F1 score of 63.0%. The Voting classifier achieved best performance with
74.0% accuracy and an F1 score of 65.0%, while the Stacking classifier reached
75.0% accuracy and an F1 score of 64.0%. ProtBert demonstrated the highest
performance among transformer models, with a accuracy 76.0% and F1 score 61.0%
which is same for all three transformer models. Advanced NLP techniques,
particularly ensemble methods and transformer models, show great potential in
protein classification. Our results demonstrate that ensemble methods,
particularly Voting Soft classifiers, achieved superior results, highlighting
the importance of sufficient training data and addressing sequence similarity
across different classes.
",2024-09-06T13:16:16Z,http://arxiv.org/abs/2409.04491v1,"Huma Perveen, Julie Weeds"
Benchmark of Deep Learning Models on Large Healthcare MIMIC Datasets,"  Deep learning models (aka Deep Neural Networks) have revolutionized many
fields including computer vision, natural language processing, speech
recognition, and is being increasingly used in clinical healthcare
applications. However, few works exist which have benchmarked the performance
of the deep learning models with respect to the state-of-the-art machine
learning models and prognostic scoring systems on publicly available healthcare
datasets. In this paper, we present the benchmarking results for several
clinical prediction tasks such as mortality prediction, length of stay
prediction, and ICD-9 code group prediction using Deep Learning models,
ensemble of machine learning models (Super Learner algorithm), SAPS II and SOFA
scores. We used the Medical Information Mart for Intensive Care III (MIMIC-III)
(v1.4) publicly available dataset, which includes all patients admitted to an
ICU at the Beth Israel Deaconess Medical Center from 2001 to 2012, for the
benchmarking tasks. Our results show that deep learning models consistently
outperform all the other approaches especially when the `raw' clinical time
series data is used as input features to the models.
",2017-10-23T22:23:34Z,http://arxiv.org/abs/1710.08531v1,"Sanjay Purushotham, Chuizheng Meng, Zhengping Che, Yan Liu"
Uzbek Sentiment Analysis based on local Restaurant Reviews,"  Extracting useful information for sentiment analysis and classification
problems from a big amount of user-generated feedback, such as restaurant
reviews, is a crucial task of natural language processing, which is not only
for customer satisfaction where it can give personalized services, but can also
influence the further development of a company. In this paper, we present a
work done on collecting restaurant reviews data as a sentiment analysis dataset
for the Uzbek language, a member of the Turkic family which is heavily affected
by the low-resource constraint, and provide some further analysis of the novel
dataset by evaluation using different techniques, from logistic regression
based models, to support vector machines, and even deep learning models, such
as recurrent neural networks, as well as convolutional neural networks. The
paper includes detailed information on how the data was collected, how it was
pre-processed for better quality optimization, as well as experimental setups
for the evaluation process. The overall evaluation results indicate that by
performing pre-processing steps, such as stemming for agglutinative languages,
the system yields better results, eventually achieving 91% accuracy result in
the best performing model
",2022-05-31T16:21:00Z,http://arxiv.org/abs/2205.15930v1,"Sanatbek Matlatipov, Hulkar Rahimboeva, Jaloliddin Rajabov, Elmurod Kuriyozov"
Knowledge Graph Extraction from Videos,"  Nearly all existing techniques for automated video annotation (or captioning)
describe videos using natural language sentences. However, this has several
shortcomings: (i) it is very hard to then further use the generated natural
language annotations in automated data processing, (ii) generating natural
language annotations requires to solve the hard subtask of generating
semantically precise and syntactically correct natural language sentences,
which is actually unrelated to the task of video annotation, (iii) it is
difficult to quantitatively measure performance, as standard metrics (e.g.,
accuracy and F1-score) are inapplicable, and (iv) annotations are
language-specific. In this paper, we propose the new task of knowledge graph
extraction from videos, i.e., producing a description in the form of a
knowledge graph of the contents of a given video. Since no datasets exist for
this task, we also include a method to automatically generate them, starting
from datasets where videos are annotated with natural language. We then
describe an initial deep-learning model for knowledge graph extraction from
videos, and report results on MSVD* and MSR-VTT*, two datasets obtained from
MSVD and MSR-VTT using our method.
",2020-07-20T12:23:39Z,http://arxiv.org/abs/2007.10040v1,"Louis Mahon, Eleonora Giunchiglia, Bowen Li, Thomas Lukasiewicz"
TensorLayer: A Versatile Library for Efficient Deep Learning Development,"  Deep learning has enabled major advances in the fields of computer vision,
natural language processing, and multimedia among many others. Developing a
deep learning system is arduous and complex, as it involves constructing neural
network architectures, managing training/trained models, tuning optimization
process, preprocessing and organizing data, etc. TensorLayer is a versatile
Python library that aims at helping researchers and engineers efficiently
develop deep learning systems. It offers rich abstractions for neural networks,
model and data management, and parallel workflow mechanism. While boosting
efficiency, TensorLayer maintains both performance and scalability. TensorLayer
was released in September 2016 on GitHub, and has helped people from academia
and industry develop real-world applications of deep learning.
",2017-07-26T17:29:49Z,http://arxiv.org/abs/1707.08551v3,"Hao Dong, Akara Supratak, Luo Mai, Fangde Liu, Axel Oehmichen, Simiao Yu, Yike Guo"
Deviation bound for non-causal machine learning,"  Concentration inequalities are widely used for analyzing machine learning
algorithms. However, current concentration inequalities cannot be applied to
some of the most popular deep neural networks, notably in natural language
processing. This is mostly due to the non-causal nature of such involved data,
in the sense that each data point depends on other neighbor data points. In
this paper, a framework for modeling non-causal random fields is provided and a
Hoeffding-type concentration inequality is obtained for this framework. The
proof of this result relies on a local approximation of the non-causal random
field by a function of a finite number of i.i.d. random variables.
",2020-09-18T15:57:59Z,http://arxiv.org/abs/2009.08905v2,"Rémy Garnier, Raphaël Langhendries"
"Natural Language Processing 4 All (NLP4All): A New Online Platform for
  Teaching and Learning NLP Concepts","  Natural Language Processing offers new insights into language data across
almost all disciplines and domains, and allows us to corroborate and/or
challenge existing knowledge. The primary hurdles to widening participation in
and use of these new research tools are, first, a lack of coding skills in
students across K-16, and in the population at large, and second, a lack of
knowledge of how NLP-methods can be used to answer questions of disciplinary
interest outside of linguistics and/or computer science. To broaden
participation in NLP and improve NLP-literacy, we introduced a new tool
web-based tool called Natural Language Processing 4 All (NLP4All). The intended
purpose of NLP4All is to help teachers facilitate learning with and about NLP,
by providing easy-to-use interfaces to NLP-methods, data, and analyses, making
it possible for non- and novice-programmers to learn NLP concepts
interactively.
",2021-05-28T09:57:22Z,http://arxiv.org/abs/2105.13704v1,"Rebekah Baglini, Arthur Hjorth"
"Evaluating the Translation Performance of Large Language Models Based on
  Euas-20","  In recent years, with the rapid development of deep learning technology,
large language models (LLMs) such as BERT and GPT have achieved breakthrough
results in natural language processing tasks. Machine translation (MT), as one
of the core tasks of natural language processing, has also benefited from the
development of large language models and achieved a qualitative leap. Despite
the significant progress in translation performance achieved by large language
models, machine translation still faces many challenges. Therefore, in this
paper, we construct the dataset Euas-20 to evaluate the performance of large
language models on translation tasks, the translation ability on different
languages, and the effect of pre-training data on the translation ability of
LLMs for researchers and developers.
",2024-08-06T11:49:11Z,http://arxiv.org/abs/2408.03119v1,"Yan Huang, Wei Liu"
"Learning Convolutional Text Representations for Visual Question
  Answering","  Visual question answering is a recently proposed artificial intelligence task
that requires a deep understanding of both images and texts. In deep learning,
images are typically modeled through convolutional neural networks, and texts
are typically modeled through recurrent neural networks. While the requirement
for modeling images is similar to traditional computer vision tasks, such as
object recognition and image classification, visual question answering raises a
different need for textual representation as compared to other natural language
processing tasks. In this work, we perform a detailed analysis on natural
language questions in visual question answering. Based on the analysis, we
propose to rely on convolutional neural networks for learning textual
representations. By exploring the various properties of convolutional neural
networks specialized for text data, such as width and depth, we present our
""CNN Inception + Gate"" model. We show that our model improves question
representations and thus the overall accuracy of visual question answering
models. We also show that the text representation requirement in visual
question answering is more complicated and comprehensive than that in
conventional natural language processing tasks, making it a better task to
evaluate textual representation methods. Shallow models like fastText, which
can obtain comparable results with deep learning models in tasks like text
classification, are not suitable in visual question answering.
",2017-05-18T22:51:44Z,http://arxiv.org/abs/1705.06824v2,"Zhengyang Wang, Shuiwang Ji"
"Deep Learning-based Software Engineering: Progress, Challenges, and
  Opportunities","  Researchers have recently achieved significant advances in deep learning
techniques, which in turn has substantially advanced other research
disciplines, such as natural language processing, image processing, speech
recognition, and software engineering. Various deep learning techniques have
been successfully employed to facilitate software engineering tasks, including
code generation, software refactoring, and fault localization. Many papers have
also been presented in top conferences and journals, demonstrating the
applications of deep learning techniques in resolving various software
engineering tasks. However, although several surveys have provided overall
pictures of the application of deep learning techniques in software
engineering, they focus more on learning techniques, that is, what kind of deep
learning techniques are employed and how deep models are trained or fine-tuned
for software engineering tasks. We still lack surveys explaining the advances
of subareas in software engineering driven by deep learning techniques, as well
as challenges and opportunities in each subarea. To this end, in this paper, we
present the first task-oriented survey on deep learning-based software
engineering. It covers twelve major software engineering subareas significantly
impacted by deep learning techniques. Such subareas spread out the through the
whole lifecycle of software development and maintenance, including requirements
engineering, software development, testing, maintenance, and developer
collaboration. As we believe that deep learning may provide an opportunity to
revolutionize the whole discipline of software engineering, providing one
survey covering as many subareas as possible in software engineering can help
future research push forward the frontier of deep learning-based software
engineering more systematically.
",2024-10-17T00:46:00Z,http://arxiv.org/abs/2410.13110v1,"Xiangping Chen, Xing Hu, Yuan Huang, He Jiang, Weixing Ji, Yanjie Jiang, Yanyan Jiang, Bo Liu, Hui Liu, Xiaochen Li, Xiaoli Lian, Guozhu Meng, Xin Peng, Hailong Sun, Lin Shi, Bo Wang, Chong Wang, Jiayi Wang, Tiantian Wang, Jifeng Xuan, Xin Xia, Yibiao Yang, Yixin Yang, Li Zhang, Yuming Zhou, Lu Zhang"
"Proceedings 36th International Conference on Logic Programming
  (Technical Communications)","  Since the first conference held in Marseille in 1982, ICLP has been the
premier international event for presenting research in logic programming.
Contributions are solicited in all areas of logic programming and related
areas, including but not restricted to:
  - Foundations: Semantics, Formalisms, Answer-Set Programming, Non-monotonic
Reasoning, Knowledge Representation.
  - Declarative Programming: Inference engines, Analysis, Type and mode
inference, Partial evaluation, Abstract interpretation, Transformation,
Validation, Verification, Debugging, Profiling, Testing, Logic-based
domain-specific languages, constraint handling rules.
  - Related Paradigms and Synergies: Inductive and Co-inductive Logic
Programming, Constraint Logic Programming, Interaction with SAT, SMT and CSP
solvers, Logic programming techniques for type inference and theorem proving,
Argumentation, Probabilistic Logic Programming, Relations to object-oriented
and Functional programming, Description logics, Neural-Symbolic Machine
Learning, Hybrid Deep Learning and Symbolic Reasoning.
  - Implementation: Concurrency and distribution, Objects, Coordination,
Mobility, Virtual machines, Compilation, Higher Order, Type systems, Modules,
Constraint handling rules, Meta-programming, Foreign interfaces, User
interfaces.
  - Applications: Databases, Big Data, Data Integration and Federation,
Software Engineering, Natural Language Processing, Web and Semantic Web,
Agents, Artificial Intelligence, Bioinformatics, Education, Computational life
sciences, Education, Cybersecurity, and Robotics.
",2020-09-19T04:18:41Z,http://arxiv.org/abs/2009.09158v1,"Francesco Ricca, Alessandra Russo, Sergio Greco, Nicola Leone, Alexander Artikis, Gerhard Friedrich, Paul Fodor, Angelika Kimmig, Francesca Lisi, Marco Maratea, Alessandra Mileo, Fabrizio Riguzzi"
NegDL: Privacy-Preserving Deep Learning Based on Negative Database,"  In the era of big data, deep learning has become an increasingly popular
topic. It has outstanding achievements in the fields of image recognition,
object detection, and natural language processing et al. The first priority of
deep learning is exploiting valuable information from a large amount of data,
which will inevitably induce privacy issues that are worthy of attention.
Presently, several privacy-preserving deep learning methods have been proposed,
but most of them suffer from a non-negligible degradation of either efficiency
or accuracy. Negative database (\textit{NDB}) is a new type of data
representation which can protect data privacy by storing and utilizing the
complementary form of original data. In this paper, we propose a
privacy-preserving deep learning method named NegDL based on \textit{NDB}.
Specifically, private data are first converted to \textit{NDB} as the input of
deep learning models by a generation algorithm called \textit{QK}-hidden
algorithm, and then the sketches of \textit{NDB} are extracted for training and
inference. We demonstrate that the computational complexity of NegDL is the
same as the original deep learning model without privacy protection.
Experimental results on Breast Cancer, MNIST, and CIFAR-10 benchmark datasets
demonstrate that the accuracy of NegDL could be comparable to the original deep
learning model in most cases, and it performs better than the method based on
differential privacy.
",2021-03-10T03:34:03Z,http://arxiv.org/abs/2103.05854v5,"Dongdong Zhao, Pingchuan Zhang, Jianwen Xiang, Jing Tian"
"A Novel Method for improving accuracy in neural network by reinstating
  traditional back propagation technique","  Deep learning has revolutionized industries like computer vision, natural
language processing, and speech recognition. However, back propagation, the
main method for training deep neural networks, faces challenges like
computational overhead and vanishing gradients. In this paper, we propose a
novel instant parameter update methodology that eliminates the need for
computing gradients at each layer. Our approach accelerates learning, avoids
the vanishing gradient problem, and outperforms state-of-the-art methods on
benchmark data sets. This research presents a promising direction for efficient
and effective deep neural network training.
",2023-08-09T16:41:00Z,http://arxiv.org/abs/2308.05059v1,Gokulprasath R
"A Survey of Methods for Addressing Class Imbalance in Deep-Learning
  Based Natural Language Processing","  Many natural language processing (NLP) tasks are naturally imbalanced, as
some target categories occur much more frequently than others in the real
world. In such scenarios, current NLP models still tend to perform poorly on
less frequent classes. Addressing class imbalance in NLP is an active research
topic, yet, finding a good approach for a particular task and imbalance
scenario is difficult.
  With this survey, the first overview on class imbalance in deep-learning
based NLP, we provide guidance for NLP researchers and practitioners dealing
with imbalanced data. We first discuss various types of controlled and
real-world class imbalance. Our survey then covers approaches that have been
explicitly proposed for class-imbalanced NLP tasks or, originating in the
computer vision community, have been evaluated on them. We organize the methods
by whether they are based on sampling, data augmentation, choice of loss
function, staged learning, or model design. Finally, we discuss open problems
such as dealing with multi-label scenarios, and propose systematic benchmarking
and reporting in order to move forward on this problem as a community.
",2022-10-10T13:26:40Z,http://arxiv.org/abs/2210.04675v2,"Sophie Henning, William Beluch, Alexander Fraser, Annemarie Friedrich"
Deep Active Learning for Named Entity Recognition,"  Deep learning has yielded state-of-the-art performance on many natural
language processing tasks including named entity recognition (NER). However,
this typically requires large amounts of labeled data. In this work, we
demonstrate that the amount of labeled training data can be drastically reduced
when deep learning is combined with active learning. While active learning is
sample-efficient, it can be computationally expensive since it requires
iterative retraining. To speed this up, we introduce a lightweight architecture
for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and
word encoders and a long short term memory (LSTM) tag decoder. The model
achieves nearly state-of-the-art performance on standard datasets for the task
while being computationally much more efficient than best performing models. We
carry out incremental active learning, during the training process, and are
able to nearly match state-of-the-art performance with just 25\% of the
original training data.
",2017-07-19T03:18:40Z,http://arxiv.org/abs/1707.05928v3,"Yanyao Shen, Hyokun Yun, Zachary C. Lipton, Yakov Kronrod, Animashree Anandkumar"
Low-Rank Deep Convolutional Neural Network for Multi-Task Learning,"  In this paper, we propose a novel multi-task learning method based on the
deep convolutional network. The proposed deep network has four convolutional
layers, three max-pooling layers, and two parallel fully connected layers. To
adjust the deep network to multi-task learning problem, we propose to learn a
low-rank deep network so that the relation among different tasks can be
explored. We proposed to minimize the number of independent parameter rows of
one fully connected layer to explore the relations among different tasks, which
is measured by the nuclear norm of the parameter of one fully connected layer,
and seek a low-rank parameter matrix. Meanwhile, we also propose to regularize
another fully connected layer by sparsity penalty, so that the useful features
learned by the lower layers can be selected. The learning problem is solved by
an iterative algorithm based on gradient descent and back-propagation
algorithms. The proposed algorithm is evaluated over benchmark data sets of
multiple face attribute prediction, multi-task natural language processing, and
joint economics index predictions. The evaluation results show the advantage of
the low-rank deep CNN model over multi-task problems.
",2019-04-12T14:20:01Z,http://arxiv.org/abs/1904.07320v1,"Fang Su, Hai-Yang Shang, Jing-Yan Wang"
An End-to-end Neural Natural Language Interface for Databases,"  The ability to extract insights from new data sets is critical for decision
making. Visual interactive tools play an important role in data exploration
since they provide non-technical users with an effective way to visually
compose queries and comprehend the results. Natural language has recently
gained traction as an alternative query interface to databases with the
potential to enable non-expert users to formulate complex questions and
information needs efficiently and effectively. However, understanding natural
language questions and translating them accurately to SQL is a challenging
task, and thus Natural Language Interfaces for Databases (NLIDBs) have not yet
made their way into practical tools and commercial products.
  In this paper, we present DBPal, a novel data exploration tool with a natural
language interface. DBPal leverages recent advances in deep models to make
query understanding more robust in the following ways: First, DBPal uses a deep
model to translate natural language statements to SQL, making the translation
process more robust to paraphrasing and other linguistic variations. Second, to
support the users in phrasing questions without knowing the database schema and
the query features, DBPal provides a learned auto-completion model that
suggests partial query extensions to users during query formulation and thus
helps to write complex queries.
",2018-04-02T05:36:38Z,http://arxiv.org/abs/1804.00401v1,"Prasetya Utama, Nathaniel Weir, Fuat Basik, Carsten Binnig, Ugur Cetintemel, Benjamin Hättasch, Amir Ilkhechi, Shekar Ramaswamy, Arif Usta"
"Subsentence Extraction from Text Using Coverage-Based Deep Learning
  Language Models","  Sentiment prediction remains a challenging and unresolved task in various
research fields, including psychology, neuroscience, and computer science. This
stems from its high degree of subjectivity and limited input sources that can
effectively capture the actual sentiment. This can be even more challenging
with only text-based input. Meanwhile, the rise of deep learning and an
unprecedented large volume of data have paved the way for artificial
intelligence to perform impressively accurate predictions or even human-level
reasoning. Drawing inspiration from this, we propose a coverage-based sentiment
and subsentence extraction system that estimates a span of input text and
recursively feeds this information back to the networks. The predicted
subsentence consists of auxiliary information expressing a sentiment. This is
an important building block for enabling vivid and epic sentiment delivery
(within the scope of this paper) and for other natural language processing
tasks such as text summarisation and Q&A. Our approach outperforms the
state-of-the-art approaches by a large margin in subsentence prediction (i.e.,
Average Jaccard scores from 0.72 to 0.89). For the evaluation, we designed
rigorous experiments consisting of 24 ablation studies. Finally, our learned
lessons are returned to the community by sharing software packages and a public
dataset that can reproduce the results presented in this paper.
",2021-04-20T06:24:49Z,http://arxiv.org/abs/2104.09777v2,"JongYoon Lim, Inkyu Sa, Ho Seok Ahn, Norina Gasteiger, Sanghyub John Lee, Bruce MacDonald"
"Frontiers of Deep Learning: From Novel Application to Real-World
  Deployment","  Deep learning continues to re-shape numerous fields, from natural language
processing and imaging to data analytics and recommendation systems. This
report studies two research papers that represent recent progress on deep
learning from two largely different aspects: The first paper applied the
transformer networks, which are typically used in language models, to improve
the quality of synthetic aperture radar image by effectively reducing the
speckle noise. The second paper presents an in-storage computing design
solution to enable cost-efficient and high-performance implementations of deep
learning recommendation systems. In addition to summarizing each paper in terms
of motivation, key ideas and techniques, and evaluation results, this report
also presents thoughts and discussions about possible future research
directions. By carrying out in-depth study on these two representative papers
and related references, this doctoral candidate has developed better
understanding on the far-reaching impact and efficient implementation of deep
learning models.
",2024-07-19T15:11:55Z,http://arxiv.org/abs/2407.14386v1,Rui Xie
"Superiorities of Deep Extreme Learning Machines against Convolutional
  Neural Networks","  Deep Learning (DL) is a machine learning procedure for artificial
intelligence that analyzes the input data in detail by increasing neuron sizes
and number of the hidden layers. DL has a popularity with the common
improvements on the graphical processing unit capabilities. Increasing number
of the neuron sizes at each layer and hidden layers is directly related to the
computation time and training speed of the classifier models. The
classification parameters including neuron weights, output weights, and biases
need to be optimized for obtaining an optimum model. Most of the popular DL
algorithms require long training times for optimization of the parameters with
feature learning progresses and back-propagated training procedures. Reducing
the training time and providing a real-time decision system are the basic focus
points of the novel approaches. Deep Extreme Learning machines (Deep ELM)
classifier model is one of the fastest and effective way to meet fast
classification problems. In this study, Deep ELM model, its superiorities and
weaknesses are discussed, the problems that are more suitable for the
classifiers against Convolutional neural network based DL algorithms.
",2021-01-21T08:22:18Z,http://arxiv.org/abs/2101.10265v1,"Gokhan Altan, Yakup Kutlu"
Ranking Creative Language Characteristics in Small Data Scenarios,"  The ability to rank creative natural language provides an important general
tool for downstream language understanding and generation. However, current
deep ranking models require substantial amounts of labeled data that are
difficult and expensive to obtain for different domains, languages and creative
characteristics. A recent neural approach, the DirectRanker, promises to reduce
the amount of training data needed but its application to text isn't fully
explored. We therefore adapt the DirectRanker to provide a new deep model for
ranking creative language with small data. We compare DirectRanker with a
Bayesian approach, Gaussian process preference learning (GPPL), which has
previously been shown to work well with sparse data. Our experiments with
sparse training data show that while the performance of standard neural ranking
approaches collapses with small training datasets, DirectRanker remains
effective. We find that combining DirectRanker with GPPL increases performance
across different settings by leveraging the complementary benefits of both
models. Our combined approach outperforms the previous state-of-the-art on
humor and metaphor novelty tasks, increasing Spearman's $\rho$ by 14% and 16%
on average.
",2020-10-23T18:57:47Z,http://arxiv.org/abs/2010.12613v1,"Julia Siekiera, Marius Köppel, Edwin Simpson, Kevin Stowe, Iryna Gurevych, Stefan Kramer"
Deep Reinforcement Learning: An Overview,"  In recent years, a specific machine learning method called deep learning has
gained huge attraction, as it has obtained astonishing results in broad
applications such as pattern recognition, speech recognition, computer vision,
and natural language processing. Recent research has also been shown that deep
learning techniques can be combined with reinforcement learning methods to
learn useful representations for the problems with high dimensional raw data
input. This chapter reviews the recent advances in deep reinforcement learning
with a focus on the most used deep architectures such as autoencoders,
convolutional neural networks and recurrent neural networks which have
successfully been come together with the reinforcement learning framework.
",2018-06-23T02:18:26Z,http://arxiv.org/abs/1806.08894v1,"Seyed Sajad Mousavi, Michael Schukat, Enda Howley"
The Natural Language Decathlon: Multitask Learning as Question Answering,"  Deep learning has improved performance on many natural language processing
(NLP) tasks individually. However, general NLP models cannot emerge within a
paradigm that focuses on the particularities of a single metric, dataset, and
task. We introduce the Natural Language Decathlon (decaNLP), a challenge that
spans ten tasks: question answering, machine translation, summarization,
natural language inference, sentiment analysis, semantic role labeling,
zero-shot relation extraction, goal-oriented dialogue, semantic parsing, and
commonsense pronoun resolution. We cast all tasks as question answering over a
context. Furthermore, we present a new Multitask Question Answering Network
(MQAN) jointly learns all tasks in decaNLP without any task-specific modules or
parameters in the multitask setting. MQAN shows improvements in transfer
learning for machine translation and named entity recognition, domain
adaptation for sentiment analysis and natural language inference, and zero-shot
capabilities for text classification. We demonstrate that the MQAN's
multi-pointer-generator decoder is key to this success and performance further
improves with an anti-curriculum training strategy. Though designed for
decaNLP, MQAN also achieves state of the art results on the WikiSQL semantic
parsing task in the single-task setting. We also release code for procuring and
processing data, training and evaluating models, and reproducing all
experiments for decaNLP.
",2018-06-20T16:39:26Z,http://arxiv.org/abs/1806.08730v1,"Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, Richard Socher"
Transformers for scientific data: a pedagogical review for astronomers,"  The deep learning architecture associated with ChatGPT and related generative
AI products is known as transformers. Initially applied to Natural Language
Processing, transformers and the self-attention mechanism they exploit have
gained widespread interest across the natural sciences. The goal of this
pedagogical and informal review is to introduce transformers to scientists. The
review includes the mathematics underlying the attention mechanism, a
description of the original transformer architecture, and a section on
applications to time series and imaging data in astronomy. We include a
Frequently Asked Questions section for readers who are curious about generative
AI or interested in getting started with transformers for their research
problem.
",2023-10-18T16:02:32Z,http://arxiv.org/abs/2310.12069v2,"Dimitrios Tanoglidis, Bhuvnesh Jain, Helen Qu"
"Harnessing Transfer Learning from Swahili: Advancing Solutions for
  Comorian Dialects","  If today some African languages like Swahili have enough resources to develop
high-performing Natural Language Processing (NLP) systems, many other languages
spoken on the continent are still lacking such support. For these languages,
still in their infancy, several possibilities exist to address this critical
lack of data. Among them is Transfer Learning, which allows low-resource
languages to benefit from the good representation of other languages that are
similar to them. In this work, we adopt a similar approach, aiming to pioneer
NLP technologies for Comorian, a group of four languages or dialects belonging
to the Bantu family.
  Our approach is initially motivated by the hypothesis that if a human can
understand a different language from their native language with little or no
effort, it would be entirely possible to model this process on a machine. To
achieve this, we consider ways to construct Comorian datasets mixed with
Swahili. One thing to note here is that in terms of Swahili data, we only focus
on elements that are closest to Comorian by calculating lexical distances
between candidate and source data. We empirically test this hypothesis in two
use cases: Automatic Speech Recognition (ASR) and Machine Translation (MT). Our
MT model achieved ROUGE-1, ROUGE-2, and ROUGE-L scores of 0.6826, 0.42, and
0.6532, respectively, while our ASR system recorded a WER of 39.50\% and a CER
of 13.76\%. This research is crucial for advancing NLP in underrepresented
languages, with potential to preserve and promote Comorian linguistic heritage
in the digital age.
",2024-12-09T22:47:41Z,http://arxiv.org/abs/2412.12143v1,"Naira Abdou Mohamed, Zakarya Erraji, Abdessalam Bahafid, Imade Benelallam"
"Robustness Evaluation of Deep Unsupervised Learning Algorithms for
  Intrusion Detection Systems","  Recently, advances in deep learning have been observed in various fields,
including computer vision, natural language processing, and cybersecurity.
Machine learning (ML) has demonstrated its ability as a potential tool for
anomaly detection-based intrusion detection systems to build secure computer
networks. Increasingly, ML approaches are widely adopted than heuristic
approaches for cybersecurity because they learn directly from data. Data is
critical for the development of ML systems, and becomes potential targets for
attackers. Basically, data poisoning or contamination is one of the most common
techniques used to fool ML models through data. This paper evaluates the
robustness of six recent deep learning algorithms for intrusion detection on
contaminated data. Our experiments suggest that the state-of-the-art algorithms
used in this study are sensitive to data contamination and reveal the
importance of self-defense against data perturbation when developing novel
models, especially for intrusion detection systems.
",2022-06-25T02:28:39Z,http://arxiv.org/abs/2207.03576v2,"D'Jeff Kanda Nkashama, Arian Soltani, Jean-Charles Verdier, Marc Frappier, Pierre-Martin Tardif, Froduald Kabanza"
Deep Natural Language Processing for LinkedIn Search,"  Many search systems work with large amounts of natural language data, e.g.,
search queries, user profiles, and documents. Building a successful search
system requires a thorough understanding of textual data semantics, where deep
learning based natural language processing techniques (deep NLP) can be of
great help. In this paper, we introduce a comprehensive study for applying deep
NLP techniques to five representative tasks in search systems: query intent
prediction (classification), query tagging (sequential tagging), document
ranking (ranking), query auto completion (language modeling), and query
suggestion (sequence to sequence). We also introduce BERT pre-training as a
sixth task that can be applied to many of the other tasks. Through the model
design and experiments of the six tasks, readers can find answers to four
important questions: (1). When is deep NLP helpful/not helpful in search
systems? (2). How to address latency challenges? (3). How to ensure model
robustness? This work builds on existing efforts of LinkedIn search, and is
tested at scale on LinkedIn's commercial search engines. We believe our
experiences can provide useful insights for the industry and research
communities.
",2021-08-16T23:37:33Z,http://arxiv.org/abs/2108.13300v1,"Weiwei Guo, Xiaowei Liu, Sida Wang, Michaeel Kazi, Zhiwei Wang, Zhoutong Fu, Jun Jia, Liang Zhang, Huiji Gao, Bo Long"
Deep Learning on Graphs: A Survey,"  Deep learning has been shown to be successful in a number of domains, ranging
from acoustics, images, to natural language processing. However, applying deep
learning to the ubiquitous graph data is non-trivial because of the unique
characteristics of graphs. Recently, substantial research efforts have been
devoted to applying deep learning methods to graphs, resulting in beneficial
advances in graph analysis techniques. In this survey, we comprehensively
review the different types of deep learning methods on graphs. We divide the
existing methods into five categories based on their model architectures and
training strategies: graph recurrent neural networks, graph convolutional
networks, graph autoencoders, graph reinforcement learning, and graph
adversarial methods. We then provide a comprehensive overview of these methods
in a systematic manner mainly by following their development history. We also
analyze the differences and compositions of different methods. Finally, we
briefly outline the applications in which they have been used and discuss
potential future research directions.
",2018-12-11T03:16:57Z,http://arxiv.org/abs/1812.04202v3,"Ziwei Zhang, Peng Cui, Wenwu Zhu"
Towards More Robust Natural Language Understanding,"  Natural Language Understanding (NLU) is a branch of Natural Language
Processing (NLP) that uses intelligent computer software to understand texts
that encode human knowledge. Recent years have witnessed notable progress
across various NLU tasks with deep learning techniques, especially with
pretrained language models. Besides proposing more advanced model
architectures, constructing more reliable and trustworthy datasets also plays a
huge role in improving NLU systems, without which it would be impossible to
train a decent NLU model. It's worth noting that the human ability of
understanding natural language is flexible and robust. On the contrary, most of
existing NLU systems fail to achieve desirable performance on out-of-domain
data or struggle on handling challenging items (e.g., inherently ambiguous
items, adversarial items) in the real world. Therefore, in order to have NLU
models understand human language more effectively, it is expected to prioritize
the study on robust natural language understanding. In this thesis, we deem
that NLU systems are consisting of two components: NLU models and NLU datasets.
As such, we argue that, to achieve robust NLU, the model architecture/training
and the dataset are equally important. Specifically, we will focus on three NLU
tasks to illustrate the robustness problem in different NLU tasks and our
contributions (i.e., novel models and new datasets) to help achieve more robust
natural language understanding. Moving forward, the ultimate goal for robust
natural language understanding is to build NLU models which can behave humanly.
That is, it's expected that robust NLU systems are capable to transfer the
knowledge from training corpus to unseen documents more reliably and survive
when encountering challenging items even if the system doesn't know a priori of
users' inputs.
",2021-12-01T17:27:19Z,http://arxiv.org/abs/2112.02992v2,Xinliang Frederick Zhang
"Robotic Planning under Uncertainty in Spatiotemporal Environments in
  Expeditionary Science","  In the expeditionary sciences, spatiotemporally varying environments --
hydrothermal plumes, algal blooms, lava flows, or animal migrations -- are
ubiquitous. Mobile robots are uniquely well-suited to study these dynamic,
mesoscale natural environments. We formalize expeditionary science as a
sequential decision-making problem, modeled using the language of
partially-observable Markov decision processes (POMDPs). Solving the
expeditionary science POMDP under real-world constraints requires efficient
probabilistic modeling and decision-making in problems with complex dynamics
and observational models. Previous work in informative path planning, adaptive
sampling, and experimental design have shown compelling results, largely in
static environments, using data-driven models and information-based rewards.
However, these methodologies do not trivially extend to expeditionary science
in spatiotemporal environments: they generally do not make use of scientific
knowledge such as equations of state dynamics, they focus on information
gathering as opposed to scientific task execution, and they make use of
decision-making approaches that scale poorly to large, continuous problems with
long planning horizons and real-time operational constraints. In this work, we
discuss these and other challenges related to probabilistic modeling and
decision-making in expeditionary science, and present some of our preliminary
work that addresses these gaps. We ground our results in a real expeditionary
science deployment of an autonomous underwater vehicle (AUV) in the deep ocean
for hydrothermal vent discovery and characterization. Our concluding thoughts
highlight remaining work to be done, and the challenges that merit
consideration by the reinforcement learning and decision-making community.
",2022-06-03T02:04:15Z,http://arxiv.org/abs/2206.01364v1,"Victoria Preston, Genevieve Flaspohler, Anna P. M. Michel, John W. Fisher III, Nicholas Roy"
"Surf at MEDIQA 2019: Improving Performance of Natural Language Inference
  in the Clinical Domain by Adopting Pre-trained Language Model","  While deep learning techniques have shown promising results in many natural
language processing (NLP) tasks, it has not been widely applied to the clinical
domain. The lack of large datasets and the pervasive use of domain-specific
language (i.e. abbreviations and acronyms) in the clinical domain causes slower
progress in NLP tasks than that of the general NLP tasks. To fill this gap, we
employ word/subword-level based models that adopt large-scale data-driven
methods such as pre-trained language models and transfer learning in analyzing
text for the clinical domain. Empirical results demonstrate the superiority of
the proposed methods by achieving 90.6% accuracy in medical domain natural
language inference task. Furthermore, we inspect the independent strengths of
the proposed approaches in quantitative and qualitative manners. This analysis
will help researchers to select necessary components in building models for the
medical domain.
",2019-06-19T00:13:04Z,http://arxiv.org/abs/1906.07854v1,"Jiin Nam, Seunghyun Yoon, Kyomin Jung"
"Uncertainty over Uncertainty: Investigating the Assumptions,
  Annotations, and Text Measurements of Economic Policy Uncertainty","  Methods and applications are inextricably linked in science, and in
particular in the domain of text-as-data. In this paper, we examine one such
text-as-data application, an established economic index that measures economic
policy uncertainty from keyword occurrences in news. This index, which is shown
to correlate with firm investment, employment, and excess market returns, has
had substantive impact in both the private sector and academia. Yet, as we
revisit and extend the original authors' annotations and text measurements we
find interesting text-as-data methodological research questions: (1) Are
annotator disagreements a reflection of ambiguity in language? (2) Do
alternative text measurements correlate with one another and with measures of
external predictive validity? We find for this application (1) some annotator
disagreements of economic policy uncertainty can be attributed to ambiguity in
language, and (2) switching measurements from keyword-matching to supervised
machine learning classifiers results in low correlation, a concerning
implication for the validity of the index.
",2020-10-09T17:50:29Z,http://arxiv.org/abs/2010.04706v1,"Katherine A. Keith, Christoph Teichmann, Brendan O'Connor, Edgar Meij"
Deep Reinforcement Learning,"  We discuss deep reinforcement learning in an overview style. We draw a big
picture, filled with details. We discuss six core elements, six important
mechanisms, and twelve applications, focusing on contemporary work, and in
historical contexts. We start with background of artificial intelligence,
machine learning, deep learning, and reinforcement learning (RL), with
resources. Next we discuss RL core elements, including value function, policy,
reward, model, exploration vs. exploitation, and representation. Then we
discuss important mechanisms for RL, including attention and memory,
unsupervised learning, hierarchical RL, multi-agent RL, relational RL, and
learning to learn. After that, we discuss RL applications, including games,
robotics, natural language processing (NLP), computer vision, finance, business
management, healthcare, education, energy, transportation, computer systems,
and, science, engineering, and art. Finally we summarize briefly, discuss
challenges and opportunities, and close with an epilogue.
",2018-10-15T13:20:56Z,http://arxiv.org/abs/1810.06339v1,Yuxi Li
"To lemmatize or not to lemmatize: how word normalisation affects ELMo
  performance in word sense disambiguation","  We critically evaluate the widespread assumption that deep learning NLP
models do not require lemmatized input. To test this, we trained versions of
contextualised word embedding ELMo models on raw tokenized corpora and on the
corpora with word tokens replaced by their lemmas. Then, these models were
evaluated on the word sense disambiguation task. This was done for the English
and Russian languages.
  The experiments showed that while lemmatization is indeed not necessary for
English, the situation is different for Russian. It seems that for
rich-morphology languages, using lemmatized training and testing data yields
small but consistent improvements: at least for word sense disambiguation. This
means that the decisions about text pre-processing before training ELMo should
consider the linguistic nature of the language in question.
",2019-09-06T21:49:47Z,http://arxiv.org/abs/1909.03135v1,"Andrey Kutuzov, Elizaveta Kuzmenko"
"Leveraging Large Language Models for Knowledge-free Weak Supervision in
  Clinical Natural Language Processing","  The performance of deep learning-based natural language processing systems is
based on large amounts of labeled training data which, in the clinical domain,
are not easily available or affordable. Weak supervision and in-context
learning offer partial solutions to this issue, particularly using large
language models (LLMs), but their performance still trails traditional
supervised methods with moderate amounts of gold-standard data. In particular,
inferencing with LLMs is computationally heavy. We propose an approach
leveraging fine-tuning LLMs and weak supervision with virtually no domain
knowledge that still achieves consistently dominant performance. Using a
prompt-based approach, the LLM is used to generate weakly-labeled data for
training a downstream BERT model. The weakly supervised model is then further
fine-tuned on small amounts of gold standard data. We evaluate this approach
using Llama2 on three different n2c2 datasets. With no more than 10 gold
standard notes, our final BERT models weakly supervised by fine-tuned
Llama2-13B consistently outperformed out-of-the-box PubMedBERT by 4.7% to 47.9%
in F1 scores. With only 50 gold standard notes, our models achieved close
performance to fully fine-tuned systems.
",2024-06-10T18:34:48Z,http://arxiv.org/abs/2406.06723v1,"Enshuo Hsu, Kirk Roberts"
"RECKONition: a NLP-based system for Industrial Accidents at Work
  Prevention","  Extracting patterns and useful information from Natural Language datasets is
a challenging task, especially when dealing with data written in a language
different from English, like Italian. Machine and Deep Learning, together with
Natural Language Processing (NLP) techniques have widely spread and improved
lately, providing a plethora of useful methods to address both Supervised and
Unsupervised problems on textual information. We propose RECKONition, a
NLP-based system for Industrial Accidents at Work Prevention. RECKONition,
which is meant to provide Natural Language Understanding, Clustering and
Inference, is the result of a joint partnership with the Italian National
Institute for Insurance against Accidents at Work (INAIL). The obtained results
showed the ability to process textual data written in Italian describing
industrial accidents dynamics and consequences.
",2021-04-29T07:13:07Z,http://arxiv.org/abs/2104.14150v1,"Patrizia Agnello, Silvia M. Ansaldi, Emilia Lenzi, Alessio Mongelluzzo, Manuel Roveri"
"A Survey of Deep Graph Clustering: Taxonomy, Challenge, Application, and
  Open Resource","  Graph clustering, which aims to divide nodes in the graph into several
distinct clusters, is a fundamental yet challenging task. Benefiting from the
powerful representation capability of deep learning, deep graph clustering
methods have achieved great success in recent years. However, the corresponding
survey paper is relatively scarce, and it is imminent to make a summary of this
field. From this motivation, we conduct a comprehensive survey of deep graph
clustering. Firstly, we introduce formulaic definition, evaluation, and
development in this field. Secondly, the taxonomy of deep graph clustering
methods is presented based on four different criteria, including graph type,
network architecture, learning paradigm, and clustering method. Thirdly, we
carefully analyze the existing methods via extensive experiments and summarize
the challenges and opportunities from five perspectives, including graph data
quality, stability, scalability, discriminative capability, and unknown cluster
number. Besides, the applications of deep graph clustering methods in six
domains, including computer vision, natural language processing, recommendation
systems, social network analyses, bioinformatics, and medical science, are
presented. Last but not least, this paper provides open resource supports,
including 1) a collection
(\url{https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering}) of
state-of-the-art deep graph clustering methods (papers, codes, and datasets)
and 2) a unified framework
(\url{https://github.com/Marigoldwu/A-Unified-Framework-for-Deep-Attribute-Graph-Clustering})
of deep graph clustering. We hope this work can serve as a quick guide and help
researchers overcome challenges in this vibrant field.
",2022-11-23T11:31:11Z,http://arxiv.org/abs/2211.12875v4,"Yue Liu, Jun Xia, Sihang Zhou, Xihong Yang, Ke Liang, Chenchen Fan, Yan Zhuang, Stan Z. Li, Xinwang Liu, Kunlun He"
"Large language models can be zero-shot anomaly detectors for time
  series?","  Recent studies have shown the ability of large language models to perform a
variety of tasks, including time series forecasting. The flexible nature of
these models allows them to be used for many applications. In this paper, we
present a novel study of large language models used for the challenging task of
time series anomaly detection. This problem entails two aspects novel for LLMs:
the need for the model to identify part of the input sequence (or multiple
parts) as anomalous; and the need for it to work with time series data rather
than the traditional text input. We introduce sigllm, a framework for time
series anomaly detection using large language models. Our framework includes a
time-series-to-text conversion module, as well as end-to-end pipelines that
prompt language models to perform time series anomaly detection. We investigate
two paradigms for testing the abilities of large language models to perform the
detection task. First, we present a prompt-based detection method that directly
asks a language model to indicate which elements of the input are anomalies.
Second, we leverage the forecasting capability of a large language model to
guide the anomaly detection process. We evaluated our framework on 11 datasets
spanning various sources and 10 pipelines. We show that the forecasting method
significantly outperformed the prompting method in all 11 datasets with respect
to the F1 score. Moreover, while large language models are capable of finding
anomalies, state-of-the-art deep learning models are still superior in
performance, achieving results 30% better than large language models.
",2024-05-23T16:21:57Z,http://arxiv.org/abs/2405.14755v3,"Sarah Alnegheimish, Linh Nguyen, Laure Berti-Equille, Kalyan Veeramachaneni"
RPC Considered Harmful: Fast Distributed Deep Learning on RDMA,"  Deep learning emerges as an important new resource-intensive workload and has
been successfully applied in computer vision, speech, natural language
processing, and so on. Distributed deep learning is becoming a necessity to
cope with growing data and model sizes. Its computation is typically
characterized by a simple tensor data abstraction to model multi-dimensional
matrices, a data-flow graph to model computation, and iterative executions with
relatively frequent synchronizations, thereby making it substantially different
from Map/Reduce style distributed big data computation.
  RPC, commonly used as the communication primitive, has been adopted by
popular deep learning frameworks such as TensorFlow, which uses gRPC. We show
that RPC is sub-optimal for distributed deep learning computation, especially
on an RDMA-capable network. The tensor abstraction and data-flow graph, coupled
with an RDMA network, offers the opportunity to reduce the unnecessary overhead
(e.g., memory copy) without sacrificing programmability and generality. In
particular, from a data access point of view, a remote machine is abstracted
just as a ""device"" on an RDMA channel, with a simple memory interface for
allocating, reading, and writing memory regions. Our graph analyzer looks at
both the data flow graph and the tensors to optimize memory allocation and
remote data access using this interface. The result is up to 25 times speedup
in representative deep learning benchmarks against the standard gRPC in
TensorFlow and up to 169% improvement even against an RPC implementation
optimized for RDMA, leading to faster convergence in the training process.
",2018-05-22T07:42:33Z,http://arxiv.org/abs/1805.08430v1,"Jilong Xue, Youshan Miao, Cheng Chen, Ming Wu, Lintao Zhang, Lidong Zhou"
"Deep Learning and Machine Learning -- Natural Language Processing: From
  Theory to Application","  With a focus on natural language processing (NLP) and the role of large
language models (LLMs), we explore the intersection of machine learning, deep
learning, and artificial intelligence. As artificial intelligence continues to
revolutionize fields from healthcare to finance, NLP techniques such as
tokenization, text classification, and entity recognition are essential for
processing and understanding human language. This paper discusses advanced data
preprocessing techniques and the use of frameworks like Hugging Face for
implementing transformer-based models. Additionally, it highlights challenges
such as handling multilingual data, reducing bias, and ensuring model
robustness. By addressing key aspects of data processing and model fine-tuning,
this work aims to provide insights into deploying effective and ethically sound
AI solutions.
",2024-10-30T09:35:35Z,http://arxiv.org/abs/2411.05026v2,"Keyu Chen, Cheng Fei, Ziqian Bi, Junyu Liu, Benji Peng, Sen Zhang, Xuanhe Pan, Jiawei Xu, Jinlang Wang, Caitlyn Heqi Yin, Yichao Zhang, Pohsun Feng, Yizhu Wen, Tianyang Wang, Ming Li, Jintao Ren, Qian Niu, Silin Chen, Weiche Hsieh, Lawrence K. Q. Yan, Chia Xin Liang, Han Xu, Hong-Ming Tseng, Xinyuan Song, Ming Liu"
"Enhancing Essay Scoring with Adversarial Weights Perturbation and
  Metric-specific AttentionPooling","  The objective of this study is to improve automated feedback tools designed
for English Language Learners (ELLs) through the utilization of data science
techniques encompassing machine learning, natural language processing, and
educational data analytics. Automated essay scoring (AES) research has made
strides in evaluating written essays, but it often overlooks the specific needs
of English Language Learners (ELLs) in language development. This study
explores the application of BERT-related techniques to enhance the assessment
of ELLs' writing proficiency within AES.
  To address the specific needs of ELLs, we propose the use of DeBERTa, a
state-of-the-art neural language model, for improving automated feedback tools.
DeBERTa, pretrained on large text corpora using self-supervised learning,
learns universal language representations adaptable to various natural language
understanding tasks. The model incorporates several innovative techniques,
including adversarial training through Adversarial Weights Perturbation (AWP)
and Metric-specific AttentionPooling (6 kinds of AP) for each label in the
competition.
  The primary focus of this research is to investigate the impact of
hyperparameters, particularly the adversarial learning rate, on the performance
of the model. By fine-tuning the hyperparameter tuning process, including the
influence of 6AP and AWP, the resulting models can provide more accurate
evaluations of language proficiency and support tailored learning tasks for
ELLs. This work has the potential to significantly benefit ELLs by improving
their English language proficiency and facilitating their educational journey.
",2024-01-06T06:05:12Z,http://arxiv.org/abs/2401.05433v1,"Jiaxin Huang, Xinyu Zhao, Chang Che, Qunwei Lin, Bo Liu"
"Recent Trends in the Use of Deep Learning Models for Grammar Error
  Handling","  Grammar error handling (GEH) is an important topic in natural language
processing (NLP). GEH includes both grammar error detection and grammar error
correction. Recent advances in computation systems have promoted the use of
deep learning (DL) models for NLP problems such as GEH. In this survey we focus
on two main DL approaches for GEH: neural machine translation models and editor
models. We describe the three main stages of the pipeline for these models:
data preparation, training, and inference. Additionally, we discuss different
techniques to improve the performance of these models at each stage of the
pipeline. We compare the performance of different models and conclude with
proposed future directions.
",2020-09-04T18:50:13Z,http://arxiv.org/abs/2009.02358v1,"Mina Naghshnejad, Tarun Joshi, Vijayan N. Nair"
"Potrika: Raw and Balanced Newspaper Datasets in the Bangla Language with
  Eight Topics and Five Attributes","  Knowledge is central to human and scientific developments. Natural Language
Processing (NLP) allows automated analysis and creation of knowledge. Data is a
crucial NLP and machine learning ingredient. The scarcity of open datasets is a
well-known problem in machine and deep learning research. This is very much the
case for textual NLP datasets in English and other major world languages. For
the Bangla language, the situation is even more challenging and the number of
large datasets for NLP research is practically nil. We hereby present Potrika,
a large single-label Bangla news article textual dataset curated for NLP
research from six popular online news portals in Bangladesh (Jugantor,
Jaijaidin, Ittefaq, Kaler Kontho, Inqilab, and Somoyer Alo) for the period
2014-2020. The articles are classified into eight distinct categories
(National, Sports, International, Entertainment, Economy, Education, Politics,
and Science \& Technology) providing five attributes (News Article, Category,
Headline, Publication Date, and Newspaper Source). The raw dataset contains
185.51 million words and 12.57 million sentences contained in 664,880 news
articles. Moreover, using NLP augmentation techniques, we create from the raw
(unbalanced) dataset another (balanced) dataset comprising 320,000 news
articles with 40,000 articles in each of the eight news categories. Potrika
contains both the datasets (raw and balanced) to suit a wide range of NLP
research. By far, to the best of our knowledge, Potrika is the largest and the
most extensive dataset for news classification.
",2022-10-17T19:37:42Z,http://arxiv.org/abs/2210.09389v1,"Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood"
"Deep learning model for Mongolian Citizens Feedback Analysis using Word
  Vector Embeddings","  A large amount of feedback was collected over the years. Many feedback
analysis models have been developed focusing on the English language.
Recognizing the concept of feedback is challenging and crucial in languages
which do not have applicable corpus and tools employed in Natural Language
Processing (i.e., vocabulary corpus, sentence structure rules, etc). However,
in this paper, we study a feedback classification in Mongolian language using
two different word embeddings for deep learning. We compare the results of
proposed approaches. We use feedback data in Cyrillic collected from 2012-2018.
The result indicates that word embeddings using their own dataset improve the
deep learning based proposed model with the best accuracy of 80.1% and 82.7%
for two classification tasks.
",2023-02-23T14:49:31Z,http://arxiv.org/abs/2302.12069v1,"Zolzaya Dashdorj, Tsetsentsengel Munkhbayar, Stanislav Grigorev"
A Perspective on Deep Learning for Molecular Modeling and Simulations,"  Deep learning is transforming many areas in science, and it has great
potential in modeling molecular systems. However, unlike the mature deployment
of deep learning in computer vision and natural language processing, its
development in molecular modeling and simulations is still at an early stage,
largely because the inductive biases of molecules are completely different from
those of images or texts. Footed on these differences, we first reviewed the
limitations of traditional deep learning models from the perspective of
molecular physics, and wrapped up some relevant technical advancement at the
interface between molecular modeling and deep learning. We do not focus merely
on the ever more complex neural network models, instead, we emphasize the
theories and ideas behind modern deep learning. We hope that transacting these
ideas into molecular modeling will create new opportunities. For this purpose,
we summarized several representative applications, ranging from supervised to
unsupervised and reinforcement learning, and discussed their connections with
the emerging trends in deep learning. Finally, we outlook promising directions
which may help address the existing issues in the current framework of deep
molecular modeling.
",2020-04-25T22:58:25Z,http://arxiv.org/abs/2004.13011v1,"Jun Zhang, Yao-Kun Lei, Zhen Zhang, Junhan Chang, Maodong Li, Xu Han, Lijiang Yang, Yi Isaac Yang, Yi Qin Gao"
"Natural Language Processing Methods to Identify Oncology Patients at
  High Risk for Acute Care with Clinical Notes","  Clinical notes are an essential component of a health record. This paper
evaluates how natural language processing (NLP) can be used to identify the
risk of acute care use (ACU) in oncology patients, once chemotherapy starts.
Risk prediction using structured health data (SHD) is now standard, but
predictions using free-text formats are complex. This paper explores the use of
free-text notes for the prediction of ACU instead of SHD. Deep Learning models
were compared to manually engineered language features. Results show that SHD
models minimally outperform NLP models; an l1-penalised logistic regression
with SHD achieved a C-statistic of 0.748 (95%-CI: 0.735, 0.762), while the same
model with language features achieved 0.730 (95%-CI: 0.717, 0.745) and a
transformer-based model achieved 0.702 (95%-CI: 0.688, 0.717). This paper shows
how language models can be used in clinical applications and underlines how
risk bias is different for diverse patient groups, even using only free-text
data.
",2022-09-28T06:31:19Z,http://arxiv.org/abs/2209.13860v2,"Claudio Fanconi, Marieke van Buchem, Tina Hernandez-Boussard"
"A Legal Framework for Natural Language Processing Model Training in
  Portugal","  Recent advances in deep learning have promoted the advent of many
computational systems capable of performing intelligent actions that, until
then, were restricted to the human intellect. In the particular case of human
languages, these advances allowed the introduction of applications like ChatGPT
that are capable of generating coherent text without being explicitly
programmed to do so. Instead, these models use large volumes of textual data to
learn meaningful representations of human languages. Associated with these
advances, concerns about copyright and data privacy infringements caused by
these applications have emerged. Despite these concerns, the pace at which new
natural language processing applications continued to be developed largely
outperformed the introduction of new regulations. Today, communication barriers
between legal experts and computer scientists motivate many unintentional legal
infringements during the development of such applications. In this paper, a
multidisciplinary team intends to bridge this communication gap and promote
more compliant Portuguese NLP research by presenting a series of everyday NLP
use cases, while highlighting the Portuguese legislation that may arise during
its development.
",2024-05-01T14:18:50Z,http://arxiv.org/abs/2405.00536v1,"Rúben Almeida, Evelin Amorim"
A Robust Hybrid Approach for Textual Document Classification,"  Text document classification is an important task for diverse natural
language processing based applications. Traditional machine learning approaches
mainly focused on reducing dimensionality of textual data to perform
classification. This although improved the overall classification accuracy, the
classifiers still faced sparsity problem due to lack of better data
representation techniques. Deep learning based text document classification, on
the other hand, benefitted greatly from the invention of word embeddings that
have solved the sparsity problem and researchers focus mainly remained on the
development of deep architectures. Deeper architectures, however, learn some
redundant features that limit the performance of deep learning based solutions.
In this paper, we propose a two stage text document classification methodology
which combines traditional feature engineering with automatic feature
engineering (using deep learning). The proposed methodology comprises a filter
based feature selection (FSE) algorithm followed by a deep convolutional neural
network. This methodology is evaluated on the two most commonly used public
datasets, i.e., 20 Newsgroups data and BBC news data. Evaluation results reveal
that the proposed methodology outperforms the state-of-the-art of both the
(traditional) machine learning and deep learning based text document
classification methodologies with a significant margin of 7.7% on 20 Newsgroups
and 6.6% on BBC news datasets.
",2019-09-12T06:39:07Z,http://arxiv.org/abs/1909.05478v1,"Muhammad Nabeel Asim, Muhammad Usman Ghani Khan, Muhammad Imran Malik, Andreas Dengel, Sheraz Ahmed"
"Text-to-Battery Recipe: A language modeling-based protocol for automatic
  battery recipe extraction and retrieval","  Recent studies have increasingly applied natural language processing (NLP) to
automatically extract experimental research data from the extensive battery
materials literature. Despite the complex process involved in battery
manufacturing -- from material synthesis to cell assembly -- there has been no
comprehensive study systematically organizing this information. In response, we
propose a language modeling-based protocol, Text-to-Battery Recipe (T2BR), for
the automatic extraction of end-to-end battery recipes, validated using a case
study on batteries containing LiFePO4 cathode material. We report machine
learning-based paper filtering models, screening 2,174 relevant papers from the
keyword-based search results, and unsupervised topic models to identify 2,876
paragraphs related to cathode synthesis and 2,958 paragraphs related to cell
assembly. Then, focusing on the two topics, two deep learning-based named
entity recognition models are developed to extract a total of 30 entities --
including precursors, active materials, and synthesis methods -- achieving F1
scores of 88.18% and 94.61%. The accurate extraction of entities enables the
systematic generation of 165 end-toend recipes of LiFePO4 batteries. Our
protocol and results offer valuable insights into specific trends, such as
associations between precursor materials and synthesis methods, or combinations
between different precursor materials. We anticipate that our findings will
serve as a foundational knowledge base for facilitating battery-recipe
information retrieval. The proposed protocol will significantly accelerate the
review of battery material literature and catalyze innovations in battery
design and development.
",2024-07-22T08:15:02Z,http://arxiv.org/abs/2407.15459v1,"Daeun Lee, Jaewoong Choi, Hiroshi Mizuseki, Byungju Lee"
"Multimodal Quantum Natural Language Processing: A Novel Framework for
  using Quantum Methods to Analyse Real Data","  Despite significant advances in quantum computing across various domains,
research on applying quantum approaches to language compositionality - such as
modeling linguistic structures and interactions - remains limited. This gap
extends to the integration of quantum language data with real-world data from
sources like images, video, and audio. This thesis explores how quantum
computational methods can enhance the compositional modeling of language
through multimodal data integration. Specifically, it advances Multimodal
Quantum Natural Language Processing (MQNLP) by applying the Lambeq toolkit to
conduct a comparative analysis of four compositional models and evaluate their
influence on image-text classification tasks. Results indicate that
syntax-based models, particularly DisCoCat and TreeReader, excel in effectively
capturing grammatical structures, while bag-of-words and sequential models
struggle due to limited syntactic awareness. These findings underscore the
potential of quantum methods to enhance language modeling and drive
breakthroughs as quantum technology evolves.
",2024-10-29T19:03:43Z,http://arxiv.org/abs/2411.05023v1,Hala Hawashin
"Feature Extraction of Text for Deep Learning Algorithms: Application on
  Fake News Detection","  Feature extraction is an important process of machine learning and deep
learning, as the process make algorithms function more efficiently, and also
accurate. In natural language processing used in deception detection such as
fake news detection, several ways of feature extraction in statistical aspect
had been introduced (e.g. N-gram). In this research, it will be shown that by
using deep learning algorithms and alphabet frequencies of the original text of
a news without any information about the sequence of the alphabet can actually
be used to classify fake news and trustworthy ones in high accuracy (85\%). As
this pre-processing method makes the data notably compact but also include the
feature that is needed for the classifier, it seems that alphabet frequencies
contains some useful features for understanding complex context or meaning of
the original text.
",2020-10-12T07:43:01Z,http://arxiv.org/abs/2010.05496v2,HyeonJun Kim
A streamable large-scale clinical EEG dataset for Deep Learning,"  Deep Learning has revolutionized various fields, including Computer Vision,
Natural Language Processing, as well as Biomedical research. Within the field
of neuroscience, specifically in electrophysiological neuroimaging, researchers
are starting to explore leveraging deep learning to make predictions on their
data without extensive feature engineering. The availability of large-scale
datasets is a crucial aspect of allowing the experimentation of Deep Learning
models. We are publishing the first large-scale clinical EEG dataset that
simplifies data access and management for Deep Learning. This dataset contains
eyes-closed EEG data prepared from a collection of 1,574 juvenile participants
from the Healthy Brain Network. We demonstrate a use case integrating this
framework, and discuss why providing such neuroinformatics infrastructure to
the community is critical for future scientific discoveries.
",2022-03-04T20:05:50Z,http://arxiv.org/abs/2203.02552v2,"Dung Truong, Manisha Sinha, Kannan Umadevi Venkataraju, Michael Milham, Arnaud Delorme"
"A Natural Language Processing Pipeline for Detecting Informal Data
  References in Academic Literature","  Discovering authoritative links between publications and the datasets that
they use can be a labor-intensive process. We introduce a natural language
processing pipeline that retrieves and reviews publications for informal
references to research datasets, which complements the work of data librarians.
We first describe the components of the pipeline and then apply it to expand an
authoritative bibliography linking thousands of social science studies to the
data-related publications in which they are used. The pipeline increases recall
for literature to review for inclusion in data-related collections of
publications and makes it possible to detect informal data references at scale.
We contribute (1) a novel Named Entity Recognition (NER) model that reliably
detects informal data references and (2) a dataset connecting items from social
science literature with datasets they reference. Together, these contributions
enable future work on data reference, data citation networks, and data reuse.
",2022-05-23T22:06:46Z,http://arxiv.org/abs/2205.11651v1,"Sara Lafia, Lizhou Fan, Libby Hemphill"
Deep Latent-Variable Models for Text Generation,"  Text generation aims to produce human-like natural language output for
down-stream tasks. It covers a wide range of applications like machine
translation, document summarization, dialogue generation and so on. Recently
deep neural network-based end-to-end architectures have been widely adopted.
The end-to-end approach conflates all sub-modules, which used to be designed by
complex handcrafted rules, into a holistic encode-decode architecture. Given
enough training data, it is able to achieve state-of-the-art performance yet
avoiding the need of language/domain-dependent knowledge. Nonetheless, deep
learning models are known to be extremely data-hungry, and text generated from
them usually suffer from low diversity, interpretability and controllability.
As a result, it is difficult to trust the output from them in real-life
applications. Deep latent-variable models, by specifying the probabilistic
distribution over an intermediate latent process, provide a potential way of
addressing these problems while maintaining the expressive power of deep neural
networks. This dissertation presents how deep latent-variable models can
improve over the standard encoder-decoder model for text generation.
",2022-03-03T23:06:39Z,http://arxiv.org/abs/2203.02055v1,Xiaoyu Shen
"Development of deep learning algorithms to categorize free-text notes
  pertaining to diabetes: convolution neural networks achieve higher accuracy
  than support vector machines","  Health professionals can use natural language processing (NLP) technologies
when reviewing electronic health records (EHR). Machine learning free-text
classifiers can help them identify problems and make critical decisions. We aim
to develop deep learning neural network algorithms that identify EHR progress
notes pertaining to diabetes and validate the algorithms at two institutions.
The data used are 2,000 EHR progress notes retrieved from patients with
diabetes and all notes were annotated manually as diabetic or non-diabetic.
Several deep learning classifiers were developed, and their performances were
evaluated with the area under the ROC curve (AUC). The convolutional neural
network (CNN) model with a separable convolution layer accurately identified
diabetes-related notes in the Brigham and Womens Hospital testing set with the
highest AUC of 0.975. Deep learning classifiers can be used to identify EHR
progress notes pertaining to diabetes. In particular, the CNN-based classifier
can achieve a higher AUC than an SVM-based classifier.
",2018-09-16T04:21:38Z,http://arxiv.org/abs/1809.05814v1,"Boyi Yang, Adam Wright"
Proactive Schemes: A Survey of Adversarial Attacks for Social Good,"  Adversarial attacks in computer vision exploit the vulnerabilities of machine
learning models by introducing subtle perturbations to input data, often
leading to incorrect predictions or classifications. These attacks have evolved
in sophistication with the advent of deep learning, presenting significant
challenges in critical applications, which can be harmful for society. However,
there is also a rich line of research from a transformative perspective that
leverages adversarial techniques for social good. Specifically, we examine the
rise of proactive schemes-methods that encrypt input data using additional
signals termed templates, to enhance the performance of deep learning models.
By embedding these imperceptible templates into digital media, proactive
schemes are applied across various applications, from simple image enhancements
to complicated deep learning frameworks to aid performance, as compared to the
passive schemes, which don't change the input data distribution for their
framework. The survey delves into the methodologies behind these proactive
schemes, the encryption and learning processes, and their application to modern
computer vision and natural language processing applications. Additionally, it
discusses the challenges, potential vulnerabilities, and future directions for
proactive schemes, ultimately highlighting their potential to foster the
responsible and secure advancement of deep learning technologies.
",2024-09-24T22:31:56Z,http://arxiv.org/abs/2409.16491v1,"Vishal Asnani, Xi Yin, Xiaoming Liu"
Towards Zero-Shot Knowledge Distillation for Natural Language Processing,"  Knowledge Distillation (KD) is a common knowledge transfer algorithm used for
model compression across a variety of deep learning based natural language
processing (NLP) solutions. In its regular manifestations, KD requires access
to the teacher's training data for knowledge transfer to the student network.
However, privacy concerns, data regulations and proprietary reasons may prevent
access to such data. We present, to the best of our knowledge, the first work
on Zero-Shot Knowledge Distillation for NLP, where the student learns from the
much larger teacher without any task specific data. Our solution combines out
of domain data and adversarial training to learn the teacher's output
distribution. We investigate six tasks from the GLUE benchmark and demonstrate
that we can achieve between 75% and 92% of the teacher's classification score
(accuracy or F1) while compressing the model 30 times.
",2020-12-31T08:16:29Z,http://arxiv.org/abs/2012.15495v1,"Ahmad Rashid, Vasileios Lioutas, Abbas Ghaddar, Mehdi Rezagholizadeh"
"MatSci-NLP: Evaluating Scientific Language Models on Materials Science
  Language Tasks Using Text-to-Schema Modeling","  We present MatSci-NLP, a natural language benchmark for evaluating the
performance of natural language processing (NLP) models on materials science
text. We construct the benchmark from publicly available materials science text
data to encompass seven different NLP tasks, including conventional NLP tasks
like named entity recognition and relation classification, as well as NLP tasks
specific to materials science, such as synthesis action retrieval which relates
to creating synthesis procedures for materials. We study various BERT-based
models pretrained on different scientific text corpora on MatSci-NLP to
understand the impact of pretraining strategies on understanding materials
science text. Given the scarcity of high-quality annotated data in the
materials science domain, we perform our fine-tuning experiments with limited
training data to encourage the generalize across MatSci-NLP tasks. Our
experiments in this low-resource training setting show that language models
pretrained on scientific text outperform BERT trained on general text. MatBERT,
a model pretrained specifically on materials science journals, generally
performs best for most tasks. Moreover, we propose a unified text-to-schema for
multitask learning on \benchmark and compare its performance with traditional
fine-tuning methods. In our analysis of different training methods, we find
that our proposed text-to-schema methods inspired by question-answering
consistently outperform single and multitask NLP fine-tuning methods. The code
and datasets are publicly available at
\url{https://github.com/BangLab-UdeM-Mila/NLP4MatSci-ACL23}.
",2023-05-14T22:01:24Z,http://arxiv.org/abs/2305.08264v1,"Yu Song, Santiago Miret, Bang Liu"
"Requirement Formalisation using Natural Language Processing and Machine
  Learning: A Systematic Review","  Improvement of software development methodologies attracts developers to
automatic Requirement Formalisation (RF) in the Requirement Engineering (RE)
field. The potential advantages by applying Natural Language Processing (NLP)
and Machine Learning (ML) in reducing the ambiguity and incompleteness of
requirement written in natural languages is reported in different studies. The
goal of this paper is to survey and classify existing work on NLP and ML for
RF, identifying challenges in this domain and providing promising future
research directions. To achieve this, we conducted a systematic literature
review to outline the current state-of-the-art of NLP and ML techniques in RF
by selecting 257 papers from common used libraries. The search result is
filtered by defining inclusion and exclusion criteria and 47 relevant studies
between 2012 and 2022 are selected. We found that heuristic NLP approaches are
the most common NLP techniques used for automatic RF, primary operating on
structured and semi-structured data. This study also revealed that Deep
Learning (DL) technique are not widely used, instead classical ML techniques
are predominant in the surveyed studies. More importantly, we identified the
difficulty of comparing the performance of different approaches due to the lack
of standard benchmark cases for RF.
",2023-03-18T17:36:21Z,http://arxiv.org/abs/2303.13365v1,"Shekoufeh Kolahdouz-Rahimi, Kevin Lano, Chenghua Lin"
"Automatic Detection of Industry Sectors in Legal Articles Using Machine
  Learning Approaches","  The ability to automatically identify industry sector coverage in articles on
legal developments, or any kind of news articles for that matter, can bring
plentiful of benefits both to the readers and the content creators themselves.
By having articles tagged based on industry coverage, readers from all around
the world would be able to get to legal news that are specific to their region
and professional industry. Simultaneously, writers would benefit from
understanding which industries potentially lack coverage or which industries
readers are currently mostly interested in and thus, they would focus their
writing efforts towards more inclusive and relevant legal news coverage. In
this paper, a Machine Learning-powered industry analysis approach which
combined Natural Language Processing (NLP) with Statistical and Machine
Learning (ML) techniques was investigated. A dataset consisting of over 1,700
annotated legal articles was created for the identification of six industry
sectors. Text and legal based features were extracted from the text. Both
traditional ML methods (e.g. gradient boosting machine algorithms, and
decision-tree based algorithms) and deep neural network (e.g. transformer
models) were applied for performance comparison of predictive models. The
system achieved promising results with area under the receiver operating
characteristic curve scores above 0.90 and F-scores above 0.81 with respect to
the six industry sectors. The experimental results show that the suggested
automated industry analysis which employs ML techniques allows the processing
of large collections of text data in an easy, efficient, and scalable way.
Traditional ML methods perform better than deep neural networks when only a
small and domain-specific training data is available for the study.
",2023-03-08T12:41:56Z,http://arxiv.org/abs/2303.05387v1,"Hui Yang, Stella Hadjiantoni, Yunfei Long, Ruta Petraityte, Berthold Lausen"
"Is Word Segmentation Necessary for Deep Learning of Chinese
  Representations?","  Segmenting a chunk of text into words is usually the first step of processing
Chinese text, but its necessity has rarely been explored. In this paper, we ask
the fundamental question of whether Chinese word segmentation (CWS) is
necessary for deep learning-based Chinese Natural Language Processing. We
benchmark neural word-based models which rely on word segmentation against
neural char-based models which do not involve word segmentation in four
end-to-end NLP benchmark tasks: language modeling, machine translation,
sentence matching/paraphrase and text classification. Through direct
comparisons between these two types of models, we find that char-based models
consistently outperform word-based models. Based on these observations, we
conduct comprehensive experiments to study why word-based models underperform
char-based models in these deep learning-based NLP tasks. We show that it is
because word-based models are more vulnerable to data sparsity and the presence
of out-of-vocabulary (OOV) words, and thus more prone to overfitting. We hope
this paper could encourage researchers in the community to rethink the
necessity of word segmentation in deep learning-based Chinese Natural Language
Processing. \footnote{Yuxian Meng and Xiaoya Li contributed equally to this
paper.}
",2019-05-14T11:39:43Z,http://arxiv.org/abs/1905.05526v2,"Xiaoya Li, Yuxian Meng, Xiaofei Sun, Qinghong Han, Arianna Yuan, Jiwei Li"
Emergence of Numeric Concepts in Multi-Agent Autonomous Communication,"  With the rapid development of deep learning, most of current state-of-the-art
techniques in natural langauge processing are based on deep learning models
trained with argescaled static textual corpora. However, we human beings learn
and understand in a different way. Thus, grounded language learning argues that
models need to learn and understand language by the experience and perceptions
obtained by interacting with enviroments, like how humans do. With the help of
deep reinforcement learning techniques, there are already lots of works
focusing on facilitating the emergence of communication protocols that have
compositionalities like natural languages among computational agents
population. Unlike these works, we, on the other hand, focus on the numeric
concepts which correspond to abstractions in cognition and function words in
natural language. Based on a specifically designed language game, we verify
that computational agents are capable of transmitting numeric concepts during
autonomous communication, and the emergent communication protocols can reflect
the underlying structure of meaning space. Although their encodeing method is
not compositional like natural languages from a perspective of human beings,
the emergent languages can be generalised to unseen inputs and, more
importantly, are easier for models to learn. Besides, iterated learning can
help further improving the compositionality of the emergent languages, under
the measurement of topological similarity. Furthermore, we experiment another
representation method, i.e. directly encode numerals into concatenations of
one-hot vectors, and find that the emergent languages would become
compositional like human natural languages. Thus, we argue that there are 2
important factors for the emergence of compositional languages.
",2019-11-04T09:58:23Z,http://arxiv.org/abs/1911.01098v1,Shangmin Guo
"Compressible Dynamics in Deep Overparameterized Low-Rank Learning &
  Adaptation","  While overparameterization in machine learning models offers great benefits
in terms of optimization and generalization, it also leads to increased
computational requirements as model sizes grow. In this work, we show that by
leveraging the inherent low-dimensional structures of data and compressible
dynamics within the model parameters, we can reap the benefits of
overparameterization without the computational burdens. In practice, we
demonstrate the effectiveness of this approach for deep low-rank matrix
completion as well as fine-tuning language models. Our approach is grounded in
theoretical findings for deep overparameterized low-rank matrix recovery, where
we show that the learning dynamics of each weight matrix are confined to an
invariant low-dimensional subspace. Consequently, we can construct and train
compact, highly compressed factorizations possessing the same benefits as their
overparameterized counterparts. In the context of deep matrix completion, our
technique substantially improves training efficiency while retaining the
advantages of overparameterization. For language model fine-tuning, we propose
a method called ""Deep LoRA"", which improves the existing low-rank adaptation
(LoRA) technique, leading to reduced overfitting and a simplified
hyperparameter setup, while maintaining comparable efficiency. We validate the
effectiveness of Deep LoRA on natural language tasks, particularly when
fine-tuning with limited data. Our code is available at
https://github.com/cjyaras/deep-lora-transformers.
",2024-06-06T14:29:49Z,http://arxiv.org/abs/2406.04112v2,"Can Yaras, Peng Wang, Laura Balzano, Qing Qu"
Word and Document Embeddings based on Neural Network Approaches,"  Data representation is a fundamental task in machine learning. The
representation of data affects the performance of the whole machine learning
system. In a long history, the representation of data is done by feature
engineering, and researchers aim at designing better features for specific
tasks. Recently, the rapid development of deep learning and representation
learning has brought new inspiration to various domains.
  In natural language processing, the most widely used feature representation
is the Bag-of-Words model. This model has the data sparsity problem and cannot
keep the word order information. Other features such as part-of-speech tagging
or more complex syntax features can only fit for specific tasks in most cases.
This thesis focuses on word representation and document representation. We
compare the existing systems and present our new model.
  First, for generating word embeddings, we make comprehensive comparisons
among existing word embedding models. In terms of theory, we figure out the
relationship between the two most important models, i.e., Skip-gram and GloVe.
In our experiments, we analyze three key points in generating word embeddings,
including the model construction, the training corpus and parameter design. We
evaluate word embeddings with three types of tasks, and we argue that they
cover the existing use of word embeddings. Through theory and practical
experiments, we present some guidelines for how to generate a good word
embedding.
  Second, in Chinese character or word representation. We introduce the joint
training of Chinese character and word. ...
  Third, for document representation, we analyze the existing document
representation models, including recursive NNs, recurrent NNs and convolutional
NNs. We point out the drawbacks of these models and present our new model, the
recurrent convolutional neural networks. ...
",2016-11-18T03:21:28Z,http://arxiv.org/abs/1611.05962v1,Siwei Lai
Deep Learning Models to Study Sentence Comprehension in the Human Brain,"  Recent artificial neural networks that process natural language achieve
unprecedented performance in tasks requiring sentence-level understanding. As
such, they could be interesting models of the integration of linguistic
information in the human brain. We review works that compare these artificial
language models with human brain activity and we assess the extent to which
this approach has improved our understanding of the neural processes involved
in natural language comprehension. Two main results emerge. First, the neural
representation of word meaning aligns with the context-dependent, dense word
vectors used by the artificial neural networks. Second, the processing
hierarchy that emerges within artificial neural networks broadly matches the
brain, but is surprisingly inconsistent across studies. We discuss current
challenges in establishing artificial neural networks as process models of
natural language comprehension. We suggest exploiting the highly structured
representational geometry of artificial neural networks when mapping
representations to brain data.
",2023-01-16T10:31:25Z,http://arxiv.org/abs/2301.06340v2,"Sophie Arana, Jacques Pesnot Lerousseau, Peter Hagoort"
Fine-tuning BERT-based models for Plant Health Bulletin Classification,"  In the era of digitization, different actors in agriculture produce numerous
data. Such data contains already latent historical knowledge in the domain.
This knowledge enables us to precisely study natural hazards within global or
local aspects, and then improve the risk prevention tasks and augment the
yield, which helps to tackle the challenge of growing population and changing
alimentary habits. In particular, French Plants Health Bulletins (BSV, for its
name in French Bulletin de Sant{\'e} du V{\'e}g{\'e}tal) give information about
the development stages of phytosanitary risks in agricultural production.
However, they are written in natural language, thus, machines and human cannot
exploit them as efficiently as it could be. Natural language processing (NLP)
technologies aim to automatically process and analyze large amounts of natural
language data. Since the 2010s, with the increases in computational power and
parallelization, representation learning and deep learning methods became
widespread in NLP. Recent advancements Bidirectional Encoder Representations
from Transformers (BERT) inspire us to rethink of knowledge representation and
natural language understanding in plant health management domain. The goal in
this work is to propose a BERT-based approach to automatically classify the BSV
to make their data easily indexable. We sampled 200 BSV to finetune the
pretrained BERT language models and classify them as pest or/and disease and we
show preliminary results.
",2021-01-29T08:14:35Z,http://arxiv.org/abs/2102.00838v1,"Shufan Jiang, Rafael Angarita, Stephane Cormier, Francis Rousseaux"
"Adversarial Robustness for Machine Learning Cyber Defenses Using Log
  Data","  There has been considerable and growing interest in applying machine learning
for cyber defenses. One promising approach has been to apply natural language
processing techniques to analyze logs data for suspicious behavior. A natural
question arises to how robust these systems are to adversarial attacks. Defense
against sophisticated attack is of particular concern for cyber defenses. In
this paper, we develop a testing framework to evaluate adversarial robustness
of machine learning cyber defenses, particularly those focused on log data. Our
framework uses techniques from deep reinforcement learning and adversarial
natural language processing. We validate our framework using a publicly
available dataset and demonstrate that our adversarial attack does succeed
against the target systems, revealing a potential vulnerability. We apply our
framework to analyze the influence of different levels of dropout
regularization and find that higher dropout levels increases robustness.
Moreover 90% dropout probability exhibited the highest level of robustness by a
significant margin, which suggests unusually high dropout may be necessary to
properly protect against adversarial attacks.
",2020-07-29T17:51:29Z,http://arxiv.org/abs/2007.14983v1,"Kai Steverson, Jonathan Mullin, Metin Ahiskali"
"Deep Cross-Modal Correlation Learning for Audio and Lyrics in Music
  Retrieval","  Little research focuses on cross-modal correlation learning where temporal
structures of different data modalities such as audio and lyrics are taken into
account. Stemming from the characteristic of temporal structures of music in
nature, we are motivated to learn the deep sequential correlation between audio
and lyrics. In this work, we propose a deep cross-modal correlation learning
architecture involving two-branch deep neural networks for audio modality and
text modality (lyrics). Different modality data are converted to the same
canonical space where inter modal canonical correlation analysis is utilized as
an objective function to calculate the similarity of temporal structures. This
is the first study on understanding the correlation between language and music
audio through deep architectures for learning the paired temporal correlation
of audio and lyrics. Pre-trained Doc2vec model followed by fully-connected
layers (fully-connected deep neural network) is used to represent lyrics. Two
significant contributions are made in the audio branch, as follows: i)
pre-trained CNN followed by fully-connected layers is investigated for
representing music audio. ii) We further suggest an end-to-end architecture
that simultaneously trains convolutional layers and fully-connected layers to
better learn temporal structures of music audio. Particularly, our end-to-end
deep architecture contains two properties: simultaneously implementing feature
learning and cross-modal correlation learning, and learning joint
representation by considering temporal structures. Experimental results, using
audio to retrieve lyrics or using lyrics to retrieve audio, verify the
effectiveness of the proposed deep correlation learning architectures in
cross-modal music retrieval.
",2017-11-24T14:21:46Z,http://arxiv.org/abs/1711.08976v2,"Yi Yu, Suhua Tang, Francisco Raposo, Lei Chen"
"Arabic Speech Emotion Recognition Employing Wav2vec2.0 and HuBERT Based
  on BAVED Dataset","  Recently, there have been tremendous research outcomes in the fields of
speech recognition and natural language processing. This is due to the
well-developed multi-layers deep learning paradigms such as wav2vec2.0,
Wav2vecU, WavBERT, and HuBERT that provide better representation learning and
high information capturing. Such paradigms run on hundreds of unlabeled data,
then fine-tuned on a small dataset for specific tasks. This paper introduces a
deep learning constructed emotional recognition model for Arabic speech
dialogues. The developed model employs the state of the art audio
representations include wav2vec2.0 and HuBERT. The experiment and performance
results of our model overcome the previous known outcomes.
",2021-10-09T00:58:12Z,http://arxiv.org/abs/2110.04425v1,"Omar Mohamed, Salah A. Aly"
Low-Resource Text Classification using Domain-Adversarial Learning,"  Deep learning techniques have recently shown to be successful in many natural
language processing tasks forming state-of-the-art systems. They require,
however, a large amount of annotated data which is often missing. This paper
explores the use of domain-adversarial learning as a regularizer to avoid
overfitting when training domain invariant features for deep, complex neural
networks in low-resource and zero-resource settings in new target domains or
languages. In case of new languages, we show that monolingual word vectors can
be directly used for training without prealignment. Their projection into a
common space can be learnt ad-hoc at training time reaching the final
performance of pretrained multilingual word vectors.
",2018-07-13T17:30:32Z,http://arxiv.org/abs/1807.05195v2,"Daniel Grießhaber, Ngoc Thang Vu, Johannes Maucher"
"Spatio-temporal Storytelling? Leveraging Generative Models for Semantic
  Trajectory Analysis","  In this paper, we lay out a vision for analysing semantic trajectory traces
and generating synthetic semantic trajectory data (SSTs) using generative
language model. Leveraging the advancements in deep learning, as evident by
progress in the field of natural language processing (NLP), computer vision,
etc. we intend to create intelligent models that can study the semantic
trajectories in various contexts, predicting future trends, increasing machine
understanding of the movement of animals, humans, goods, etc. enhancing
human-computer interactions, and contributing to an array of applications
ranging from urban-planning to personalized recommendation engines and business
strategy.
",2023-06-24T08:45:47Z,http://arxiv.org/abs/2306.13905v1,"Shreya Ghosh, Saptarshi Sengupta, Prasenjit Mitra"
"A Comprehensive Review on Sentiment Analysis: Tasks, Approaches and
  Applications","  Sentiment analysis (SA) is an emerging field in text mining. It is the
process of computationally identifying and categorizing opinions expressed in a
piece of text over different social media platforms. Social media plays an
essential role in knowing the customer mindset towards a product, services, and
the latest market trends. Most organizations depend on the customer's response
and feedback to upgrade their offered products and services. SA or opinion
mining seems to be a promising research area for various domains. It plays a
vital role in analyzing big data generated daily in structured and unstructured
formats over the internet. This survey paper defines sentiment and its recent
research and development in different domains, including voice, images, videos,
and text. The challenges and opportunities of sentiment analysis are also
discussed in the paper.
  \keywords{Sentiment Analysis, Machine Learning, Lexicon-based approach, Deep
Learning, Natural Language Processing}
",2023-11-19T06:29:41Z,http://arxiv.org/abs/2311.11250v1,"Sudhanshu Kumar, Partha Pratim Roy, Debi Prosad Dogra, Byung-Gyu Kim"
Grammar-based Game Description Generation using Large Language Models,"  To lower the barriers to game design development, automated game design,
which generates game designs through computational processes, has been
explored. In automated game design, machine learning-based techniques such as
evolutionary algorithms have achieved success. Benefiting from the remarkable
advancements in deep learning, applications in computer vision and natural
language processing have progressed in level generation. However, due to the
limited amount of data in game design, the application of deep learning has
been insufficient for tasks such as game description generation. To pioneer a
new approach for handling limited data in automated game design, we focus on
the in-context learning of large language models (LLMs). LLMs can capture the
features of a task from a few demonstration examples and apply the capabilities
acquired during pre-training. We introduce the grammar of game descriptions,
which effectively structures the game design space, into the LLMs' reasoning
process. Grammar helps LLMs capture the characteristics of the complex task of
game description generation. Furthermore, we propose a decoding method that
iteratively improves the generated output by leveraging the grammar. Our
experiments demonstrate that this approach performs well in generating game
descriptions.
",2024-07-24T16:36:02Z,http://arxiv.org/abs/2407.17404v1,"Tsunehiko Tanaka, Edgar Simo-Serra"
On Uncertainty In Natural Language Processing,"  The last decade in deep learning has brought on increasingly capable systems
that are deployed on a wide variety of applications. In natural language
processing, the field has been transformed by a number of breakthroughs
including large language models, which are used in increasingly many
user-facing applications. In order to reap the benefits of this technology and
reduce potential harms, it is important to quantify the reliability of model
predictions and the uncertainties that shroud their development.
  This thesis studies how uncertainty in natural language processing can be
characterized from a linguistic, statistical and neural perspective, and how it
can be reduced and quantified through the design of the experimental pipeline.
We further explore uncertainty quantification in modeling by theoretically and
empirically investigating the effect of inductive model biases in text
classification tasks. The corresponding experiments include data for three
different languages (Danish, English and Finnish) and tasks as well as a large
set of different uncertainty quantification approaches. Additionally, we
propose a method for calibrated sampling in natural language generation based
on non-exchangeable conformal prediction, which provides tighter token sets
with better coverage of the actual continuation. Lastly, we develop an approach
to quantify confidence in large black-box language models using auxiliary
predictors, where the confidence is predicted from the input to and generated
output text of the target model alone.
",2024-10-04T14:08:02Z,http://arxiv.org/abs/2410.03446v1,Dennis Ulmer
"A Systematic Literature Review about Idea Mining: The Use of
  Machine-driven Analytics to Generate Ideas","  Idea generation is the core activity of innovation. Digital data sources,
which are sources of innovation, such as patents, publications, social media,
websites, etc., are increasingly growing at unprecedented volume. Manual idea
generation is time-consuming and is affected by the subjectivity of the
individuals involved. Therefore, the use machine-driven data analytics
techniques to analyze data to generate ideas and support idea generation by
serving users is useful. The objective of this study is to study state-of
the-art machine-driven analytics for idea generation and data sources, hence
the result of this study will generally server as a guideline for choosing
techniques and data sources. A systematic literature review is conducted to
identify relevant scholarly literature from IEEE, Scopus, Web of Science and
Google Scholar. We selected a total of 71 articles and analyzed them
thematically. The results of this study indicate that idea generation through
machine-driven analytics applies text mining, information retrieval (IR),
artificial intelligence (AI), deep learning, machine learning, statistical
techniques, natural language processing (NLP), NLP-based morphological
analysis, network analysis, and bibliometric to support idea generation. The
results include a list of techniques and procedures in idea generation through
machine-driven idea analytics. Additionally, characterization and heuristics
used in idea generation are summarized. For the future, tools designed to
generate ideas could be explored.
",2022-01-30T21:46:21Z,http://arxiv.org/abs/2202.12826v1,"Workneh Y. Ayele, Gustaf Juell-Skielse"
Deep learning for photoacoustic imaging: a survey,"  Machine learning has been developed dramatically and witnessed a lot of
applications in various fields over the past few years. This boom originated in
2009, when a new model emerged, that is, the deep artificial neural network,
which began to surpass other established mature models on some important
benchmarks. Later, it was widely used in academia and industry. Ranging from
image analysis to natural language processing, it fully exerted its magic and
now become the state-of-the-art machine learning models. Deep neural networks
have great potential in medical imaging technology, medical data analysis,
medical diagnosis and other healthcare issues, and is promoted in both
pre-clinical and even clinical stages. In this review, we performed an overview
of some new developments and challenges in the application of machine learning
to medical image analysis, with a special focus on deep learning in
photoacoustic imaging. The aim of this review is threefold: (i) introducing
deep learning with some important basics, (ii) reviewing recent works that
apply deep learning in the entire ecological chain of photoacoustic imaging,
from image reconstruction to disease diagnosis, (iii) providing some open
source materials and other resources for researchers interested in applying
deep learning to photoacoustic imaging.
",2020-08-10T15:53:30Z,http://arxiv.org/abs/2008.04221v4,"Changchun Yang, Hengrong Lan, Feng Gao, Fei Gao"
Continual Lifelong Learning in Natural Language Processing: A Survey,"  Continual learning (CL) aims to enable information systems to learn from a
continuous data stream across time. However, it is difficult for existing deep
learning architectures to learn a new task without largely forgetting
previously acquired knowledge. Furthermore, CL is particularly challenging for
language learning, as natural language is ambiguous: it is discrete,
compositional, and its meaning is context-dependent. In this work, we look at
the problem of CL through the lens of various NLP tasks. Our survey discusses
major challenges in CL and current methods applied in neural network models. We
also provide a critical review of the existing CL evaluation methods and
datasets in NLP. Finally, we present our outlook on future research directions.
",2020-12-17T18:44:36Z,http://arxiv.org/abs/2012.09823v1,"Magdalena Biesialska, Katarzyna Biesialska, Marta R. Costa-jussà"
Not Enough Data? Deep Learning to the Rescue!,"  Based on recent advances in natural language modeling and those in text
generation capabilities, we propose a novel data augmentation method for text
classification tasks. We use a powerful pre-trained neural network model to
artificially synthesize new labeled data for supervised learning. We mainly
focus on cases with scarce labeled data. Our method, referred to as
language-model-based data augmentation (LAMBADA), involves fine-tuning a
state-of-the-art language generator to a specific task through an initial
training phase on the existing (usually small) labeled data. Using the
fine-tuned model and given a class label, new sentences for the class are
generated. Our process then filters these new sentences by using a classifier
trained on the original data. In a series of experiments, we show that LAMBADA
improves classifiers' performance on a variety of datasets. Moreover, LAMBADA
significantly improves upon the state-of-the-art techniques for data
augmentation, specifically those applicable to text classification tasks with
little data.
",2019-11-08T08:30:22Z,http://arxiv.org/abs/1911.03118v2,"Ateret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich, Amir Kantor, George Kour, Segev Shlomov, Naama Tepper, Naama Zwerdling"
"Deep Learning in the Automotive Industry: Recent Advances and
  Application Examples","  One of the most exciting technology breakthroughs in the last few years has
been the rise of deep learning. State-of-the-art deep learning models are being
widely deployed in academia and industry, across a variety of areas, from image
analysis to natural language processing. These models have grown from fledgling
research subjects to mature techniques in real-world use. The increasing scale
of data, computational power and the associated algorithmic innovations are the
main drivers for the progress we see in this field. These developments also
have a huge potential for the automotive industry and therefore the interest in
deep learning-based technology is growing. A lot of the product innovations,
such as self-driving cars, parking and lane-change assist or safety functions,
such as autonomous emergency braking, are powered by deep learning algorithms.
Deep learning is poised to offer gains in performance and functionality for
most ADAS (Advanced Driver Assistance System) solutions. Virtual sensing for
vehicle dynamics application, vehicle inspection/heath monitoring, automated
driving and data-driven product development are key areas that are expected to
get the most attention. This article provides an overview of the recent
advances and some associated challenges in deep learning techniques in the
context of automotive applications.
",2019-06-20T20:30:39Z,http://arxiv.org/abs/1906.08834v2,"Kanwar Bharat Singh, Mustafa Ali Arat"
"LiPCoT: Linear Predictive Coding based Tokenizer for Self-supervised
  Learning of Time Series Data via Language Models","  Language models have achieved remarkable success in various natural language
processing tasks. However, their application to time series data, a crucial
component in many domains, remains limited. This paper proposes LiPCoT (Linear
Predictive Coding based Tokenizer for time series), a novel tokenizer that
encodes time series data into a sequence of tokens, enabling self-supervised
learning of time series using existing Language model architectures such as
BERT. Unlike traditional time series tokenizers that rely heavily on CNN
encoder for time series feature generation, LiPCoT employs stochastic modeling
through linear predictive coding to create a latent space for time series
providing a compact yet rich representation of the inherent stochastic nature
of the data. Furthermore, LiPCoT is computationally efficient and can
effectively handle time series data with varying sampling rates and lengths,
overcoming common limitations of existing time series tokenizers. In this
proof-of-concept work, we present the effectiveness of LiPCoT in classifying
Parkinson's disease (PD) using an EEG dataset from 46 participants. In
particular, we utilize LiPCoT to encode EEG data into a small vocabulary of
tokens and then use BERT for self-supervised learning and the downstream task
of PD classification. We benchmark our approach against several
state-of-the-art CNN-based deep learning architectures for PD detection. Our
results reveal that BERT models utilizing self-supervised learning outperformed
the best-performing existing method by 7.1% in precision, 2.3% in recall, 5.5%
in accuracy, 4% in AUC, and 5% in F1-score highlighting the potential for
self-supervised learning even on small datasets. Our work will inform future
foundational models for time series, particularly for self-supervised learning.
",2024-08-14T04:51:33Z,http://arxiv.org/abs/2408.07292v1,Md Fahim Anjum
Combining Representation Learning with Logic for Language Processing,"  The current state-of-the-art in many natural language processing and
automated knowledge base completion tasks is held by representation learning
methods which learn distributed vector representations of symbols via
gradient-based optimization. They require little or no hand-crafted features,
thus avoiding the need for most preprocessing steps and task-specific
assumptions. However, in many cases representation learning requires a large
amount of annotated training data to generalize well to unseen data. Such
labeled training data is provided by human annotators who often use formal
logic as the language for specifying annotations. This thesis investigates
different combinations of representation learning methods with logic for
reducing the need for annotated training data, and for improving
generalization.
",2017-12-27T21:09:36Z,http://arxiv.org/abs/1712.09687v1,Tim Rocktäschel
Lecture Notes on Neural Information Retrieval,"  These lecture notes focus on the recent advancements in neural information
retrieval, with particular emphasis on the systems and models exploiting
transformer networks. These networks, originally proposed by Google in 2017,
have seen a large success in many natural language processing and information
retrieval tasks. While there are many fantastic textbook on information
retrieval and natural language processing as well as specialised books for a
more advanced audience, these lecture notes target people aiming at developing
a basic understanding of the main information retrieval techniques and
approaches based on deep learning. These notes have been prepared for a IR
graduate course of the MSc program in Artificial Intelligence and Data
Engineering at the University of Pisa, Italy.
",2022-07-27T10:43:27Z,http://arxiv.org/abs/2207.13443v2,Nicola Tonellotto
Deep Learning Based Speech Beamforming,"  Multi-channel speech enhancement with ad-hoc sensors has been a challenging
task. Speech model guided beamforming algorithms are able to recover natural
sounding speech, but the speech models tend to be oversimplified or the
inference would otherwise be too complicated. On the other hand, deep learning
based enhancement approaches are able to learn complicated speech distributions
and perform efficient inference, but they are unable to deal with variable
number of input channels. Also, deep learning approaches introduce a lot of
errors, particularly in the presence of unseen noise types and settings. We
have therefore proposed an enhancement framework called DEEPBEAM, which
combines the two complementary classes of algorithms. DEEPBEAM introduces a
beamforming filter to produce natural sounding speech, but the filter
coefficients are determined with the help of a monaural speech enhancement
neural network. Experiments on synthetic and real-world data show that DEEPBEAM
is able to produce clean, dry and natural sounding speech, and is robust
against unseen noise.
",2018-02-15T02:00:54Z,http://arxiv.org/abs/1802.05383v1,"Kaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang, Dinei Florencio, Mark Hasegawa-Johnson"
"Deep Learning versus Traditional Classifiers on Vietnamese Students'
  Feedback Corpus","  Student's feedback is an important source of collecting students' opinions to
improve the quality of training activities. Implementing sentiment analysis
into student feedback data, we can determine sentiments polarities which
express all problems in the institution since changes necessary will be applied
to improve the quality of teaching and learning. This study focused on machine
learning and natural language processing techniques (NaiveBayes, Maximum
Entropy, Long Short-Term Memory, Bi-Directional Long Short-Term Memory) on the
VietnameseStudents' Feedback Corpus collected from a university. The final
results were compared and evaluated to find the most effective model based on
different evaluation criteria. The experimental results show that the
Bi-Directional LongShort-Term Memory algorithm outperformed than three other
algorithms in terms of the F1-score measurement with 92.0% on the sentiment
classification task and 89.6% on the topic classification task. In addition, we
developed a sentiment analysis application analyzing student feedback. The
application will help the institution to recognize students' opinions about a
problem and identify shortcomings that still exist. With the use of this
application, the institution can propose an appropriate method to improve the
quality of training activities in the future.
",2019-11-17T12:32:50Z,http://arxiv.org/abs/1911.07223v1,"Phu X. V. Nguyen, Tham T. T. Hong, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen"
"Comparative Analysis of Contextual Relation Extraction based on Deep
  Learning Models","  Contextual Relation Extraction (CRE) is mainly used for constructing a
knowledge graph with a help of ontology. It performs various tasks such as
semantic search, query answering, and textual entailment. Relation extraction
identifies the entities from raw texts and the relations among them. An
efficient and accurate CRE system is essential for creating domain knowledge in
the biomedical industry. Existing Machine Learning and Natural Language
Processing (NLP) techniques are not suitable to predict complex relations from
sentences that consist of more than two relations and unspecified entities
efficiently. In this work, deep learning techniques have been used to identify
the appropriate semantic relation based on the context from multiple sentences.
Even though various machine learning models have been used for relation
extraction, they provide better results only for binary relations, i.e.,
relations occurred exactly between the two entities in a sentence. Machine
learning models are not suited for complex sentences that consist of the words
that have various meanings. To address these issues, hybrid deep learning
models have been used to extract the relations from complex sentence
effectively. This paper explores the analysis of various deep learning models
that are used for relation extraction.
",2023-09-13T09:05:09Z,http://arxiv.org/abs/2309.06814v1,"R. Priyadharshini, G. Jeyakodi, P. Shanthi Bala"
Recent Trends in Named Entity Recognition (NER),"  The availability of large amounts of computer-readable textual data and
hardware that can process the data has shifted the focus of knowledge projects
towards deep learning architecture. Natural Language Processing, particularly
the task of Named Entity Recognition is no exception. The bulk of the learning
methods that have produced state-of-the-art results have changed the deep
learning model, the training method used, the training data itself or the
encoding of the output of the NER system. In this paper, we review significant
learning methods that have been employed for NER in the recent past and how
they came about from the linear learning methods of the past. We also cover the
progress of related tasks that are upstream or downstream to NER, e.g.,
sequence tagging, entity linking, etc., wherever the processes in question have
also improved NER results.
",2021-01-25T14:18:24Z,http://arxiv.org/abs/2101.11420v1,Arya Roy
"Deep Collective Learning: Learning Optimal Inputs and Weights Jointly in
  Deep Neural Networks","  It is well observed that in deep learning and computer vision literature,
visual data are always represented in a manually designed coding scheme (eg.,
RGB images are represented as integers ranging from 0 to 255 for each channel)
when they are input to an end-to-end deep neural network (DNN) for any learning
task. We boldly question whether the manually designed inputs are good for DNN
training for different tasks and study whether the input to a DNN can be
optimally learned end-to-end together with learning the weights of the DNN. In
this paper, we propose the paradigm of {\em deep collective learning} which
aims to learn the weights of DNNs and the inputs to DNNs simultaneously for
given tasks. We note that collective learning has been implicitly but widely
used in natural language processing while it has almost never been studied in
computer vision. Consequently, we propose the lookup vision networks
(Lookup-VNets) as a solution to deep collective learning in computer vision.
This is achieved by associating each color in each channel with a vector in
lookup tables. As learning inputs in computer vision has almost never been
studied in the existing literature, we explore several aspects of this question
through varieties of experiments on image classification tasks. Experimental
results on four benchmark datasets, i.e., CIFAR-10, CIFAR-100, Tiny ImageNet,
and ImageNet (ILSVRC2012) have shown several surprising characteristics of
Lookup-VNets and have demonstrated the advantages and promise of Lookup-VNets
and deep collective learning.
",2020-09-17T00:33:04Z,http://arxiv.org/abs/2009.07988v1,"Xiang Deng, Zhongfei, Zhang"
"A Universal Prompting Strategy for Extracting Process Model Information
  from Natural Language Text using Large Language Models","  Over the past decade, extensive research efforts have been dedicated to the
extraction of information from textual process descriptions. Despite the
remarkable progress witnessed in natural language processing (NLP), information
extraction within the Business Process Management domain remains predominantly
reliant on rule-based systems and machine learning methodologies. Data scarcity
has so far prevented the successful application of deep learning techniques.
However, the rapid progress in generative large language models (LLMs) makes it
possible to solve many NLP tasks with very high quality without the need for
extensive data. Therefore, we systematically investigate the potential of LLMs
for extracting information from textual process descriptions, targeting the
detection of process elements such as activities and actors, and relations
between them. Using a heuristic algorithm, we demonstrate the suitability of
the extracted information for process model generation. Based on a novel
prompting strategy, we show that LLMs are able to outperform state-of-the-art
machine learning approaches with absolute performance improvements of up to 8\%
$F_1$ score across three different datasets. We evaluate our prompting strategy
on eight different LLMs, showing it is universally applicable, while also
analyzing the impact of certain prompt parts on extraction quality. The number
of example texts, the specificity of definitions, and the rigour of format
instructions are identified as key for improving the accuracy of extracted
information. Our code, prompts, and data are publicly available.
",2024-07-26T06:39:35Z,http://arxiv.org/abs/2407.18540v1,"Julian Neuberger, Lars Ackermann, Han van der Aa, Stefan Jablonski"
"One Deep Music Representation to Rule Them All? : A comparative analysis
  of different representation learning strategies","  Inspired by the success of deploying deep learning in the fields of Computer
Vision and Natural Language Processing, this learning paradigm has also found
its way into the field of Music Information Retrieval. In order to benefit from
deep learning in an effective, but also efficient manner, deep transfer
learning has become a common approach. In this approach, it is possible to
reuse the output of a pre-trained neural network as the basis for a new
learning task. The underlying hypothesis is that if the initial and new
learning tasks show commonalities and are applied to the same type of input
data (e.g. music audio), the generated deep representation of the data is also
informative for the new task. Since, however, most of the networks used to
generate deep representations are trained using a single initial learning
source, their representation is unlikely to be informative for all possible
future tasks. In this paper, we present the results of our investigation of
what are the most important factors to generate deep representations for the
data and learning tasks in the music domain. We conducted this investigation
via an extensive empirical study that involves multiple learning sources, as
well as multiple deep learning architectures with varying levels of information
sharing between sources, in order to learn music representations. We then
validate these representations considering multiple target datasets for
evaluation. The results of our experiments yield several insights on how to
approach the design of methods for learning widely deployable deep data
representations in the music domain.
",2018-02-12T14:08:54Z,http://arxiv.org/abs/1802.04051v4,"Jaehun Kim, Julián Urbano, Cynthia C. S. Liem, Alan Hanjalic"
Matching Text with Deep Mutual Information Estimation,"  Text matching is a core natural language processing research problem. How to
retain sufficient information on both content and structure information is one
important challenge. In this paper, we present a neural approach for
general-purpose text matching with deep mutual information estimation
incorporated. Our approach, Text matching with Deep Info Max (TIM), is
integrated with a procedure of unsupervised learning of representations by
maximizing the mutual information between text matching neural network's input
and output. We use both global and local mutual information to learn text
representations. We evaluate our text matching approach on several tasks
including natural language inference, paraphrase identification, and answer
selection. Compared to the state-of-the-art approaches, the experiments show
that our method integrated with mutual information estimation learns better
text representation and achieves better experimental results of text matching
tasks without exploiting pretraining on external data.
",2020-03-09T15:25:37Z,http://arxiv.org/abs/2003.11521v1,"Xixi Zhou, Chengxi Li, Jiajun Bu, Chengwei Yao, Keyue Shi, Zhi Yu, Zhou Yu"
"Detecting ESG topics using domain-specific language models and data
  augmentation approaches","  Despite recent advances in deep learning-based language modelling, many
natural language processing (NLP) tasks in the financial domain remain
challenging due to the paucity of appropriately labelled data. Other issues
that can limit task performance are differences in word distribution between
the general corpora - typically used to pre-train language models - and
financial corpora, which often exhibit specialized language and symbology.
Here, we investigate two approaches that may help to mitigate these issues.
Firstly, we experiment with further language model pre-training using large
amounts of in-domain data from business and financial news. We then apply
augmentation approaches to increase the size of our dataset for model
fine-tuning. We report our findings on an Environmental, Social and Governance
(ESG) controversies dataset and demonstrate that both approaches are beneficial
to accuracy in classification tasks.
",2020-10-16T11:20:07Z,http://arxiv.org/abs/2010.08319v1,"Tim Nugent, Nicole Stelea, Jochen L. Leidner"
A Survey on Transfer Learning in Natural Language Processing,"  Deep learning models usually require a huge amount of data. However, these
large datasets are not always attainable. This is common in many challenging
NLP tasks. Consider Neural Machine Translation, for instance, where curating
such large datasets may not be possible specially for low resource languages.
Another limitation of deep learning models is the demand for huge computing
resources. These obstacles motivate research to question the possibility of
knowledge transfer using large trained models. The demand for transfer learning
is increasing as many large models are emerging. In this survey, we feature the
recent transfer learning advances in the field of NLP. We also provide a
taxonomy for categorizing different transfer learning approaches from the
literature.
",2020-05-31T21:52:31Z,http://arxiv.org/abs/2007.04239v1,"Zaid Alyafeai, Maged Saeed AlShaibani, Irfan Ahmad"
EnSyth: A Pruning Approach to Synthesis of Deep Learning Ensembles,"  Deep neural networks have achieved state-of-art performance in many domains
including computer vision, natural language processing and self-driving cars.
However, they are very computationally expensive and memory intensive which
raises significant challenges when it comes to deploy or train them on strict
latency applications or resource-limited environments. As a result, many
attempts have been introduced to accelerate and compress deep learning models,
however the majority were not able to maintain the same accuracy of the
baseline models. In this paper, we describe EnSyth, a deep learning ensemble
approach to enhance the predictability of compact neural network's models.
First, we generate a set of diverse compressed deep learning models using
different hyperparameters for a pruning method, after that we utilise ensemble
learning to synthesise the outputs of the compressed models to compose a new
pool of classifiers. Finally, we apply backward elimination on the generated
pool to explore the best performing combinations of models. On CIFAR-10,
CIFAR-5 data-sets with LeNet-5, EnSyth outperforms the predictability of the
baseline model.
",2019-07-22T12:44:46Z,http://arxiv.org/abs/1907.09286v1,"Besher Alhalabi, Mohamed Medhat Gaber, Shadi Basurra"
"MessageNet: Message Classification using Natural Language Processing and
  Meta-data","  In this paper we propose a new Deep Learning (DL) approach for message
classification. Our method is based on the state-of-the-art Natural Language
Processing (NLP) building blocks, combined with a novel technique for infusing
the meta-data input that is typically available in messages such as the sender
information, timestamps, attached image, audio, affiliations, and more. As we
demonstrate throughout the paper, going beyond the mere text by leveraging all
available channels in the message, could yield an improved representation and
higher classification accuracy. To achieve message representation, each type of
input is processed in a dedicated block in the neural network architecture that
is suitable for the data type. Such an implementation enables training all
blocks together simultaneously, and forming cross channels features in the
network. We show in the Experiments Section that in some cases, message's
meta-data holds an additional information that cannot be extracted just from
the text, and when using this information we achieve better performance.
Furthermore, we demonstrate that our multi-modality block approach outperforms
other approaches for injecting the meta data to the the text classifier.
",2023-01-04T20:11:00Z,http://arxiv.org/abs/2301.01808v1,"Adar Kahana, Oren Elisha"
"Deep Learning and Machine Learning, Advancing Big Data Analytics and
  Management: Handy Appetizer","  This book explores the role of Artificial Intelligence (AI), Machine Learning
(ML), and Deep Learning (DL) in driving the progress of big data analytics and
management. The book focuses on simplifying the complex mathematical concepts
behind deep learning, offering intuitive visualizations and practical case
studies to help readers understand how neural networks and technologies like
Convolutional Neural Networks (CNNs) work. It introduces several classic models
and technologies such as Transformers, GPT, ResNet, BERT, and YOLO,
highlighting their applications in fields like natural language processing,
image recognition, and autonomous driving. The book also emphasizes the
importance of pre-trained models and how they can enhance model performance and
accuracy, with instructions on how to apply these models in various real-world
scenarios. Additionally, it provides an overview of key big data management
technologies like SQL and NoSQL databases, as well as distributed computing
frameworks such as Apache Hadoop and Spark, explaining their importance in
managing and processing vast amounts of data. Ultimately, the book underscores
the value of mastering deep learning and big data management skills as critical
tools for the future workforce, making it an essential resource for both
beginners and experienced professionals.
",2024-09-25T17:31:45Z,http://arxiv.org/abs/2409.17120v1,"Benji Peng, Xuanhe Pan, Yizhu Wen, Ziqian Bi, Keyu Chen, Ming Li, Ming Liu, Qian Niu, Junyu Liu, Jinlang Wang, Sen Zhang, Jiawei Xu, Pohsun Feng"
"Leveraging pre-trained language models for conversational information
  seeking from text","  Recent advances in Natural Language Processing, and in particular on the
construction of very large pre-trained language representation models, is
opening up new perspectives on the construction of conversational information
seeking (CIS) systems. In this paper we investigate the usage of in-context
learning and pre-trained language representation models to address the problem
of information extraction from process description documents, in an incremental
question and answering oriented fashion. In particular we investigate the usage
of the native GPT-3 (Generative Pre-trained Transformer 3) model, together with
two in-context learning customizations that inject conceptual definitions and a
limited number of samples in a few shot-learning fashion. The results highlight
the potential of the approach and the usefulness of the in-context learning
customizations, which can substantially contribute to address the ""training
data challenge"" of deep learning based NLP techniques the BPM field. It also
highlight the challenge posed by control flow relations for which further
training needs to be devised.
",2022-03-31T09:00:46Z,http://arxiv.org/abs/2204.03542v1,"Patrizio Bellan, Mauro Dragoni, Chiara Ghidini"
Training Larger Networks for Deep Reinforcement Learning,"  The success of deep learning in the computer vision and natural language
processing communities can be attributed to training of very deep neural
networks with millions or billions of parameters which can then be trained with
massive amounts of data. However, similar trend has largely eluded training of
deep reinforcement learning (RL) algorithms where larger networks do not lead
to performance improvement. Previous work has shown that this is mostly due to
instability during training of deep RL agents when using larger networks. In
this paper, we make an attempt to understand and address training of larger
networks for deep RL. We first show that naively increasing network capacity
does not improve performance. Then, we propose a novel method that consists of
1) wider networks with DenseNet connection, 2) decoupling representation
learning from training of RL, 3) a distributed training method to mitigate
overfitting problems. Using this three-fold technique, we show that we can
train very large networks that result in significant performance gains. We
present several ablation studies to demonstrate the efficacy of the proposed
method and some intuitive understanding of the reasons for performance gain. We
show that our proposed method outperforms other baseline algorithms on several
challenging locomotion tasks.
",2021-02-16T02:16:54Z,http://arxiv.org/abs/2102.07920v1,"Kei Ota, Devesh K. Jha, Asako Kanezaki"
A Survey on Uncertainty Quantification Methods for Deep Learning,"  Deep neural networks (DNNs) have achieved tremendous success in making
accurate predictions for computer vision, natural language processing, as well
as science and engineering domains. However, it is also well-recognized that
DNNs sometimes make unexpected, incorrect, but overconfident predictions. This
can cause serious consequences in high-stake applications, such as autonomous
driving, medical diagnosis, and disaster response. Uncertainty quantification
(UQ) aims to estimate the confidence of DNN predictions beyond prediction
accuracy. In recent years, many UQ methods have been developed for DNNs. It is
of great practical value to systematically categorize these UQ methods and
compare their advantages and disadvantages. However, existing surveys mostly
focus on categorizing UQ methodologies from a neural network architecture
perspective or a Bayesian perspective and ignore the source of uncertainty that
each methodology can incorporate, making it difficult to select an appropriate
UQ method in practice. To fill the gap, this paper presents a systematic
taxonomy of UQ methods for DNNs based on the types of uncertainty sources (data
uncertainty versus model uncertainty). We summarize the advantages and
disadvantages of methods in each category. We show how our taxonomy of UQ
methodologies can potentially help guide the choice of UQ method in different
machine learning problems (e.g., active learning, robustness, and reinforcement
learning). We also identify current research gaps and propose several future
research directions.
",2023-02-26T22:30:08Z,http://arxiv.org/abs/2302.13425v5,"Wenchong He, Zhe Jiang, Tingsong Xiao, Zelin Xu, Yukun Li"
Hopular: Modern Hopfield Networks for Tabular Data,"  While Deep Learning excels in structured data as encountered in vision and
natural language processing, it failed to meet its expectations on tabular
data. For tabular data, Support Vector Machines (SVMs), Random Forests, and
Gradient Boosting are the best performing techniques with Gradient Boosting in
the lead. Recently, we saw a surge of Deep Learning methods that were tailored
to tabular data but still underperform compared to Gradient Boosting on
small-sized datasets. We suggest ""Hopular"", a novel Deep Learning architecture
for medium- and small-sized datasets, where each layer is equipped with
continuous modern Hopfield networks. The modern Hopfield networks use stored
data to identify feature-feature, feature-target, and sample-sample
dependencies. Hopular's novelty is that every layer can directly access the
original input as well as the whole training set via stored data in the
Hopfield networks. Therefore, Hopular can step-wise update its current model
and the resulting prediction at every layer like standard iterative learning
algorithms. In experiments on small-sized tabular datasets with less than 1,000
samples, Hopular surpasses Gradient Boosting, Random Forests, SVMs, and in
particular several Deep Learning methods. In experiments on medium-sized
tabular data with about 10,000 samples, Hopular outperforms XGBoost, CatBoost,
LightGBM and a state-of-the art Deep Learning method designed for tabular data.
Thus, Hopular is a strong alternative to these methods on tabular data.
",2022-06-01T17:57:44Z,http://arxiv.org/abs/2206.00664v1,"Bernhard Schäfl, Lukas Gruber, Angela Bitto-Nemling, Sepp Hochreiter"
A New Method for Cross-Lingual-based Semantic Role Labeling,"  Semantic role labeling is a crucial task in natural language processing,
enabling better comprehension of natural language. However, the lack of
annotated data in multiple languages has posed a challenge for researchers. To
address this, a deep learning algorithm based on model transfer has been
proposed. The algorithm utilizes a dataset consisting of the English portion of
CoNLL2009 and a corpus of semantic roles in Persian. To optimize the efficiency
of training, only ten percent of the educational data from each language is
used. The results of the proposed model demonstrate significant improvements
compared to Niksirt et al.'s model. In monolingual mode, the proposed model
achieved a 2.05 percent improvement on F1-score, while in cross-lingual mode,
the improvement was even more substantial, reaching 6.23 percent. Worth noting
is that the compared model only trained two of the four stages of semantic role
labeling and employed golden data for the remaining two stages. This suggests
that the actual superiority of the proposed model surpasses the reported
numbers by a significant margin. The development of cross-lingual methods for
semantic role labeling holds promise, particularly in addressing the scarcity
of annotated data for various languages. These advancements pave the way for
further research in understanding and processing natural language across
different linguistic contexts.
",2024-08-28T16:06:12Z,http://arxiv.org/abs/2408.15896v1,"Mohammad Ebrahimi, Behrouz Minaei Bidgoli, Nasim Khozouei"
"Emulating Spatio-Temporal Realizations of Three-Dimensional Isotropic
  Turbulence via Deep Sequence Learning Models","  We use a data-driven approach to model a three-dimensional turbulent flow
using cutting-edge Deep Learning techniques. The deep learning framework
incorporates physical constraints on the flow, such as preserving
incompressibility and global statistical invariants of velocity gradient
tensor. The accuracy of the model is assessed using statistical and
physics-based metrics. The data set comes from Direct Numerical Simulation of
an incompressible, statistically stationary, isotropic turbulent flow in a
cubic box. Since the size of the dataset is memory intensive, we first generate
a low-dimensional representation of the velocity data, and then pass it to a
sequence prediction network that learns the spatial and temporal correlations
of the underlying data. The dimensionality reduction is performed via
extraction using Vector-Quantized Autoencoder (VQ-AE), which learns the
discrete latent variables. For the sequence forecasting, the idea of
Transformer architecture from natural language processing is used, and its
performance compared against more standard Recurrent Networks (such as
Convolutional LSTM). These architectures are designed and trained to perform a
sequence to sequence multi-class classification task in which they take an
input sequence with a fixed length (k) and predict a sequence with a fixed
length (p), representing the future time instants of the flow. Our results for
the short-term predictions show that the accuracy of results for both models
deteriorates across predicted snapshots due to autoregressive nature of the
predictions. Based on our diagnostics tests, the trained Conv-Transformer model
outperforms the Conv-LSTM one and can accurately, both quantitatively and
qualitatively, retain the large scales and capture well the inertial scales of
flow but fails at recovering the small and intermittent fluid motions.
",2021-12-07T03:33:39Z,http://arxiv.org/abs/2112.03469v1,"Mohammadreza Momenifar, Enmao Diao, Vahid Tarokh, Andrew D. Bragg"
"Deep Learning Approaches for Improving Question Answering Systems in
  Hepatocellular Carcinoma Research","  In recent years, advancements in natural language processing (NLP) have been
fueled by deep learning techniques, particularly through the utilization of
powerful computing resources like GPUs and TPUs. Models such as BERT and GPT-3,
trained on vast amounts of data, have revolutionized language understanding and
generation. These pre-trained models serve as robust bases for various tasks
including semantic understanding, intelligent writing, and reasoning, paving
the way for a more generalized form of artificial intelligence. NLP, as a vital
application of AI, aims to bridge the gap between humans and computers through
natural language interaction. This paper delves into the current landscape and
future prospects of large-scale model-based NLP, focusing on the
question-answering systems within this domain. Practical cases and developments
in artificial intelligence-driven question-answering systems are analyzed to
foster further exploration and research in the realm of large-scale NLP.
",2024-02-25T09:32:17Z,http://arxiv.org/abs/2402.16038v1,"Shuning Huo, Yafei Xiang, Hanyi Yu, Mengran Zhu, Yulu Gong"
"EXPLORER: Exploration-guided Reasoning for Textual Reinforcement
  Learning","  Text-based games (TBGs) have emerged as an important collection of NLP tasks,
requiring reinforcement learning (RL) agents to combine natural language
understanding with reasoning. A key challenge for agents attempting to solve
such tasks is to generalize across multiple games and demonstrate good
performance on both seen and unseen objects. Purely deep-RL-based approaches
may perform well on seen objects; however, they fail to showcase the same
performance on unseen objects. Commonsense-infused deep-RL agents may work
better on unseen data; unfortunately, their policies are often not
interpretable or easily transferable. To tackle these issues, in this paper, we
present EXPLORER which is an exploration-guided reasoning agent for textual
reinforcement learning. EXPLORER is neurosymbolic in nature, as it relies on a
neural module for exploration and a symbolic module for exploitation. It can
also learn generalized symbolic policies and perform well over unseen data. Our
experiments show that EXPLORER outperforms the baseline agents on Text-World
cooking (TW-Cooking) and Text-World Commonsense (TWC) games.
",2024-03-15T21:22:37Z,http://arxiv.org/abs/2403.10692v1,"Kinjal Basu, Keerthiram Murugesan, Subhajit Chaudhury, Murray Campbell, Kartik Talamadupula, Tim Klinger"
Predicting Process Behaviour using Deep Learning,"  Predicting business process behaviour is an important aspect of business
process management. Motivated by research in natural language processing, this
paper describes an application of deep learning with recurrent neural networks
to the problem of predicting the next event in a business process. This is both
a novel method in process prediction, which has largely relied on explicit
process models, and also a novel application of deep learning methods. The
approach is evaluated on two real datasets and our results surpass the
state-of-the-art in prediction precision.
",2016-12-14T12:33:28Z,http://arxiv.org/abs/1612.04600v2,"Joerg Evermann, Jana-Rebecca Rehse, Peter Fettke"
"Sample-efficient Actor-Critic Reinforcement Learning with Supervised
  Data for Dialogue Management","  Deep reinforcement learning (RL) methods have significant potential for
dialogue policy optimisation. However, they suffer from a poor performance in
the early stages of learning. This is especially problematic for on-line
learning with real users. Two approaches are introduced to tackle this problem.
Firstly, to speed up the learning process, two sample-efficient neural networks
algorithms: trust region actor-critic with experience replay (TRACER) and
episodic natural actor-critic with experience replay (eNACER) are presented.
For TRACER, the trust region helps to control the learning step size and avoid
catastrophic model changes. For eNACER, the natural gradient identifies the
steepest ascent direction in policy space to speed up the convergence. Both
models employ off-policy learning with experience replay to improve
sample-efficiency. Secondly, to mitigate the cold start issue, a corpus of
demonstration data is utilised to pre-train the models prior to on-line
reinforcement learning. Combining these two approaches, we demonstrate a
practical approach to learn deep RL-based dialogue policies and demonstrate
their effectiveness in a task-oriented information seeking domain.
",2017-07-01T09:56:31Z,http://arxiv.org/abs/1707.00130v2,"Pei-Hao Su, Pawel Budzianowski, Stefan Ultes, Milica Gasic, Steve Young"
"What makes a language easy to deep-learn? Deep neural networks and
  humans similarly benefit from compositional structure","  Deep neural networks drive the success of natural language processing. A
fundamental property of language is its compositional structure, allowing
humans to systematically produce forms for new meanings. For humans, languages
with more compositional and transparent structures are typically easier to
learn than those with opaque and irregular structures. However, this
learnability advantage has not yet been shown for deep neural networks,
limiting their use as models for human language learning. Here, we directly
test how neural networks compare to humans in learning and generalizing
different languages that vary in their degree of compositional structure. We
evaluate the memorization and generalization capabilities of a large language
model and recurrent neural networks, and show that both deep neural networks
exhibit a learnability advantage for more structured linguistic input: neural
networks exposed to more compositional languages show more systematic
generalization, greater agreement between different agents, and greater
similarity to human learners.
",2023-02-23T18:57:34Z,http://arxiv.org/abs/2302.12239v4,"Lukas Galke, Yoav Ram, Limor Raviv"
Text segmentation with character-level text embeddings,"  Learning word representations has recently seen much success in computational
linguistics. However, assuming sequences of word tokens as input to linguistic
analysis is often unjustified. For many languages word segmentation is a
non-trivial task and naturally occurring text is sometimes a mixture of natural
language strings and other character data. We propose to learn text
representations directly from raw character sequences by training a Simple
recurrent Network to predict the next character in text. The network uses its
hidden layer to evolve abstract representations of the character sequences it
sees. To demonstrate the usefulness of the learned text embeddings, we use them
as features in a supervised character level text segmentation and labeling
task: recognizing spans of text containing programming language code. By using
the embeddings as features we are able to substantially improve over a baseline
which uses only surface character n-grams.
",2013-09-18T12:38:34Z,http://arxiv.org/abs/1309.4628v1,Grzegorz Chrupała
"Continually Learn to Map Visual Concepts to Large Language Models in
  Resource-constrained Environments","  Learning continually from a stream of non-i.i.d. data is an open challenge in
deep learning, even more so when working in resource-constrained environments
such as embedded devices. Visual models that are continually updated through
supervised learning are often prone to overfitting, catastrophic forgetting,
and biased representations. On the other hand, large language models contain
knowledge about multiple concepts and their relations, which can foster a more
robust, informed and coherent learning process. This work proposes Continual
Visual Mapping (CVM), an approach that continually ground vision
representations to a knowledge space extracted from a fixed Language model.
Specifically, CVM continually trains a small and efficient visual model to map
its representations into a conceptual space established by a fixed Large
Language Model. Due to their smaller nature, CVM can be used when directly
adapting large visual pre-trained models is unfeasible due to computational or
data constraints. CVM overcome state-of-the-art continual learning methods on
five benchmarks and offers a promising avenue for addressing generalization
capabilities in continual learning, even in computationally constrained
devices.
",2024-07-11T08:28:40Z,http://arxiv.org/abs/2407.08279v1,"Clea Rebillard, Julio Hurtado, Andrii Krutsylo, Lucia Passaro, Vincenzo Lomonaco"
Operationalising Representation in Natural Language Processing,"  Despite its centrality in the philosophy of cognitive science, there has been
little prior philosophical work engaging with the notion of representation in
contemporary NLP practice. This paper attempts to fill that lacuna: drawing on
ideas from cognitive science, I introduce a framework for evaluating the
representational claims made about components of neural NLP models, proposing
three criteria with which to evaluate whether a component of a model represents
a property and operationalising these criteria using probing classifiers, a
popular analysis technique in NLP (and deep learning more broadly).
  The project of operationalising a philosophically-informed notion of
representation should be of interest to both philosophers of science and NLP
practitioners. It affords philosophers a novel testing-ground for claims about
the nature of representation, and helps NLPers organise the large literature on
probing experiments, suggesting novel avenues for empirical research.
",2023-06-14T01:34:16Z,http://arxiv.org/abs/2306.08193v2,Jacqueline Harding
"Fine-tuning Vision Transformers for the Prediction of State Variables in
  Ising Models","  Transformers are state-of-the-art deep learning models that are composed of
stacked attention and point-wise, fully connected layers designed for handling
sequential data. Transformers are not only ubiquitous throughout Natural
Language Processing (NLP), but, recently, they have inspired a new wave of
Computer Vision (CV) applications research. In this work, a Vision Transformer
(ViT) is applied to predict the state variables of 2-dimensional Ising model
simulations. Our experiments show that ViT outperform state-of-the-art
Convolutional Neural Networks (CNN) when using a small number of microstate
images from the Ising model corresponding to various boundary conditions and
temperatures. This work opens the possibility of applying ViT to other
simulations, and raises interesting research directions on how attention maps
can learn about the underlying physics governing different phenomena.
",2021-09-28T00:23:31Z,http://arxiv.org/abs/2109.13925v2,"Onur Kara, Arijit Sehanobish, Hector H Corzo"
"A Systematic Literature Review on the Use of Machine Learning in
  Software Engineering","  Software engineering (SE) is a dynamic field that involves multiple phases
all of which are necessary to develop sustainable software systems. Machine
learning (ML), a branch of artificial intelligence (AI), has drawn a lot of
attention in recent years thanks to its ability to analyze massive volumes of
data and extract useful patterns from data. Several studies have focused on
examining, categorising, and assessing the application of ML in SE processes.
We conducted a literature review on primary studies to address this gap. The
study was carried out following the objective and the research questions to
explore the current state of the art in applying machine learning techniques in
software engineering processes. The review identifies the key areas within
software engineering where ML has been applied, including software quality
assurance, software maintenance, software comprehension, and software
documentation. It also highlights the specific ML techniques that have been
leveraged in these domains, such as supervised learning, unsupervised learning,
and deep learning.
  Keywords: machine learning, deep learning, software engineering, natural
language processing, source code
",2024-06-19T23:04:27Z,http://arxiv.org/abs/2406.13877v1,"Nyaga Fred, I. O. Temkin"
Word Embedding based Edit Distance,"  Text similarity calculation is a fundamental problem in natural language
processing and related fields. In recent years, deep neural networks have been
developed to perform the task and high performances have been achieved. The
neural networks are usually trained with labeled data in supervised learning,
and creation of labeled data is usually very costly. In this short paper, we
address unsupervised learning for text similarity calculation. We propose a new
method called Word Embedding based Edit Distance (WED), which incorporates word
embedding into edit distance. Experiments on three benchmark datasets show WED
outperforms state-of-the-art unsupervised methods including edit distance,
TF-IDF based cosine, word embedding based cosine, Jaccard index, etc.
",2018-10-25T07:50:17Z,http://arxiv.org/abs/1810.10752v1,"Yilin Niu, Chao Qiao, Hang Li, Minlie Huang"
"Improving a neural network model by explanation-guided training for
  glioma classification based on MRI data","  In recent years, artificial intelligence (AI) systems have come to the
forefront. These systems, mostly based on Deep learning (DL), achieve excellent
results in areas such as image processing, natural language processing, or
speech recognition. Despite the statistically high accuracy of deep learning
models, their output is often a decision of ""black box"". Thus, Interpretability
methods have become a popular way to gain insight into the decision-making
process of deep learning models. Explanation of a deep learning model is
desirable in the medical domain since the experts have to justify their
judgments to the patient. In this work, we proposed a method for
explanation-guided training that uses a Layer-wise relevance propagation (LRP)
technique to force the model to focus only on the relevant part of the image.
We experimentally verified our method on a convolutional neural network (CNN)
model for low-grade and high-grade glioma classification problems. Our
experiments show promising results in a way to use interpretation techniques in
the model training process.
",2021-07-05T13:27:28Z,http://arxiv.org/abs/2107.02008v2,"Frantisek Sefcik, Wanda Benesova"
"Bridging Text and Molecule: A Survey on Multimodal Frameworks for
  Molecule","  Artificial intelligence has demonstrated immense potential in scientific
research. Within molecular science, it is revolutionizing the traditional
computer-aided paradigm, ushering in a new era of deep learning. With recent
progress in multimodal learning and natural language processing, an emerging
trend has targeted at building multimodal frameworks to jointly model molecules
with textual domain knowledge. In this paper, we present the first systematic
survey on multimodal frameworks for molecules research. Specifically,we begin
with the development of molecular deep learning and point out the necessity to
involve textual modality. Next, we focus on recent advances in text-molecule
alignment methods, categorizing current models into two groups based on their
architectures and listing relevant pre-training tasks. Furthermore, we delves
into the utilization of large language models and prompting techniques for
molecular tasks and present significant applications in drug discovery.
Finally, we discuss the limitations in this field and highlight several
promising directions for future research.
",2024-03-07T03:03:13Z,http://arxiv.org/abs/2403.13830v1,"Yi Xiao, Xiangxin Zhou, Qiang Liu, Liang Wang"
"Predicting ATP binding sites in protein sequences using Deep Learning
  and Natural Language Processing","  Predicting ATP-Protein Binding sites in genes is of great significance in the
field of Biology and Medicine. The majority of research in this field has been
conducted through time- and resource-intensive 'wet experiments' in
laboratories. Over the years, researchers have been investigating computational
methods computational methods to accomplish the same goals, utilising the
strength of advanced Deep Learning and NLP algorithms. In this paper, we
propose to develop methods to classify ATP-Protein binding sites. We conducted
various experiments mainly using PSSMs and several word embeddings as features.
We used 2D CNNs and LightGBM classifiers as our chief Deep Learning Algorithms.
The MP3Vec and BERT models have also been subjected to testing in our study.
The outcomes of our experiments demonstrated improvement over the
state-of-the-art benchmarks.
",2024-02-02T18:42:39Z,http://arxiv.org/abs/2402.01829v1,"Shreyas V, Swati Agarwal"
"Categorization in the Wild: Generalizing Cognitive Models to
  Naturalistic Data across Languages","  Categories such as animal or furniture are acquired at an early age and play
an important role in processing, organizing, and communicating world knowledge.
Categories exist across cultures: they allow to efficiently represent the
complexity of the world, and members of a community strongly agree on their
nature, revealing a shared mental representation. Models of category learning
and representation, however, are typically tested on data from small-scale
experiments involving small sets of concepts with artificially restricted
features; and experiments predominantly involve participants of selected
cultural and socio-economical groups (very often involving western native
speakers of English such as U.S. college students) . This work investigates
whether models of categorization generalize (a) to rich and noisy data
approximating the environment humans live in; and (b) across languages and
cultures. We present a Bayesian cognitive model designed to jointly learn
categories and their structured representation from natural language text which
allows us to (a) evaluate performance on a large scale, and (b) apply our model
to a diverse set of languages. We show that meaningful categories comprising
hundreds of concepts and richly structured featural representations emerge
across languages. Our work illustrates the potential of recent advances in
computational modeling and large scale naturalistic datasets for cognitive
science research.
",2019-02-23T19:21:08Z,http://arxiv.org/abs/1902.08830v1,"Lea Frermann, Mirella Lapata"
"MineObserver 2.0: A Deep Learning & In-Game Framework for Assessing
  Natural Language Descriptions of Minecraft Imagery","  MineObserver 2.0 is an AI framework that uses Computer Vision and Natural
Language Processing for assessing the accuracy of learner-generated
descriptions of Minecraft images that include some scientifically relevant
content. The system automatically assesses the accuracy of participant
observations, written in natural language, made during science learning
activities that take place in Minecraft. We demonstrate our system working in
real-time and describe a teacher support dashboard to showcase observations,
both of which advance our previous work. We present the results of a study
showing that MineObserver 2.0 improves over its predecessor both in perceived
accuracy of the system's generated descriptions as well as in usefulness of the
system's feedback. In future work we intend improve system-generated
descriptions, give teachers more control and upgrade the system to perform
continuous learning to more effectively and rapidly respond to novel
observations made by learners.
",2023-12-19T00:15:35Z,http://arxiv.org/abs/2312.11761v1,"Jay Mahajan, Samuel Hum, Jack Henhapl, Diya Yunus, Matthew Gadbury, Emi Brown, Jeff Ginger, H. Chad Lane"
"Look Ma, Only 400 Samples! Revisiting the Effectiveness of Automatic
  N-Gram Rule Generation for Spelling Normalization in Filipino","  With 84.75 million Filipinos online, the ability for models to process online
text is crucial for developing Filipino NLP applications. To this end, spelling
correction is a crucial preprocessing step for downstream processing. However,
the lack of data prevents the use of language models for this task. In this
paper, we propose an N-Gram + Damerau Levenshtein distance model with automatic
rule extraction. We train the model on 300 samples, and show that despite
limited training data, it achieves good performance and outperforms other deep
learning approaches in terms of accuracy and edit distance. Moreover, the model
(1) requires little compute power, (2) trains in little time, thus allowing for
retraining, and (3) is easily interpretable, allowing for direct
troubleshooting, highlighting the success of traditional approaches over more
complex deep learning models in settings where data is unavailable.
",2022-10-06T04:41:26Z,http://arxiv.org/abs/2210.02675v2,"Lorenzo Jaime Yu Flores, Dragomir Radev"
MMREC: LLM Based Multi-Modal Recommender System,"  The importance of recommender systems is growing rapidly due to the
exponential increase in the volume of content generated daily. This surge in
content presents unique challenges for designing effective recommender systems.
Key among these challenges is the need to effectively leverage the vast amounts
of natural language data and images that represent user preferences. This paper
presents a novel approach to enhancing recommender systems by leveraging Large
Language Models (LLMs) and deep learning techniques. The proposed framework
aims to improve the accuracy and relevance of recommendations by incorporating
multi-modal information processing and by the use of unified latent space
representation. The study explores the potential of LLMs to better understand
and utilize natural language data in recommendation contexts, addressing the
limitations of previous methods. The framework efficiently extracts and
integrates text and image information through LLMs, unifying diverse modalities
in a latent space to simplify the learning process for the ranking model.
Experimental results demonstrate the enhanced discriminative power of the model
when utilizing multi-modal information. This research contributes to the
evolving field of recommender systems by showcasing the potential of LLMs and
multi-modal data integration to create more personalized and contextually
relevant recommendations.
",2024-08-08T04:31:29Z,http://arxiv.org/abs/2408.04211v1,"Jiahao Tian, Jinman Zhao, Zhenkai Wang, Zhicheng Ding"
"Reverse Transfer Learning: Can Word Embeddings Trained for Different NLP
  Tasks Improve Neural Language Models?","  Natural language processing (NLP) tasks tend to suffer from a paucity of
suitably annotated training data, hence the recent success of transfer learning
across a wide variety of them. The typical recipe involves: (i) training a
deep, possibly bidirectional, neural network with an objective related to
language modeling, for which training data is plentiful; and (ii) using the
trained network to derive contextual representations that are far richer than
standard linear word embeddings such as word2vec, and thus result in important
gains. In this work, we wonder whether the opposite perspective is also true:
can contextual representations trained for different NLP tasks improve language
modeling itself? Since language models (LMs) are predominantly locally
optimized, other NLP tasks may help them make better predictions based on the
entire semantic fabric of a document. We test the performance of several types
of pre-trained embeddings in neural LMs, and we investigate whether it is
possible to make the LM more aware of global semantic information through
embeddings pre-trained with a domain classification model. Initial experiments
suggest that as long as the proper objective criterion is used during training,
pre-trained embeddings are likely to be beneficial for neural language
modeling.
",2019-09-09T20:01:51Z,http://arxiv.org/abs/1909.04130v1,"Lyan Verwimp, Jerome R. Bellegarda"
"ParticleGrid: Enabling Deep Learning using 3D Representation of
  Materials","  From AlexNet to Inception, autoencoders to diffusion models, the development
of novel and powerful deep learning models and learning algorithms has
proceeded at breakneck speeds. In part, we believe that rapid iteration of
model architecture and learning techniques by a large community of researchers
over a common representation of the underlying entities has resulted in
transferable deep learning knowledge. As a result, model scale, accuracy,
fidelity, and compute performance have dramatically increased in computer
vision and natural language processing. On the other hand, the lack of a common
representation for chemical structure has hampered similar progress. To enable
transferable deep learning, we identify the need for a robust 3-dimensional
representation of materials such as molecules and crystals. The goal is to
enable both materials property prediction and materials generation with 3D
structures. While computationally costly, such representations can model a
large set of chemical structures. We propose $\textit{ParticleGrid}$, a
SIMD-optimized library for 3D structures, that is designed for deep learning
applications and to seamlessly integrate with deep learning frameworks. Our
highly optimized grid generation allows for generating grids on the fly on the
CPU, reducing storage and GPU compute and memory requirements. We show the
efficacy of 3D grids generated via $\textit{ParticleGrid}$ and accurately
predict molecular energy properties using a 3D convolutional neural network.
Our model is able to get 0.006 mean square error and nearly match the values
calculated using computationally costly density functional theory at a fraction
of the time.
",2022-11-15T21:03:34Z,http://arxiv.org/abs/2211.08506v1,"Shehtab Zaman, Ethan Ferguson, Cecile Pereira, Denis Akhiyarov, Mauricio Araya-Polo, Kenneth Chiu"
"Deep Learning -- A first Meta-Survey of selected Reviews across
  Scientific Disciplines, their Commonalities, Challenges and Research Impact","  Deep learning belongs to the field of artificial intelligence, where machines
perform tasks that typically require some kind of human intelligence. Similar
to the basic structure of a brain, a deep learning algorithm consists of an
artificial neural network, which resembles the biological brain structure.
Mimicking the learning process of humans with their senses, deep learning
networks are fed with (sensory) data, like texts, images, videos or sounds.
These networks outperform the state-of-the-art methods in different tasks and,
because of this, the whole field saw an exponential growth during the last
years. This growth resulted in way over 10,000 publications per year in the
last years. For example, the search engine PubMed alone, which covers only a
sub-set of all publications in the medical field, provides already over 11,000
results in Q3 2020 for the search term 'deep learning', and around 90% of these
results are from the last three years. Consequently, a complete overview over
the field of deep learning is already impossible to obtain and, in the near
future, it will potentially become difficult to obtain an overview over a
subfield. However, there are several review articles about deep learning, which
are focused on specific scientific fields or applications, for example deep
learning advances in computer vision or in specific tasks like object
detection. With these surveys as a foundation, the aim of this contribution is
to provide a first high-level, categorized meta-survey of selected reviews on
deep learning across different scientific disciplines. The categories (computer
vision, language processing, medical informatics and additional works) have
been chosen according to the underlying data sources (image, language, medical,
mixed). In addition, we review the common architectures, methods, pros, cons,
evaluations, challenges and future directions for every sub-category.
",2020-11-16T13:14:18Z,http://arxiv.org/abs/2011.08184v2,"Jan Egger, Antonio Pepe, Christina Gsaxner, Yuan Jin, Jianning Li, Roman Kern"
"iReason: Multimodal Commonsense Reasoning using Videos and Natural
  Language with Interpretability","  Causality knowledge is vital to building robust AI systems. Deep learning
models often perform poorly on tasks that require causal reasoning, which is
often derived using some form of commonsense knowledge not immediately
available in the input but implicitly inferred by humans. Prior work has
unraveled spurious observational biases that models fall prey to in the absence
of causality. While language representation models preserve contextual
knowledge within learned embeddings, they do not factor in causal relationships
during training. By blending causal relationships with the input features to an
existing model that performs visual cognition tasks (such as scene
understanding, video captioning, video question-answering, etc.), better
performance can be achieved owing to the insight causal relationships bring
about. Recently, several models have been proposed that have tackled the task
of mining causal data from either the visual or textual modality. However,
there does not exist widespread research that mines causal relationships by
juxtaposing the visual and language modalities. While images offer a rich and
easy-to-process resource for us to mine causality knowledge from, videos are
denser and consist of naturally time-ordered events. Also, textual information
offers details that could be implicit in videos. We propose iReason, a
framework that infers visual-semantic commonsense knowledge using both videos
and natural language captions. Furthermore, iReason's architecture integrates a
causal rationalization module to aid the process of interpretability, error
analysis and bias detection. We demonstrate the effectiveness of iReason using
a two-pronged comparative analysis with language representation learning models
(BERT, GPT-2) as well as current state-of-the-art multimodal causality models.
",2021-06-25T02:56:34Z,http://arxiv.org/abs/2107.10300v1,"Aman Chadha, Vinija Jain"
"Towards Automated Data Sciences with Natural Language and SageCopilot:
  Practices and Lessons Learned","  While the field of NL2SQL has made significant advancements in translating
natural language instructions into executable SQL scripts for data querying and
processing, achieving full automation within the broader data science pipeline
- encompassing data querying, analysis, visualization, and reporting - remains
a complex challenge. This study introduces SageCopilot, an advanced,
industry-grade system system that automates the data science pipeline by
integrating Large Language Models (LLMs), Autonomous Agents (AutoAgents), and
Language User Interfaces (LUIs). Specifically, SageCopilot incorporates a
two-phase design: an online component refining users' inputs into executable
scripts through In-Context Learning (ICL) and running the scripts for results
reporting & visualization, and an offline preparing demonstrations requested by
ICL in the online phase. A list of trending strategies such as Chain-of-Thought
and prompt-tuning have been used to augment SageCopilot for enhanced
performance. Through rigorous testing and comparative analysis against
prompt-based solutions, SageCopilot has been empirically validated to achieve
superior end-to-end performance in generating or executing scripts and offering
results with visualization, backed by real-world datasets. Our in-depth
ablation studies highlight the individual contributions of various components
and strategies used by SageCopilot to the end-to-end correctness for data
sciences.
",2024-07-21T08:58:18Z,http://arxiv.org/abs/2407.21040v1,"Yuan Liao, Jiang Bian, Yuhui Yun, Shuo Wang, Yubo Zhang, Jiaming Chu, Tao Wang, Kewei Li, Yuchen Li, Xuhong Li, Shilei Ji, Haoyi Xiong"
"An implementation of the ""Guess who?"" game using CLIP","  CLIP (Contrastive Language-Image Pretraining) is an efficient method for
learning computer vision tasks from natural language supervision that has
powered a recent breakthrough in deep learning due to its zero-shot transfer
capabilities. By training from image-text pairs available on the internet, the
CLIP model transfers non-trivially to most tasks without the need for any data
set specific training. In this work, we use CLIP to implement the engine of the
popular game ""Guess who?"", so that the player interacts with the game using
natural language prompts and CLIP automatically decides whether an image in the
game board fulfills that prompt or not. We study the performance of this
approach by benchmarking on different ways of prompting the questions to CLIP,
and show the limitations of its zero-shot capabilites.
",2021-11-30T13:10:52Z,http://arxiv.org/abs/2112.00599v1,"Arnau Martí Sarri, Victor Rodriguez-Fernandez"
"A Deep Learning Architecture for De-identification of Patient Notes:
  Implementation and Evaluation","  De-identification is the process of removing 18 protected health information
(PHI) from clinical notes in order for the text to be considered not
individually identifiable. Recent advances in natural language processing (NLP)
has allowed for the use of deep learning techniques for the task of
de-identification. In this paper, we present a deep learning architecture that
builds on the latest NLP advances by incorporating deep contextualized word
embeddings and variational drop out Bi-LSTMs. We test this architecture on two
gold standard datasets and show that the architecture achieves state-of-the-art
performance on both data sets while also converging faster than other systems
without the use of dictionaries or other knowledge sources.
",2018-10-03T02:53:04Z,http://arxiv.org/abs/1810.01570v1,"Kaung Khin, Philipp Burckhardt, Rema Padman"
Human Cognition and Language Processing with Neural-Lexicon Hypothesis,"  Cognition and language seem closely related to the human cognitive process,
although they have not been studied and investigated in detail. Our brain is
too complex to fully comprehend the structures and connectivity, as well as its
functions, with the currently available technology such as
electro-encephalography, positron emission tomography, or functional magnetic
resonance imaging, and neurobiological data. Therefore, the exploration of
neurobiological processes, such as cognition, requires substantially more
related evidences, especially from in-vivo human experiments. Cognition and
language are of inter-disciplinary nature and additional methodological support
is needed from other disciplines, such as deep learning in the field of
artificial intelligence, for example. In this paper, we have attempted to
explain the neural mechanisms underlying ""cognition and language processing"" or
""cognition or thinking"" using a novel neural network model with several newly
emerging developments such as neuronal resonance, in-vivo human fiber
tractography or connectivity data, Engram and Hebbian hypothesis, human memory
formation in the high brain areas, deep learning, and more recently developed
neural memory concepts, the neural lexicon. The neural lexicon is developed via
language by repeated exposure to the neural system, similar to multilayer
signal processing in deep learning. We have derived a neural model to explain
how human ""cognition and language processing"" or ""cognition and thinking""
works, with a focus on language, a universal medium of the human society.
Although the proposed hypothesis is not fully based on experimental evidences,
a substantial portion of the observations in this study is directly and
indirectly supported by recent experimental findings and the theoretical bases
of deep learning research.
",2022-10-24T05:31:09Z,http://arxiv.org/abs/2210.12960v2,"Zang-Hee Cho, Sun-Ha Paek, Young-Bo Kim, Taigyoun Cho, Hyejin Jeong, Haigun Lee"
Chatbot System Architecture,"  The conversational agents is one of the most interested topics in computer
science field in the recent decade. Which can be composite from more than one
subject in this field, which you need to apply Natural Language Processing
Concepts and some Artificial Intelligence Techniques such as Deep Learning
methods to make decision about how should be the response. This paper is
dedicated to discuss the system architecture for the conversational agent and
explain each component in details.
",2022-01-17T11:07:58Z,http://arxiv.org/abs/2201.06348v1,"Moataz Mohammed, Mostafa M. Aref"
"Leveraging Large Language Models for Wireless Symbol Detection via
  In-Context Learning","  Deep neural networks (DNNs) have made significant strides in tackling
challenging tasks in wireless systems, especially when an accurate wireless
model is not available. However, when available data is limited, traditional
DNNs often yield subpar results due to underfitting. At the same time, large
language models (LLMs) exemplified by GPT-3, have remarkably showcased their
capabilities across a broad range of natural language processing tasks. But
whether and how LLMs can benefit challenging non-language tasks in wireless
systems is unexplored. In this work, we propose to leverage the in-context
learning ability (a.k.a. prompting) of LLMs to solve wireless tasks in the low
data regime without any training or fine-tuning, unlike DNNs which require
training. We further demonstrate that the performance of LLMs varies
significantly when employed with different prompt templates. To solve this
issue, we employ the latest LLM calibration methods. Our results reveal that
using LLMs via ICL methods generally outperforms traditional DNNs on the symbol
demodulation task and yields highly confident predictions when coupled with
calibration techniques.
",2024-08-28T17:19:20Z,http://arxiv.org/abs/2409.00124v2,"Momin Abbas, Koushik Kar, Tianyi Chen"
"A Hybrid Natural Language Generation System Integrating Rules and Deep
  Learning Algorithms","  This paper proposes an enhanced natural language generation system combining
the merits of both rule-based approaches and modern deep learning algorithms,
boosting its performance to the extent where the generated textual content is
capable of exhibiting agile human-writing styles and the content logic of which
is highly controllable. We also come up with a novel approach called HMCU to
measure the performance of the natural language processing comprehensively and
precisely.
",2020-06-15T00:50:41Z,http://arxiv.org/abs/2006.09213v2,"Wei Wei, Bei Zhou, Georgios Leontidis"
"Distillation of Weighted Automata from Recurrent Neural Networks using a
  Spectral Approach","  This paper is an attempt to bridge the gap between deep learning and
grammatical inference. Indeed, it provides an algorithm to extract a
(stochastic) formal language from any recurrent neural network trained for
language modelling. In detail, the algorithm uses the already trained network
as an oracle -- and thus does not require the access to the inner
representation of the black-box -- and applies a spectral approach to infer a
weighted automaton.
  As weighted automata compute linear functions, they are computationally more
efficient than neural networks and thus the nature of the approach is the one
of knowledge distillation. We detail experiments on 62 data sets (both
synthetic and from real-world applications) that allow an in-depth study of the
abilities of the proposed algorithm. The results show the WA we extract are
good approximations of the RNN, validating the approach. Moreover, we show how
the process provides interesting insights toward the behavior of RNN learned on
data, enlarging the scope of this work to the one of explainability of deep
learning models.
",2020-09-28T07:04:15Z,http://arxiv.org/abs/2009.13101v1,"Remi Eyraud, Stephane Ayache"
"Cross-lingual Adaption Model-Agnostic Meta-Learning for Natural Language
  Understanding","  Meta learning with auxiliary languages has demonstrated promising
improvements for cross-lingual natural language processing. However, previous
studies sample the meta-training and meta-testing data from the same language,
which limits the ability of the model for cross-lingual transfer. In this
paper, we propose XLA-MAML, which performs direct cross-lingual adaption in the
meta-learning stage. We conduct zero-shot and few-shot experiments on Natural
Language Inference and Question Answering. The experimental results demonstrate
the effectiveness of our method across different languages, tasks, and
pretrained models. We also give analysis on various cross-lingual specific
settings for meta-learning including sampling strategy and parallelism.
",2021-11-10T16:53:50Z,http://arxiv.org/abs/2111.05805v1,"Qianying Liu, Fei Cheng, Sadao Kurohashi"
"Harlequin: Color-driven Generation of Synthetic Data for Referring
  Expression Comprehension","  Referring Expression Comprehension (REC) aims to identify a particular object
in a scene by a natural language expression, and is an important topic in
visual language understanding. State-of-the-art methods for this task are based
on deep learning, which generally requires expensive and manually labeled
annotations. Some works tackle the problem with limited-supervision learning or
relying on Large Vision and Language Models. However, the development of
techniques to synthesize labeled data is overlooked. In this paper, we propose
a novel framework that generates artificial data for the REC task, taking into
account both textual and visual modalities. At first, our pipeline processes
existing data to create variations in the annotations. Then, it generates an
image using altered annotations as guidance. The result of this pipeline is a
new dataset, called Harlequin, made by more than 1M queries. This approach
eliminates manual data collection and annotation, enabling scalability and
facilitating arbitrary complexity. We pre-train three REC models on Harlequin,
then fine-tuned and evaluated on human-annotated datasets. Our experiments show
that the pre-training on artificial data is beneficial for performance.
",2024-11-22T09:08:36Z,http://arxiv.org/abs/2411.14807v1,"Luca Parolari, Elena Izzo, Lamberto Ballan"
Advanced Multimodal Deep Learning Architecture for Image-Text Matching,"  Image-text matching is a key multimodal task that aims to model the semantic
association between images and text as a matching relationship. With the advent
of the multimedia information age, image, and text data show explosive growth,
and how to accurately realize the efficient and accurate semantic
correspondence between them has become the core issue of common concern in
academia and industry. In this study, we delve into the limitations of current
multimodal deep learning models in processing image-text pairing tasks.
Therefore, we innovatively design an advanced multimodal deep learning
architecture, which combines the high-level abstract representation ability of
deep neural networks for visual information with the advantages of natural
language processing models for text semantic understanding. By introducing a
novel cross-modal attention mechanism and hierarchical feature fusion strategy,
the model achieves deep fusion and two-way interaction between image and text
feature space. In addition, we also optimize the training objectives and loss
functions to ensure that the model can better map the potential association
structure between images and text during the learning process. Experiments show
that compared with existing image-text matching models, the optimized new model
has significantly improved performance on a series of benchmark data sets. In
addition, the new model also shows excellent generalization and robustness on
large and diverse open scenario datasets and can maintain high matching
performance even in the face of previously unseen complex situations.
",2024-06-13T08:32:24Z,http://arxiv.org/abs/2406.15306v1,"Jinyin Wang, Haijing Zhang, Yihao Zhong, Yingbin Liang, Rongwei Ji, Yiru Cang"
"GUIDO: A Hybrid Approach to Guideline Discovery & Ordering from Natural
  Language Texts","  Extracting workflow nets from textual descriptions can be used to simplify
guidelines or formalize textual descriptions of formal processes like business
processes and algorithms. The task of manually extracting processes, however,
requires domain expertise and effort. While automatic process model extraction
is desirable, annotating texts with formalized process models is expensive.
Therefore, there are only a few machine-learning-based extraction approaches.
Rule-based approaches, in turn, require domain specificity to work well and can
rarely distinguish relevant and irrelevant information in textual descriptions.
In this paper, we present GUIDO, a hybrid approach to the process model
extraction task that first, classifies sentences regarding their relevance to
the process model, using a BERT-based sentence classifier, and second, extracts
a process model from the sentences classified as relevant, using dependency
parsing. The presented approach achieves significantly better results than a
pure rule-based approach. GUIDO achieves an average behavioral similarity score
of $0.93$. Still, in comparison to purely machine-learning-based approaches,
the annotation costs stay low.
",2023-07-19T13:01:03Z,http://arxiv.org/abs/2307.09959v1,"Nils Freyer, Dustin Thewes, Matthias Meinecke"
Modeling Protein Using Large-scale Pretrain Language Model,"  Protein is linked to almost every life process. Therefore, analyzing the
biological structure and property of protein sequences is critical to the
exploration of life, as well as disease detection and drug discovery.
Traditional protein analysis methods tend to be labor-intensive and
time-consuming. The emergence of deep learning models makes modeling data
patterns in large quantities of data possible. Interdisciplinary researchers
have begun to leverage deep learning methods to model large biological
datasets, e.g. using long short-term memory and convolutional neural network
for protein sequence classification. After millions of years of evolution,
evolutionary information is encoded in protein sequences. Inspired by the
similarity between natural language and protein sequences, we use large-scale
language models to model evolutionary-scale protein sequences, encoding protein
biology information in representation. Significant improvements are observed in
both token-level and sequence-level tasks, demonstrating that our large-scale
model can accurately capture evolution information from pretraining on
evolutionary-scale individual sequences. Our code and model are available at
https://github.com/THUDM/ProteinLM.
",2021-08-17T04:13:11Z,http://arxiv.org/abs/2108.07435v2,"Yijia Xiao, Jiezhong Qiu, Ziang Li, Chang-Yu Hsieh, Jie Tang"
Knowledge-based Deep Learning for Modeling Chaotic Systems,"  Deep Learning has received increased attention due to its unbeatable success
in many fields, such as computer vision, natural language processing,
recommendation systems, and most recently in simulating multiphysics problems
and predicting nonlinear dynamical systems. However, modeling and forecasting
the dynamics of chaotic systems remains an open research problem since training
deep learning models requires big data, which is not always available in many
cases. Such deep learners can be trained from additional information obtained
from simulated results and by enforcing the physical laws of the chaotic
systems. This paper considers extreme events and their dynamics and proposes
elegant models based on deep neural networks, called knowledge-based deep
learning (KDL). Our proposed KDL can learn the complex patterns governing
chaotic systems by jointly training on real and simulated data directly from
the dynamics and their differential equations. This knowledge is transferred to
model and forecast real-world chaotic events exhibiting extreme behavior. We
validate the efficiency of our model by assessing it on three real-world
benchmark datasets: El Nino sea surface temperature, San Juan Dengue viral
infection, and Bj{\o}rn{\o}ya daily precipitation, all governed by extreme
events' dynamics. Using prior knowledge of extreme events and physics-based
loss functions to lead the neural network learning, we ensure physically
consistent, generalizable, and accurate forecasting, even in a small data
regime.
",2022-09-09T11:46:25Z,http://arxiv.org/abs/2209.04259v1,"Zakaria Elabid, Tanujit Chakraborty, Abdenour Hadid"
"Deep Learning for Time Series Classification and Extrinsic Regression: A
  Current Survey","  Time Series Classification and Extrinsic Regression are important and
challenging machine learning tasks. Deep learning has revolutionized natural
language processing and computer vision and holds great promise in other fields
such as time series analysis where the relevant features must often be
abstracted from the raw data but are not known a priori. This paper surveys the
current state of the art in the fast-moving field of deep learning for time
series classification and extrinsic regression. We review different network
architectures and training methods used for these tasks and discuss the
challenges and opportunities when applying deep learning to time series data.
We also summarize two critical applications of time series classification and
extrinsic regression, human activity recognition and satellite earth
observation.
",2023-02-06T01:01:00Z,http://arxiv.org/abs/2302.02515v2,"Navid Mohammadi Foumani, Lynn Miller, Chang Wei Tan, Geoffrey I. Webb, Germain Forestier, Mahsa Salehi"
"Multi-task learning for natural language processing in the 2020s: where
  are we going?","  Multi-task learning (MTL) significantly pre-dates the deep learning era, and
it has seen a resurgence in the past few years as researchers have been
applying MTL to deep learning solutions for natural language tasks. While
steady MTL research has always been present, there is a growing interest driven
by the impressive successes published in the related fields of transfer
learning and pre-training, such as BERT, and the release of new challenge
problems, such as GLUE and the NLP Decathlon (decaNLP). These efforts place
more focus on how weights are shared across networks, evaluate the re-usability
of network components and identify use cases where MTL can significantly
outperform single-task solutions. This paper strives to provide a comprehensive
survey of the numerous recent MTL contributions to the field of natural
language processing and provide a forum to focus efforts on the hardest
unsolved problems in the next decade. While novel models that improve
performance on NLP benchmarks are continually produced, lasting MTL challenges
remain unsolved which could hold the key to better language understanding,
knowledge discovery and natural language interfaces.
",2020-07-22T13:44:57Z,http://arxiv.org/abs/2007.16008v1,"Joseph Worsham, Jugal Kalita"
Semi-supervised Interactive Intent Labeling,"  Building the Natural Language Understanding (NLU) modules of task-oriented
Spoken Dialogue Systems (SDS) involves a definition of intents and entities,
collection of task-relevant data, annotating the data with intents and
entities, and then repeating the same process over and over again for adding
any functionality/enhancement to the SDS. In this work, we showcase an Intent
Bulk Labeling system where SDS developers can interactively label and augment
training data from unlabeled utterance corpora using advanced clustering and
visual labeling methods. We extend the Deep Aligned Clustering work with a
better backbone BERT model, explore techniques to select the seed data for
labeling, and develop a data balancing method using an oversampling technique
that utilizes paraphrasing models. We also look at the effect of data
augmentation on the clustering process. Our results show that we can achieve
over 10% gain in clustering accuracy on some datasets using the combination of
the above techniques. Finally, we extract utterance embeddings from the
clustering model and plot the data to interactively bulk label the samples,
reducing the time and effort for data labeling of the whole dataset
significantly.
",2021-04-27T18:06:55Z,http://arxiv.org/abs/2104.13406v2,"Saurav Sahay, Eda Okur, Nagib Hakim, Lama Nachman"
EduBERT: Pretrained Deep Language Models for Learning Analytics,"  The use of large pretrained neural networks to create contextualized word
embeddings has drastically improved performance on several natural language
processing (NLP) tasks. These computationally expensive models have begun to be
applied to domain-specific NLP tasks such as re-hospitalization prediction from
clinical notes. This paper demonstrates that using large pretrained models
produces excellent results on common learning analytics tasks. Pre-training
deep language models using student forum data from a wide array of online
courses improves performance beyond the state of the art on three text
classification tasks. We also show that a smaller, distilled version of our
model produces the best results on two of the three tasks while limiting
computational cost. We make both models available to the research community at
large.
",2019-12-02T11:32:53Z,http://arxiv.org/abs/1912.00690v1,"Benjamin Clavié, Kobi Gal"
BabyBear: Cheap inference triage for expensive language models,"  Transformer language models provide superior accuracy over previous models
but they are computationally and environmentally expensive. Borrowing the
concept of model cascading from computer vision, we introduce BabyBear, a
framework for cascading models for natural language processing (NLP) tasks to
minimize cost. The core strategy is inference triage, exiting early when the
least expensive model in the cascade achieves a sufficiently high-confidence
prediction. We test BabyBear on several open source data sets related to
document classification and entity recognition. We find that for common NLP
tasks a high proportion of the inference load can be accomplished with cheap,
fast models that have learned by observing a deep learning model. This allows
us to reduce the compute cost of large-scale classification jobs by more than
50% while retaining overall accuracy. For named entity recognition, we save 33%
of the deep learning compute while maintaining an F1 score higher than 95% on
the CoNLL benchmark.
",2022-05-24T03:21:07Z,http://arxiv.org/abs/2205.11747v1,"Leila Khalili, Yao You, John Bohannon"
A Survey on Deep Learning for Theorem Proving,"  Theorem proving is a fundamental aspect of mathematics, spanning from
informal reasoning in natural language to rigorous derivations in formal
systems. In recent years, the advancement of deep learning, especially the
emergence of large language models, has sparked a notable surge of research
exploring these techniques to enhance the process of theorem proving. This
paper presents a comprehensive survey of deep learning for theorem proving by
offering (i) a thorough review of existing approaches across various tasks such
as autoformalization, premise selection, proofstep generation, and proof
search; (ii) an extensive summary of curated datasets and strategies for
synthetic data generation; (iii) a detailed analysis of evaluation metrics and
the performance of state-of-the-art methods; and (iv) a critical discussion on
the persistent challenges and the promising avenues for future exploration. Our
survey aims to serve as a foundational reference for deep learning approaches
in theorem proving, inspiring and catalyzing further research endeavors in this
rapidly growing field. A curated list of papers is available at
https://github.com/zhaoyu-li/DL4TP.
",2024-04-15T17:07:55Z,http://arxiv.org/abs/2404.09939v3,"Zhaoyu Li, Jialiang Sun, Logan Murphy, Qidong Su, Zenan Li, Xian Zhang, Kaiyu Yang, Xujie Si"
"Training point-based deep learning networks for forest segmentation with
  synthetic data","  Remote sensing through unmanned aerial systems (UAS) has been increasing in
forestry in recent years, along with using machine learning for data
processing. Deep learning architectures, extensively applied in natural
language and image processing, have recently been extended to the point cloud
domain. However, the availability of point cloud datasets for training and
testing remains limited. Creating forested environment point cloud datasets is
expensive, requires high-precision sensors, and is time-consuming as manual
point classification is required. Moreover, forest areas could be inaccessible
or dangerous for humans, further complicating data collection. Then, a question
arises whether it is possible to use synthetic data to train deep learning
networks without the need to rely on large volumes of real forest data. To
answer this question, we developed a realistic simulator that procedurally
generates synthetic forest scenes. Thanks to this, we have conducted a
comparative study of different state-of-the-art point-based deep learning
networks for forest segmentation. Using created datasets, we determined the
feasibility of using synthetic data to train deep learning networks to classify
point clouds from real forest datasets. Both the simulator and the datasets are
released as part of this work.
",2024-03-21T04:01:26Z,http://arxiv.org/abs/2403.14115v2,"Francisco Raverta Capua, Juan Schandin, Pablo De Cristóforis"
"Automatic coding of students' writing via Contrastive Representation
  Learning in the Wasserstein space","  Qualitative analysis of verbal data is of central importance in the learning
sciences. It is labor-intensive and time-consuming, however, which limits the
amount of data researchers can include in studies. This work is a step towards
building a statistical machine learning (ML) method for achieving an automated
support for qualitative analyses of students' writing, here specifically in
score laboratory reports in introductory biology for sophistication of
argumentation and reasoning. We start with a set of lab reports from an
undergraduate biology course, scored by a four-level scheme that considers the
complexity of argument structure, the scope of evidence, and the care and
nuance of conclusions. Using this set of labeled data, we show that a popular
natural language modeling processing pipeline, namely vector representation of
words, a.k.a word embeddings, followed by Long Short Term Memory (LSTM) model
for capturing language generation as a state-space model, is able to
quantitatively capture the scoring, with a high Quadratic Weighted Kappa (QWK)
prediction score, when trained in via a novel contrastive learning set-up. We
show that the ML algorithm approached the inter-rater reliability of human
analysis. Ultimately, we conclude, that machine learning (ML) for natural
language processing (NLP) holds promise for assisting learning sciences
researchers in conducting qualitative studies at much larger scales than is
currently possible.
",2020-11-26T16:52:48Z,http://arxiv.org/abs/2011.13384v2,"Ruijie Jiang, Julia Gouvea, David Hammer, Eric Miller, Shuchin Aeron"
Unsupervised Ranking Model for Entity Coreference Resolution,"  Coreference resolution is one of the first stages in deep language
understanding and its importance has been well recognized in the natural
language processing community. In this paper, we propose a generative,
unsupervised ranking model for entity coreference resolution by introducing
resolution mode variables. Our unsupervised system achieves 58.44% F1 score of
the CoNLL metric on the English data from the CoNLL-2012 shared task (Pradhan
et al., 2012), outperforming the Stanford deterministic system (Lee et al.,
2013) by 3.01%.
",2016-03-15T04:39:15Z,http://arxiv.org/abs/1603.04553v1,"Xuezhe Ma, Zhengzhong Liu, Eduard Hovy"
"Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe
  Noise","  The growing importance of massive datasets used for deep learning makes
robustness to label noise a critical property for classifiers to have. Sources
of label noise include automatic labeling, non-expert labeling, and label
corruption by data poisoning adversaries. Numerous previous works assume that
no source of labels can be trusted. We relax this assumption and assume that a
small subset of the training data is trusted. This enables substantial label
corruption robustness performance gains. In addition, particularly severe label
noise can be combated by using a set of trusted data with clean labels. We
utilize trusted data by proposing a loss correction technique that utilizes
trusted examples in a data-efficient manner to mitigate the effects of label
noise on deep neural network classifiers. Across vision and natural language
processing tasks, we experiment with various label noises at several strengths,
and show that our method significantly outperforms existing methods.
",2018-02-14T19:48:50Z,http://arxiv.org/abs/1802.05300v4,"Dan Hendrycks, Mantas Mazeika, Duncan Wilson, Kevin Gimpel"
"STA: Self-controlled Text Augmentation for Improving Text
  Classifications","  Despite recent advancements in Machine Learning, many tasks still involve
working in low-data regimes which can make solving natural language problems
difficult. Recently, a number of text augmentation techniques have emerged in
the field of Natural Language Processing (NLP) which can enrich the training
data with new examples, though they are not without their caveats. For
instance, simple rule-based heuristic methods are effective, but lack variation
in semantic content and syntactic structure with respect to the original text.
On the other hand, more complex deep learning approaches can cause extreme
shifts in the intrinsic meaning of the text and introduce unwanted noise into
the training data. To more reliably control the quality of the augmented
examples, we introduce a state-of-the-art approach for Self-Controlled Text
Augmentation (STA). Our approach tightly controls the generation process by
introducing a self-checking procedure to ensure that generated examples retain
the semantic content of the original text. Experimental results on multiple
benchmarking datasets demonstrate that STA substantially outperforms existing
state-of-the-art techniques, whilst qualitative analysis reveals that the
generated examples are both lexically diverse and semantically reliable.
",2023-02-24T17:54:12Z,http://arxiv.org/abs/2302.12784v1,"Congcong Wang, Gonzalo Fiz Pontiveros, Steven Derby, Tri Kurniawan Wijaya"
"An Uncertainty-aware Deep Learning Framework-based Robust Design
  Optimization of Metamaterial Units","  Mechanical metamaterials represent an innovative class of artificial
structures, distinguished by their extraordinary mechanical characteristics,
which are beyond the scope of traditional natural materials. The use of deep
generative models has become increasingly popular in the design of metamaterial
units. The effectiveness of using deep generative models lies in their capacity
to compress complex input data into a simplified, lower-dimensional latent
space, while also enabling the creation of novel optimal designs through
sampling within this space. However, the design process does not take into
account the effect of model uncertainty due to data sparsity or the effect of
input data uncertainty due to inherent randomness in the data. This might lead
to the generation of undesirable structures with high sensitivity to the
uncertainties in the system. To address this issue, a novel uncertainty-aware
deep learning framework-based robust design approach is proposed for the design
of metamaterial units with optimal target properties. The proposed approach
utilizes the probabilistic nature of the deep learning framework and quantifies
both aleatoric and epistemic uncertainties associated with surrogate-based
design optimization. We demonstrate that the proposed design approach is
capable of designing high-performance metamaterial units with high reliability.
To showcase the effectiveness of the proposed design approach, a
single-objective design optimization problem and a multi-objective design
optimization problem are presented. The optimal robust designs obtained are
validated by comparing them to the designs obtained from the topology
optimization method as well as the designs obtained from a deterministic deep
learning framework-based design optimization where none of the uncertainties in
the system are explicitly considered.
",2024-07-19T22:21:27Z,http://arxiv.org/abs/2407.20251v1,"Zihan Wang, Anindya Bhaduri, Hongyi Xu, Liping Wang"
"Multimodal machine learning for materials science: composition-structure
  bimodal learning for experimentally measured properties","  The widespread application of multimodal machine learning models like GPT-4
has revolutionized various research fields including computer vision and
natural language processing. However, its implementation in materials
informatics remains underexplored, despite the presence of materials data
across diverse modalities, such as composition and structure. The effectiveness
of machine learning models trained on large calculated datasets depends on the
accuracy of calculations, while experimental datasets often have limited data
availability and incomplete information. This paper introduces a novel approach
to multimodal machine learning in materials science via composition-structure
bimodal learning. The proposed COmposition-Structure Bimodal Network (COSNet)
is designed to enhance learning and predictions of experimentally measured
materials properties that have incomplete structure information. Bimodal
learning significantly reduces prediction errors across distinct materials
properties including Li conductivity in solid electrolyte, band gap, refractive
index, dielectric constant, energy, and magnetic moment, surpassing
composition-only learning methods. Furthermore, we identified that data
augmentation based on modal availability plays a pivotal role in the success of
bimodal learning.
",2023-08-04T02:04:52Z,http://arxiv.org/abs/2309.04478v1,"Sheng Gong, Shuo Wang, Taishan Zhu, Yang Shao-Horn, Jeffrey C. Grossman"
"GIT-Mol: A Multi-modal Large Language Model for Molecular Science with
  Graph, Image, and Text","  Large language models have made significant strides in natural language
processing, enabling innovative applications in molecular science by processing
textual representations of molecules. However, most existing language models
cannot capture the rich information with complex molecular structures or
images. In this paper, we introduce GIT-Mol, a multi-modal large language model
that integrates the Graph, Image, and Text information. To facilitate the
integration of multi-modal molecular data, we propose GIT-Former, a novel
architecture that is capable of aligning all modalities into a unified latent
space. We achieve a 5%-10% accuracy increase in properties prediction and a
20.2% boost in molecule generation validity compared to the baselines. With the
any-to-language molecular translation strategy, our model has the potential to
perform more downstream tasks, such as compound name recognition and chemical
reaction prediction.
",2023-08-14T03:12:29Z,http://arxiv.org/abs/2308.06911v3,"Pengfei Liu, Yiming Ren, Jun Tao, Zhixiang Ren"
"Active Learning for Sequence Tagging with Deep Pre-trained Models and
  Bayesian Uncertainty Estimates","  Annotating training data for sequence tagging of texts is usually very
time-consuming. Recent advances in transfer learning for natural language
processing in conjunction with active learning open the possibility to
significantly reduce the necessary annotation budget. We are the first to
thoroughly investigate this powerful combination for the sequence tagging task.
We conduct an extensive empirical study of various Bayesian uncertainty
estimation methods and Monte Carlo dropout options for deep pre-trained models
in the active learning framework and find the best combinations for different
types of models. Besides, we also demonstrate that to acquire instances during
active learning, a full-size Transformer can be substituted with a distilled
version, which yields better computational performance and reduces obstacles
for applying deep active learning in practice.
",2021-01-20T13:59:25Z,http://arxiv.org/abs/2101.08133v2,"Artem Shelmanov, Dmitri Puzyrev, Lyubov Kupriyanova, Denis Belyakov, Daniil Larionov, Nikita Khromov, Olga Kozlova, Ekaterina Artemova, Dmitry V. Dylov, Alexander Panchenko"
A Selective Overview of Deep Learning,"  Deep learning has arguably achieved tremendous success in recent years. In
simple words, deep learning uses the composition of many nonlinear functions to
model the complex dependency between input features and labels. While neural
networks have a long history, recent advances have greatly improved their
performance in computer vision, natural language processing, etc. From the
statistical and scientific perspective, it is natural to ask: What is deep
learning? What are the new characteristics of deep learning, compared with
classical methods? What are the theoretical foundations of deep learning? To
answer these questions, we introduce common neural network models (e.g.,
convolutional neural nets, recurrent neural nets, generative adversarial nets)
and training techniques (e.g., stochastic gradient descent, dropout, batch
normalization) from a statistical point of view. Along the way, we highlight
new characteristics of deep learning (including depth and over-parametrization)
and explain their practical and theoretical benefits. We also sample recent
results on theories of deep learning, many of which are only suggestive. While
a complete understanding of deep learning remains elusive, we hope that our
perspectives and discussions serve as a stimulus for new statistical research.
",2019-04-10T17:53:15Z,http://arxiv.org/abs/1904.05526v2,"Jianqing Fan, Cong Ma, Yiqiao Zhong"
Fine-grained Sentiment Classification using BERT,"  Sentiment classification is an important process in understanding people's
perception towards a product, service, or topic. Many natural language
processing models have been proposed to solve the sentiment classification
problem. However, most of them have focused on binary sentiment classification.
In this paper, we use a promising deep learning model called BERT to solve the
fine-grained sentiment classification task. Experiments show that our model
outperforms other popular models for this task without sophisticated
architecture. We also demonstrate the effectiveness of transfer learning in
natural language processing in the process.
",2019-10-04T09:20:48Z,http://arxiv.org/abs/1910.03474v1,"Manish Munikar, Sushil Shakya, Aakash Shrestha"
Deep Active Learning for Data Mining from Conflict Text Corpora,"  High-resolution event data on armed conflict and related processes have
revolutionized the study of political contention with datasets like UCDP GED,
ACLED etc. However, most of these datasets limit themselves to collecting
spatio-temporal (high-resolution) and intensity data. Information on dynamics,
such as targets, tactics, purposes etc. are rarely collected owing to the
extreme workload of collecting data. However, most datasets rely on a rich
corpus of textual data allowing further mining of further information connected
to each event. This paper proposes one such approach that is inexpensive and
high performance, leveraging active learning - an iterative process of
improving a machine learning model based on sequential (guided) human input.
Active learning is employed to then step-wise train (fine-tuning) of a large,
encoder-only language model adapted for extracting sub-classes of events
relating to conflict dynamics. The approach shows performance similar to human
(gold-standard) coding while reducing the amount of required human annotation
by as much as 99%.
",2024-02-02T17:16:23Z,http://arxiv.org/abs/2402.01577v1,Mihai Croicu
"Explainable machine learning multi-label classification of Spanish legal
  judgements","  Artificial Intelligence techniques such as Machine Learning (ML) have not
been exploited to their maximum potential in the legal domain. This has been
partially due to the insufficient explanations they provided about their
decisions. Automatic expert systems with explanatory capabilities can be
specially useful when legal practitioners search jurisprudence to gather
contextual knowledge for their cases. Therefore, we propose a hybrid system
that applies ML for multi-label classification of judgements (sentences) and
visual and natural language descriptions for explanation purposes, boosted by
Natural Language Processing techniques and deep legal reasoning to identify the
entities, such as the parties, involved. We are not aware of any prior work on
automatic multi-label classification of legal judgements also providing natural
language explanations to the end-users with comparable overall quality. Our
solution achieves over 85 % micro precision on a labelled data set annotated by
legal experts. This endorses its interest to relieve human experts from
monotonous labour-intensive legal classification tasks.
",2024-05-27T19:16:42Z,http://arxiv.org/abs/2405.17610v1,"Francisco de Arriba-Pérez, Silvia García-Méndez, Francisco J. González-Castaño, Jaime González-González"
"Sensitive Data Detection and Classification in Spanish Clinical Text:
  Experiments with BERT","  Massive digital data processing provides a wide range of opportunities and
benefits, but at the cost of endangering personal data privacy. Anonymisation
consists in removing or replacing sensitive information from data, enabling its
exploitation for different purposes while preserving the privacy of
individuals. Over the years, a lot of automatic anonymisation systems have been
proposed; however, depending on the type of data, the target language or the
availability of training documents, the task remains challenging still. The
emergence of novel deep-learning models during the last two years has brought
large improvements to the state of the art in the field of Natural Language
Processing. These advancements have been most noticeably led by BERT, a model
proposed by Google in 2018, and the shared language models pre-trained on
millions of documents. In this paper, we use a BERT-based sequence labelling
model to conduct a series of anonymisation experiments on several clinical
datasets in Spanish. We also compare BERT to other algorithms. The experiments
show that a simple BERT-based model with general-domain pre-training obtains
highly competitive results without any domain specific feature engineering.
",2020-03-06T09:46:51Z,http://arxiv.org/abs/2003.03106v2,"Aitor García-Pablos, Naiara Perez, Montse Cuadros"
"Deep Unsupervised Domain Adaptation: A Review of Recent Advances and
  Perspectives","  Deep learning has become the method of choice to tackle real-world problems
in different domains, partly because of its ability to learn from data and
achieve impressive performance on a wide range of applications. However, its
success usually relies on two assumptions: (i) vast troves of labeled datasets
are required for accurate model fitting, and (ii) training and testing data are
independent and identically distributed. Its performance on unseen target
domains, thus, is not guaranteed, especially when encountering
out-of-distribution data at the adaptation stage. The performance drop on data
in a target domain is a critical problem in deploying deep neural networks that
are successfully trained on data in a source domain. Unsupervised domain
adaptation (UDA) is proposed to counter this, by leveraging both labeled source
domain data and unlabeled target domain data to carry out various tasks in the
target domain. UDA has yielded promising results on natural image processing,
video analysis, natural language processing, time-series data analysis, medical
image analysis, etc. In this review, as a rapidly evolving topic, we provide a
systematic comparison of its methods and applications. In addition, the
connection of UDA with its closely related tasks, e.g., domain generalization
and out-of-distribution detection, has also been discussed. Furthermore,
deficiencies in current methods and possible promising directions are
highlighted.
",2022-08-15T20:05:07Z,http://arxiv.org/abs/2208.07422v1,"Xiaofeng Liu, Chaehwa Yoo, Fangxu Xing, Hyejin Oh, Georges El Fakhri, Je-Won Kang, Jonghye Woo"
"Kencorpus: A Kenyan Language Corpus of Swahili, Dholuo and Luhya for
  Natural Language Processing Tasks","  Indigenous African languages are categorized as under-served in Natural
Language Processing. They therefore experience poor digital inclusivity and
information access. The processing challenge with such languages has been how
to use machine learning and deep learning models without the requisite data.
The Kencorpus project intends to bridge this gap by collecting and storing text
and speech data that is good enough for data-driven solutions in applications
such as machine translation, question answering and transcription in
multilingual communities. The Kencorpus dataset is a text and speech corpus for
three languages predominantly spoken in Kenya: Swahili, Dholuo and Luhya. Data
collection was done by researchers from communities, schools, media, and
publishers. The Kencorpus' dataset has a collection of 5,594 items - 4,442
texts (5.6M words) and 1,152 speech files (177hrs). Based on this data, Part of
Speech tagging sets for Dholuo and Luhya (50,000 and 93,000 words respectively)
were developed. We developed 7,537 Question-Answer pairs for Swahili and
created a text translation set of 13,400 sentences from Dholuo and Luhya into
Swahili. The datasets are useful for downstream machine learning tasks such as
model training and translation. We also developed two proof of concept systems:
for Kiswahili speech-to-text and machine learning system for Question Answering
task, with results of 18.87% word error rate and 80% Exact Match (EM)
respectively. These initial results give great promise to the usability of
Kencorpus to the machine learning community. Kencorpus is one of few public
domain corpora for these three low resource languages and forms a basis of
learning and sharing experiences for similar works especially for low resource
languages.
",2022-08-25T13:27:14Z,http://arxiv.org/abs/2208.12081v2,"Barack Wanjawa, Lilian Wanzare, Florence Indede, Owen McOnyango, Edward Ombui, Lawrence Muchemi"
Language Semantic Graph Guided Data-Efficient Learning,"  Developing generalizable models that can effectively learn from limited data
and with minimal reliance on human supervision is a significant objective
within the machine learning community, particularly in the era of deep neural
networks. Therefore, to achieve data-efficient learning, researchers typically
explore approaches that can leverage more related or unlabeled data without
necessitating additional manual labeling efforts, such as Semi-Supervised
Learning (SSL), Transfer Learning (TL), and Data Augmentation (DA). SSL
leverages unlabeled data in the training process, while TL enables the transfer
of expertise from related data distributions. DA broadens the dataset by
synthesizing new data from existing examples. However, the significance of
additional knowledge contained within labels has been largely overlooked in
research. In this paper, we propose a novel perspective on data efficiency that
involves exploiting the semantic information contained in the labels of the
available data. Specifically, we introduce a Language Semantic Graph (LSG)
which is constructed from labels manifest as natural language descriptions.
Upon this graph, an auxiliary graph neural network is trained to extract
high-level semantic relations and then used to guide the training of the
primary model, enabling more adequate utilization of label knowledge. Across
image, video, and audio modalities, we utilize the LSG method in both TL and
SSL scenarios and illustrate its versatility in significantly enhancing
performance compared to other data-efficient learning approaches. Additionally,
our in-depth analysis shows that the LSG method also expedites the training
process.
",2023-11-15T08:54:57Z,http://arxiv.org/abs/2311.08782v1,"Wenxuan Ma, Shuang Li, Lincan Cai, Jingxuan Kang"
Neural Networks for Entity Matching: A Survey,"  Entity matching is the problem of identifying which records refer to the same
real-world entity. It has been actively researched for decades, and a variety
of different approaches have been developed. Even today, it remains a
challenging problem, and there is still generous room for improvement. In
recent years we have seen new methods based upon deep learning techniques for
natural language processing emerge.
  In this survey, we present how neural networks have been used for entity
matching. Specifically, we identify which steps of the entity matching process
existing work have targeted using neural networks, and provide an overview of
the different techniques used at each step. We also discuss contributions from
deep learning in entity matching compared to traditional methods, and propose a
taxonomy of deep neural networks for entity matching.
",2020-10-21T15:36:03Z,http://arxiv.org/abs/2010.11075v2,"Nils Barlaug, Jon Atle Gulla"
A Deep Learning System for Domain-specific Speech Recognition,"  As human-machine voice interfaces provide easy access to increasingly
intelligent machines, many state-of-the-art automatic speech recognition (ASR)
systems are proposed. However, commercial ASR systems usually have poor
performance on domain-specific speech especially under low-resource settings.
The author works with pre-trained DeepSpeech2 and Wav2Vec2 acoustic models to
develop benefit-specific ASR systems. The domain-specific data are collected
using proposed semi-supervised learning annotation with little human
intervention. The best performance comes from a fine-tuned Wav2Vec2-Large-LV60
acoustic model with an external KenLM, which surpasses the Google and AWS ASR
systems on benefit-specific speech. The viability of using error prone ASR
transcriptions as part of spoken language understanding (SLU) is also
investigated. Results of a benefit-specific natural language understanding
(NLU) task show that the domain-specific fine-tuned ASR system can outperform
the commercial ASR systems even when its transcriptions have higher word error
rate (WER), and the results between fine-tuned ASR and human transcriptions are
similar.
",2023-03-18T22:19:09Z,http://arxiv.org/abs/2303.10510v2,Yanan Jia
"DataAgent: Evaluating Large Language Models' Ability to Answer
  Zero-Shot, Natural Language Queries","  Conventional processes for analyzing datasets and extracting meaningful
information are often time-consuming and laborious. Previous work has
identified manual, repetitive coding and data collection as major obstacles
that hinder data scientists from undertaking more nuanced labor and high-level
projects. To combat this, we evaluated OpenAI's GPT-3.5 as a ""Language Data
Scientist"" (LDS) that can extrapolate key findings, including correlations and
basic information, from a given dataset. The model was tested on a diverse set
of benchmark datasets to evaluate its performance across multiple standards,
including data science code-generation based tasks involving libraries such as
NumPy, Pandas, Scikit-Learn, and TensorFlow, and was broadly successful in
correctly answering a given data science query related to the benchmark
dataset. The LDS used various novel prompt engineering techniques to
effectively answer a given question, including Chain-of-Thought reinforcement
and SayCan prompt engineering. Our findings demonstrate great potential for
leveraging Large Language Models for low-level, zero-shot data analysis.
",2024-03-29T22:59:34Z,http://arxiv.org/abs/2404.00188v1,"Manit Mishra, Abderrahman Braham, Charles Marsom, Bryan Chung, Gavin Griffin, Dakshesh Sidnerlikar, Chatanya Sarin, Arjun Rajaram"
"Enable Deep Learning on Mobile Devices: Methods, Systems, and
  Applications","  Deep neural networks (DNNs) have achieved unprecedented success in the field
of artificial intelligence (AI), including computer vision, natural language
processing and speech recognition. However, their superior performance comes at
the considerable cost of computational complexity, which greatly hinders their
applications in many resource-constrained devices, such as mobile phones and
Internet of Things (IoT) devices. Therefore, methods and techniques that are
able to lift the efficiency bottleneck while preserving the high accuracy of
DNNs are in great demand in order to enable numerous edge AI applications. This
paper provides an overview of efficient deep learning methods, systems and
applications. We start from introducing popular model compression methods,
including pruning, factorization, quantization as well as compact model design.
To reduce the large design cost of these manual solutions, we discuss the
AutoML framework for each of them, such as neural architecture search (NAS) and
automated pruning and quantization. We then cover efficient on-device training
to enable user customization based on the local data on mobile devices. Apart
from general acceleration techniques, we also showcase several task-specific
accelerations for point cloud, video and natural language processing by
exploiting their spatial sparsity and temporal/token redundancy. Finally, to
support all these algorithmic advancements, we introduce the efficient deep
learning system design from both software and hardware perspectives.
",2022-04-25T16:52:48Z,http://arxiv.org/abs/2204.11786v1,"Han Cai, Ji Lin, Yujun Lin, Zhijian Liu, Haotian Tang, Hanrui Wang, Ligeng Zhu, Song Han"
Multi-Task Learning in Natural Language Processing: An Overview,"  Deep learning approaches have achieved great success in the field of Natural
Language Processing (NLP). However, directly training deep neural models often
suffer from overfitting and data scarcity problems that are pervasive in NLP
tasks. In recent years, Multi-Task Learning (MTL), which can leverage useful
information of related tasks to achieve simultaneous performance improvement on
these tasks, has been used to handle these problems. In this paper, we give an
overview of the use of MTL in NLP tasks. We first review MTL architectures used
in NLP tasks and categorize them into four classes, including parallel
architecture, hierarchical architecture, modular architecture, and generative
adversarial architecture. Then we present optimization techniques on loss
construction, gradient regularization, data sampling, and task scheduling to
properly train a multi-task model. After presenting applications of MTL in a
variety of NLP tasks, we introduce some benchmark datasets. Finally, we make a
conclusion and discuss several possible research directions in this field.
",2021-09-19T14:51:51Z,http://arxiv.org/abs/2109.09138v2,"Shijie Chen, Yu Zhang, Qiang Yang"
"Potential, Challenges and Future Directions for Deep Learning in
  Prognostics and Health Management Applications","  Deep learning applications have been thriving over the last decade in many
different domains, including computer vision and natural language
understanding. The drivers for the vibrant development of deep learning have
been the availability of abundant data, breakthroughs of algorithms and the
advancements in hardware. Despite the fact that complex industrial assets have
been extensively monitored and large amounts of condition monitoring signals
have been collected, the application of deep learning approaches for detecting,
diagnosing and predicting faults of complex industrial assets has been limited.
The current paper provides a thorough evaluation of the current developments,
drivers, challenges, potential solutions and future research needs in the field
of deep learning applied to Prognostics and Health Management (PHM)
applications.
",2020-05-05T13:35:28Z,http://arxiv.org/abs/2005.02144v1,"Olga Fink, Qin Wang, Markus Svensén, Pierre Dersin, Wan-Jui Lee, Melanie Ducoffe"
"The Vulnerability of the Neural Networks Against Adversarial Examples in
  Deep Learning Algorithms","  With further development in the fields of computer vision, network security,
natural language processing and so on so forth, deep learning technology
gradually exposed certain security risks. The existing deep learning algorithms
cannot effectively describe the essential characteristics of data, making the
algorithm unable to give the correct result in the face of malicious input.
Based on current security threats faced by deep learning, this paper introduces
the problem of adversarial examples in deep learning, sorts out the existing
attack and defense methods of the black box and white box, and classifies them.
It briefly describes the application of some adversarial examples in different
scenarios in recent years, compares several defense technologies of adversarial
examples, and finally summarizes the problems in this research field and
prospects for its future development. This paper introduces the common white
box attack methods in detail, and further compares the similarities and
differences between the attack of the black and white box. Correspondingly, the
author also introduces the defense methods, and analyzes the performance of
these methods against the black and white box attack.
",2020-11-02T04:41:08Z,http://arxiv.org/abs/2011.05976v2,Rui Zhao
"Word class representations spontaneously emerge in a deep neural network
  trained on next word prediction","  How do humans learn language, and can the first language be learned at all?
These fundamental questions are still hotly debated. In contemporary
linguistics, there are two major schools of thought that give completely
opposite answers. According to Chomsky's theory of universal grammar, language
cannot be learned because children are not exposed to sufficient data in their
linguistic environment. In contrast, usage-based models of language assume a
profound relationship between language structure and language use. In
particular, contextual mental processing and mental representations are assumed
to have the cognitive capacity to capture the complexity of actual language use
at all levels. The prime example is syntax, i.e., the rules by which words are
assembled into larger units such as sentences. Typically, syntactic rules are
expressed as sequences of word classes. However, it remains unclear whether
word classes are innate, as implied by universal grammar, or whether they
emerge during language acquisition, as suggested by usage-based approaches.
Here, we address this issue from a machine learning and natural language
processing perspective. In particular, we trained an artificial deep neural
network on predicting the next word, provided sequences of consecutive words as
input. Subsequently, we analyzed the emerging activation patterns in the hidden
layers of the neural network. Strikingly, we find that the internal
representations of nine-word input sequences cluster according to the word
class of the tenth word to be predicted as output, even though the neural
network did not receive any explicit information about syntactic rules or word
classes during training. This surprising result suggests, that also in the
human brain, abstract representational categories such as word classes may
naturally emerge as a consequence of predictive coding and processing during
language acquisition.
",2023-02-15T11:02:50Z,http://arxiv.org/abs/2302.07588v1,"Kishore Surendra, Achim Schilling, Paul Stoewer, Andreas Maier, Patrick Krauss"
GPT-3 Models are Poor Few-Shot Learners in the Biomedical Domain,"  Deep neural language models have set new breakthroughs in many tasks of
Natural Language Processing (NLP). Recent work has shown that deep transformer
language models (pretrained on large amounts of texts) can achieve high levels
of task-specific few-shot performance comparable to state-of-the-art models.
However, the ability of these large language models in few-shot transfer
learning has not yet been explored in the biomedical domain. We investigated
the performance of two powerful transformer language models, i.e. GPT-3 and
BioBERT, in few-shot settings on various biomedical NLP tasks. The experimental
results showed that, to a great extent, both the models underperform a language
model fine-tuned on the full training data. Although GPT-3 had already achieved
near state-of-the-art results in few-shot knowledge transfer on open-domain NLP
tasks, it could not perform as effectively as BioBERT, which is orders of
magnitude smaller than GPT-3. Regarding that BioBERT was already pretrained on
large biomedical text corpora, our study suggests that language models may
largely benefit from in-domain pretraining in task-specific few-shot learning.
However, in-domain pretraining seems not to be sufficient; novel pretraining
and few-shot learning strategies are required in the biomedical NLP domain.
",2021-09-06T15:50:37Z,http://arxiv.org/abs/2109.02555v2,"Milad Moradi, Kathrin Blagec, Florian Haberl, Matthias Samwald"
Paraphrase Generation with Deep Reinforcement Learning,"  Automatic generation of paraphrases from a given sentence is an important yet
challenging task in natural language processing (NLP), and plays a key role in
a number of applications such as question answering, search, and dialogue. In
this paper, we present a deep reinforcement learning approach to paraphrase
generation. Specifically, we propose a new framework for the task, which
consists of a \textit{generator} and an \textit{evaluator}, both of which are
learned from data. The generator, built as a sequence-to-sequence learning
model, can produce paraphrases given a sentence. The evaluator, constructed as
a deep matching model, can judge whether two sentences are paraphrases of each
other. The generator is first trained by deep learning and then further
fine-tuned by reinforcement learning in which the reward is given by the
evaluator. For the learning of the evaluator, we propose two methods based on
supervised learning and inverse reinforcement learning respectively, depending
on the type of available training data. Empirical study shows that the learned
evaluator can guide the generator to produce more accurate paraphrases.
Experimental results demonstrate the proposed models (the generators)
outperform the state-of-the-art methods in paraphrase generation in both
automatic evaluation and human evaluation.
",2017-11-01T10:40:42Z,http://arxiv.org/abs/1711.00279v3,"Zichao Li, Xin Jiang, Lifeng Shang, Hang Li"
"Natural Language Processing in Electronic Health Records in Relation to
  Healthcare Decision-making: A Systematic Review","  Background: Natural Language Processing (NLP) is widely used to extract
clinical insights from Electronic Health Records (EHRs). However, the lack of
annotated data, automated tools, and other challenges hinder the full
utilisation of NLP for EHRs. Various Machine Learning (ML), Deep Learning (DL)
and NLP techniques are studied and compared to understand the limitations and
opportunities in this space comprehensively.
  Methodology: After screening 261 articles from 11 databases, we included 127
papers for full-text review covering seven categories of articles: 1) medical
note classification, 2) clinical entity recognition, 3) text summarisation, 4)
deep learning (DL) and transfer learning architecture, 5) information
extraction, 6) Medical language translation and 7) other NLP applications. This
study follows the Preferred Reporting Items for Systematic Reviews and
Meta-Analyses (PRISMA) guidelines.
  Result and Discussion: EHR was the most commonly used data type among the
selected articles, and the datasets were primarily unstructured. Various ML and
DL methods were used, with prediction or classification being the most common
application of ML or DL. The most common use cases were: the International
Classification of Diseases, Ninth Revision (ICD-9) classification, clinical
note analysis, and named entity recognition (NER) for clinical descriptions and
research on psychiatric disorders.
  Conclusion: We find that the adopted ML models were not adequately assessed.
In addition, the data imbalance problem is quite important, yet we must find
techniques to address this underlining problem. Future studies should address
key limitations in studies, primarily identifying Lupus Nephritis, Suicide
Attempts, perinatal self-harmed and ICD-9 classification.
",2023-06-22T12:10:41Z,http://arxiv.org/abs/2306.12834v1,"Elias Hossain, Rajib Rana, Niall Higgins, Jeffrey Soar, Prabal Datta Barua, Anthony R. Pisani, Ph. D, Kathryn Turner}"
V-CNN: When Convolutional Neural Network encounters Data Visualization,"  In recent years, deep learning poses a deep technical revolution in almost
every field and attracts great attentions from industry and academia.
Especially, the convolutional neural network (CNN), one representative model of
deep learning, achieves great successes in computer vision and natural language
processing. However, simply or blindly applying CNN to the other fields results
in lower training effects or makes it quite difficult to adjust the model
parameters. In this poster, we propose a general methodology named V-CNN by
introducing data visualizing for CNN. V-CNN introduces a data visualization
model prior to CNN modeling to make sure the data after processing is fit for
the features of images as well as CNN modeling. We apply V-CNN to the network
intrusion detection problem based on a famous practical dataset: AWID.
Simulation results confirm V-CNN significantly outperforms other studies and
the recall rate of each invasion category is more than 99.8%.
",2018-06-12T10:57:57Z,http://arxiv.org/abs/1807.02164v1,"Mao Yang, Bo Li, Guanxiong Feng, Zhongjiang Yan"
"Security Vulnerability Detection Using Deep Learning Natural Language
  Processing","  Detecting security vulnerabilities in software before they are exploited has
been a challenging problem for decades. Traditional code analysis methods have
been proposed, but are often ineffective and inefficient. In this work, we
model software vulnerability detection as a natural language processing (NLP)
problem with source code treated as texts, and address the automated software
venerability detection with recent advanced deep learning NLP models assisted
by transfer learning on written English. For training and testing, we have
preprocessed the NIST NVD/SARD databases and built a dataset of over 100,000
files in $C$ programming language with 123 types of vulnerabilities. The
extensive experiments generate the best performance of over 93\% accuracy in
detecting security vulnerabilities.
",2021-05-06T01:28:21Z,http://arxiv.org/abs/2105.02388v1,"Noah Ziems, Shaoen Wu"
"Multimodal Deep Neural Networks using Both Engineered and Learned
  Representations for Biodegradability Prediction","  Deep learning algorithms excel at extracting patterns from raw data, and with
large datasets, they have been very successful in computer vision and natural
language applications. However, in other domains, large datasets on which to
learn representations from may not exist. In this work, we develop a novel
multimodal CNN-MLP neural network architecture that utilizes both
domain-specific feature engineering as well as learned representations from raw
data. We illustrate the effectiveness of such network designs in the chemical
sciences, for predicting biodegradability. DeepBioD, a multimodal CNN-MLP
network is more accurate than either standalone network designs, and achieves
an error classification rate of 0.125 that is 27% lower than the current
state-of-the-art. Thus, our work indicates that combining traditional feature
engineering with representation learning can be effective, particularly in
situations where labeled data is limited.
",2018-08-13T20:36:08Z,http://arxiv.org/abs/1808.04456v2,"Garrett B. Goh, Khushmeen Sakloth, Charles Siegel, Abhinav Vishnu, Jim Pfaendtner"
"Processamento de linguagem natural em Português e aprendizagem
  profunda para o domínio de Óleo e Gás","  Over the last few decades, institutions around the world have been challenged
to deal with the sheer volume of information captured in unstructured formats,
especially in textual documents. The so called Digital Transformation age,
characterized by important technological advances and the advent of disruptive
methods in Artificial Intelligence, offers opportunities to make better use of
this information. Recent techniques in Natural Language Processing (NLP) with
Deep Learning approaches allow to efficiently process a large volume of data in
order to obtain relevant information, to identify patterns, classify text,
among other applications. In this context, the highly technical vocabulary of
Oil and Gas (O&G) domain represents a challenge for these NLP algorithms, in
which terms can assume a very different meaning in relation to common sense
understanding. The search for suitable mathematical representations and
specific models requires a large amount of representative corpora in the O&G
domain. However, public access to this material is scarce in the scientific
literature, especially considering the Portuguese language. This paper presents
a literature review about the main techniques for deep learning NLP and their
major applications for O&G domain in Portuguese.
",2019-08-05T15:05:48Z,http://arxiv.org/abs/1908.01674v2,"Diogo Gomes, Alexandre Evsukoff"
"Librarian-in-the-Loop: A Natural Language Processing Paradigm for
  Detecting Informal Mentions of Research Data in Academic Literature","  Data citations provide a foundation for studying research data impact.
Collecting and managing data citations is a new frontier in archival science
and scholarly communication. However, the discovery and curation of research
data citations is labor intensive. Data citations that reference unique
identifiers (i.e. DOIs) are readily findable; however, informal mentions made
to research data are more challenging to infer. We propose a natural language
processing (NLP) paradigm to support the human task of identifying informal
mentions made to research datasets. The work of discovering informal data
mentions is currently performed by librarians and their staff in the
Inter-university Consortium for Political and Social Research (ICPSR), a large
social science data archive that maintains a large bibliography of data-related
literature. The NLP model is bootstrapped from data citations actively
collected by librarians at ICPSR. The model combines pattern matching with
multiple iterations of human annotations to learn additional rules for
detecting informal data mentions. These examples are then used to train an NLP
pipeline. The librarian-in-the-loop paradigm is centered in the data work
performed by ICPSR librarians, supporting broader efforts to build a more
comprehensive bibliography of data-related literature that reflects the
scholarly communities of research data users.
",2022-03-10T02:11:30Z,http://arxiv.org/abs/2203.05112v1,"Lizhou Fan, Sara Lafia, David Bleckley, Elizabeth Moss, Andrea Thomer, Libby Hemphill"
A Robust Deep Ensemble Classifier for Figurative Language Detection,"  Recognition and classification of Figurative Language (FL) is an open problem
of Sentiment Analysis in the broader field of Natural Language Processing (NLP)
due to the contradictory meaning contained in phrases with metaphorical
content. The problem itself contains three interrelated FL recognition tasks:
sarcasm, irony and metaphor which, in the present paper, are dealt with
advanced Deep Learning (DL) techniques. First, we introduce a data
prepossessing framework towards efficient data representation formats so that
to optimize the respective inputs to the DL models. In addition, special
features are extracted in order to characterize the syntactic, expressive,
emotional and temper content reflected in the respective social media text
references. These features aim to capture aspects of the social network user's
writing method. Finally, features are fed to a robust, Deep Ensemble Soft
Classifier (DESC) which is based on the combination of different DL techniques.
Using three different benchmark datasets (one of them containing various FL
forms) we conclude that the DESC model achieves a very good performance, worthy
of comparison with relevant methodologies and state-of-the-art technologies in
the challenging field of FL recognition.
",2021-07-09T11:26:37Z,http://arxiv.org/abs/2107.04372v1,"Rolandos Alexandros Potamias, Georgios Siolas, Andreas - Georgios Stafylopatis"
"Background-aware Multi-source Fusion Financial Trend Forecasting
  Mechanism","  Stock prices, as an economic indicator, reflect changes in economic
development and market conditions. Traditional stock price prediction models
often only consider time-series data and are limited by the mechanisms of the
models themselves. Some deep learning models have high computational costs,
depend on a large amount of high-quality data, and have poor interpretations,
making it difficult to intuitively understand the driving factors behind the
predictions. Some studies have used deep learning models to extract text
features and combine them with price data to make joint predictions, but there
are issues with dealing with information noise, accurate extraction of text
sentiment, and how to efficiently fuse text and numerical data. To address
these issues in this paper, we propose a background-aware multi-source fusion
financial trend forecasting mechanism. The system leverages a large language
model to extract key information from policy and stock review texts, utilizing
the MacBERT model to generate feature vectors. These vectors are then
integrated with stock price data to form comprehensive feature representations.
These integrated features are input into a neural network comprising various
deep learning architectures. By integrating multiple data sources, the system
offers a holistic view of market dynamics. It harnesses the comprehensive
analytical and interpretative capabilities of large language models, retaining
deep semantic and sentiment information from policy texts to provide richer
input features for stock trend prediction. Additionally, we compare the
accuracy of six models (LSTM, BiLSTM, MogrifierLSTM, GRU, ST-LSTM, SwinLSTM).
The results demonstrate that our system achieves generally better accuracy in
predicting stock movements, attributed to the incorporation of large language
model processing, policy information, and other influential features.
",2024-07-01T02:10:17Z,http://arxiv.org/abs/2407.00904v1,"Fengting Mo, Shanshan Yan, Yinhao Xiao"
"Crystal Transformer: Self-learning neural language model for Generative
  and Tinkering Design of Materials","  Self-supervised neural language models have recently achieved unprecedented
success, from natural language processing to learning the languages of
biological sequences and organic molecules. These models have demonstrated
superior performance in the generation, structure classification, and
functional predictions for proteins and molecules with learned representations.
However, most of the masking-based pre-trained language models are not designed
for generative design, and their black-box nature makes it difficult to
interpret their design logic. Here we propose BLMM Crystal Transformer, a
neural network based probabilistic generative model for generative and
tinkering design of inorganic materials. Our model is built on the blank
filling language model for text generation and has demonstrated unique
advantages in learning the ""materials grammars"" together with high-quality
generation, interpretability, and data efficiency. It can generate chemically
valid materials compositions with as high as 89.7\% charge neutrality and
84.8\% balanced electronegativity, which are more than 4 and 8 times higher
compared to a pseudo random sampling baseline. The probabilistic generation
process of BLMM allows it to recommend tinkering operations based on learned
materials chemistry and makes it useful for materials doping. Combined with the
TCSP crysal structure prediction algorithm, We have applied our model to
discover a set of new materials as validated using DFT calculations. Our work
thus brings the unsupervised transformer language models based generative
artificial intelligence to inorganic materials. A user-friendly web app has
been developed for computational materials doping and can be accessed freely at
\url{www.materialsatlas.org/blmtinker}.
",2022-04-25T20:20:26Z,http://arxiv.org/abs/2204.11953v1,"Lai Wei, Qinyang Li, Yuqi Song, Stanislav Stefanov, Edirisuriya M. D. Siriwardane, Fanglin Chen, Jianjun Hu"
"Diatom-inspired architected materials using language-based deep
  learning: Perception, transformation and manufacturing","  Learning from nature has been a quest of humanity for millennia. While this
has taken the form of humans assessing natural designs such as bones, butterfly
wings, or spider webs, we can now achieve generating designs using advanced
computational algorithms. In this paper we report novel biologically inspired
designs of diatom structures, enabled using transformer neural networks, using
natural language models to learn, process and transfer insights across
manifestations. We illustrate a series of novel diatom-based designs and also
report a manufactured specimen, created using additive manufacturing. The
method applied here could be expanded to focus on other biological design cues,
implement a systematic optimization to meet certain design targets, and include
a hybrid set of material design sets.
",2023-01-14T10:02:51Z,http://arxiv.org/abs/2301.05875v1,Markus J. Buehler
"A neural document language modeling framework for spoken document
  retrieval","  Recent developments in deep learning have led to a significant innovation in
various classic and practical subjects, including speech recognition, computer
vision, question answering, information retrieval and so on. In the context of
natural language processing (NLP), language representations have shown giant
successes in many downstream tasks, so the school of studies have become a
major stream of research recently. Because the immenseness of multimedia data
along with speech have spread around the world in our daily life, spoken
document retrieval (SDR) has become an important research subject in the past
decades. Targeting on enhancing the SDR performance, the paper concentrates on
proposing a neural retrieval framework, which assembles the merits of using
language modeling (LM) mechanism in SDR and leveraging the abstractive
information learned by the language representation models. Consequently, to our
knowledge, this is a pioneer study on supervised training of a neural LM-based
SDR framework, especially combined with the pretrained language representation
methods.
",2019-10-31T07:50:41Z,http://arxiv.org/abs/1910.14286v1,"Li-Phen Yen, Zhen-Yu Wu, Kuan-Yu Chen"
A Survey on Dialogue Systems: Recent Advances and New Frontiers,"  Dialogue systems have attracted more and more attention. Recent advances on
dialogue systems are overwhelmingly contributed by deep learning techniques,
which have been employed to enhance a wide range of big data applications such
as computer vision, natural language processing, and recommender systems. For
dialogue systems, deep learning can leverage a massive amount of data to learn
meaningful feature representations and response generation strategies, while
requiring a minimum amount of hand-crafting. In this article, we give an
overview to these recent advances on dialogue systems from various perspectives
and discuss some possible research directions. In particular, we generally
divide existing dialogue systems into task-oriented and non-task-oriented
models, then detail how deep learning techniques help them with representative
algorithms and finally discuss some appealing research directions that can
bring the dialogue system research into a new frontier.
",2017-11-06T05:20:54Z,http://arxiv.org/abs/1711.01731v3,"Hongshen Chen, Xiaorui Liu, Dawei Yin, Jiliang Tang"
Deep Anomaly Detection with Outlier Exposure,"  It is important to detect anomalous inputs when deploying machine learning
systems. The use of larger and more complex inputs in deep learning magnifies
the difficulty of distinguishing between anomalous and in-distribution
examples. At the same time, diverse image and text data are available in
enormous quantities. We propose leveraging these data to improve deep anomaly
detection by training anomaly detectors against an auxiliary dataset of
outliers, an approach we call Outlier Exposure (OE). This enables anomaly
detectors to generalize and detect unseen anomalies. In extensive experiments
on natural language processing and small- and large-scale vision tasks, we find
that Outlier Exposure significantly improves detection performance. We also
observe that cutting-edge generative models trained on CIFAR-10 may assign
higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to
mitigate this issue. We also analyze the flexibility and robustness of Outlier
Exposure, and identify characteristics of the auxiliary dataset that improve
performance.
",2018-12-11T18:49:50Z,http://arxiv.org/abs/1812.04606v3,"Dan Hendrycks, Mantas Mazeika, Thomas Dietterich"
"Attention mechanisms and deep learning for machine vision: A survey of
  the state of the art","  With the advent of state of the art nature-inspired pure attention based
models i.e. transformers, and their success in natural language processing
(NLP), their extension to machine vision (MV) tasks was inevitable and much
felt. Subsequently, vision transformers (ViTs) were introduced which are giving
quite a challenge to the established deep learning based machine vision
techniques. However, pure attention based models/architectures like
transformers require huge data, large training times and large computational
resources. Some recent works suggest that combinations of these two varied
fields can prove to build systems which have the advantages of both these
fields. Accordingly, this state of the art survey paper is introduced which
hopefully will help readers get useful information about this interesting and
potential research area. A gentle introduction to attention mechanisms is
given, followed by a discussion of the popular attention based deep
architectures. Subsequently, the major categories of the intersection of
attention mechanisms and deep learning for machine vision (MV) based are
discussed. Afterwards, the major algorithms, issues and trends within the scope
of the paper are discussed.
",2021-06-03T10:23:32Z,http://arxiv.org/abs/2106.07550v1,"Abdul Mueed Hafiz, Shabir Ahmad Parah, Rouf Ul Alam Bhat"
Natural Language Adversarial Defense through Synonym Encoding,"  In the area of natural language processing, deep learning models are recently
known to be vulnerable to various types of adversarial perturbations, but
relatively few works are done on the defense side. Especially, there exists few
effective defense method against the successful synonym substitution based
attacks that preserve the syntactic structure and semantic information of the
original text while fooling the deep learning models. We contribute in this
direction and propose a novel adversarial defense method called Synonym
Encoding Method (SEM). Specifically, SEM inserts an encoder before the input
layer of the target model to map each cluster of synonyms to a unique encoding
and trains the model to eliminate possible adversarial perturbations without
modifying the network architecture or adding extra data. Extensive experiments
demonstrate that SEM can effectively defend the current synonym substitution
based attacks and block the transferability of adversarial examples. SEM is
also easy and efficient to scale to large models and big datasets.
",2019-09-15T03:35:18Z,http://arxiv.org/abs/1909.06723v4,"Xiaosen Wang, Hao Jin, Yichen Yang, Kun He"
Learning Unification-Based Natural Language Grammars,"  When parsing unrestricted language, wide-covering grammars often
undergenerate. Undergeneration can be tackled either by sentence correction, or
by grammar correction. This thesis concentrates upon automatic grammar
correction (or machine learning of grammar) as a solution to the problem of
undergeneration. Broadly speaking, grammar correction approaches can be
classified as being either {\it data-driven}, or {\it model-based}. Data-driven
learners use data-intensive methods to acquire grammar. They typically use
grammar formalisms unsuited to the needs of practical text processing and
cannot guarantee that the resulting grammar is adequate for subsequent semantic
interpretation. That is, data-driven learners acquire grammars that generate
strings that humans would judge to be grammatically ill-formed (they {\it
overgenerate}) and fail to assign linguistically plausible parses. Model-based
learners are knowledge-intensive and are reliant for success upon the
completeness of a {\it model of grammaticality}. But in practice, the model
will be incomplete. Given that in this thesis we deal with undergeneration by
learning, we hypothesise that the combined use of data-driven and model-based
learning would allow data-driven learning to compensate for model-based
learning's incompleteness, whilst model-based learning would compensate for
data-driven learning's unsoundness. We describe a system that we have used to
test the hypothesis empirically. The system combines data-driven and
model-based learning to acquire unification-based grammars that are more
suitable for practical text parsing. Using the Spoken English Corpus as data,
and by quantitatively measuring undergeneration, overgeneration and parse
plausibility, we show that this hypothesis is correct.
",1995-02-03T12:17:28Z,http://arxiv.org/abs/cmp-lg/9502002v1,Miles Osborne
Privacy in Deep Learning: A Survey,"  The ever-growing advances of deep learning in many areas including vision,
recommendation systems, natural language processing, etc., have led to the
adoption of Deep Neural Networks (DNNs) in production systems. The availability
of large datasets and high computational power are the main contributors to
these advances. The datasets are usually crowdsourced and may contain sensitive
information. This poses serious privacy concerns as this data can be misused or
leaked through various vulnerabilities. Even if the cloud provider and the
communication link is trusted, there are still threats of inference attacks
where an attacker could speculate properties of the data used for training, or
find the underlying model architecture and parameters. In this survey, we
review the privacy concerns brought by deep learning, and the mitigating
techniques introduced to tackle these issues. We also show that there is a gap
in the literature regarding test-time inference privacy, and propose possible
future research directions.
",2020-04-25T23:47:25Z,http://arxiv.org/abs/2004.12254v5,"Fatemehsadat Mireshghallah, Mohammadkazem Taram, Praneeth Vepakomma, Abhishek Singh, Ramesh Raskar, Hadi Esmaeilzadeh"
"Surrogate Modeling of Trajectory Map-matching in Urban Road Networks
  using Transformer Sequence-to-Sequence Model","  Large-scale geolocation telematics data acquired from connected vehicles has
the potential to significantly enhance mobility infrastructures and operational
systems within smart cities. To effectively utilize this data, it is essential
to accurately match the geolocation data to the road segments. However, this
matching is often not trivial due to the low sampling rate and errors
exacerbated by multipath effects in urban environments. Traditionally,
statistical modeling techniques such as Hidden-Markov models incorporating
domain knowledge into the matching process have been extensively used for
map-matching tasks. However, rule-based map-matching tasks are noise-sensitive
and inefficient in processing large-scale trajectory data. Deep learning
techniques directly learn the relationship between observed data and road
networks from the data, often without the need for hand-crafted rules or domain
knowledge. This renders them an efficient approach for map-matching large-scale
datasets and more robust to the noise. This paper introduces a deep-learning
model, specifically the transformer-based encoder-decoder model, to perform as
a surrogate for offline map-matching algorithms. The encoder-decoder
architecture initially encodes the series of noisy GPS points into a
representation that automatically captures hidden contextual structures and
spatial correlations between GPS points. Subsequently, the decoder associates
data points with the road network features and thus transforms these
representations into a sequence of road segments. The model is trained and
evaluated using GPS traces collected in Manhattan, New York. Achieving an
accuracy of 75%, transformer-based encoder-decoder models extensively employed
in natural language processing presented a promising performance for
translating noisy GPS data to the navigated routes in urban road networks.
",2024-04-18T18:39:23Z,http://arxiv.org/abs/2404.12460v3,"Sevin Mohammadi, Andrew W. Smyth"
One-shot and few-shot learning of word embeddings,"  Standard deep learning systems require thousands or millions of examples to
learn a concept, and cannot integrate new concepts easily. By contrast, humans
have an incredible ability to do one-shot or few-shot learning. For instance,
from just hearing a word used in a sentence, humans can infer a great deal
about it, by leveraging what the syntax and semantics of the surrounding words
tells us. Here, we draw inspiration from this to highlight a simple technique
by which deep recurrent networks can similarly exploit their prior knowledge to
learn a useful representation for a new word from little data. This could make
natural language processing systems much more flexible, by allowing them to
learn continually from the new words they encounter.
",2017-10-27T18:05:22Z,http://arxiv.org/abs/1710.10280v2,"Andrew K. Lampinen, James L. McClelland"
"A Review of Different Word Embeddings for Sentiment Classification using
  Deep Learning","  The web is loaded with textual content, and Natural Language Processing is a
standout amongst the most vital fields in Machine Learning. But when data is
huge simple Machine Learning algorithms are not able to handle it and that is
when Deep Learning comes into play which based on Neural Networks. However
since neural networks cannot process raw text, we have to change over them
through some diverse strategies of word embedding. This paper demonstrates
those distinctive word embedding strategies implemented on an Amazon Review
Dataset, which has two sentiments to be classified: Happy and Unhappy based on
numerous customer reviews. Moreover we demonstrate the distinction in accuracy
with a discourse about which word embedding to apply when.
",2018-07-05T07:17:21Z,http://arxiv.org/abs/1807.02471v1,Debadri Dutta
From LIMA to DeepLIMA: following a new path of interoperability,"  In this article, we describe the architecture of the LIMA (Libre Multilingual
Analyzer) framework and its recent evolution with the addition of new text
analysis modules based on deep neural networks. We extended the functionality
of LIMA in terms of the number of supported languages while preserving existing
configurable architecture and the availability of previously developed
rule-based and statistical analysis components. Models were trained for more
than 60 languages on the Universal Dependencies 2.5 corpora, WikiNer corpora,
and CoNLL-03 dataset. Universal Dependencies allowed us to increase the number
of supported languages and to generate models that could be integrated into
other platforms. This integration of ubiquitous Deep Learning Natural Language
Processing models and the use of standard annotated collections using Universal
Dependencies can be viewed as a new path of interoperability, through the
normalization of models and data, that are complementary to a more standard
technical interoperability, implemented in LIMA through services available in
Docker containers on Docker Hub.
",2024-09-10T14:26:12Z,http://arxiv.org/abs/2409.06550v1,"Victor Bocharov, Romaric Besançon, Gaël de Chalendar, Olivier Ferret, Nasredine Semmar"
"Unlocking Futures: A Natural Language Driven Career Prediction System
  for Computer Science and Software Engineering Students","  A career is a crucial aspect for any person to fulfill their desires through
hard work. During their studies, students cannot find the best career
suggestions unless they receive meaningful guidance tailored to their skills.
Therefore, we developed an AI-assisted model for early prediction to provide
better career suggestions. Although the task is difficult, proper guidance can
make it easier. Effective career guidance requires understanding a student's
academic skills, interests, and skill-related activities. In this research, we
collected essential information from Computer Science (CS) and Software
Engineering (SWE) students to train a machine learning (ML) model that predicts
career paths based on students' career-related information. To adequately train
the models, we applied Natural Language Processing (NLP) techniques and
completed dataset pre-processing. For comparative analysis, we utilized
multiple classification ML algorithms and deep learning (DL) algorithms. This
study contributes valuable insights to educational advising by providing
specific career suggestions based on the unique features of CS and SWE
students. Additionally, the research helps individual CS and SWE students find
suitable jobs that match their skills, interests, and skill-related activities.
",2024-05-28T12:56:57Z,http://arxiv.org/abs/2405.18139v1,"Sakir Hossain Faruque, Sharun Akter Khushbu, Sharmin Akter"
Visualizing and Understanding Deep Neural Networks in CTR Prediction,"  Although deep learning techniques have been successfully applied to many
tasks, interpreting deep neural network models is still a big challenge to us.
Recently, many works have been done on visualizing and analyzing the mechanism
of deep neural networks in the areas of image processing and natural language
processing. In this paper, we present our approaches to visualize and
understand deep neural networks for a very important commercial task--CTR
(Click-through rate) prediction. We conduct experiments on the productive data
from our online advertising system with daily varying distribution. To
understand the mechanism and the performance of the model, we inspect the
model's inner status at neuron level. Also, a probe approach is implemented to
measure the layer-wise performance of the model. Moreover, to measure the
influence from the input features, we calculate saliency scores based on the
back-propagated gradients. Practical applications are also discussed, for
example, in understanding, monitoring, diagnosing and refining models and
algorithms.
",2018-06-22T08:03:35Z,http://arxiv.org/abs/1806.08541v1,"Lin Guo, Hui Ye, Wenbo Su, Henhuan Liu, Kai Sun, Hang Xiang"
"Incorporating Dictionaries into Deep Neural Networks for the Chinese
  Clinical Named Entity Recognition","  Clinical Named Entity Recognition (CNER) aims to identify and classify
clinical terms such as diseases, symptoms, treatments, exams, and body parts in
electronic health records, which is a fundamental and crucial task for clinical
and translational research. In recent years, deep neural networks have achieved
significant success in named entity recognition and many other Natural Language
Processing (NLP) tasks. Most of these algorithms are trained end to end, and
can automatically learn features from large scale labeled datasets. However,
these data-driven methods typically lack the capability of processing rare or
unseen entities. Previous statistical methods and feature engineering practice
have demonstrated that human knowledge can provide valuable information for
handling rare and unseen cases. In this paper, we address the problem by
incorporating dictionaries into deep neural networks for the Chinese CNER task.
Two different architectures that extend the Bi-directional Long Short-Term
Memory (Bi-LSTM) neural network and five different feature representation
schemes are proposed to handle the task. Computational results on the CCKS-2017
Task 2 benchmark dataset show that the proposed method achieves the highly
competitive performance compared with the state-of-the-art deep learning
methods.
",2018-04-13T16:36:44Z,http://arxiv.org/abs/1804.05017v1,"Qi Wang, Yuhang Xia, Yangming Zhou, Tong Ruan, Daqi Gao, Ping He"
"Describing Semantic Representations of Brain Activity Evoked by Visual
  Stimuli","  Quantitative modeling of human brain activity based on language
representations has been actively studied in systems neuroscience. However,
previous studies examined word-level representation, and little is known about
whether we could recover structured sentences from brain activity. This study
attempts to generate natural language descriptions of semantic contents from
human brain activity evoked by visual stimuli. To effectively use a small
amount of available brain activity data, our proposed method employs a
pre-trained image-captioning network model using a deep learning framework. To
apply brain activity to the image-captioning network, we train regression
models that learn the relationship between brain activity and deep-layer image
features. The results demonstrate that the proposed model can decode brain
activity and generate descriptions using natural language sentences. We also
conducted several experiments with data from different subsets of brain regions
known to process visual stimuli. The results suggest that semantic information
for sentence generations is widespread across the entire cortex.
",2018-01-19T05:12:59Z,http://arxiv.org/abs/1802.02210v1,"Eri Matsuo, Ichiro Kobayashi, Shinji Nishimoto, Satoshi Nishida, Hideki Asoh"
Transferable Models for Bioacoustics with Human Language Supervision,"  Passive acoustic monitoring offers a scalable, non-invasive method for
tracking global biodiversity and anthropogenic impacts on species. Although
deep learning has become a vital tool for processing this data, current models
are inflexible, typically cover only a handful of species, and are limited by
data scarcity. In this work, we propose BioLingual, a new model for
bioacoustics based on contrastive language-audio pretraining. We first
aggregate bioacoustic archives into a language-audio dataset, called
AnimalSpeak, with over a million audio-caption pairs holding information on
species, vocalization context, and animal behavior. After training on this
dataset to connect language and audio representations, our model can identify
over a thousand species' calls across taxa, complete bioacoustic tasks
zero-shot, and retrieve animal vocalization recordings from natural text
queries. When fine-tuned, BioLingual sets a new state-of-the-art on nine tasks
in the Benchmark of Animal Sounds. Given its broad taxa coverage and ability to
be flexibly queried in human language, we believe this model opens new
paradigms in ecological monitoring and research, including free-text search on
the world's acoustic monitoring archives. We open-source our models, dataset,
and code.
",2023-08-09T14:22:18Z,http://arxiv.org/abs/2308.04978v1,"David Robinson, Adelaide Robinson, Lily Akrapongpisak"
"Prior Knowledge Driven Label Embedding for Slot Filling in Natural
  Language Understanding","  Traditional slot filling in natural language understanding (NLU) predicts a
one-hot vector for each word. This form of label representation lacks semantic
correlation modelling, which leads to severe data sparsity problem, especially
when adapting an NLU model to a new domain. To address this issue, a novel
label embedding based slot filling framework is proposed in this paper. Here,
distributed label embedding is constructed for each slot using prior knowledge.
Three encoding methods are investigated to incorporate different kinds of prior
knowledge about slots: atomic concepts, slot descriptions, and slot exemplars.
The proposed label embeddings tend to share text patterns and reuses data with
different slot labels. This makes it useful for adaptive NLU with limited data.
Also, since label embedding is independent of NLU model, it is compatible with
almost all deep learning based slot filling models. The proposed approaches are
evaluated on three datasets. Experiments on single domain and domain adaptation
tasks show that label embedding achieves significant performance improvement
over traditional one-hot label representation as well as advanced zero-shot
approaches.
",2020-03-22T07:27:07Z,http://arxiv.org/abs/2003.09831v1,"Su Zhu, Zijian Zhao, Rao Ma, Kai Yu"
"Enhancing Deep Knowledge Tracing via Diffusion Models for Personalized
  Adaptive Learning","  In contrast to pedagogies like evidence-based teaching, personalized adaptive
learning (PAL) distinguishes itself by closely monitoring the progress of
individual students and tailoring the learning path to their unique knowledge
and requirements. A crucial technique for effective PAL implementation is
knowledge tracing, which models students' evolving knowledge to predict their
future performance. Based on these predictions, personalized recommendations
for resources and learning paths can be made to meet individual needs. Recent
advancements in deep learning have successfully enhanced knowledge tracking
through Deep Knowledge Tracing (DKT). This paper introduces generative AI
models to further enhance DKT. Generative AI models, rooted in deep learning,
are trained to generate synthetic data, addressing data scarcity challenges in
various applications across fields such as natural language processing (NLP)
and computer vision (CV). This study aims to tackle data shortage issues in
student learning records to enhance DKT performance for PAL. Specifically, it
employs TabDDPM, a diffusion model, to generate synthetic educational records
to augment training data for enhancing DKT. The proposed method's effectiveness
is validated through extensive experiments on ASSISTments datasets. The
experimental results demonstrate that the AI-generated data by TabDDPM
significantly improves DKT performance, particularly in scenarios with small
data for training and large data for testing.
",2024-04-25T00:23:20Z,http://arxiv.org/abs/2405.05134v1,"Ming Kuo, Shouvon Sarker, Lijun Qian, Yujian Fu, Xiangfang Li, Xishuang Dong"
"Using Focal Loss to Fight Shallow Heuristics: An Empirical Analysis of
  Modulated Cross-Entropy in Natural Language Inference","  There is no such thing as a perfect dataset. In some datasets, deep neural
networks discover underlying heuristics that allow them to take shortcuts in
the learning process, resulting in poor generalization capability. Instead of
using standard cross-entropy, we explore whether a modulated version of
cross-entropy called focal loss can constrain the model so as not to use
heuristics and improve generalization performance. Our experiments in natural
language inference show that focal loss has a regularizing impact on the
learning process, increasing accuracy on out-of-distribution data, but slightly
decreasing performance on in-distribution data. Despite the improved
out-of-distribution performance, we demonstrate the shortcomings of focal loss
and its inferiority in comparison to the performance of methods such as
unbiased focal loss and self-debiasing ensembles.
",2022-11-23T22:19:00Z,http://arxiv.org/abs/2211.13331v1,"Frano Rajič, Ivan Stresec, Axel Marmet, Tim Poštuvan"
"DeepEmotex: Classifying Emotion in Text Messages using Deep Transfer
  Learning","  Transfer learning has been widely used in natural language processing through
deep pretrained language models, such as Bidirectional Encoder Representations
from Transformers and Universal Sentence Encoder. Despite the great success,
language models get overfitted when applied to small datasets and are prone to
forgetting when fine-tuned with a classifier. To remedy this problem of
forgetting in transferring deep pretrained language models from one domain to
another domain, existing efforts explore fine-tuning methods to forget less. We
propose DeepEmotex an effective sequential transfer learning method to detect
emotion in text. To avoid forgetting problem, the fine-tuning step is
instrumented by a large amount of emotion-labeled data collected from Twitter.
We conduct an experimental study using both curated Twitter data sets and
benchmark data sets. DeepEmotex models achieve over 91% accuracy for
multi-class emotion classification on test dataset. We evaluate the performance
of the fine-tuned DeepEmotex models in classifying emotion in EmoInt and
Stimulus benchmark datasets. The models correctly classify emotion in 73% of
the instances in the benchmark datasets. The proposed DeepEmotex-BERT model
outperforms Bi-LSTM result on the benchmark datasets by 23%. We also study the
effect of the size of the fine-tuning dataset on the accuracy of our models.
Our evaluation results show that fine-tuning with a large set of
emotion-labeled data improves both the robustness and effectiveness of the
resulting target task model.
",2022-06-12T03:23:40Z,http://arxiv.org/abs/2206.06775v1,"Maryam Hasan, Elke Rundensteiner, Emmanuel Agu"
"Content-driven, unsupervised clustering of news articles through
  multiscale graph partitioning","  The explosion in the amount of news and journalistic content being generated
across the globe, coupled with extended and instantaneous access to information
through online media, makes it difficult and time-consuming to monitor news
developments and opinion formation in real time. There is an increasing need
for tools that can pre-process, analyse and classify raw text to extract
interpretable content; specifically, identifying topics and content-driven
groupings of articles. We present here such a methodology that brings together
powerful vector embeddings from Natural Language Processing with tools from
Graph Theory that exploit diffusive dynamics on graphs to reveal natural
partitions across scales. Our framework uses a recent deep neural network text
analysis methodology (Doc2vec) to represent text in vector form and then
applies a multi-scale community detection method (Markov Stability) to
partition a similarity graph of document vectors. The method allows us to
obtain clusters of documents with similar content, at different levels of
resolution, in an unsupervised manner. We showcase our approach with the
analysis of a corpus of 9,000 news articles published by Vox Media over one
year. Our results show consistent groupings of documents according to content
without a priori assumptions about the number or type of clusters to be found.
The multilevel clustering reveals a quasi-hierarchy of topics and subtopics
with increased intelligibility and improved topic coherence as compared to
external taxonomy services and standard topic detection methods.
",2018-08-03T12:57:15Z,http://arxiv.org/abs/1808.01175v1,"M. Tarik Altuncu, Sophia N. Yaliraki, Mauricio Barahona"
Detecting Bias in Transfer Learning Approaches for Text Classification,"  Classification is an essential and fundamental task in machine learning,
playing a cardinal role in the field of natural language processing (NLP) and
computer vision (CV). In a supervised learning setting, labels are always
needed for the classification task. Especially for deep neural models, a large
amount of high-quality labeled data are required for training. However, when a
new domain comes out, it is usually hard or expensive to acquire the labels.
Transfer learning could be an option to transfer the knowledge from a source
domain to a target domain. A challenge is that these two domains can be
different, either on the feature distribution, or the class distribution for
the nature of the samples. In this work, we evaluate some existing transfer
learning approaches on detecting the bias of imbalanced classes including
traditional and deep models. Besides, we propose an approach to bridge the gap
of the domain class imbalance issue.
",2021-02-03T15:48:21Z,http://arxiv.org/abs/2102.02114v1,Irene Li
"Combination of Domain Knowledge and Deep Learning for Sentiment Analysis
  of Short and Informal Messages on Social Media","  Sentiment analysis has been emerging recently as one of the major natural
language processing (NLP) tasks in many applications. Especially, as social
media channels (e.g. social networks or forums) have become significant sources
for brands to observe user opinions about their products, this task is thus
increasingly crucial. However, when applied with real data obtained from social
media, we notice that there is a high volume of short and informal messages
posted by users on those channels. This kind of data makes the existing works
suffer from many difficulties to handle, especially ones using deep learning
approaches. In this paper, we propose an approach to handle this problem. This
work is extended from our previous work, in which we proposed to combine the
typical deep learning technique of Convolutional Neural Networks with domain
knowledge. The combination is used for acquiring additional training data
augmentation and a more reasonable loss function. In this work, we further
improve our architecture by various substantial enhancements, including
negation-based data augmentation, transfer learning for word embeddings, the
combination of word-level embeddings and character-level embeddings, and using
multitask learning technique for attaching domain knowledge rules in the
learning process. Those enhancements, specifically aiming to handle short and
informal messages, help us to enjoy significant improvement in performance once
experimenting on real datasets.
",2019-02-16T06:03:57Z,http://arxiv.org/abs/1902.06050v2,"Khuong Vo, Tri Nguyen, Dang Pham, Mao Nguyen, Minh Truong, Trung Mai, Tho Quan"
UQ for Credit Risk Management: A deep evidence regression approach,"  Machine Learning has invariantly found its way into various Credit Risk
applications. Due to the intrinsic nature of Credit Risk, quantifying the
uncertainty of the predicted risk metrics is essential, and applying
uncertainty-aware deep learning models to credit risk settings can be very
helpful. In this work, we have explored the application of a scalable UQ-aware
deep learning technique, Deep Evidence Regression and applied it to predicting
Loss Given Default. We contribute to the literature by extending the Deep
Evidence Regression methodology to learning target variables generated by a
Weibull process and provide the relevant learning framework. We demonstrate the
application of our approach to both simulated and real-world data.
",2023-05-08T18:03:01Z,http://arxiv.org/abs/2305.04967v2,Ashish Dhiman
"Language Models are Drummers: Drum Composition with Natural Language
  Pre-Training","  Automatic music generation with artificial intelligence typically requires a
large amount of data which is hard to obtain for many less common genres and
musical instruments. To tackle this issue, we present ongoing work and
preliminary findings on the possibility for deep models to transfer knowledge
from language to music, by finetuning large language models pre-trained on a
massive text corpus on only hundreds of MIDI files of drum performances. We
show that by doing so, one of the largest, state-of-the-art models (GPT3) is
capable of generating reasonable drum grooves, while models that are not
pre-trained (Transformer) shows no such ability beyond naive repetition.
Evaluating generated music is a challenging task, more so is evaluating drum
grooves with little precedence in literature. Hence, we propose a tailored
structural evaluation method and analyze drum grooves produced by GPT3 compared
to those played by human professionals, exposing the strengths and weaknesses
of such generation by language-to-music transfer. Our findings suggest that
language-to-music transfer learning with large language models is viable and
promising.
",2023-01-03T15:47:53Z,http://arxiv.org/abs/2301.01162v1,"Li Zhang, Chris Callison-Burch"
Continual learning on 3D point clouds with random compressed rehearsal,"  Contemporary deep neural networks offer state-of-the-art results when applied
to visual reasoning, e.g., in the context of 3D point cloud data. Point clouds
are important datatype for precise modeling of three-dimensional environments,
but effective processing of this type of data proves to be challenging. In the
world of large, heavily-parameterized network architectures and
continuously-streamed data, there is an increasing need for machine learning
models that can be trained on additional data. Unfortunately, currently
available models cannot fully leverage training on additional data without
losing their past knowledge. Combating this phenomenon, called catastrophic
forgetting, is one of the main objectives of continual learning. Continual
learning for deep neural networks has been an active field of research,
primarily in 2D computer vision, natural language processing, reinforcement
learning, and robotics. However, in 3D computer vision, there are hardly any
continual learning solutions specifically designed to take advantage of point
cloud structure. This work proposes a novel neural network architecture capable
of continual learning on 3D point cloud data. We utilize point cloud structure
properties for preserving a heavily compressed set of past data. By using
rehearsal and reconstruction as regularization methods of the learning process,
our approach achieves a significant decrease of catastrophic forgetting
compared to the existing solutions on several most popular point cloud datasets
considering two continual learning settings: when a task is known beforehand,
and in the challenging scenario of when task information is unknown to the
model.
",2022-05-16T22:59:52Z,http://arxiv.org/abs/2205.08013v2,"Maciej Zamorski, Michał Stypułkowski, Konrad Karanowski, Tomasz Trzciński, Maciej Zięba"
Deep Learning in Science,"  Much of the recent success of Artificial Intelligence (AI) has been spurred
on by impressive achievements within a broader family of machine learning
methods, commonly referred to as Deep Learning (DL). This paper provides
insights on the diffusion and impact of DL in science. Through a Natural
Language Processing (NLP) approach on the arXiv.org publication corpus, we
delineate the emerging DL technology and identify a list of relevant search
terms. These search terms allow us to retrieve DL-related publications from Web
of Science across all sciences. Based on that sample, we document the DL
diffusion process in the scientific system. We find i) an exponential growth in
the adoption of DL as a research tool across all sciences and all over the
world, ii) regional differentiation in DL application domains, and iii) a
transition from interdisciplinary DL applications to disciplinary research
within application domains. In a second step, we investigate how the adoption
of DL methods affects scientific development. Therefore, we empirically assess
how DL adoption relates to re-combinatorial novelty and scientific impact in
the health sciences. We find that DL adoption is negatively correlated with
re-combinatorial novelty, but positively correlated with expectation as well as
variance of citation performance. Our findings suggest that DL does not (yet?)
work as an autopilot to navigate complex knowledge landscapes and overthrow
their structure. However, the 'DL principle' qualifies for its versatility as
the nucleus of a general scientific method that advances science in a
measurable way.
",2020-09-03T10:41:29Z,http://arxiv.org/abs/2009.01575v2,"Stefano Bianchini, Moritz Müller, Pierre Pelletier"
A Unified Review of Deep Learning for Automated Medical Coding,"  Automated medical coding, an essential task for healthcare operation and
delivery, makes unstructured data manageable by predicting medical codes from
clinical documents. Recent advances in deep learning and natural language
processing have been widely applied to this task. However, deep learning-based
medical coding lacks a unified view of the design of neural network
architectures. This review proposes a unified framework to provide a general
understanding of the building blocks of medical coding models and summarizes
recent advanced models under the proposed framework. Our unified framework
decomposes medical coding into four main components, i.e., encoder modules for
text feature extraction, mechanisms for building deep encoder architectures,
decoder modules for transforming hidden representations into medical codes, and
the usage of auxiliary information. Finally, we introduce the benchmarks and
real-world usage and discuss key research challenges and future directions.
",2022-01-08T09:37:23Z,http://arxiv.org/abs/2201.02797v5,"Shaoxiong Ji, Wei Sun, Xiaobo Li, Hang Dong, Ara Taalas, Yijia Zhang, Honghan Wu, Esa Pitkänen, Pekka Marttinen"
"Speech representation learning: Learning bidirectional encoders with
  single-view, multi-view, and multi-task methods","  This thesis focuses on representation learning for sequence data over time or
space, aiming to improve downstream sequence prediction tasks by using the
learned representations. Supervised learning has been the most dominant
approach for training deep neural networks for learning good sequential
representations. However, one limiting factor to scale supervised learning is
the lack of enough annotated data. Motivated by this challenge, it is natural
to explore representation learning methods that can utilize large amounts of
unlabeled and weakly labeled data, as well as an additional data modality. I
describe my broad study of representation learning for speech data. Unlike most
other works that focus on a single learning setting, this thesis studies
multiple settings: supervised learning with auxiliary losses, unsupervised
learning, semi-supervised learning, and multi-view learning. Besides different
learning problems, I also explore multiple approaches for representation
learning. Though I focus on speech data, the methods described in this thesis
can also be applied to other domains. Overall, the field of representation
learning is developing rapidly. State-of-the-art results on speech related
tasks are typically based on Transformers pre-trained with large-scale
self-supervised learning, which aims to learn generic representations that can
benefit multiple downstream tasks. Since 2020, large-scale pre-training has
been the de facto choice to achieve good performance. This delayed thesis does
not attempt to summarize and compare with the latest results on speech
representation learning; instead, it presents a unique study on speech
representation learning before the Transformer era, that covers multiple
learning settings. Some of the findings in this thesis can still be useful
today.
",2023-07-25T20:38:55Z,http://arxiv.org/abs/2308.00129v1,Qingming Tang
Topics to Avoid: Demoting Latent Confounds in Text Classification,"  Despite impressive performance on many text classification tasks, deep neural
networks tend to learn frequent superficial patterns that are specific to the
training data and do not always generalize well. In this work, we observe this
limitation with respect to the task of native language identification. We find
that standard text classifiers which perform well on the test set end up
learning topical features which are confounds of the prediction task (e.g., if
the input text mentions Sweden, the classifier predicts that the author's
native language is Swedish). We propose a method that represents the latent
topical confounds and a model which ""unlearns"" confounding features by
predicting both the label of the input text and the confound; but we train the
two predictors adversarially in an alternating fashion to learn a text
representation that predicts the correct label but is less prone to using
information about the confound. We show that this model generalizes better and
learns features that are indicative of the writing style rather than the
content.
",2019-09-01T19:18:44Z,http://arxiv.org/abs/1909.00453v2,"Sachin Kumar, Shuly Wintner, Noah A. Smith, Yulia Tsvetkov"
"A Natural Language Processing and Deep Learning based Model for
  Automated Vehicle Diagnostics using Free-Text Customer Service Reports","  Initial fault detection and diagnostics are imperative measures to improve
the efficiency, safety, and stability of vehicle operation. In recent years,
numerous studies have investigated data-driven approaches to improve the
vehicle diagnostics process using available vehicle data. Moreover, data-driven
methods are employed to enhance customer-service agent interactions. In this
study, we demonstrate a machine learning pipeline to improve automated vehicle
diagnostics. First, Natural Language Processing (NLP) is used to automate the
extraction of crucial information from free-text failure reports (generated
during customers' calls to the service department). Then, deep learning
algorithms are employed to validate service requests and filter vague or
misleading claims. Ultimately, different classification algorithms are
implemented to classify service requests so that valid service requests can be
directed to the relevant service department. The proposed model- Bidirectional
Long Short Term Memory (BiLSTM) along with Convolution Neural Network (CNN)-
shows more than 18\% accuracy improvement in validating service requests
compared to technicians' capabilities. In addition, using domain-based NLP
techniques at preprocessing and feature extraction stages along with CNN-BiLSTM
based request validation enhanced the accuracy ($>25\%$), sensitivity
($>39\%$), specificity ($>11\%$), and precision ($>11\%$) of Gradient Tree
Boosting (GTB) service classification model. The Receiver Operating
Characteristic Area Under the Curve (ROC-AUC) reached 0.82.
",2021-11-29T21:41:34Z,http://arxiv.org/abs/2111.14977v1,"Ali Khodadadi, Soroush Ghandiparsi, Chen-Nee Chuah"
"Unnatural Language Processing: Bridging the Gap Between Synthetic and
  Natural Language Data","  Large, human-annotated datasets are central to the development of natural
language processing models. Collecting these datasets can be the most
challenging part of the development process. We address this problem by
introducing a general purpose technique for ``simulation-to-real'' transfer in
language understanding problems with a delimited set of target behaviors,
making it possible to develop models that can interpret natural utterances
without natural training data. We begin with a synthetic data generation
procedure, and train a model that can accurately interpret utterances produced
by the data generator. To generalize to natural utterances, we automatically
find projections of natural language utterances onto the support of the
synthetic language, using learned sentence embeddings to define a distance
metric. With only synthetic training data, our approach matches or outperforms
state-of-the-art models trained on natural language data in several domains.
These results suggest that simulation-to-real transfer is a practical framework
for developing NLP applications, and that improved models for transfer might
provide wide-ranging improvements in downstream tasks.
",2020-04-28T16:41:00Z,http://arxiv.org/abs/2004.13645v1,"Alana Marzoev, Samuel Madden, M. Frans Kaashoek, Michael Cafarella, Jacob Andreas"
Deep Clustering with Measure Propagation,"  Deep models have improved state-of-the-art for both supervised and
unsupervised learning. For example, deep embedded clustering (DEC) has greatly
improved the unsupervised clustering performance, by using stacked autoencoders
for representation learning. However, one weakness of deep modeling is that the
local neighborhood structure in the original space is not necessarily preserved
in the latent space. To preserve local geometry, various methods have been
proposed in the supervised and semi-supervised learning literature (e.g.,
spectral clustering and label propagation) using graph Laplacian
regularization. In this paper, we combine the strength of deep representation
learning with measure propagation (MP), a KL-divergence based graph
regularization method originally used in the semi-supervised scenario. The main
assumption of MP is that if two data points are close in the original space,
they are likely to belong to the same class, measured by KL-divergence of class
membership distribution. By taking the same assumption in the unsupervised
learning scenario, we propose our Deep Embedded Clustering Aided by Measure
Propagation (DECAMP) model. We evaluate DECAMP on short text clustering tasks.
On three public datasets, DECAMP performs competitively with other
state-of-the-art baselines, including baselines using additional data to
generate word embeddings used in the clustering process. As an example, on the
Stackoverflow dataset, DECAMP achieved a clustering accuracy of 79%, which is
about 5% higher than all existing baselines. These empirical results suggest
that DECAMP is a very effective method for unsupervised learning.
",2021-04-18T22:02:43Z,http://arxiv.org/abs/2104.08967v3,"Minhua Chen, Badrinath Jayakumar, Padmasundari Gopalakrishnan, Qiming Huang, Michael Johnston, Patrick Haffner"
"Natural Language Processing with Deep Learning for Medical Adverse Event
  Detection from Free-Text Medical Narratives: A Case Study of Detecting Total
  Hip Replacement Dislocation","  Accurate and timely detection of medical adverse events (AEs) from free-text
medical narratives is challenging. Natural language processing (NLP) with deep
learning has already shown great potential for analyzing free-text data, but
its application for medical AE detection has been limited. In this study we
proposed deep learning based NLP (DL-NLP) models for efficient and accurate hip
dislocation AE detection following total hip replacement from standard
(radiology notes) and non-standard (follow-up telephone notes) free-text
medical narratives. We benchmarked these proposed models with a wide variety of
traditional machine learning based NLP (ML-NLP) models, and also assessed the
accuracy of International Classification of Diseases (ICD) and Current
Procedural Terminology (CPT) codes in capturing these hip dislocation AEs in a
multi-center orthopaedic registry. All DL-NLP models out-performed all of the
ML-NLP models, with a convolutional neural network (CNN) model achieving the
best overall performance (Kappa = 0.97 for radiology notes, and Kappa = 1.00
for follow-up telephone notes). On the other hand, the ICD/CPT codes of the
patients who sustained a hip dislocation AE were only 75.24% accurate, showing
the potential of the proposed model to be used in largescale orthopaedic
registries for accurate and efficient hip dislocation AE detection to improve
the quality of care and patient outcome.
",2020-04-17T16:25:36Z,http://arxiv.org/abs/2004.08333v2,"Alireza Borjali, Martin Magneli, David Shin, Henrik Malchau, Orhun K. Muratoglu, Kartik M. Varadarajan"
"APALU: A Trainable, Adaptive Activation Function for Deep Learning
  Networks","  Activation function is a pivotal component of deep learning, facilitating the
extraction of intricate data patterns. While classical activation functions
like ReLU and its variants are extensively utilized, their static nature and
simplicity, despite being advantageous, often limit their effectiveness in
specialized tasks. The trainable activation functions also struggle sometimes
to adapt to the unique characteristics of the data. Addressing these
limitations, we introduce a novel trainable activation function, adaptive
piecewise approximated activation linear unit (APALU), to enhance the learning
performance of deep learning across a broad range of tasks. It presents a
unique set of features that enable it to maintain stability and efficiency in
the learning process while adapting to complex data representations.
Experiments reveal significant improvements over widely used activation
functions for different tasks. In image classification, APALU increases
MobileNet and GoogleNet accuracy by 0.37% and 0.04%, respectively, on the
CIFAR10 dataset. In anomaly detection, it improves the average area under the
curve of One-CLASS Deep SVDD by 0.8% on the MNIST dataset, 1.81% and 1.11%
improvements with DifferNet, and knowledge distillation, respectively, on the
MVTech dataset. Notably, APALU achieves 100% accuracy on a sign language
recognition task with a limited dataset. For regression tasks, APALU enhances
the performance of deep neural networks and recurrent neural networks on
different datasets. These improvements highlight the robustness and
adaptability of APALU across diverse deep-learning applications.
",2024-02-13T06:18:42Z,http://arxiv.org/abs/2402.08244v1,"Barathi Subramanian, Rathinaraja Jeyaraj, Rakhmonov Akhrorjon Akhmadjon Ugli, Jeonghong Kim"
"Model Performance Prediction for Hyperparameter Optimization of Deep
  Learning Models Using High Performance Computing and Quantum Annealing","  Hyperparameter Optimization (HPO) of Deep Learning-based models tends to be a
compute resource intensive process as it usually requires to train the target
model with many different hyperparameter configurations. We show that
integrating model performance prediction with early stopping methods holds
great potential to speed up the HPO process of deep learning models. Moreover,
we propose a novel algorithm called Swift-Hyperband that can use either
classical or quantum support vector regression for performance prediction and
benefit from distributed High Performance Computing environments. This
algorithm is tested not only for the Machine-Learned Particle Flow model used
in High Energy Physics, but also for a wider range of target models from
domains such as computer vision and natural language processing.
Swift-Hyperband is shown to find comparable (or better) hyperparameters as well
as using less computational resources in all test cases.
",2023-11-29T10:32:40Z,http://arxiv.org/abs/2311.17508v1,"Juan Pablo García Amboage, Eric Wulff, Maria Girone, Tomás F. Pena"
"Reconstructing Materials Tetrahedron: Challenges in Materials
  Information Extraction","  The discovery of new materials has a documented history of propelling human
progress for centuries and more. The behaviour of a material is a function of
its composition, structure, and properties, which further depend on its
processing and testing conditions. Recent developments in deep learning and
natural language processing have enabled information extraction at scale from
published literature such as peer-reviewed publications, books, and patents.
However, this information is spread in multiple formats, such as tables, text,
and images, and with little or no uniformity in reporting style giving rise to
several machine learning challenges. Here, we discuss, quantify, and document
these challenges in automated information extraction (IE) from materials
science literature towards the creation of a large materials science knowledge
base. Specifically, we focus on IE from text and tables and outline several
challenges with examples. We hope the present work inspires researchers to
address the challenges in a coherent fashion, providing a fillip to IE towards
developing a materials knowledge base.
",2023-10-12T14:57:24Z,http://arxiv.org/abs/2310.08383v3,"Kausik Hira, Mohd Zaki, Dhruvil Sheth, Mausam, N M Anoop Krishnan"
"Advancements in eHealth Data Analytics through Natural Language
  Processing and Deep Learning","  The healthcare environment is commonly referred to as ""information-rich"" but
also ""knowledge poor"". Healthcare systems collect huge amounts of data from
various sources: lab reports, medical letters, logs of medical tools or
programs, medical prescriptions, etc. These massive sets of data can provide
great knowledge and information that can improve the medical services, and
overall the healthcare domain, such as disease prediction by analyzing the
patient's symptoms or disease prevention, by facilitating the discovery of
behavioral factors for diseases. Unfortunately, only a relatively small volume
of the textual eHealth data is processed and interpreted, an important factor
being the difficulty in efficiently performing Big Data operations. In the
medical field, detecting domain-specific multi-word terms is a crucial task as
they can define an entire concept with a few words. A term can be defined as a
linguistic structure or a concept, and it is composed of one or more words with
a specific meaning to a domain. All the terms of a domain create its
terminology. This chapter offers a critical study of the current, most
performant solutions for analyzing unstructured (image and textual) eHealth
data. This study also provides a comparison of the current Natural Language
Processing and Deep Learning techniques in the eHealth context. Finally, we
examine and discuss some of the current issues, and we define a set of research
directions in this area.
",2024-01-19T17:51:11Z,http://arxiv.org/abs/2401.10850v1,"Elena-Simona Apostol, Ciprian-Octavian Truică"
"ECGBERT: Understanding Hidden Language of ECGs with Self-Supervised
  Representation Learning","  In the medical field, current ECG signal analysis approaches rely on
supervised deep neural networks trained for specific tasks that require
substantial amounts of labeled data. However, our paper introduces ECGBERT, a
self-supervised representation learning approach that unlocks the underlying
language of ECGs. By unsupervised pre-training of the model, we mitigate
challenges posed by the lack of well-labeled and curated medical data. ECGBERT,
inspired by advances in the area of natural language processing and large
language models, can be fine-tuned with minimal additional layers for various
ECG-based problems. Through four tasks, including Atrial Fibrillation
arrhythmia detection, heartbeat classification, sleep apnea detection, and user
authentication, we demonstrate ECGBERT's potential to achieve state-of-the-art
results on a wide variety of tasks.
",2023-06-10T04:23:08Z,http://arxiv.org/abs/2306.06340v1,"Seokmin Choi, Sajad Mousavi, Phillip Si, Haben G. Yhdego, Fatemeh Khadem, Fatemeh Afghah"
"Deep Speech Based End-to-End Automated Speech Recognition (ASR) for
  Indian-English Accents","  Automated Speech Recognition (ASR) is an interdisciplinary application of
computer science and linguistics that enable us to derive the transcription
from the uttered speech waveform. It finds several applications in Military
like High-performance fighter aircraft, helicopters, air-traffic controller.
Other than military speech recognition is used in healthcare, persons with
disabilities and many more. ASR has been an active research area. Several
models and algorithms for speech to text (STT) have been proposed. One of the
most recent is Mozilla Deep Speech, it is based on the Deep Speech research
paper by Baidu. Deep Speech is a state-of-art speech recognition system is
developed using end-to-end deep learning, it is trained using well-optimized
Recurrent Neural Network (RNN) training system utilizing multiple Graphical
Processing Units (GPUs). This training is mostly done using American-English
accent datasets, which results in poor generalizability to other English
accents. India is a land of vast diversity. This can even be seen in the
speech, there are several English accents which vary from state to state. In
this work, we have used transfer learning approach using most recent Deep
Speech model i.e., deepspeech-0.9.3 to develop an end-to-end speech recognition
system for Indian-English accents. This work utilizes fine-tuning and data
argumentation to further optimize and improve the Deep Speech ASR system. Indic
TTS data of Indian-English accents is used for transfer learning and
fine-tuning the pre-trained Deep Speech model. A general comparison is made
among the untrained model, our trained model and other available speech
recognition services for Indian-English Accents.
",2022-04-03T03:11:21Z,http://arxiv.org/abs/2204.00977v1,"Priyank Dubey, Bilal Shah"
Neural Unsupervised Domain Adaptation in NLP---A Survey,"  Deep neural networks excel at learning from labeled data and achieve
state-of-the-art resultson a wide array of Natural Language Processing tasks.
In contrast, learning from unlabeled data, especially under domain shift,
remains a challenge. Motivated by the latest advances, in this survey we review
neural unsupervised domain adaptation techniques which do not require labeled
target domain data. This is a more challenging yet a more widely applicable
setup. We outline methods, from early traditional non-neural methods to
pre-trained model transfer. We also revisit the notion of domain, and we
uncover a bias in the type of Natural Language Processing tasks which received
most attention. Lastly, we outline future directions, particularly the broader
need for out-of-distribution generalization of future NLP.
",2020-05-31T22:34:14Z,http://arxiv.org/abs/2006.00632v2,"Alan Ramponi, Barbara Plank"
"Artificial intelligence-aided protein engineering: from topological data
  analysis to deep protein language models","  Protein engineering is an emerging field in biotechnology that has the
potential to revolutionize various areas, such as antibody design, drug
discovery, food security, ecology, and more. However, the mutational space
involved is too vast to be handled through experimental means alone. Leveraging
accumulative protein databases, machine learning (ML) models, particularly
those based on natural language processing (NLP), have considerably expedited
protein engineering. Moreover, advances in topological data analysis (TDA) and
artificial intelligence-based protein structure prediction, such as AlphaFold2,
have made more powerful structure-based ML-assisted protein engineering
strategies possible. This review aims to offer a comprehensive, systematic, and
indispensable set of methodological components, including TDA and NLP, for
protein engineering and to facilitate their future development.
",2023-07-27T02:14:09Z,http://arxiv.org/abs/2307.14587v1,"Yuchi Qiu, Guo-Wei Wei"
Do Neural Nets Learn Statistical Laws behind Natural Language?,"  The performance of deep learning in natural language processing has been
spectacular, but the reasons for this success remain unclear because of the
inherent complexity of deep learning. This paper provides empirical evidence of
its effectiveness and of a limitation of neural networks for language
engineering. Precisely, we demonstrate that a neural language model based on
long short-term memory (LSTM) effectively reproduces Zipf's law and Heaps' law,
two representative statistical properties underlying natural language. We
discuss the quality of reproducibility and the emergence of Zipf's law and
Heaps' law as training progresses. We also point out that the neural language
model has a limitation in reproducing long-range correlation, another
statistical property of natural language. This understanding could provide a
direction for improving the architectures of neural networks.
",2017-07-16T09:08:42Z,http://arxiv.org/abs/1707.04848v2,"Shuntaro Takahashi, Kumiko Tanaka-Ishii"
Knowledge Graphs Querying,"  Knowledge graphs (KGs) such as DBpedia, Freebase, YAGO, Wikidata, and NELL
were constructed to store large-scale, real-world facts as (subject, predicate,
object) triples -- that can also be modeled as a graph, where a node (a subject
or an object) represents an entity with attributes, and a directed edge (a
predicate) is a relationship between two entities. Querying KGs is critical in
web search, question answering (QA), semantic search, personal assistants, fact
checking, and recommendation. While significant progress has been made on KG
construction and curation, thanks to deep learning recently we have seen a
surge of research on KG querying and QA. The objectives of our survey are
two-fold. First, research on KG querying has been conducted by several
communities, such as databases, data mining, semantic web, machine learning,
information retrieval, and natural language processing (NLP), with different
focus and terminologies; and also in diverse topics ranging from graph
databases, query languages, join algorithms, graph patterns matching, to more
sophisticated KG embedding and natural language questions (NLQs). We aim at
uniting different interdisciplinary topics and concepts that have been
developed for KG querying. Second, many recent advances on KG and query
embedding, multimodal KG, and KG-QA come from deep learning, IR, NLP, and
computer vision domains. We identify important challenges of KG querying that
received less attention by graph databases, and by the DB community in general,
e.g., incomplete KG, semantic matching, multimodal data, and NLQs. We conclude
by discussing interesting opportunities for the data management community, for
instance, KG as a unified data model and vector-based query processing.
",2023-05-23T19:32:42Z,http://arxiv.org/abs/2305.14485v1,Arijit Khan
Tree Edit Distance Learning via Adaptive Symbol Embeddings,"  Metric learning has the aim to improve classification accuracy by learning a
distance measure which brings data points from the same class closer together
and pushes data points from different classes further apart. Recent research
has demonstrated that metric learning approaches can also be applied to trees,
such as molecular structures, abstract syntax trees of computer programs, or
syntax trees of natural language, by learning the cost function of an edit
distance, i.e. the costs of replacing, deleting, or inserting nodes in a tree.
However, learning such costs directly may yield an edit distance which violates
metric axioms, is challenging to interpret, and may not generalize well. In
this contribution, we propose a novel metric learning approach for trees which
we call embedding edit distance learning (BEDL) and which learns an edit
distance indirectly by embedding the tree nodes as vectors, such that the
Euclidean distance between those vectors supports class discrimination. We
learn such embeddings by reducing the distance to prototypical trees from the
same class and increasing the distance to prototypical trees from different
classes. In our experiments, we show that BEDL improves upon the
state-of-the-art in metric learning for trees on six benchmark data sets,
ranging from computer science over biomedical data to a natural-language
processing data set containing over 300,000 nodes.
",2018-06-13T13:08:16Z,http://arxiv.org/abs/1806.05009v3,"Benjamin Paaßen, Claudio Gallicchio, Alessio Micheli, Barbara Hammer"
"The Evolution of Distributed Systems for Graph Neural Networks and their
  Origin in Graph Processing and Deep Learning: A Survey","  Graph Neural Networks (GNNs) are an emerging research field. This specialized
Deep Neural Network (DNN) architecture is capable of processing graph
structured data and bridges the gap between graph processing and Deep Learning
(DL). As graphs are everywhere, GNNs can be applied to various domains
including recommendation systems, computer vision, natural language processing,
biology and chemistry. With the rapid growing size of real world graphs, the
need for efficient and scalable GNN training solutions has come. Consequently,
many works proposing GNN systems have emerged throughout the past few years.
However, there is an acute lack of overview, categorization and comparison of
such systems. We aim to fill this gap by summarizing and categorizing important
methods and techniques for large-scale GNN solutions. In addition, we establish
connections between GNN systems, graph processing systems and DL systems.
",2023-05-23T09:22:33Z,http://arxiv.org/abs/2305.13854v1,"Jana Vatter, Ruben Mayer, Hans-Arno Jacobsen"
"The Role of CNL and AMR in Scalable Abstractive Summarization for
  Multilingual Media Monitoring","  In the era of Big Data and Deep Learning, there is a common view that machine
learning approaches are the only way to cope with the robust and scalable
information extraction and summarization. It has been recently proposed that
the CNL approach could be scaled up, building on the concept of embedded CNL
and, thus, allowing for CNL-based information extraction from e.g. normative or
medical texts that are rather controlled by nature but still infringe the
boundaries of CNL. Although it is arguable if CNL can be exploited to approach
the robust wide-coverage semantic parsing for use cases like media monitoring,
its potential becomes much more obvious in the opposite direction: generation
of story highlights from the summarized AMR graphs, which is in the focus of
this position paper.
",2016-06-20T07:15:55Z,http://arxiv.org/abs/1606.05994v1,"Normunds Gruzitis, Guntis Barzdins"
"Beyond the Status Quo: A Contemporary Survey of Advances and Challenges
  in Audio Captioning","  Automated audio captioning (AAC), a task that mimics human perception as well
as innovatively links audio processing and natural language processing, has
overseen much progress over the last few years. AAC requires recognizing
contents such as the environment, sound events and the temporal relationships
between sound events and describing these elements with a fluent sentence.
Currently, an encoder-decoder-based deep learning framework is the standard
approach to tackle this problem. Plenty of works have proposed novel network
architectures and training schemes, including extra guidance, reinforcement
learning, audio-text self-supervised learning and diverse or controllable
captioning. Effective data augmentation techniques, especially based on large
language models are explored. Benchmark datasets and AAC-oriented evaluation
metrics also accelerate the improvement of this field. This paper situates
itself as a comprehensive survey covering the comparison between AAC and its
related tasks, the existing deep learning techniques, datasets, and the
evaluation metrics in AAC, with insights provided to guide potential future
research directions.
",2022-05-11T09:09:15Z,http://arxiv.org/abs/2205.05357v2,"Xuenan Xu, Zeyu Xie, Mengyue Wu, Kai Yu"
"Extracting Chemical-Protein Interactions via Calibrated Deep Neural
  Network and Self-training","  The extraction of interactions between chemicals and proteins from several
biomedical articles is important in many fields of biomedical research such as
drug development and prediction of drug side effects. Several natural language
processing methods, including deep neural network (DNN) models, have been
applied to address this problem. However, these methods were trained with
hard-labeled data, which tend to become over-confident, leading to degradation
of the model reliability. To estimate the data uncertainty and improve the
reliability, ""calibration"" techniques have been applied to deep learning
models. In this study, to extract chemical--protein interactions, we propose a
DNN-based approach incorporating uncertainty information and calibration
techniques. Our model first encodes the input sequence using a pre-trained
language-understanding model, following which it is trained using two
calibration methods: mixup training and addition of a confidence penalty loss.
Finally, the model is re-trained with augmented data that are extracted using
the estimated uncertainties. Our approach has achieved state-of-the-art
performance with regard to the Biocreative VI ChemProt task, while preserving
higher calibration abilities than those of previous approaches. Furthermore,
our approach also presents the possibilities of using uncertainty estimation
for performance improvement.
",2020-11-04T10:14:31Z,http://arxiv.org/abs/2011.02207v1,"Dongha Choi, Hyunju Lee"
"Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit","  Code intelligence leverages machine learning techniques to extract knowledge
from extensive code corpora, with the aim of developing intelligent tools to
improve the quality and productivity of computer programming. Currently, there
is already a thriving research community focusing on code intelligence, with
efforts ranging from software engineering, machine learning, data mining,
natural language processing, and programming languages. In this paper, we
conduct a comprehensive literature review on deep learning for code
intelligence, from the aspects of code representation learning, deep learning
techniques, and application tasks. We also benchmark several state-of-the-art
neural models for code intelligence, and provide an open-source toolkit
tailored for the rapid prototyping of deep-learning-based code intelligence
models. In particular, we inspect the existing code intelligence models under
the basis of code representation learning, and provide a comprehensive overview
to enhance comprehension of the present state of code intelligence.
Furthermore, we publicly release the source code and data resources to provide
the community with a ready-to-use benchmark, which can facilitate the
evaluation and comparison of existing and future code intelligence models
(https://xcodemind.github.io). At last, we also point out several challenging
and promising directions for future research.
",2023-12-30T17:48:37Z,http://arxiv.org/abs/2401.00288v1,"Yao Wan, Yang He, Zhangqian Bi, Jianguo Zhang, Hongyu Zhang, Yulei Sui, Guandong Xu, Hai Jin, Philip S. Yu"
Distilling Task-Specific Knowledge from BERT into Simple Neural Networks,"  In the natural language processing literature, neural networks are becoming
increasingly deeper and complex. The recent poster child of this trend is the
deep language representation model, which includes BERT, ELMo, and GPT. These
developments have led to the conviction that previous-generation, shallower
neural networks for language understanding are obsolete. In this paper,
however, we demonstrate that rudimentary, lightweight neural networks can still
be made competitive without architecture changes, external training data, or
additional input features. We propose to distill knowledge from BERT, a
state-of-the-art language representation model, into a single-layer BiLSTM, as
well as its siamese counterpart for sentence-pair tasks. Across multiple
datasets in paraphrasing, natural language inference, and sentiment
classification, we achieve comparable results with ELMo, while using roughly
100 times fewer parameters and 15 times less inference time.
",2019-03-28T17:23:50Z,http://arxiv.org/abs/1903.12136v1,"Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, Jimmy Lin"
Financial data analysis application via multi-strategy text processing,"  Maintaining financial system stability is critical to economic development,
and early identification of risks and opportunities is essential. The financial
industry contains a wide variety of data, such as financial statements,
customer information, stock trading data, news, etc. Massive heterogeneous data
calls for intelligent algorithms for machines to process and understand. This
paper mainly focuses on the stock trading data and news about China A-share
companies. We present a financial data analysis application, Financial Quotient
Porter, designed to combine textual and numerical data by using a
multi-strategy data mining approach. Additionally, we present our efforts and
plans in deep learning financial text processing application scenarios using
natural language processing (NLP) and knowledge graph (KG) technologies. Based
on KG technology, risks and opportunities can be identified from heterogeneous
data. NLP technology can be used to extract entities, relations, and events
from unstructured text, and analyze market sentiment. Experimental results show
market sentiments towards a company and an industry, as well as news-level
associations between companies.
",2022-04-25T01:56:36Z,http://arxiv.org/abs/2204.11394v1,Hongyin Zhu
A Law of Data Separation in Deep Learning,"  While deep learning has enabled significant advances in many areas of
science, its black-box nature hinders architecture design for future artificial
intelligence applications and interpretation for high-stakes decision makings.
We addressed this issue by studying the fundamental question of how deep neural
networks process data in the intermediate layers. Our finding is a simple and
quantitative law that governs how deep neural networks separate data according
to class membership throughout all layers for classification. This law shows
that each layer improves data separation at a constant geometric rate, and its
emergence is observed in a collection of network architectures and datasets
during training. This law offers practical guidelines for designing
architectures, improving model robustness and out-of-sample performance, as
well as interpreting the predictions.
",2022-10-31T02:25:38Z,http://arxiv.org/abs/2210.17020v2,"Hangfeng He, Weijie J. Su"
Building Advanced Dialogue Managers for Goal-Oriented Dialogue Systems,"  Goal-Oriented (GO) Dialogue Systems, colloquially known as goal oriented
chatbots, help users achieve a predefined goal (e.g. book a movie ticket)
within a closed domain. A first step is to understand the user's goal by using
natural language understanding techniques. Once the goal is known, the bot must
manage a dialogue to achieve that goal, which is conducted with respect to a
learnt policy. The success of the dialogue system depends on the quality of the
policy, which is in turn reliant on the availability of high-quality training
data for the policy learning method, for instance Deep Reinforcement Learning.
  Due to the domain specificity, the amount of available data is typically too
low to allow the training of good dialogue policies. In this master thesis we
introduce a transfer learning method to mitigate the effects of the low
in-domain data availability. Our transfer learning based approach improves the
bot's success rate by $20\%$ in relative terms for distant domains and we more
than double it for close domains, compared to the model without transfer
learning. Moreover, the transfer learning chatbots learn the policy up to 5 to
10 times faster. Finally, as the transfer learning approach is complementary to
additional processing such as warm-starting, we show that their joint
application gives the best outcomes.
",2018-06-03T12:36:06Z,http://arxiv.org/abs/1806.00780v1,Vladimir Ilievski
"Mapping Complex Technologies via Science-Technology Linkages; The Case
  of Neuroscience -- A transformer based keyword extraction approach","  In this paper, we present an efficient deep learning based approach to
extract technology-related topics and keywords within scientific literature,
and identify corresponding technologies within patent applications.
Specifically, we utilize transformer based language models, tailored for use
with scientific text, to detect coherent topics over time and describe these by
relevant keywords that are automatically extracted from a large text corpus. We
identify these keywords using Named Entity Recognition, distinguishing between
those describing methods, applications and other scientific terminology. We
create a large amount of search queries based on combinations of method- and
application-keywords, which we use to conduct semantic search and identify
related patents. By doing so, we aim at contributing to the growing body of
research on text-based technology mapping and forecasting that leverages latest
advances in natural language processing and deep learning. We are able to map
technologies identified in scientific literature to patent applications,
thereby providing an empirical foundation for the study of science-technology
linkages. We illustrate the workflow as well as results obtained by mapping
publications within the field of neuroscience to related patent applications.
",2022-05-19T09:32:09Z,http://arxiv.org/abs/2205.10153v1,"Daniel Hain, Roman Jurowetzki, Mariagrazia Squicciarini"
"Knowledge Distillation in Automated Annotation: Supervised Text
  Classification with LLM-Generated Training Labels","  Computational social science (CSS) practitioners often rely on human-labeled
data to fine-tune supervised text classifiers. We assess the potential for
researchers to augment or replace human-generated training data with surrogate
training labels from generative large language models (LLMs). We introduce a
recommended workflow and test this LLM application by replicating 14
classification tasks and measuring performance. We employ a novel corpus of
English-language text classification data sets from recent CSS articles in
high-impact journals. Because these data sets are stored in password-protected
archives, our analyses are less prone to issues of contamination. For each
task, we compare supervised classifiers fine-tuned using GPT-4 labels against
classifiers fine-tuned with human annotations and against labels from GPT-4 and
Mistral-7B with few-shot in-context learning. Our findings indicate that
supervised classification models fine-tuned on LLM-generated labels perform
comparably to models fine-tuned with labels from human annotators. Fine-tuning
models using LLM-generated labels can be a fast, efficient and cost-effective
method of building supervised text classifiers.
",2024-06-25T15:20:25Z,http://arxiv.org/abs/2406.17633v1,"Nicholas Pangakis, Samuel Wolken"
"Resource Allocation and Workload Scheduling for Large-Scale Distributed
  Deep Learning: A Survey","  With rapidly increasing distributed deep learning workloads in large-scale
data centers, efficient distributed deep learning framework strategies for
resource allocation and workload scheduling have become the key to
high-performance deep learning. The large-scale environment with large volumes
of datasets, models, and computational and communication resources raises
various unique challenges for resource allocation and workload scheduling in
distributed deep learning, such as scheduling complexity, resource and workload
heterogeneity, and fault tolerance. To uncover these challenges and
corresponding solutions, this survey reviews the literature, mainly from 2019
to 2024, on efficient resource allocation and workload scheduling strategies
for large-scale distributed DL. We explore these strategies by focusing on
various resource types, scheduling granularity levels, and performance goals
during distributed training and inference processes. We highlight critical
challenges for each topic and discuss key insights of existing technologies. To
illustrate practical large-scale resource allocation and workload scheduling in
real distributed deep learning scenarios, we use a case study of training large
language models. This survey aims to encourage computer science, artificial
intelligence, and communications researchers to understand recent advances and
explore future research directions for efficient framework strategies for
large-scale distributed deep learning.
",2024-06-12T11:51:44Z,http://arxiv.org/abs/2406.08115v1,"Feng Liang, Zhen Zhang, Haifeng Lu, Chengming Li, Victor C. M. Leung, Yanyi Guo, Xiping Hu"
"Med7: a transferable clinical natural language processing model for
  electronic health records","  The field of clinical natural language processing has been advanced
significantly since the introduction of deep learning models. The
self-supervised representation learning and the transfer learning paradigm
became the methods of choice in many natural language processing application,
in particular in the settings with the dearth of high quality manually
annotated data. Electronic health record systems are ubiquitous and the
majority of patients' data are now being collected electronically and in
particular in the form of free text. Identification of medical concepts and
information extraction is a challenging task, yet important ingredient for
parsing unstructured data into structured and tabulated format for downstream
analytical tasks. In this work we introduced a named-entity recognition model
for clinical natural language processing. The model is trained to recognise
seven categories: drug names, route, frequency, dosage, strength, form,
duration. The model was first self-supervisedly pre-trained by predicting the
next word, using a collection of 2 million free-text patients' records from
MIMIC-III corpora and then fine-tuned on the named-entity recognition task. The
model achieved a lenient (strict) micro-averaged F1 score of 0.957 (0.893)
across all seven categories. Additionally, we evaluated the transferability of
the developed model using the data from the Intensive Care Unit in the US to
secondary care mental health records (CRIS) in the UK. A direct application of
the trained NER model to CRIS data resulted in reduced performance of F1=0.762,
however after fine-tuning on a small sample from CRIS, the model achieved a
reasonable performance of F1=0.944. This demonstrated that despite a close
similarity between the data sets and the NER tasks, it is essential to
fine-tune on the target domain data in order to achieve more accurate results.
",2020-03-03T00:55:43Z,http://arxiv.org/abs/2003.01271v2,"Andrey Kormilitzin, Nemanja Vaci, Qiang Liu, Alejo Nevado-Holgado"
"Robust Task-Oriented Dialogue Generation with Contrastive Pre-training
  and Adversarial Filtering","  Data artifacts incentivize machine learning models to learn non-transferable
generalizations by taking advantage of shortcuts in the data, and there is
growing evidence that data artifacts play a role for the strong results that
deep learning models achieve in recent natural language processing benchmarks.
In this paper, we focus on task-oriented dialogue and investigate whether
popular datasets such as MultiWOZ contain such data artifacts. We found that by
only keeping frequent phrases in the training examples, state-of-the-art models
perform similarly compared to the variant trained with full data, suggesting
they exploit these spurious correlations to solve the task. Motivated by this,
we propose a contrastive learning based framework to encourage the model to
ignore these cues and focus on learning generalisable patterns. We also
experiment with adversarial filtering to remove ""easy"" training instances so
that the model would focus on learning from the ""harder"" instances. We conduct
a number of generalization experiments -- e.g., cross-domain/dataset and
adversarial tests -- to assess the robustness of our approach and found that it
works exceptionally well.
",2022-05-20T03:13:02Z,http://arxiv.org/abs/2205.10363v1,"Shiquan Yang, Xinting Huang, Jey Han Lau, Sarah Erfani"
"Using Error Decay Prediction to Overcome Practical Issues of Deep Active
  Learning for Named Entity Recognition","  Existing deep active learning algorithms achieve impressive sampling
efficiency on natural language processing tasks. However, they exhibit several
weaknesses in practice, including (a) inability to use uncertainty sampling
with black-box models, (b) lack of robustness to labeling noise, and (c) lack
of transparency. In response, we propose a transparent batch active sampling
framework by estimating the error decay curves of multiple feature-defined
subsets of the data. Experiments on four named entity recognition (NER) tasks
demonstrate that the proposed methods significantly outperform
diversification-based methods for black-box NER taggers, and can make the
sampling process more robust to labeling noise when combined with
uncertainty-based methods. Furthermore, the analysis of experimental results
sheds light on the weaknesses of different active sampling strategies, and when
traditional uncertainty-based or diversification-based methods can be expected
to work well.
",2019-11-17T20:41:32Z,http://arxiv.org/abs/1911.07335v2,"Haw-Shiuan Chang, Shankar Vembu, Sunil Mohan, Rheeya Uppaal, Andrew McCallum"
Deep Active Learning with Noise Stability,"  Uncertainty estimation for unlabeled data is crucial to active learning. With
a deep neural network employed as the backbone model, the data selection
process is highly challenging due to the potential over-confidence of the model
inference. Existing methods resort to special learning fashions (e.g.
adversarial) or auxiliary models to address this challenge. This tends to
result in complex and inefficient pipelines, which would render the methods
impractical. In this work, we propose a novel algorithm that leverages noise
stability to estimate data uncertainty. The key idea is to measure the output
derivation from the original observation when the model parameters are randomly
perturbed by noise. We provide theoretical analyses by leveraging the small
Gaussian noise theory and demonstrate that our method favors a subset with
large and diverse gradients. Our method is generally applicable in various
tasks, including computer vision, natural language processing, and structural
data analysis. It achieves competitive performance compared against
state-of-the-art active learning baselines.
",2022-05-26T13:21:01Z,http://arxiv.org/abs/2205.13340v2,"Xingjian Li, Pengkun Yang, Yangcheng Gu, Xueying Zhan, Tianyang Wang, Min Xu, Chengzhong Xu"
"A Deep Learning Model with Hierarchical LSTMs and Supervised Attention
  for Anti-Phishing","  Anti-phishing aims to detect phishing content/documents in a pool of textual
data. This is an important problem in cybersecurity that can help to guard
users from fraudulent information. Natural language processing (NLP) offers a
natural solution for this problem as it is capable of analyzing the textual
content to perform intelligent recognition. In this work, we investigate
state-of-the-art techniques for text categorization in NLP to address the
problem of anti-phishing for emails (i.e, predicting if an email is phishing or
not). These techniques are based on deep learning models that have attracted
much attention from the community recently. In particular, we present a
framework with hierarchical long short-term memory networks (H-LSTMs) and
attention mechanisms to model the emails simultaneously at the word and the
sentence level. Our expectation is to produce an effective model for
anti-phishing and demonstrate the effectiveness of deep learning for problems
in cybersecurity.
",2018-05-03T21:53:09Z,http://arxiv.org/abs/1805.01554v1,"Minh Nguyen, Toan Nguyen, Thien Huu Nguyen"
Deep Spatial Learning with Molecular Vibration,"  Machine learning over-fitting caused by data scarcity greatly limits the
application of machine learning for molecules. Due to manufacturing processes
difference, big data is not always rendered available through computational
chemistry methods for some tasks, causing data scarcity problem for machine
learning algorithms. Here we propose to extract the natural features of
molecular structures and rationally distort them to augment the data
availability. This method allows a machine learning project to leverage the
powerful fit of physics-informed augmentation for providing significant boost
to predictive accuracy. Successfully verified by the prediction of rejection
rate and flux of thin film polyamide nanofiltration membranes, with the
relative error dropping from 16.34% to 6.71% and the coefficient of
determination rising from 0.16 to 0.75, the proposed deep spatial learning with
molecular vibration is widely instructive for molecular science. Experimental
comparison unequivocally demonstrates its superiority over common learning
algorithms.
",2020-11-14T02:46:43Z,http://arxiv.org/abs/2011.07200v1,"Ziyang Zhang, Yingtao Luo"
Self-Knowledge Distillation in Natural Language Processing,"  Since deep learning became a key player in natural language processing (NLP),
many deep learning models have been showing remarkable performances in a
variety of NLP tasks, and in some cases, they are even outperforming humans.
Such high performance can be explained by efficient knowledge representation of
deep learning models. While many methods have been proposed to learn more
efficient representation, knowledge distillation from pretrained deep networks
suggest that we can use more information from the soft target probability to
train other neural networks. In this paper, we propose a new knowledge
distillation method self-knowledge distillation, based on the soft target
probabilities of the training model itself, where multimode information is
distilled from the word embedding space right below the softmax layer. Due to
the time complexity, our method approximates the soft target probabilities. In
experiments, we applied the proposed method to two different and fundamental
NLP tasks: language model and neural machine translation. The experiment
results show that our proposed method improves performance on the tasks.
",2019-08-02T15:17:27Z,http://arxiv.org/abs/1908.01851v1,"Sangchul Hahn, Heeyoul Choi"
"Positional Attention-based Frame Identification with BERT: A Deep
  Learning Approach to Target Disambiguation and Semantic Frame Selection","  Semantic parsing is the task of transforming sentences from natural language
into formal representations of predicate-argument structures. Under this
research area, frame-semantic parsing has attracted much interest. This parsing
approach leverages the lexical information defined in FrameNet to associate
marked predicates or targets with semantic frames, thereby assigning semantic
roles to sentence components based on pre-specified frame elements in FrameNet.
In this paper, a deep neural network architecture known as Positional
Attention-based Frame Identification with BERT (PAFIBERT) is presented as a
solution to the frame identification subtask in frame-semantic parsing.
Although the importance of this subtask is well-established, prior research has
yet to find a robust solution that works satisfactorily for both in-domain and
out-of-domain data. This study thus set out to improve frame identification in
light of recent advancements of language modeling and transfer learning in
natural language processing. The proposed method is partially empowered by
BERT, a pre-trained language model that excels at capturing contextual
information in texts. By combining the language representation power of BERT
with a position-based attention mechanism, PAFIBERT is able to attend to
target-specific contexts in sentences for disambiguating targets and
associating them with the most suitable semantic frames. Under various
experimental settings, PAFIBERT outperformed existing solutions by a
significant margin, achieving new state-of-the-art results for both in-domain
and out-of-domain benchmark test sets.
",2019-10-31T15:51:04Z,http://arxiv.org/abs/1910.14549v1,"Sang-Sang Tan, Jin-Cheon Na"
Tackling Morphological Analogies Using Deep Learning -- Extended Version,"  Analogical proportions are statements of the form ""A is to B as C is to D"".
They constitute an inference tool that provides a logical framework to address
learning, transfer, and explainability concerns and that finds useful
applications in artificial intelligence and natural language processing. In
this paper, we address two problems, namely, analogy detection and resolution
in morphology. Multiple symbolic approaches tackle the problem of analogies in
morphology and achieve competitive performance. We show that it is possible to
use a data-driven strategy to outperform those models. We propose an approach
using deep learning to detect and solve morphological analogies. It encodes
structural properties of analogical proportions and relies on a specifically
designed embedding model capturing morphological characteristics of words. We
demonstrate our model's competitive performance on analogy detection and
resolution over multiple languages. We provide an empirical study to analyze
the impact of balancing training data and evaluate the robustness of our
approach to input perturbation.
",2021-11-09T13:45:23Z,http://arxiv.org/abs/2111.05147v1,"Safa Alsaidi, Amandine Decker, Esteban Marquer, Pierre-Alexandre Murena, Miguel Couceiro"
LSICC: A Large Scale Informal Chinese Corpus,"  Deep learning based natural language processing model is proven powerful, but
need large-scale dataset. Due to the significant gap between the real-world
tasks and existing Chinese corpus, in this paper, we introduce a large-scale
corpus of informal Chinese. This corpus contains around 37 million book reviews
and 50 thousand netizen's comments to the news. We explore the informal words
frequencies of the corpus and show the difference between our corpus and the
existing ones. The corpus can be further used to train deep learning based
natural language processing tasks such as Chinese word segmentation, sentiment
analysis.
",2018-11-26T03:58:09Z,http://arxiv.org/abs/1811.10167v1,"Jianyu Zhao, Zhuoran Ji"
"Res-CNN-BiLSTM Network for overcoming Mental Health Disturbances caused
  due to Cyberbullying through Social Media","  Mental Health Disturbance has many reasons and cyberbullying is one of the
major causes that does exploitation using social media as an instrument. The
cyberbullying is done on the basis of Religion, Ethnicity, Age and Gender which
is a sensitive psychological issue. This can be addressed using Natural
Language Processing with Deep Learning, since social media is the medium and it
generates massive form of data in textual form. Such data can be leveraged to
find the semantics and derive what type of cyberbullying is done and who are
the people involved for early measures. Since deriving semantics is essential
we proposed a Hybrid Deep Learning Model named 1-Dimensional
CNN-Bidirectional-LSTMs with Residuals shortly known as Res-CNN-BiLSTM. In this
paper we have proposed the architecture and compared its performance with
different approaches of Embedding Deep Learning Algorithms.
",2022-04-20T18:40:39Z,http://arxiv.org/abs/2204.09738v1,"Raunak Joshi, Abhishek Gupta, Nandan Kanvinde"
A Hybrid Deep Learning Model for Arabic Text Recognition,"  Arabic text recognition is a challenging task because of the cursive nature
of Arabic writing system, its joint writing scheme, the large number of
ligatures and many other challenges. Deep Learning DL models achieved
significant progress in numerous domains including computer vision and sequence
modelling. This paper presents a model that can recognize Arabic text that was
printed using multiple font types including fonts that mimic Arabic handwritten
scripts. The proposed model employs a hybrid DL network that can recognize
Arabic printed text without the need for character segmentation. The model was
tested on a custom dataset comprised of over two million word samples that were
generated using 18 different Arabic font types. The objective of the testing
process was to assess the model capability in recognizing a diverse set of
Arabic fonts representing a varied cursive styles. The model achieved good
results in recognizing characters and words and it also achieved promising
results in recognizing characters when it was tested on unseen data. The
prepared model, the custom datasets and the toolkit for generating similar
datasets are made publicly available, these tools can be used to prepare models
for recognizing other font types as well as to further extend and enhance the
performance of the proposed model.
",2020-09-04T02:49:17Z,http://arxiv.org/abs/2009.01987v1,"Mohammad Fasha, Bassam Hammo, Nadim Obeid, Jabir Widian"
"Architectures of Topological Deep Learning: A Survey of Message-Passing
  Topological Neural Networks","  The natural world is full of complex systems characterized by intricate
relations between their components: from social interactions between
individuals in a social network to electrostatic interactions between atoms in
a protein. Topological Deep Learning (TDL) provides a comprehensive framework
to process and extract knowledge from data associated with these systems, such
as predicting the social community to which an individual belongs or predicting
whether a protein can be a reasonable target for drug development. TDL has
demonstrated theoretical and practical advantages that hold the promise of
breaking ground in the applied sciences and beyond. However, the rapid growth
of the TDL literature for relational systems has also led to a lack of
unification in notation and language across message-passing Topological Neural
Network (TNN) architectures. This presents a real obstacle for building upon
existing works and for deploying message-passing TNNs to new real-world
problems. To address this issue, we provide an accessible introduction to TDL
for relational systems, and compare the recently published message-passing TNNs
using a unified mathematical and graphical notation. Through an intuitive and
critical review of the emerging field of TDL, we extract valuable insights into
current challenges and exciting opportunities for future development.
",2023-04-20T01:02:13Z,http://arxiv.org/abs/2304.10031v3,"Mathilde Papillon, Sophia Sanborn, Mustafa Hajij, Nina Miolane"
Data-driven geophysics: from dictionary learning to deep learning,"  Understanding the principles of geophysical phenomena is an essential and
challenging task. ""Model-driven"" approaches have supported the development of
geophysics for a long time; however, such methods suffer from the curse of
dimensionality and may inaccurately model the subsurface. ""Data-driven""
techniques may overcome these issues with increasingly available geophysical
data. In this article, we review the basic concepts of and recent advances in
data-driven approaches from dictionary learning to deep learning in a variety
of geophysical scenarios. Explorational geophysics including data processing,
inversion and interpretation will be mainly focused. Artificial intelligence
applications on geoscience involving deep Earth, earthquake, water resource,
atmospheric science, satellite remoe sensing and space sciences are also
reviewed. We present a coding tutorial and a summary of tips for beginners and
interested geophysical readers to rapidly explore deep learning. Some promising
directions are provided for future research involving deep learning in
geophysics, such as unsupervised learning, transfer learning, multimodal deep
learning, federated learning, uncertainty estimation, and activate learning.
",2020-07-13T04:39:49Z,http://arxiv.org/abs/2007.06183v2,"Siwei Yu, Jianwei Ma"
"On Linearizing Structured Data in Encoder-Decoder Language Models:
  Insights from Text-to-SQL","  Structured data, prevalent in tables, databases, and knowledge graphs, poses
a significant challenge in its representation. With the advent of large
language models (LLMs), there has been a shift towards linearization-based
methods, which process structured data as sequential token streams, diverging
from approaches that explicitly model structure, often as a graph. Crucially,
there remains a gap in our understanding of how these linearization-based
methods handle structured data, which is inherently non-linear. This work
investigates the linear handling of structured data in encoder-decoder language
models, specifically T5. Our findings reveal the model's ability to mimic
human-designed processes such as schema linking and syntax prediction,
indicating a deep, meaningful learning of structure beyond simple token
sequencing. We also uncover insights into the model's internal mechanisms,
including the ego-centric nature of structure node encodings and the potential
for model compression due to modality fusion redundancy. Overall, this work
sheds light on the inner workings of linearization-based methods and could
potentially provide guidance for future research.
",2024-04-03T01:16:20Z,http://arxiv.org/abs/2404.02389v1,"Yutong Shao, Ndapa Nakashole"
"Deep Learning and NLP in Cryptocurrency Forecasting: Integrating
  Financial, Blockchain, and Social Media Data","  We introduce novel approaches to cryptocurrency price forecasting, leveraging
Machine Learning (ML) and Natural Language Processing (NLP) techniques, with a
focus on Bitcoin and Ethereum. By analysing news and social media content,
primarily from Twitter and Reddit, we assess the impact of public sentiment on
cryptocurrency markets. A distinctive feature of our methodology is the
application of the BART MNLI zero-shot classification model to detect bullish
and bearish trends, significantly advancing beyond traditional sentiment
analysis. Additionally, we systematically compare a range of pre-trained and
fine-tuned deep learning NLP models against conventional dictionary-based
sentiment analysis methods. Another key contribution of our work is the
adoption of local extrema alongside daily price movements as predictive
targets, reducing trading frequency and portfolio volatility. Our findings
demonstrate that integrating textual data into cryptocurrency price forecasting
not only improves forecasting accuracy but also consistently enhances the
profitability and Sharpe ratio across various validation scenarios,
particularly when applying deep learning NLP techniques. The entire codebase of
our experiments is made available via an online repository:
https://anonymous.4open.science/r/crypto-forecasting-public
",2023-11-23T16:14:44Z,http://arxiv.org/abs/2311.14759v2,"Vincent Gurgul, Stefan Lessmann, Wolfgang Karl Härdle"
"Natural Language Interfaces for Tabular Data Querying and Visualization:
  A Survey","  The emergence of natural language processing has revolutionized the way users
interact with tabular data, enabling a shift from traditional query languages
and manual plotting to more intuitive, language-based interfaces. The rise of
large language models (LLMs) such as ChatGPT and its successors has further
advanced this field, opening new avenues for natural language processing
techniques. This survey presents a comprehensive overview of natural language
interfaces for tabular data querying and visualization, which allow users to
interact with data using natural language queries. We introduce the fundamental
concepts and techniques underlying these interfaces with a particular emphasis
on semantic parsing, the key technology facilitating the translation from
natural language to SQL queries or data visualization commands. We then delve
into the recent advancements in Text-to-SQL and Text-to-Vis problems from the
perspectives of datasets, methodologies, metrics, and system designs. This
includes a deep dive into the influence of LLMs, highlighting their strengths,
limitations, and potential for future improvements. Through this survey, we aim
to provide a roadmap for researchers and practitioners interested in developing
and applying natural language interfaces for data interaction in the era of
large language models.
",2023-10-27T05:01:20Z,http://arxiv.org/abs/2310.17894v3,"Weixu Zhang, Yifei Wang, Yuanfeng Song, Victor Junqiu Wei, Yuxing Tian, Yiyan Qi, Jonathan H. Chan, Raymond Chi-Wing Wong, Haiqin Yang"
A Survey on Backdoor Attack and Defense in Natural Language Processing,"  Deep learning is becoming increasingly popular in real-life applications,
especially in natural language processing (NLP). Users often choose training
outsourcing or adopt third-party data and models due to data and computation
resources being limited. In such a situation, training data and models are
exposed to the public. As a result, attackers can manipulate the training
process to inject some triggers into the model, which is called backdoor
attack. Backdoor attack is quite stealthy and difficult to be detected because
it has little inferior influence on the model's performance for the clean
samples. To get a precise grasp and understanding of this problem, in this
paper, we conduct a comprehensive review of backdoor attacks and defenses in
the field of NLP. Besides, we summarize benchmark datasets and point out the
open issues to design credible systems to defend against backdoor attacks.
",2022-11-22T02:35:12Z,http://arxiv.org/abs/2211.11958v1,"Xuan Sheng, Zhaoyang Han, Piji Li, Xiangmao Chang"
Fidelity-Weighted Learning,"  Training deep neural networks requires many training samples, but in practice
training labels are expensive to obtain and may be of varying quality, as some
may be from trusted expert labelers while others might be from heuristics or
other sources of weak supervision such as crowd-sourcing. This creates a
fundamental quality versus-quantity trade-off in the learning process. Do we
learn from the small amount of high-quality data or the potentially large
amount of weakly-labeled data? We argue that if the learner could somehow know
and take the label-quality into account when learning the data representation,
we could get the best of both worlds. To this end, we propose
""fidelity-weighted learning"" (FWL), a semi-supervised student-teacher approach
for training deep neural networks using weakly-labeled data. FWL modulates the
parameter updates to a student network (trained on the task we care about) on a
per-sample basis according to the posterior confidence of its label-quality
estimated by a teacher (who has access to the high-quality labels). Both
student and teacher are learned from the data. We evaluate FWL on two tasks in
information retrieval and natural language processing where we outperform
state-of-the-art alternative semi-supervised methods, indicating that our
approach makes better use of strong and weak labels, and leads to better
task-dependent data representations.
",2017-11-08T02:05:11Z,http://arxiv.org/abs/1711.02799v2,"Mostafa Dehghani, Arash Mehrjou, Stephan Gouws, Jaap Kamps, Bernhard Schölkopf"
"Tree Edit Distance Learning via Adaptive Symbol Embeddings:
  Supplementary Materials and Results","  Metric learning has the aim to improve classification accuracy by learning a
distance measure which brings data points from the same class closer together
and pushes data points from different classes further apart. Recent research
has demonstrated that metric learning approaches can also be applied to trees,
such as molecular structures, abstract syntax trees of computer programs, or
syntax trees of natural language, by learning the cost function of an edit
distance, i.e. the costs of replacing, deleting, or inserting nodes in a tree.
However, learning such costs directly may yield an edit distance which violates
metric axioms, is challenging to interpret, and may not generalize well. In
this contribution, we propose a novel metric learning approach for trees which
learns an edit distance indirectly by embedding the tree nodes as vectors, such
that the Euclidean distance between those vectors supports class
discrimination. We learn such embeddings by reducing the distance to
prototypical trees from the same class and increasing the distance to
prototypical trees from different classes. In our experiments, we show that our
proposed metric learning approach improves upon the state-of-the-art in metric
learning for trees on six benchmark data sets, ranging from computer science
over biomedical data to a natural-language processing data set containing over
300,000 nodes.
",2018-05-18T10:09:41Z,http://arxiv.org/abs/1805.07123v1,Benjamin Paaßen
Value Prediction for Spatiotemporal Gait Data Using Deep Learning,"  Human gait has been commonly used for the diagnosis and evaluation of medical
conditions and for monitoring the progress during treatment and rehabilitation.
The use of wearable sensors that capture pressure or motion has yielded
techniques that analyze the gait data to aid recovery, identify activity
performed, or identify individuals. Deep learning, usually employing
classification, has been successfully utilized in a variety of applications
such as computer vision, biomedical imaging analysis, and natural language
processing. We expand the application of deep learning to value prediction of
time-series of spatiotemporal gait data. Moreover, we explore several deep
learning architectures (Recurrent Neural Networks (RNN) and RNN combined with
Convolutional Neural Networks (CNN)) to make short- and long-distance
predictions using two different experimental setups. Our results show that
short-distance prediction has an RMSE as low as 0.060675, and long-distance
prediction RMSE as low as 0.106365. Additionally, the results show that the
proposed deep learning models are capable of predicting the entire trial when
trained and validated using the trials from the same participant. The proposed,
customized models, used with value prediction open possibilities for additional
applications, such as fall prediction, in-home progress monitoring, aiding of
exoskeleton movement, and authentication.
",2024-02-29T18:30:13Z,http://arxiv.org/abs/2403.07926v1,"Ryan Cavanagh, Jelena Trajkovic, Wenlu Zhang, I-Hung Khoo, Vennila Krishnan"
"Unmasking Transformers: A Theoretical Approach to Data Recovery via
  Attention Weights","  In the realm of deep learning, transformers have emerged as a dominant
architecture, particularly in natural language processing tasks. However, with
their widespread adoption, concerns regarding the security and privacy of the
data processed by these models have arisen. In this paper, we address a pivotal
question: Can the data fed into transformers be recovered using their attention
weights and outputs? We introduce a theoretical framework to tackle this
problem. Specifically, we present an algorithm that aims to recover the input
data $X \in \mathbb{R}^{d \times n}$ from given attention weights $W = QK^\top
\in \mathbb{R}^{d \times d}$ and output $B \in \mathbb{R}^{n \times n}$ by
minimizing the loss function $L(X)$. This loss function captures the
discrepancy between the expected output and the actual output of the
transformer. Our findings have significant implications for the Localized
Layer-wise Mechanism (LLM), suggesting potential vulnerabilities in the model's
design from a security and privacy perspective. This work underscores the
importance of understanding and safeguarding the internal workings of
transformers to ensure the confidentiality of processed data.
",2023-10-19T04:41:01Z,http://arxiv.org/abs/2310.12462v1,"Yichuan Deng, Zhao Song, Shenghao Xie, Chiwun Yang"
Social Analysis of Young Basque Speaking Communities in Twitter,"  In this paper we take into account both social and linguistic aspects to
perform demographic analysis by processing a large amount of tweets in Basque
language. The study of demographic characteristics and social relationships are
approached by applying machine learning and modern deep-learning Natural
Language Processing (NLP) techniques, combining social sciences with automatic
text processing. More specifically, our main objective is to combine
demographic inference and social analysis in order to detect young Basque
Twitter users and to identify the communities that arise from their
relationships or shared content. This social and demographic analysis will be
entirely based on the~automatically collected tweets using NLP to convert
unstructured textual information into interpretable knowledge.
",2021-09-08T08:19:08Z,http://arxiv.org/abs/2109.03487v1,"J. Fernandez de Landa, R. Agerri"
ZeroBERTo: Leveraging Zero-Shot Text Classification by Topic Modeling,"  Traditional text classification approaches often require a good amount of
labeled data, which is difficult to obtain, especially in restricted domains or
less widespread languages. This lack of labeled data has led to the rise of
low-resource methods, that assume low data availability in natural language
processing. Among them, zero-shot learning stands out, which consists of
learning a classifier without any previously labeled data. The best results
reported with this approach use language models such as Transformers, but fall
into two problems: high execution time and inability to handle long texts as
input. This paper proposes a new model, ZeroBERTo, which leverages an
unsupervised clustering step to obtain a compressed data representation before
the classification task. We show that ZeroBERTo has better performance for long
inputs and shorter execution time, outperforming XLM-R by about 12% in the F1
score in the FolhaUOL dataset. Keywords: Low-Resource NLP, Unlabeled data,
Zero-Shot Learning, Topic Modeling, Transformers.
",2022-01-04T20:08:17Z,http://arxiv.org/abs/2201.01337v3,"Alexandre Alcoforado, Thomas Palmeira Ferraz, Rodrigo Gerber, Enzo Bustos, André Seidel Oliveira, Bruno Miguel Veloso, Fabio Levy Siqueira, Anna Helena Reali Costa"
Emotion Based Hate Speech Detection using Multimodal Learning,"  In recent years, monitoring hate speech and offensive language on social
media platforms has become paramount due to its widespread usage among all age
groups, races, and ethnicities. Consequently, there have been substantial
research efforts towards automated detection of such content using Natural
Language Processing (NLP). While successfully filtering textual data, no
research has focused on detecting hateful content in multimedia data. With
increased ease of data storage and the exponential growth of social media
platforms, multimedia content proliferates the internet as much as text data.
Nevertheless, it escapes the automatic filtering systems. Hate speech and
offensiveness can be detected in multimedia primarily via three modalities,
i.e., visual, acoustic, and verbal. Our preliminary study concluded that the
most essential features in classifying hate speech would be the speaker's
emotional state and its influence on the spoken words, therefore limiting our
current research to these modalities. This paper proposes the first multimodal
deep learning framework to combine the auditory features representing emotion
and the semantic features to detect hateful content. Our results demonstrate
that incorporating emotional attributes leads to significant improvement over
text-based models in detecting hateful multimedia content. This paper also
presents a new Hate Speech Detection Video Dataset (HSDVD) collected for the
purpose of multimodal learning as no such dataset exists today.
",2022-02-13T05:39:47Z,http://arxiv.org/abs/2202.06218v1,"Aneri Rana, Sonali Jha"
A Survey on Green Deep Learning,"  In recent years, larger and deeper models are springing up and continuously
pushing state-of-the-art (SOTA) results across various fields like natural
language processing (NLP) and computer vision (CV). However, despite promising
results, it needs to be noted that the computations required by SOTA models
have been increased at an exponential rate. Massive computations not only have
a surprisingly large carbon footprint but also have negative effects on
research inclusiveness and deployment on real-world applications.
  Green deep learning is an increasingly hot research field that appeals to
researchers to pay attention to energy usage and carbon emission during model
training and inference. The target is to yield novel results with lightweight
and efficient technologies. Many technologies can be used to achieve this goal,
like model compression and knowledge distillation. This paper focuses on
presenting a systematic review of the development of Green deep learning
technologies. We classify these approaches into four categories: (1) compact
networks, (2) energy-efficient training strategies, (3) energy-efficient
inference approaches, and (4) efficient data usage. For each category, we
discuss the progress that has been achieved and the unresolved challenges.
",2021-11-08T16:55:03Z,http://arxiv.org/abs/2111.05193v2,"Jingjing Xu, Wangchunshu Zhou, Zhiyi Fu, Hao Zhou, Lei Li"
"Textual Data Mining for Financial Fraud Detection: A Deep Learning
  Approach","  In this report, I present a deep learning approach to conduct a natural
language processing (hereafter NLP) binary classification task for analyzing
financial-fraud texts. First, I searched for regulatory announcements and
enforcement bulletins from HKEX news to define fraudulent companies and to
extract their MD&A reports before I organized the sentences from the reports
with labels and reporting time. My methodology involved different kinds of
neural network models, including Multilayer Perceptrons with Embedding layers,
vanilla Recurrent Neural Network (RNN), Long-Short Term Memory (LSTM), and
Gated Recurrent Unit (GRU) for the text classification task. By utilizing this
diverse set of models, I aim to perform a comprehensive comparison of their
accuracy in detecting financial fraud. My results bring significant
implications for financial fraud detection as this work contributes to the
growing body of research at the intersection of deep learning, NLP, and
finance, providing valuable insights for industry practitioners, regulators,
and researchers in the pursuit of more robust and effective fraud detection
methodologies.
",2023-08-05T15:33:10Z,http://arxiv.org/abs/2308.03800v1,Qiuru Li
Using Sequences of Life-events to Predict Human Lives,"  Over the past decade, machine learning has revolutionized computers' ability
to analyze text through flexible computational models. Due to their structural
similarity to written language, transformer-based architectures have also shown
promise as tools to make sense of a range of multi-variate sequences from
protein-structures, music, electronic health records to weather-forecasts. We
can also represent human lives in a way that shares this structural similarity
to language. From one perspective, lives are simply sequences of events: People
are born, visit the pediatrician, start school, move to a new location, get
married, and so on. Here, we exploit this similarity to adapt innovations from
natural language processing to examine the evolution and predictability of
human lives based on detailed event sequences. We do this by drawing on
arguably the most comprehensive registry data in existence, available for an
entire nation of more than six million individuals across decades. Our data
include information about life-events related to health, education, occupation,
income, address, and working hours, recorded with day-to-day resolution. We
create embeddings of life-events in a single vector space showing that this
embedding space is robust and highly structured. Our models allow us to predict
diverse outcomes ranging from early mortality to personality nuances,
outperforming state-of-the-art models by a wide margin. Using methods for
interpreting deep learning models, we probe the algorithm to understand the
factors that enable our predictions. Our framework allows researchers to
identify new potential mechanisms that impact life outcomes and associated
possibilities for personalized interventions.
",2023-06-05T16:19:48Z,http://arxiv.org/abs/2306.03009v1,"Germans Savcisens, Tina Eliassi-Rad, Lars Kai Hansen, Laust Mortensen, Lau Lilleholt, Anna Rogers, Ingo Zettler, Sune Lehmann"
PassTSL: Modeling Human-Created Passwords through Two-Stage Learning,"  Textual passwords are still the most widely used user authentication
mechanism. Due to the close connections between textual passwords and natural
languages, advanced technologies in natural language processing (NLP) and
machine learning (ML) could be used to model passwords for different purposes
such as studying human password-creation behaviors and developing more advanced
password cracking methods for informing better defence mechanisms. In this
paper, we propose PassTSL (modeling human-created Passwords through Two-Stage
Learning), inspired by the popular pretraining-finetuning framework in NLP and
deep learning (DL). We report how different pretraining settings affected
PassTSL and proved its effectiveness by applying it to six large leaked
password databases. Experimental results showed that it outperforms five
state-of-the-art (SOTA) password cracking methods on password guessing by a
significant margin ranging from 4.11% to 64.69% at the maximum point. Based on
PassTSL, we also implemented a password strength meter (PSM), and our
experiments showed that it was able to estimate password strength more
accurately, causing fewer unsafe errors (overestimating the password strength)
than two other SOTA PSMs when they produce the same rate of safe errors
(underestimating the password strength): a neural-network based method and
zxcvbn. Furthermore, we explored multiple finetuning settings, and our
evaluations showed that, even a small amount of additional training data, e.g.,
only 0.1% of the pretrained data, can lead to over 3% improvement in password
guessing on average. We also proposed a heuristic approach to selecting
finetuning passwords based on JS (Jensen-Shannon) divergence and experimental
results validated its usefulness. In summary, our contributions demonstrate the
potential and feasibility of applying advanced NLP and ML methods to password
modeling and cracking.
",2024-07-19T09:23:30Z,http://arxiv.org/abs/2407.14145v1,"Yangde Wang, Haozhang Li, Weidong Qiu, Shujun Li, Peng Tang"
"Tinto: Multisensor Benchmark for 3D Hyperspectral Point Cloud
  Segmentation in the Geosciences","  The increasing use of deep learning techniques has reduced interpretation
time and, ideally, reduced interpreter bias by automatically deriving
geological maps from digital outcrop models. However, accurate validation of
these automated mapping approaches is a significant challenge due to the
subjective nature of geological mapping and the difficulty in collecting
quantitative validation data. Additionally, many state-of-the-art deep learning
methods are limited to 2D image data, which is insufficient for 3D digital
outcrops, such as hyperclouds. To address these challenges, we present Tinto, a
multi-sensor benchmark digital outcrop dataset designed to facilitate the
development and validation of deep learning approaches for geological mapping,
especially for non-structured 3D data like point clouds. Tinto comprises two
complementary sets: 1) a real digital outcrop model from Corta Atalaya (Spain),
with spectral attributes and ground-truth data, and 2) a synthetic twin that
uses latent features in the original datasets to reconstruct realistic spectral
data (including sensor noise and processing artifacts) from the ground-truth.
The point cloud is dense and contains 3,242,964 labeled points. We used these
datasets to explore the abilities of different deep learning approaches for
automated geological mapping. By making Tinto publicly available, we hope to
foster the development and adaptation of new deep learning tools for 3D
applications in Earth sciences. The dataset can be accessed through this link:
https://doi.org/10.14278/rodare.2256.
",2023-05-17T03:24:08Z,http://arxiv.org/abs/2305.09928v2,"Ahmed J. Afifi, Samuel T. Thiele, Aldino Rizaldy, Sandra Lorenz, Pedram Ghamisi, Raimon Tolosana-Delgado, Moritz Kirsch, Richard Gloaguen, Michael Heizmann"
"Machine and Deep Learning Methods with Manual and Automatic Labelling
  for News Classification in Bangla Language","  Research in Natural Language Processing (NLP) has increasingly become
important due to applications such as text classification, text mining,
sentiment analysis, POS tagging, named entity recognition, textual entailment,
and many others. This paper introduces several machine and deep learning
methods with manual and automatic labelling for news classification in the
Bangla language. We implemented several machine (ML) and deep learning (DL)
algorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient
Descent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest
Neighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document
Frequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long
Short-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit
(GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and
FastText word embedding models. We develop automatic labelling methods using
Latent Dirichlet Allocation (LDA) and investigate the performance of
single-label and multi-label article classification methods. To investigate
performance, we developed from scratch Potrika, the largest and the most
extensive dataset for news classification in the Bangla language, comprising
185.51 million words and 12.57 million sentences contained in 664,880 news
articles in eight distinct categories, curated from six popular online news
portals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83%
achieve the highest accuracy for manually-labelled data. For the automatic
labelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy
for single-label and multi-label data, respectively. The methods developed in
this paper are expected to advance research in Bangla and other languages.
",2022-10-19T21:53:49Z,http://arxiv.org/abs/2210.10903v1,"Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood"
"An open access NLP dataset for Arabic dialects : Data collection,
  labeling, and model construction","  Natural Language Processing (NLP) is today a very active field of research
and innovation. Many applications need however big sets of data for supervised
learning, suitably labelled for the training purpose. This includes
applications for the Arabic language and its national dialects. However, such
open access labeled data sets in Arabic and its dialects are lacking in the
Data Science ecosystem and this lack can be a burden to innovation and research
in this field. In this work, we present an open data set of social data content
in several Arabic dialects. This data was collected from the Twitter social
network and consists on +50K twits in five (5) national dialects. Furthermore,
this data was labeled for several applications, namely dialect detection, topic
detection and sentiment analysis. We publish this data as an open access data
to encourage innovation and encourage other works in the field of NLP for
Arabic dialects and social media. A selection of models were built using this
data set and are presented in this paper along with their performances.
",2021-02-07T01:39:52Z,http://arxiv.org/abs/2102.11000v1,"ElMehdi Boujou, Hamza Chataoui, Abdellah El Mekki, Saad Benjelloun, Ikram Chairi, Ismail Berrada"
"Integrating Chemical Language and Molecular Graph in Multimodal Fused
  Deep Learning for Drug Property Prediction","  Accurately predicting molecular properties is a challenging but essential
task in drug discovery. Recently, many mono-modal deep learning methods have
been successfully applied to molecular property prediction. However, the
inherent limitation of mono-modal learning arises from relying solely on one
modality of molecular representation, which restricts a comprehensive
understanding of drug molecules and hampers their resilience against data
noise. To overcome the limitations, we construct multimodal deep learning
models to cover different molecular representations. We convert drug molecules
into three molecular representations, SMILES-encoded vectors, ECFP
fingerprints, and molecular graphs. To process the modal information,
Transformer-Encoder, bi-directional gated recurrent units (BiGRU), and graph
convolutional network (GCN) are utilized for feature learning respectively,
which can enhance the model capability to acquire complementary and naturally
occurring bioinformatics information. We evaluated our triple-modal model on
six molecule datasets. Different from bi-modal learning models, we adopt five
fusion methods to capture the specific features and leverage the contribution
of each modal information better. Compared with mono-modal models, our
multimodal fused deep learning (MMFDL) models outperform single models in
accuracy, reliability, and resistance capability against noise. Moreover, we
demonstrate its generalization ability in the prediction of binding constants
for protein-ligand complex molecules in the refined set of PDBbind. The
advantage of the multimodal model lies in its ability to process diverse
sources of data using proper models and suitable fusion methods, which would
enhance the noise resistance of the model while obtaining data diversity.
",2023-12-29T07:19:42Z,http://arxiv.org/abs/2312.17495v2,"Xiaohua Lu, Liangxu Xie, Lei Xu, Rongzhi Mao, Shan Chang, Xiaojun Xu"
"Survey on Automated Short Answer Grading with Deep Learning: from Word
  Embeddings to Transformers","  Automated short answer grading (ASAG) has gained attention in education as a
means to scale educational tasks to the growing number of students. Recent
progress in Natural Language Processing and Machine Learning has largely
influenced the field of ASAG, of which we survey the recent research
advancements. We complement previous surveys by providing a comprehensive
analysis of recently published methods that deploy deep learning approaches. In
particular, we focus our analysis on the transition from hand engineered
features to representation learning approaches, which learn representative
features for the task at hand automatically from large corpora of data. We
structure our analysis of deep learning methods along three categories: word
embeddings, sequential models, and attention-based methods. Deep learning
impacted ASAG differently than other fields of NLP, as we noticed that the
learned representations alone do not contribute to achieve the best results,
but they rather show to work in a complementary way with hand-engineered
features. The best performance are indeed achieved by methods that combine the
carefully hand-engineered features with the power of the semantic descriptions
provided by the latest models, like transformers architectures. We identify
challenges and provide an outlook on research direction that can be addressed
in the future
",2022-03-11T13:47:08Z,http://arxiv.org/abs/2204.03503v1,"Stefan Haller, Adina Aldea, Christin Seifert, Nicola Strisciuglio"
Machine Translation : From Statistical to modern Deep-learning practices,"  Machine translation (MT) is an area of study in Natural Language processing
which deals with the automatic translation of human language, from one language
to another by the computer. Having a rich research history spanning nearly
three decades, Machine translation is one of the most sought after area of
research in the linguistics and computational community. In this paper, we
investigate the models based on deep learning that have achieved substantial
progress in recent years and becoming the prominent method in MT. We shall
discuss the two main deep-learning based Machine Translation methods, one at
component or domain level which leverages deep learning models to enhance the
efficacy of Statistical Machine Translation (SMT) and end-to-end deep learning
models in MT which uses neural networks to find correspondence between the
source and target languages using the encoder-decoder architecture. We conclude
this paper by providing a time line of the major research problems solved by
the researchers and also provide a comprehensive overview of present areas of
research in Neural Machine Translation.
",2018-12-11T07:04:44Z,http://arxiv.org/abs/1812.04238v1,"Siddhant Srivastava, Anupam Shukla, Ritu Tiwari"
IndicSTR12: A Dataset for Indic Scene Text Recognition,"  The importance of Scene Text Recognition (STR) in today's increasingly
digital world cannot be overstated. Given the significance of STR, data
intensive deep learning approaches that auto-learn feature mappings have
primarily driven the development of STR solutions. Several benchmark datasets
and substantial work on deep learning models are available for Latin languages
to meet this need. On more complex, syntactically and semantically, Indian
languages spoken and read by 1.3 billion people, there is less work and
datasets available. This paper aims to address the Indian space's lack of a
comprehensive dataset by proposing the largest and most comprehensive real
dataset - IndicSTR12 - and benchmarking STR performance on 12 major Indian
languages. A few works have addressed the same issue, but to the best of our
knowledge, they focused on a small number of Indian languages. The size and
complexity of the proposed dataset are comparable to those of existing Latin
contemporaries, while its multilingualism will catalyse the development of
robust text detection and recognition models. It was created specifically for a
group of related languages with different scripts. The dataset contains over
27000 word-images gathered from various natural scenes, with over 1000
word-images for each language. Unlike previous datasets, the images cover a
broader range of realistic conditions, including blur, illumination changes,
occlusion, non-iconic texts, low resolution, perspective text etc. Along with
the new dataset, we provide a high-performing baseline on three models -
PARSeq, CRNN, and STARNet.
",2024-03-12T18:14:48Z,http://arxiv.org/abs/2403.08007v1,"Harsh Lunia, Ajoy Mondal, C V Jawahar"
"Data-driven models and computational tools for neurolinguistics: a
  language technology perspective","  In this paper, our focus is the connection and influence of language
technologies on the research in neurolinguistics. We present a review of brain
imaging-based neurolinguistic studies with a focus on the natural language
representations, such as word embeddings and pre-trained language models.
Mutual enrichment of neurolinguistics and language technologies leads to
development of brain-aware natural language representations. The importance of
this research area is emphasized by medical applications.
",2020-03-23T20:41:51Z,http://arxiv.org/abs/2003.10540v1,"Ekaterina Artemova, Amir Bakarov, Aleksey Artemov, Evgeny Burnaev, Maxim Sharaev"
Deep Learning Model for Finding New Superconductors,"  Exploration of new superconductors still relies on the experience and
intuition of experts and is largely a process of experimental trial and error.
In one study, only 3% of the candidate materials showed superconductivity.
Here, we report the first deep learning model for finding new superconductors.
We introduced the method named ""reading periodic table"" which represented the
periodic table in a way that allows deep learning to learn to read the periodic
table and to learn the law of elements for the purpose of discovering novel
superconductors that are outside the training data. It is recognized that it is
difficult for deep learning to predict something outside the training data.
Although we used only the chemical composition of materials as information, we
obtained an $R^{2}$ value of 0.92 for predicting $T_\text{c}$ for materials in
a database of superconductors. We also introduced the method named ""garbage-in""
to create synthetic data of non-superconductors that do not exist.
Non-superconductors are not reported, but the data must be required for deep
learning to distinguish between superconductors and non-superconductors. We
obtained three remarkable results. The deep learning can predict
superconductivity for a material with a precision of 62%, which shows the
usefulness of the model; it found the recently discovered superconductor CaBi2
and another one Hf0.5Nb0.2V2Zr0.3, neither of which is in the superconductor
database; and it found Fe-based high-temperature superconductors (discovered in
2008) from the training data before 2008. These results open the way for the
discovery of new high-temperature superconductor families. The candidate
materials list, data, and method are openly available from the link
https://github.com/tomo835g/Deep-Learning-to-find-Superconductors.
",2018-12-03T05:30:34Z,http://arxiv.org/abs/1812.01995v4,"Tomohiko Konno, Hodaka Kurokawa, Fuyuki Nabeshima, Yuki Sakishita, Ryo Ogawa, Iwao Hosako, Atsutaka Maeda"
"Synergizing Unsupervised and Supervised Learning: A Hybrid Approach for
  Accurate Natural Language Task Modeling","  While supervised learning models have shown remarkable performance in various
natural language processing (NLP) tasks, their success heavily relies on the
availability of large-scale labeled datasets, which can be costly and
time-consuming to obtain. Conversely, unsupervised learning techniques can
leverage abundant unlabeled text data to learn rich representations, but they
do not directly optimize for specific NLP tasks. This paper presents a novel
hybrid approach that synergizes unsupervised and supervised learning to improve
the accuracy of NLP task modeling. While supervised models excel at specific
tasks, they rely on large labeled datasets. Unsupervised techniques can learn
rich representations from abundant unlabeled text but don't directly optimize
for tasks. Our methodology integrates an unsupervised module that learns
representations from unlabeled corpora (e.g., language models, word embeddings)
and a supervised module that leverages these representations to enhance
task-specific models. We evaluate our approach on text classification and named
entity recognition (NER), demonstrating consistent performance gains over
supervised baselines. For text classification, contextual word embeddings from
a language model pretrain a recurrent or transformer-based classifier. For NER,
word embeddings initialize a BiLSTM sequence labeler. By synergizing
techniques, our hybrid approach achieves SOTA results on benchmark datasets,
paving the way for more data-efficient and robust NLP systems.
",2024-06-03T08:31:35Z,http://arxiv.org/abs/2406.01096v1,"Wrick Talukdar, Anjanava Biswas"
"Talk2Data: A Natural Language Interface for Exploratory Visual Analysis
  via Question Decomposition","  Through a natural language interface (NLI) for exploratory visual analysis,
users can directly ""ask"" analytical questions about the given tabular data.
This process greatly improves user experience and lowers the technical barriers
of data analysis. Existing techniques focus on generating a visualization from
a concrete question. However, complex questions, requiring multiple data
queries and visualizations to answer, are frequently asked in data exploration
and analysis, which cannot be easily solved with the existing techniques. To
address this issue, in this paper, we introduce Talk2Data, a natural language
interface for exploratory visual analysis that supports answering complex
questions. It leverages an advanced deep-learning model to resolve complex
questions into a series of simple questions that could gradually elaborate on
the users' requirements. To present answers, we design a set of annotated and
captioned visualizations to represent the answers in a form that supports
interpretation and narration. We conducted an ablation study and a controlled
user study to evaluate Talk2Data's effectiveness and usefulness.
",2021-07-30T03:41:39Z,http://arxiv.org/abs/2107.14420v3,"Yi Guo, Danqing Shi, Mingjuan Guo, Yanqiu Wu, Qing Chen, Nan Cao"
Deep Learning for Text Style Transfer: A Survey,"  Text style transfer is an important task in natural language generation,
which aims to control certain attributes in the generated text, such as
politeness, emotion, humor, and many others. It has a long history in the field
of natural language processing, and recently has re-gained significant
attention thanks to the promising performance brought by deep neural models. In
this paper, we present a systematic survey of the research on neural text style
transfer, spanning over 100 representative articles since the first neural text
style transfer work in 2017. We discuss the task formulation, existing datasets
and subtasks, evaluation, as well as the rich methodologies in the presence of
parallel and non-parallel data. We also provide discussions on a variety of
important topics regarding the future development of this task. Our curated
paper list is at https://github.com/zhijing-jin/Text_Style_Transfer_Survey
",2020-11-01T04:04:43Z,http://arxiv.org/abs/2011.00416v5,"Di Jin, Zhijing Jin, Zhiting Hu, Olga Vechtomova, Rada Mihalcea"
Natural Language-Guided Programming,"  In today's software world with its cornucopia of reusable software libraries,
when a programmer is faced with a programming task that they suspect can be
completed through the use of a library, they often look for code examples using
a search engine and then manually adapt found examples to their specific
context of use. We put forward a vision based on a new breed of developer tools
that have the potential to largely automate this process. The key idea is to
adapt code autocompletion tools such that they take into account not only the
developer's already-written code but also the intent of the task the developer
is trying to achieve next, formulated in plain natural language. We call this
practice of enriching the code with natural language intent to facilitate its
completion natural language-guided programming.
  To show that this idea is feasible we design, implement and benchmark a tool
that solves this problem in the context of a specific domain (data science) and
a specific programming language (Python). Central to the tool is the use of
language models trained on a large corpus of documented code. Our initial
experiments confirm the feasibility of the idea but also make it clear that we
have only scratched the surface of what may become possible in the future. We
end the paper with a comprehensive research agenda to stimulate additional
research in the budding area of natural language-guided programming.
",2021-08-11T13:06:33Z,http://arxiv.org/abs/2108.05198v2,"Geert Heyman, Rafael Huysegems, Pascal Justen, Tom Van Cutsem"
"A Large Language Model and Denoising Diffusion Framework for Targeted
  Design of Microstructures with Commands in Natural Language","  Microstructure plays a critical role in determining the macroscopic
properties of materials, with applications spanning alloy design, MEMS devices,
and tissue engineering, among many others. Computational frameworks have been
developed to capture the complex relationship between microstructure and
material behavior. However, despite these advancements, the steep learning
curve associated with domain-specific knowledge and complex algorithms
restricts the broader application of these tools. To lower this barrier, we
propose a framework that integrates Natural Language Processing (NLP), Large
Language Models (LLMs), and Denoising Diffusion Probabilistic Models (DDPMs) to
enable microstructure design using intuitive natural language commands. Our
framework employs contextual data augmentation, driven by a pretrained LLM, to
generate and expand a diverse dataset of microstructure descriptors. A
retrained NER model extracts relevant microstructure descriptors from
user-provided natural language inputs, which are then used by the DDPM to
generate microstructures with targeted mechanical properties and topological
features. The NLP and DDPM components of the framework are modular, allowing
for separate training and validation, which ensures flexibility in adapting the
framework to different datasets and use cases. A surrogate model system is
employed to rank and filter generated samples based on their alignment with
target properties. Demonstrated on a database of nonlinear hyperelastic
microstructures, this framework serves as a prototype for accessible inverse
design of microstructures, starting from intuitive natural language commands.
",2024-09-22T14:45:22Z,http://arxiv.org/abs/2409.14473v1,"Nikita Kartashov, Nikolaos N. Vlassis"
Online Knowledge Integration for 3D Semantic Mapping: A Survey,"  Semantic mapping is a key component of robots operating in and interacting
with objects in structured environments. Traditionally, geometric and knowledge
representations within a semantic map have only been loosely integrated.
However, recent advances in deep learning now allow full integration of prior
knowledge, represented as knowledge graphs or language concepts, into sensor
data processing and semantic mapping pipelines. Semantic scene graphs and
language models enable modern semantic mapping approaches to incorporate
graph-based prior knowledge or to leverage the rich information in human
language both during and after the mapping process. This has sparked
substantial advances in semantic mapping, leading to previously impossible
novel applications. This survey reviews these recent developments
comprehensively, with a focus on online integration of knowledge into semantic
mapping. We specifically focus on methods using semantic scene graphs for
integrating symbolic prior knowledge and language models for respective capture
of implicit common-sense knowledge and natural language concepts
",2024-11-27T08:53:16Z,http://arxiv.org/abs/2411.18147v1,"Felix Igelbrink, Marian Renz, Martin Günther, Piper Powell, Lennart Niecksch, Oscar Lima, Martin Atzmueller, Joachim Hertzberg"
A Morphology-aware Network for Morphological Disambiguation,"  Agglutinative languages such as Turkish, Finnish and Hungarian require
morphological disambiguation before further processing due to the complex
morphology of words. A morphological disambiguator is used to select the
correct morphological analysis of a word. Morphological disambiguation is
important because it generally is one of the first steps of natural language
processing and its performance affects subsequent analyses. In this paper, we
propose a system that uses deep learning techniques for morphological
disambiguation. Many of the state-of-the-art results in computer vision, speech
recognition and natural language processing have been obtained through deep
learning models. However, applying deep learning techniques to morphologically
rich languages is not well studied. In this work, while we focus on Turkish
morphological disambiguation we also present results for French and German in
order to show that the proposed architecture achieves high accuracy with no
language-specific feature engineering or additional resource. In the
experiments, we achieve 84.12, 88.35 and 93.78 morphological disambiguation
accuracy among the ambiguous words for Turkish, German and French respectively.
",2017-02-13T07:08:28Z,http://arxiv.org/abs/1702.03654v1,"Eray Yildiz, Caglar Tirkaz, H. Bahadir Sahin, Mustafa Tolga Eren, Ozan Sonmez"
SKOPE: A connectionist/symbolic architecture of spoken Korean processing,"  Spoken language processing requires speech and natural language integration.
Moreover, spoken Korean calls for unique processing methodology due to its
linguistic characteristics. This paper presents SKOPE, a connectionist/symbolic
spoken Korean processing engine, which emphasizes that: 1) connectionist and
symbolic techniques must be selectively applied according to their relative
strength and weakness, and 2) the linguistic characteristics of Korean must be
fully considered for phoneme recognition, speech and language integration, and
morphological/syntactic processing. The design and implementation of SKOPE
demonstrates how connectionist/symbolic hybrid architectures can be constructed
for spoken agglutinative language processing. Also SKOPE presents many novel
ideas for speech and language processing. The phoneme recognition,
morphological analysis, and syntactic analysis experiments show that SKOPE is a
viable approach for the spoken Korean processing.
",1995-04-07T14:39:09Z,http://arxiv.org/abs/cmp-lg/9504008v2,"Geunbae Lee, Jong-Hyeok Lee"
"Accelerating materials discovery for polymer solar cells: Data-driven
  insights enabled by natural language processing","  We present a simulation of various active learning strategies for the
discovery of polymer solar cell donor/acceptor pairs using data extracted from
the literature spanning $\sim$20 years by a natural language processing
pipeline. While data-driven methods have been well established to discover
novel materials faster than Edisonian trial-and-error approaches, their
benefits have not been quantified for material discovery problems that can take
decades. Our approach demonstrates a potential reduction in discovery time by
approximately 75 %, equivalent to a 15 year acceleration in material
innovation. Our pipeline enables us to extract data from greater than 3300
papers which is $\sim$5 times larger and therefore more diverse than similar
data sets reported by others. We also trained machine learning models to
predict the power conversion efficiency and used our model to identify
promising donor-acceptor combinations that are as yet unreported. We thus
demonstrate a pipeline that goes from published literature to extracted
material property data which in turn is used to obtain data-driven insights.
Our insights include active learning strategies that can be used to train
strong predictive models of material properties or be robust to the initial
material system used. This work provides a valuable framework for data-driven
research in materials science.
",2024-02-29T18:54:46Z,http://arxiv.org/abs/2402.19462v2,"Pranav Shetty, Aishat Adeboye, Sonakshi Gupta, Chao Zhang, Rampi Ramprasad"
Homological Convolutional Neural Networks,"  Deep learning methods have demonstrated outstanding performances on
classification and regression tasks on homogeneous data types (e.g., image,
audio, and text data). However, tabular data still pose a challenge, with
classic machine learning approaches being often computationally cheaper and
equally effective than increasingly complex deep learning architectures. The
challenge arises from the fact that, in tabular data, the correlation among
features is weaker than the one from spatial or semantic relationships in
images or natural language, and the dependency structures need to be modeled
without any prior information. In this work, we propose a novel deep learning
architecture that exploits the data structural organization through
topologically constrained network representations to gain relational
information from sparse tabular inputs. The resulting model leverages the power
of convolution and is centered on a limited number of concepts from network
topology to guarantee: (i) a data-centric and deterministic building pipeline;
(ii) a high level of interpretability over the inference process; and (iii) an
adequate room for scalability. We test our model on 18 benchmark datasets
against 5 classic machine learning and 3 deep learning models, demonstrating
that our approach reaches state-of-the-art performances on these challenging
datasets. The code to reproduce all our experiments is provided at
https://github.com/FinancialComputingUCL/HomologicalCNN.
",2023-08-26T08:48:51Z,http://arxiv.org/abs/2308.13816v2,"Antonio Briola, Yuanrong Wang, Silvia Bartolucci, Tomaso Aste"
"Improving Natural-Language-based Audio Retrieval with Transfer Learning
  and Audio & Text Augmentations","  The absence of large labeled datasets remains a significant challenge in many
application areas of deep learning. Researchers and practitioners typically
resort to transfer learning and data augmentation to alleviate this issue. We
study these strategies in the context of audio retrieval with natural language
queries (Task 6b of the DCASE 2022 Challenge). Our proposed system uses
pre-trained embedding models to project recordings and textual descriptions
into a shared audio-caption space in which related examples from different
modalities are close. We employ various data augmentation techniques on audio
and text inputs and systematically tune their corresponding hyperparameters
with sequential model-based optimization. Our results show that the used
augmentations strategies reduce overfitting and improve retrieval performance.
",2022-08-24T11:54:42Z,http://arxiv.org/abs/2208.11460v3,"Paul Primus, Gerhard Widmer"
"Leveraging Sparse and Dense Feature Combinations for Sentiment
  Classification","  Neural networks are one of the most popular approaches for many natural
language processing tasks such as sentiment analysis. They often outperform
traditional machine learning models and achieve the state-of-art results on
most tasks. However, many existing deep learning models are complex, difficult
to train and provide a limited improvement over simpler methods. We propose a
simple, robust and powerful model for sentiment classification. This model
outperforms many deep learning models and achieves comparable results to other
deep learning models with complex architectures on sentiment analysis datasets.
We publish the code online.
",2017-08-13T17:38:17Z,http://arxiv.org/abs/1708.03940v1,"Tao Yu, Christopher Hidey, Owen Rambow, Kathleen McKeown"
Active Learning amidst Logical Knowledge,"  Structured prediction is ubiquitous in applications of machine learning such
as knowledge extraction and natural language processing. Structure often can be
formulated in terms of logical constraints. We consider the question of how to
perform efficient active learning in the presence of logical constraints among
variables inferred by different classifiers. We propose several methods and
provide theoretical results that demonstrate the inappropriateness of employing
uncertainty guided sampling, a commonly used active learning method.
Furthermore, experiments on ten different datasets demonstrate that the methods
significantly outperform alternatives in practice. The results are of practical
significance in situations where labeled data is scarce.
",2017-09-26T06:13:49Z,http://arxiv.org/abs/1709.08850v1,"Emmanouil Antonios Platanios, Ashish Kapoor, Eric Horvitz"
Using Machine Learning Based Models for Personality Recognition,"  Personality can be defined as the combination of behavior, emotion,
motivation, and thoughts that aim at describing various aspects of human
behavior based on a few stable and measurable characteristics. Considering the
fact that our personality has a remarkable influence in our daily life,
automatic recognition of a person's personality attributes can provide many
essential practical applications in various aspects of cognitive science. deep
learning based method for the task of personality recognition from text is
proposed in this paper. Among various deep neural networks, Convolutional
Neural Networks (CNN) have demonstrated profound efficiency in natural language
processing and especially personality detection. Owing to the fact that various
filter sizes in CNN may influence its performance, we decided to combine CNN
with AdaBoost, a classical ensemble algorithm, to consider the possibility of
using the contribution of various filter lengths and gasp their potential in
the final classification via combining various classifiers with respective
filter size using AdaBoost. Our proposed method was validated on the Essay
dataset by conducting a series of experiments and the empirical results
demonstrated the superiority of our proposed method compared to both machine
learning and deep learning methods for the task of personality recognition.
",2022-01-17T07:20:51Z,http://arxiv.org/abs/2201.06248v1,"Fatemeh Mohades Deilami, Hossein Sadr, Mojdeh Nazari"
"Machine Learning: Algorithms, Models, and Applications","  Recent times are witnessing rapid development in machine learning algorithm
systems, especially in reinforcement learning, natural language processing,
computer and robot vision, image processing, speech, and emotional processing
and understanding. In tune with the increasing importance and relevance of
machine learning models, algorithms, and their applications, and with the
emergence of more innovative uses cases of deep learning and artificial
intelligence, the current volume presents a few innovative research works and
their applications in real world, such as stock trading, medical and healthcare
systems, and software automation. The chapters in the book illustrate how
machine learning and deep learning algorithms and models are designed,
optimized, and deployed. The volume will be useful for advanced graduate and
doctoral students, researchers, faculty members of universities, practicing
data scientists and data engineers, professionals, and consultants working on
the broad areas of machine learning, deep learning, and artificial
intelligence.
",2022-01-06T07:14:02Z,http://arxiv.org/abs/2201.01943v1,"Jaydip Sen, Sidra Mehtab, Rajdeep Sen, Abhishek Dutta, Pooja Kherwa, Saheel Ahmed, Pranay Berry, Sahil Khurana, Sonali Singh, David W. W Cadotte, David W. Anderson, Kalum J. Ost, Racheal S. Akinbo, Oladunni A. Daramola, Bongs Lainjo"
"Research on Optimization of Natural Language Processing Model Based on
  Multimodal Deep Learning","  This project intends to study the image representation based on attention
mechanism and multimodal data. By adding multiple pattern layers to the
attribute model, the semantic and hidden layers of image content are
integrated. The word vector is quantified by the Word2Vec method and then
evaluated by a word embedding convolutional neural network. The published
experimental results of the two groups were tested. The experimental results
show that this method can convert discrete features into continuous characters,
thus reducing the complexity of feature preprocessing. Word2Vec and natural
language processing technology are integrated to achieve the goal of direct
evaluation of missing image features. The robustness of the image feature
evaluation model is improved by using the excellent feature analysis
characteristics of a convolutional neural network. This project intends to
improve the existing image feature identification methods and eliminate the
subjective influence in the evaluation process. The findings from the
simulation indicate that the novel approach has developed is viable,
effectively augmenting the features within the produced representations.
",2024-06-13T06:03:59Z,http://arxiv.org/abs/2406.08838v1,"Dan Sun, Yaxin Liang, Yining Yang, Yuhan Ma, Qishi Zhan, Erdi Gao"
"FineText: Text Classification via Attention-based Language Model
  Fine-tuning","  Training deep neural networks from scratch on natural language processing
(NLP) tasks requires significant amount of manually labeled text corpus and
substantial time to converge, which usually cannot be satisfied by the
customers. In this paper, we aim to develop an effective transfer learning
algorithm by fine-tuning a pre-trained language model. The goal is to provide
expressive and convenient-to-use feature extractors for downstream NLP tasks,
and achieve improvement in terms of accuracy, data efficiency, and
generalization to new domains. Therefore, we propose an attention-based
fine-tuning algorithm that automatically selects relevant contextualized
features from the pre-trained language model and uses those features on
downstream text classification tasks. We test our methods on six widely-used
benchmarking datasets, and achieve new state-of-the-art performance on all of
them. Moreover, we then introduce an alternative multi-task learning approach,
which is an end-to-end algorithm given the pre-trained model. By doing
multi-task learning, one can largely reduce the total training time by trading
off some classification accuracy.
",2019-10-25T23:13:15Z,http://arxiv.org/abs/1910.11959v1,"Yunzhe Tao, Saurabh Gupta, Satyapriya Krishna, Xiong Zhou, Orchid Majumder, Vineet Khare"
"On the Use of BERT for Automated Essay Scoring: Joint Learning of
  Multi-Scale Essay Representation","  In recent years, pre-trained models have become dominant in most natural
language processing (NLP) tasks. However, in the area of Automated Essay
Scoring (AES), pre-trained models such as BERT have not been properly used to
outperform other deep learning models such as LSTM. In this paper, we introduce
a novel multi-scale essay representation for BERT that can be jointly learned.
We also employ multiple losses and transfer learning from out-of-domain essays
to further improve the performance. Experiment results show that our approach
derives much benefit from joint learning of multi-scale essay representation
and obtains almost the state-of-the-art result among all deep learning models
in the ASAP task. Our multi-scale essay representation also generalizes well to
CommonLit Readability Prize data set, which suggests that the novel text
representation proposed in this paper may be a new and effective choice for
long-text tasks.
",2022-05-08T10:36:54Z,http://arxiv.org/abs/2205.03835v2,"Yongjie Wang, Chuan Wang, Ruobing Li, Hui Lin"
Evolutionary Algorithm for Sinhala to English Translation,"  Machine Translation (MT) is an area in natural language processing, which
focus on translating from one language to another. Many approaches ranging from
statistical methods to deep learning approaches are used in order to achieve
MT. However, these methods either require a large number of data or a clear
understanding about the language. Sinhala language has less digital text which
could be used to train a deep neural network. Furthermore, Sinhala has complex
rules therefore, it is harder to create statistical rules in order to apply
statistical methods in MT. This research focuses on Sinhala to English
translation using an Evolutionary Algorithm (EA). EA is used to identifying the
correct meaning of Sinhala text and to translate it to English. The Sinhala
text is passed to identify the meaning in order to get the correct meaning of
the sentence. With the use of the EA the translation is carried out. The
translated text is passed on to grammatically correct the sentence. This has
shown to achieve accurate results.
",2019-07-06T22:51:28Z,http://arxiv.org/abs/1907.03202v1,"J. K. Joseph, W. M. T. Chathurika, A. Nugaliyadde, Y. Mallawarachchi"
"Semantics of the Black-Box: Can knowledge graphs help make deep learning
  systems more interpretable and explainable?","  The recent series of innovations in deep learning (DL) have shown enormous
potential to impact individuals and society, both positively and negatively.
The DL models utilizing massive computing power and enormous datasets have
significantly outperformed prior historical benchmarks on increasingly
difficult, well-defined research tasks across technology domains such as
computer vision, natural language processing, signal processing, and
human-computer interactions. However, the Black-Box nature of DL models and
their over-reliance on massive amounts of data condensed into labels and dense
representations poses challenges for interpretability and explainability of the
system. Furthermore, DLs have not yet been proven in their ability to
effectively utilize relevant domain knowledge and experience critical to human
understanding. This aspect is missing in early data-focused approaches and
necessitated knowledge-infused learning and other strategies to incorporate
computational knowledge. This article demonstrates how knowledge, provided as a
knowledge graph, is incorporated into DL methods using knowledge-infused
learning, which is one of the strategies. We then discuss how this makes a
fundamental difference in the interpretability and explainability of current
approaches, and illustrate it with examples from natural language processing
for healthcare and education applications.
",2020-10-16T22:55:23Z,http://arxiv.org/abs/2010.08660v4,"Manas Gaur, Keyur Faldu, Amit Sheth"
Curriculum Learning in Deep Neural Networks for Financial Forecasting,"  For any financial organization, computing accurate quarterly forecasts for
various products is one of the most critical operations. As the granularity at
which forecasts are needed increases, traditional statistical time series
models may not scale well. We apply deep neural networks in the forecasting
domain by experimenting with techniques from Natural Language Processing
(Encoder-Decoder LSTMs) and Computer Vision (Dilated CNNs), as well as
incorporating transfer learning. A novel contribution of this paper is the
application of curriculum learning to neural network models built for time
series forecasting. We illustrate the performance of our models using
Microsoft's revenue data corresponding to Enterprise, and Small, Medium &
Corporate products, spanning approximately 60 regions across the globe for 8
different business segments, and totaling in the order of tens of billions of
USD. We compare our models' performance to the ensemble model of traditional
statistics and machine learning techniques currently used by Microsoft Finance.
With this in-production model as a baseline, our experiments yield an
approximately 30% improvement in overall accuracy on test data. We find that
our curriculum learning LSTM-based model performs best, showing that it is
reasonable to implement our proposed methods without overfitting on
medium-sized data.
",2019-04-29T18:09:31Z,http://arxiv.org/abs/1904.12887v2,"Allison Koenecke, Amita Gajewar"
"Adapt or Get Left Behind: Domain Adaptation through BERT Language Model
  Finetuning for Aspect-Target Sentiment Classification","  Aspect-Target Sentiment Classification (ATSC) is a subtask of Aspect-Based
Sentiment Analysis (ABSA), which has many applications e.g. in e-commerce,
where data and insights from reviews can be leveraged to create value for
businesses and customers. Recently, deep transfer-learning methods have been
applied successfully to a myriad of Natural Language Processing (NLP) tasks,
including ATSC. Building on top of the prominent BERT language model, we
approach ATSC using a two-step procedure: self-supervised domain-specific BERT
language model finetuning, followed by supervised task-specific finetuning. Our
findings on how to best exploit domain-specific language model finetuning
enable us to produce new state-of-the-art performance on the SemEval 2014 Task
4 restaurants dataset. In addition, to explore the real-world robustness of our
models, we perform cross-domain evaluation. We show that a cross-domain adapted
BERT language model performs significantly better than strong baseline models
like vanilla BERT-base and XLNet-base. Finally, we conduct a case study to
interpret model prediction errors.
",2019-08-30T17:44:30Z,http://arxiv.org/abs/1908.11860v2,"Alexander Rietzler, Sebastian Stabinger, Paul Opitz, Stefan Engl"
"Natural Language Processing Methods for Symbolic Music Generation and
  Information Retrieval: a Survey","  Several adaptations of Transformers models have been developed in various
domains since its breakthrough in Natural Language Processing (NLP). This trend
has spread into the field of Music Information Retrieval (MIR), including
studies processing music data. However, the practice of leveraging NLP tools
for symbolic music data is not novel in MIR. Music has been frequently compared
to language, as they share several similarities, including sequential
representations of text and music. These analogies are also reflected through
similar tasks in MIR and NLP. This survey reviews NLP methods applied to
symbolic music generation and information retrieval studies following two axes.
We first propose an overview of representations of symbolic music adapted from
natural language sequential representations. Such representations are designed
by considering the specificities of symbolic music. These representations are
then processed by models. Such models, possibly originally developed for text
and adapted for symbolic music, are trained on various tasks. We describe these
models, in particular deep learning models, through different prisms,
highlighting music-specialized mechanisms. We finally present a discussion
surrounding the effective use of NLP tools for symbolic music data. This
includes technical issues regarding NLP methods and fundamental differences
between text and music, which may open several doors for further research into
more effectively adapting NLP tools to symbolic MIR.
",2024-02-27T12:48:01Z,http://arxiv.org/abs/2402.17467v1,"Dinh-Viet-Toan Le, Louis Bigo, Mikaela Keller, Dorien Herremans"
"Deep Unsupervised Domain Adaptation for Time Series Classification: a
  Benchmark","  Unsupervised Domain Adaptation (UDA) aims to harness labeled source data to
train models for unlabeled target data. Despite extensive research in domains
like computer vision and natural language processing, UDA remains underexplored
for time series data, which has widespread real-world applications ranging from
medicine and manufacturing to earth observation and human activity recognition.
Our paper addresses this gap by introducing a comprehensive benchmark for
evaluating UDA techniques for time series classification, with a focus on deep
learning methods. We provide seven new benchmark datasets covering various
domain shifts and temporal dynamics, facilitating fair and standardized UDA
method assessments with state of the art neural network backbones (e.g.
Inception) for time series data. This benchmark offers insights into the
strengths and limitations of the evaluated approaches while preserving the
unsupervised nature of domain adaptation, making it directly applicable to
practical problems. Our paper serves as a vital resource for researchers and
practitioners, advancing domain adaptation solutions for time series data and
fostering innovation in this critical field. The implementation code of this
benchmark is available at https://github.com/EricssonResearch/UDA-4-TSC.
",2023-12-15T15:03:55Z,http://arxiv.org/abs/2312.09857v2,"Hassan Ismail Fawaz, Ganesh Del Grosso, Tanguy Kerdoncuff, Aurelie Boisbunon, Illyyne Saffar"
Does Synthetic Data Make Large Language Models More Efficient?,"  Natural Language Processing (NLP) has undergone transformative changes with
the advent of deep learning methodologies. One challenge persistently
confronting researchers is the scarcity of high-quality, annotated datasets
that drive these models. This paper explores the nuances of synthetic data
generation in NLP, with a focal point on template-based question generation. By
assessing its advantages, including data augmentation potential and the
introduction of structured variety, we juxtapose these benefits against
inherent limitations, such as the risk of overfitting and the constraints posed
by pre-defined templates. Drawing from empirical evaluations, we demonstrate
the impact of template-based synthetic data on the performance of modern
transformer models. We conclude by emphasizing the delicate balance required
between synthetic and real-world data, and the future trajectories of
integrating synthetic data in model training pipelines. The findings aim to
guide NLP practitioners in harnessing synthetic data's potential, ensuring
optimal model performance in diverse applications.
",2023-10-11T19:16:09Z,http://arxiv.org/abs/2310.07830v1,"Sia Gholami, Marwan Omar"
End-to-End Speech Translation of Arabic to English Broadcast News,"  Speech translation (ST) is the task of directly translating acoustic speech
signals in a source language into text in a foreign language. ST task has been
addressed, for a long time, using a pipeline approach with two modules : first
an Automatic Speech Recognition (ASR) in the source language followed by a
text-to-text Machine translation (MT). In the past few years, we have seen a
paradigm shift towards the end-to-end approaches using sequence-to-sequence
deep neural network models. This paper presents our efforts towards the
development of the first Broadcast News end-to-end Arabic to English speech
translation system. Starting from independent ASR and MT LDC releases, we were
able to identify about 92 hours of Arabic audio recordings for which the manual
transcription was also translated into English at the segment level. These data
was used to train and compare pipeline and end-to-end speech translation
systems under multiple scenarios including transfer learning and data
augmentation techniques.
",2022-12-11T11:35:46Z,http://arxiv.org/abs/2212.05479v1,"Fethi Bougares, Salim Jouili"
"Memristors -- from In-memory computing, Deep Learning Acceleration,
  Spiking Neural Networks, to the Future of Neuromorphic and Bio-inspired
  Computing","  Machine learning, particularly in the form of deep learning, has driven most
of the recent fundamental developments in artificial intelligence. Deep
learning is based on computational models that are, to a certain extent,
bio-inspired, as they rely on networks of connected simple computing units
operating in parallel. Deep learning has been successfully applied in areas
such as object/pattern recognition, speech and natural language processing,
self-driving vehicles, intelligent self-diagnostics tools, autonomous robots,
knowledgeable personal assistants, and monitoring. These successes have been
mostly supported by three factors: availability of vast amounts of data,
continuous growth in computing power, and algorithmic innovations. The
approaching demise of Moore's law, and the consequent expected modest
improvements in computing power that can be achieved by scaling, raise the
question of whether the described progress will be slowed or halted due to
hardware limitations. This paper reviews the case for a novel beyond CMOS
hardware technology, memristors, as a potential solution for the implementation
of power-efficient in-memory computing, deep learning accelerators, and spiking
neural networks. Central themes are the reliance on non-von-Neumann computing
architectures and the need for developing tailored learning and inference
algorithms. To argue that lessons from biology can be useful in providing
directions for further progress in artificial intelligence, we briefly discuss
an example based reservoir computing. We conclude the review by speculating on
the big picture view of future neuromorphic and brain-inspired computing
systems.
",2020-04-30T16:49:03Z,http://arxiv.org/abs/2004.14942v1,"Adnan Mehonic, Abu Sebastian, Bipin Rajendran, Osvaldo Simeone, Eleni Vasilaki, Anthony J. Kenyon"
"Deep Transfer Learning & Beyond: Transformer Language Models in
  Information Systems Research","  AI is widely thought to be poised to transform business, yet current
perceptions of the scope of this transformation may be myopic. Recent progress
in natural language processing involving transformer language models (TLMs)
offers a potential avenue for AI-driven business and societal transformation
that is beyond the scope of what most currently foresee. We review this recent
progress as well as recent literature utilizing text mining in top IS journals
to develop an outline for how future IS research can benefit from these new
techniques. Our review of existing IS literature reveals that suboptimal text
mining techniques are prevalent and that the more advanced TLMs could be
applied to enhance and increase IS research involving text data, and to enable
new IS research topics, thus creating more value for the research community.
This is possible because these techniques make it easier to develop very
powerful custom systems and their performance is superior to existing methods
for a wide range of tasks and applications. Further, multilingual language
models make possible higher quality text analytics for research in multiple
languages. We also identify new avenues for IS research, like language user
interfaces, that may offer even greater potential for future IS research.
",2021-10-18T02:01:39Z,http://arxiv.org/abs/2110.08975v2,"Ross Gruetzemacher, David Paradice"
"Neural data-to-text generation: A comparison between pipeline and
  end-to-end architectures","  Traditionally, most data-to-text applications have been designed using a
modular pipeline architecture, in which non-linguistic input data is converted
into natural language through several intermediate transformations. In
contrast, recent neural models for data-to-text generation have been proposed
as end-to-end approaches, where the non-linguistic input is rendered in natural
language with much less explicit intermediate representations in-between. This
study introduces a systematic comparison between neural pipeline and end-to-end
data-to-text approaches for the generation of text from RDF triples. Both
architectures were implemented making use of state-of-the art deep learning
methods as the encoder-decoder Gated-Recurrent Units (GRU) and Transformer.
Automatic and human evaluations together with a qualitative analysis suggest
that having explicit intermediate steps in the generation process results in
better texts than the ones generated by end-to-end approaches. Moreover, the
pipeline models generalize better to unseen inputs. Data and code are publicly
available.
",2019-08-23T20:10:36Z,http://arxiv.org/abs/1908.09022v2,"Thiago Castro Ferreira, Chris van der Lee, Emiel van Miltenburg, Emiel Krahmer"
A novel hybrid methodology of measuring sentence similarity,"  The problem of measuring sentence similarity is an essential issue in the
natural language processing (NLP) area. It is necessary to measure the
similarity between sentences accurately. There are many approaches to measuring
sentence similarity. Deep learning methodology shows a state-of-the-art
performance in many natural language processing fields and is used a lot in
sentence similarity measurement methods. However, in the natural language
processing field, considering the structure of the sentence or the word
structure that makes up the sentence is also important. In this study, we
propose a methodology combined with both deep learning methodology and a method
considering lexical relationships. Our evaluation metric is the Pearson
correlation coefficient and Spearman correlation coefficient. As a result, the
proposed method outperforms the current approaches on a KorSTS standard
benchmark Korean dataset. Moreover, it performs a maximum of 65% increase than
only using deep learning methodology. Experiments show that our proposed method
generally results in better performance than those with only a deep learning
model.
",2021-05-03T06:50:54Z,http://arxiv.org/abs/2105.00648v5,"Yongmin Yoo, Tak-Sung Heo, Yeongjoon Park, Kyungsun Kim"
"A Comparative Study of the Application of Different Learning Techniques
  to Natural Language Interfaces","  In this paper we present first results from a comparative study. Its aim is
to test the feasibility of different inductive learning techniques to perform
the automatic acquisition of linguistic knowledge within a natural language
database interface. In our interface architecture the machine learning module
replaces an elaborate semantic analysis component. The learning module learns
the correct mapping of a user's input to the corresponding database command
based on a collection of past input data. We use an existing interface to a
production planning and control system as evaluation and compare the results
achieved by different instance-based and model-based learning algorithms.
",1997-05-16T03:50:14Z,http://arxiv.org/abs/cmp-lg/9705012v1,"Werner Winiwarter, Yahiko Kambayashi"
Natural Language Processing for Policymaking,"  Language is the medium for many political activities, from campaigns to news
reports. Natural language processing (NLP) uses computational tools to parse
text into key information that is needed for policymaking. In this chapter, we
introduce common methods of NLP, including text classification, topic modeling,
event extraction, and text scaling. We then overview how these methods can be
used for policymaking through four major applications including data collection
for evidence-based policymaking, interpretation of political decisions, policy
communication, and investigation of policy effects. Finally, we highlight some
potential limitations and ethical concerns when using NLP for policymaking.
  This text is from Chapter 7 (pages 141-162) of the Handbook of Computational
Social Science for Policy (2023). Open Access on Springer:
https://doi.org/10.1007/978-3-031-16624-2
",2023-02-07T14:34:39Z,http://arxiv.org/abs/2302.03490v1,"Zhijing Jin, Rada Mihalcea"
Key Information Extraction From Documents: Evaluation And Generator,"  Extracting information from documents usually relies on natural language
processing methods working on one-dimensional sequences of text. In some cases,
for example, for the extraction of key information from semi-structured
documents, such as invoice-documents, spatial and formatting information of
text are crucial to understand the contextual meaning. Convolutional neural
networks are already common in computer vision models to process and extract
relationships in multidimensional data. Therefore, natural language processing
models have already been combined with computer vision models in the past, to
benefit from e.g. positional information and to improve performance of these
key information extraction models. Existing models were either trained on
unpublished data sets or on an annotated collection of receipts, which did not
focus on PDF-like documents. Hence, in this research project a template-based
document generator was created to compare state-of-the-art models for
information extraction. An existing information extraction model ""Chargrid""
(Katti et al., 2019) was reconstructed and the impact of a bounding box
regression decoder, as well as the impact of an NLP pre-processing step was
evaluated for information extraction from documents. The results have shown
that NLP based pre-processing is beneficial for model performance. However, the
use of a bounding box regression decoder increases the model performance only
for fields that do not follow a rectangular shape.
",2021-06-09T16:12:21Z,http://arxiv.org/abs/2106.14624v1,"Oliver Bensch, Mirela Popa, Constantin Spille"
"To Compress, or Not to Compress: Characterizing Deep Learning Model
  Compression for Embedded Inference","  The recent advances in deep neural networks (DNNs) make them attractive for
embedded systems. However, it can take a long time for DNNs to make an
inference on resource-constrained computing devices. Model compression
techniques can address the computation issue of deep inference on embedded
devices. This technique is highly attractive, as it does not rely on
specialized hardware, or computation-offloading that is often infeasible due to
privacy concerns or high latency. However, it remains unclear how model
compression techniques perform across a wide range of DNNs. To design efficient
embedded deep learning solutions, we need to understand their behaviors. This
work develops a quantitative approach to characterize model compression
techniques on a representative embedded deep learning architecture, the NVIDIA
Jetson Tx2. We perform extensive experiments by considering 11 influential
neural network architectures from the image classification and the natural
language processing domains. We experimentally show that how two mainstream
compression techniques, data quantization and pruning, perform on these network
architectures and the implications of compression techniques to the model
storage size, inference time, energy consumption and performance metrics. We
demonstrate that there are opportunities to achieve fast deep inference on
embedded systems, but one must carefully choose the compression settings. Our
results provide insights on when and how to apply model compression techniques
and guidelines for designing efficient embedded deep learning systems.
",2018-10-21T05:09:45Z,http://arxiv.org/abs/1810.08899v1,"Qing Qin, Jie Ren, Jialong Yu, Ling Gao, Hai Wang, Jie Zheng, Yansong Feng, Jianbin Fang, Zheng Wang"
A Survey on Silicon Photonics for Deep Learning,"  Deep learning has led to unprecedented successes in solving some very
difficult problems in domains such as computer vision, natural language
processing, and general pattern recognition. These achievements are the
culmination of decades-long research into better training techniques and deeper
neural network models, as well as improvements in hardware platforms that are
used to train and execute the deep neural network models. Many
application-specific integrated circuit (ASIC) hardware accelerators for deep
learning have garnered interest in recent years due to their improved
performance and energy-efficiency over conventional CPU and GPU architectures.
However, these accelerators are constrained by fundamental bottlenecks due to
1) the slowdown in CMOS scaling, which has limited computational and
performance-per-watt capabilities of emerging electronic processors, and 2) the
use of metallic interconnects for data movement, which do not scale well and
are a major cause of bandwidth, latency, and energy inefficiencies in almost
every contemporary processor. Silicon photonics has emerged as a promising
CMOS-compatible alternative to realize a new generation of deep learning
accelerators that can use light for both communication and computation. This
article surveys the landscape of silicon photonics to accelerate deep learning,
with a coverage of developments across design abstractions in a bottom-up
manner, to convey both the capabilities and limitations of the silicon
photonics paradigm in the context of deep learning acceleration.
",2021-01-05T19:35:29Z,http://arxiv.org/abs/2101.01751v2,"Febin P Sunny, Ebadollah Taheri, Mahdi Nikdast, Sudeep Pasricha"
"Semi-automatic Generation of Multilingual Datasets for Stance Detection
  in Twitter","  Popular social media networks provide the perfect environment to study the
opinions and attitudes expressed by users. While interactions in social media
such as Twitter occur in many natural languages, research on stance detection
(the position or attitude expressed with respect to a specific topic) within
the Natural Language Processing field has largely been done for English.
Although some efforts have recently been made to develop annotated data in
other languages, there is a telling lack of resources to facilitate
multilingual and crosslingual research on stance detection. This is partially
due to the fact that manually annotating a corpus of social media texts is a
difficult, slow and costly process. Furthermore, as stance is a highly domain-
and topic-specific phenomenon, the need for annotated data is specially
demanding. As a result, most of the manually labeled resources are hindered by
their relatively small size and skewed class distribution. This paper presents
a method to obtain multilingual datasets for stance detection in Twitter.
Instead of manually annotating on a per tweet basis, we leverage user-based
information to semi-automatically label large amounts of tweets. Empirical
monolingual and cross-lingual experimentation and qualitative analysis show
that our method helps to overcome the aforementioned difficulties to build
large, balanced and multilingual labeled corpora. We believe that our method
can be easily adapted to easily generate labeled social media data for other
Natural Language Processing tasks and domains.
",2021-01-28T13:05:09Z,http://arxiv.org/abs/2101.11978v1,"Elena Zotova, Rodrigo Agerri, German Rigau"
"NeCo@ALQAC 2023: Legal Domain Knowledge Acquisition for Low-Resource
  Languages through Data Enrichment","  In recent years, natural language processing has gained significant
popularity in various sectors, including the legal domain. This paper presents
NeCo Team's solutions to the Vietnamese text processing tasks provided in the
Automated Legal Question Answering Competition 2023 (ALQAC 2023), focusing on
legal domain knowledge acquisition for low-resource languages through data
enrichment. Our methods for the legal document retrieval task employ a
combination of similarity ranking and deep learning models, while for the
second task, which requires extracting an answer from a relevant legal article
in response to a question, we propose a range of adaptive techniques to handle
different question types. Our approaches achieve outstanding results on both
tasks of the competition, demonstrating the potential benefits and
effectiveness of question answering systems in the legal field, particularly
for low-resource languages.
",2023-09-11T14:43:45Z,http://arxiv.org/abs/2309.05500v1,"Hai-Long Nguyen, Dieu-Quynh Nguyen, Hoang-Trung Nguyen, Thu-Trang Pham, Huu-Dong Nguyen, Thach-Anh Nguyen, Thi-Hai-Yen Vuong, Ha-Thanh Nguyen"
"From Black Boxes to Conversations: Incorporating XAI in a Conversational
  Agent","  The goal of Explainable AI (XAI) is to design methods to provide insights
into the reasoning process of black-box models, such as deep neural networks,
in order to explain them to humans. Social science research states that such
explanations should be conversational, similar to human-to-human explanations.
In this work, we show how to incorporate XAI in a conversational agent, using a
standard design for the agent comprising natural language understanding and
generation components. We build upon an XAI question bank, which we extend by
quality-controlled paraphrases, to understand the user's information needs. We
further systematically survey the literature for suitable explanation methods
that provide the information to answer those questions, and present a
comprehensive list of suggestions. Our work is the first step towards truly
natural conversations about machine learning models with an explanation agent.
The comprehensive list of XAI questions and the corresponding explanation
methods may support other researchers in providing the necessary information to
address users' demands. To facilitate future work, we release our source code
and data.
",2022-09-06T15:01:06Z,http://arxiv.org/abs/2209.02552v3,"Van Bach Nguyen, Jörg Schlötterer, Christin Seifert"
"Visual Question Answering using Deep Learning: A Survey and Performance
  Analysis","  The Visual Question Answering (VQA) task combines challenges for processing
data with both Visual and Linguistic processing, to answer basic `common sense'
questions about given images. Given an image and a question in natural
language, the VQA system tries to find the correct answer to it using visual
elements of the image and inference gathered from textual questions. In this
survey, we cover and discuss the recent datasets released in the VQA domain
dealing with various types of question-formats and robustness of the
machine-learning models. Next, we discuss about new deep learning models that
have shown promising results over the VQA datasets. At the end, we present and
discuss some of the results computed by us over the vanilla VQA model, Stacked
Attention Network and the VQA Challenge 2017 winner model. We also provide the
detailed analysis along with the challenges and future research directions.
",2019-08-27T07:03:03Z,http://arxiv.org/abs/1909.01860v2,"Yash Srivastava, Vaishnav Murali, Shiv Ram Dubey, Snehasis Mukherjee"
"Text Mining for Processing Interview Data in Computational Social
  Science","  We use commercially available text analysis technology to process interview
text data from a computational social science study. We find that topical
clustering and terminological enrichment provide for convenient exploration and
quantification of the responses. This makes it possible to generate and test
hypotheses and to compare textual and non-textual variables, and saves analyst
effort. We encourage studies in social science to use text analysis, especially
for exploratory open-ended studies. We discuss how replicability requirements
are met by text analysis technology. We note that the most recent learning
models are not designed with transparency in mind, and that research requires a
model to be editable and its decisions to be explainable. The tools available
today, such as the one used in the present study, are not built for processing
interview texts. While many of the variables under consideration are
quantifiable using lexical statistics, we find that some interesting and
potentially valuable features are difficult or impossible to automatise
reliably at present. We note that there are some potentially interesting
applications for traditional natural language processing mechanisms such as
named entity recognition and anaphora resolution in this application area. We
conclude with a suggestion for language technologists to investigate the
challenge of processing interview data comprehensively, especially the
interplay between question and response, and we encourage social science
researchers not to hesitate to use text analysis tools, especially for the
exploratory phase of processing interview data.?
",2020-11-28T00:44:35Z,http://arxiv.org/abs/2011.14037v1,"Jussi Karlgren, Renee Li, Eva M Meyersson Milgrom"
File mapping Rule-based DBMS and Natural Language Processing,"  This paper describes the system of storage, extract and processing of
information structured similarly to the natural language. For recursive
inference the system uses the rules having the same representation, as the
data. The environment of storage of information is provided with the File
Mapping (SHM) mechanism of operating system. In the paper the main principles
of construction of dynamic data structure and language for record of the
inference rules are stated; the features of available implementation are
considered and the description of the application realizing semantic
information retrieval on the natural language is given.
",2001-06-10T14:56:51Z,http://arxiv.org/abs/cs/0106016v1,Vjacheslav M. Novikov
"Semantic maps and metrics for science Semantic maps and metrics for
  science using deep transformer encoders","  The growing deluge of scientific publications demands text analysis tools
that can help scientists and policy-makers navigate, forecast and beneficially
guide scientific research. Recent advances in natural language understanding
driven by deep transformer networks offer new possibilities for mapping
science. Because the same surface text can take on multiple and sometimes
contradictory specialized senses across distinct research communities,
sensitivity to context is critical for infometric applications. Transformer
embedding models such as BERT capture shades of association and connotation
that vary across the different linguistic contexts of any particular word or
span of text. Here we report a procedure for encoding scientific documents with
these tools, measuring their improvement over static word embeddings in a
nearest-neighbor retrieval task. We find discriminability of contextual
representations is strongly influenced by choice of pooling strategy for
summarizing the high-dimensional network activations. Importantly, we note that
fundamentals such as domain-matched training data are more important than
state-of-the-art NLP tools. Yet state-of-the-art models did offer significant
gains. The best approach we investigated combined domain-matched pretraining,
sound pooling, and state-of-the-art deep transformer network encoders. Finally,
with the goal of leveraging contextual representations from deep encoders, we
present a range of measurements for understanding and forecasting research
communities in science.
",2021-04-13T04:12:20Z,http://arxiv.org/abs/2104.05928v1,"Brendan Chambers, James Evans"
"Listen, Interact and Talk: Learning to Speak via Interaction","  One of the long-term goals of artificial intelligence is to build an agent
that can communicate intelligently with human in natural language. Most
existing work on natural language learning relies heavily on training over a
pre-collected dataset with annotated labels, leading to an agent that
essentially captures the statistics of the fixed external training data. As the
training data is essentially a static snapshot representation of the knowledge
from the annotator, the agent trained this way is limited in adaptiveness and
generalization of its behavior. Moreover, this is very different from the
language learning process of humans, where language is acquired during
communication by taking speaking action and learning from the consequences of
speaking action in an interactive manner. This paper presents an interactive
setting for grounded natural language learning, where an agent learns natural
language by interacting with a teacher and learning from feedback, thus
learning and improving language skills while taking part in the conversation.
To achieve this goal, we propose a model which incorporates both imitation and
reinforcement by leveraging jointly sentence and reward feedbacks from the
teacher. Experiments are conducted to validate the effectiveness of the
proposed approach.
",2017-05-28T07:48:14Z,http://arxiv.org/abs/1705.09906v1,"Haichao Zhang, Haonan Yu, Wei Xu"
"On the Linguistic Representational Power of Neural Machine Translation
  Models","  Despite the recent success of deep neural networks in natural language
processing (NLP), their interpretability remains a challenge. We analyze the
representations learned by neural machine translation models at various levels
of granularity and evaluate their quality through relevant extrinsic
properties. In particular, we seek answers to the following questions: (i) How
accurately is word-structure captured within the learned representations, an
important aspect in translating morphologically-rich languages? (ii) Do the
representations capture long-range dependencies, and effectively handle
syntactically divergent languages? (iii) Do the representations capture lexical
semantics? We conduct a thorough investigation along several parameters: (i)
Which layers in the architecture capture each of these linguistic phenomena;
(ii) How does the choice of translation unit (word, character, or subword unit)
impact the linguistic properties captured by the underlying representations?
(iii) Do the encoder and decoder learn differently and independently? (iv) Do
the representations learned by multilingual NMT models capture the same amount
of linguistic information as their bilingual counterparts? Our data-driven,
quantitative evaluation illuminates important aspects in NMT models and their
ability to capture various linguistic phenomena. We show that deep NMT models
learn a non-trivial amount of linguistic information. Notable findings include:
i) Word morphology and part-of-speech information are captured at the lower
layers of the model; (ii) In contrast, lexical semantics or non-local syntactic
and semantic dependencies are better represented at the higher layers; (iii)
Representations learned using characters are more informed about wordmorphology
compared to those learned using subword units; and (iv) Representations learned
by multilingual models are richer compared to bilingual models.
",2019-11-01T12:13:45Z,http://arxiv.org/abs/1911.00317v1,"Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, James Glass"
"Towards Enhancing Database Education: Natural Language Generation Meets
  Query Execution Plans","  The database systems course is offered as part of an undergraduate computer
science degree program in many major universities. A key learning goal of
learners taking such a course is to understand how SQL queries are processed in
a RDBMS in practice. Since a query execution plan (QEP) describes the execution
steps of a query, learners can acquire the understanding by perusing the QEPs
generated by a RDBMS. Unfortunately, in practice, it is often daunting for a
learner to comprehend these QEPs containing vendor-specific implementation
details, hindering her learning process. In this paper, we present a novel,
end-to-end, generic system called lantern that generates a natural language
description of a qep to facilitate understanding of the query execution steps.
It takes as input an SQL query and its QEP, and generates a natural language
description of the execution strategy deployed by the underlying RDBMS.
Specifically, it deploys a declarative framework called pool that enables
subject matter experts to efficiently create and maintain natural language
descriptions of physical operators used in QEPs. A rule-based framework called
RULE-LANTERN is proposed that exploits pool to generate natural language
descriptions of QEPs. Despite the high accuracy of RULE-LANTERN, our engagement
with learners reveal that, consistent with existing psychology theories,
perusing such rule-based descriptions lead to boredom due to repetitive
statements across different QEPs. To address this issue, we present a novel
deep learning-based language generation framework called NEURAL-LANTERN that
infuses language variability in the generated description by exploiting a set
of paraphrasing tools and word embedding. Our experimental study with real
learners shows the effectiveness of lantern in facilitating comprehension of
QEPs.
",2021-03-01T04:13:21Z,http://arxiv.org/abs/2103.00740v3,"Weiguo Wang, Sourav S Bhowmick, Hui Li, Shafiq R Joty, Siyuan Liu, Peng Chen"
Deep Anomaly Detection in Text,"  Deep anomaly detection methods have become increasingly popular in recent
years, with methods like Stacked Autoencoders, Variational Autoencoders, and
Generative Adversarial Networks greatly improving the state-of-the-art. Other
methods rely on augmenting classical models (such as the One-Class Support
Vector Machine), by learning an appropriate kernel function using Neural
Networks. Recent developments in representation learning by self-supervision
are proving to be very beneficial in the context of anomaly detection. Inspired
by the advancements in anomaly detection using self-supervised learning in the
field of computer vision, this thesis aims to develop a method for detecting
anomalies by exploiting pretext tasks tailored for text corpora. This approach
greatly improves the state-of-the-art on two datasets, 20Newsgroups, and AG
News, for both semi-supervised and unsupervised anomaly detection, thus proving
the potential for self-supervised anomaly detectors in the field of natural
language processing.
",2023-12-14T22:04:43Z,http://arxiv.org/abs/2401.02971v1,Andrei Manolache
"Recent advances in deep learning and language models for studying the
  microbiome","  Recent advancements in deep learning, particularly large language models
(LLMs), made a significant impact on how researchers study microbiome and
metagenomics data. Microbial protein and genomic sequences, like natural
languages, form a language of life, enabling the adoption of LLMs to extract
useful insights from complex microbial ecologies. In this paper, we review
applications of deep learning and language models in analyzing microbiome and
metagenomics data. We focus on problem formulations, necessary datasets, and
the integration of language modeling techniques. We provide an extensive
overview of protein/genomic language modeling and their contributions to
microbiome studies. We also discuss applications such as novel viromics
language modeling, biosynthetic gene cluster prediction, and knowledge
integration for metagenomics studies.
",2024-09-15T18:32:31Z,http://arxiv.org/abs/2409.10579v1,"Binghao Yan, Yunbi Nam, Lingyao Li, Rebecca A. Deek, Hongzhe Li, Siyuan Ma"
"A Survey in Automatic Irony Processing: Linguistic, Cognitive, and
  Multi-X Perspectives","  Irony is a ubiquitous figurative language in daily communication. Previously,
many researchers have approached irony from linguistic, cognitive science, and
computational aspects. Recently, some progress have been witnessed in automatic
irony processing due to the rapid development in deep neural models in natural
language processing (NLP). In this paper, we will provide a comprehensive
overview of computational irony, insights from linguistic theory and cognitive
science, as well as its interactions with downstream NLP tasks and newly
proposed multi-X irony processing perspectives.
",2022-09-10T17:03:34Z,http://arxiv.org/abs/2209.04712v1,"Qingcheng Zeng, An-Ran Li"
Transfer Learning for Speech and Language Processing,"  Transfer learning is a vital technique that generalizes models trained for
one setting or task to other settings or tasks. For example in speech
recognition, an acoustic model trained for one language can be used to
recognize speech in another language, with little or no re-training data.
Transfer learning is closely related to multi-task learning (cross-lingual vs.
multilingual), and is traditionally studied in the name of `model adaptation'.
Recent advance in deep learning shows that transfer learning becomes much
easier and more effective with high-level abstract features learned by deep
models, and the `transfer' can be conducted not only between data distributions
and data types, but also between model structures (e.g., shallow nets and deep
nets) or even model types (e.g., Bayesian models and neural models). This
review paper summarizes some recent prominent research towards this direction,
particularly for speech and language processing. We also report some results
from our group and highlight the potential of this very interesting research
field.
",2015-11-19T05:54:45Z,http://arxiv.org/abs/1511.06066v1,"Dong Wang, Thomas Fang Zheng"
"The RFML Ecosystem: A Look at the Unique Challenges of Applying Deep
  Learning to Radio Frequency Applications","  While deep machine learning technologies are now pervasive in
state-of-the-art image recognition and natural language processing
applications, only in recent years have these technologies started to
sufficiently mature in applications related to wireless communications. In
particular, recent research has shown deep machine learning to be an enabling
technology for cognitive radio applications as well as a useful tool for
supplementing expertly defined algorithms for spectrum sensing applications
such as signal detection, estimation, and classification (termed here as Radio
Frequency Machine Learning, or RFML). A major driver for the usage of deep
machine learning in the context of wireless communications is that little, to
no, a priori knowledge of the intended spectral environment is required, given
that there is an abundance of representative data to facilitate training and
evaluation. However, in addition to this fundamental need for sufficient data,
there are other key considerations, such as trust, security, and
hardware/software issues, that must be taken into account before deploying deep
machine learning systems in real-world wireless communication applications.
This paper provides an overview and survey of prior work related to these major
research considerations. In particular, we present their unique considerations
in the RFML application space, which are not generally present in the image,
audio, and/or text application spaces.
",2020-10-01T14:27:28Z,http://arxiv.org/abs/2010.00432v1,"Lauren J. Wong, William H. Clark IV, Bryse Flowers, R. Michael Buehrer, Alan J. Michaels, William C. Headley"
Evolution of transfer learning in natural language processing,"  In this paper, we present a study of the recent advancements which have
helped bring Transfer Learning to NLP through the use of semi-supervised
training. We discuss cutting-edge methods and architectures such as BERT, GPT,
ELMo, ULMFit among others. Classically, tasks in natural language processing
have been performed through rule-based and statistical methodologies. However,
owing to the vast nature of natural languages these methods do not generalise
well and failed to learn the nuances of language. Thus machine learning
algorithms such as Naive Bayes and decision trees coupled with traditional
models such as Bag-of-Words and N-grams were used to usurp this problem.
Eventually, with the advent of advanced recurrent neural network architectures
such as the LSTM, we were able to achieve state-of-the-art performance in
several natural language processing tasks such as text classification and
machine translation. We talk about how Transfer Learning has brought about the
well-known ImageNet moment for NLP. Several advanced architectures such as the
Transformer and its variants have allowed practitioners to leverage knowledge
gained from unrelated task to drastically fasten convergence and provide better
performance on the target task. This survey represents an effort at providing a
succinct yet complete understanding of the recent advances in natural language
processing using deep learning in with a special focus on detailing transfer
learning and its potential advantages.
",2019-10-16T14:24:37Z,http://arxiv.org/abs/1910.07370v1,"Aditya Malte, Pratik Ratadiya"
"EEGDiR: Electroencephalogram denoising network for temporal information
  storage and global modeling through Retentive Network","  Electroencephalogram (EEG) signals play a pivotal role in clinical medicine,
brain research, and neurological disease studies. However, susceptibility to
various physiological and environmental artifacts introduces noise in recorded
EEG data, impeding accurate analysis of underlying brain activity. Denoising
techniques are crucial to mitigate this challenge. Recent advancements in deep
learningbased approaches exhibit substantial potential for enhancing the
signal-to-noise ratio of EEG data compared to traditional methods. In the realm
of large-scale language models (LLMs), the Retentive Network (Retnet)
infrastructure, prevalent for some models, demonstrates robust feature
extraction and global modeling capabilities. Recognizing the temporal
similarities between EEG signals and natural language, we introduce the Retnet
from natural language processing to EEG denoising. This integration presents a
novel approach to EEG denoising, opening avenues for a profound understanding
of brain activities and accurate diagnosis of neurological diseases.
Nonetheless, direct application of Retnet to EEG denoising is unfeasible due to
the one-dimensional nature of EEG signals, while natural language processing
deals with two-dimensional data. To facilitate Retnet application to EEG
denoising, we propose the signal embedding method, transforming one-dimensional
EEG signals into two dimensions for use as network inputs. Experimental results
validate the substantial improvement in denoising effectiveness achieved by the
proposed method.
",2024-03-20T15:04:21Z,http://arxiv.org/abs/2404.15289v2,"Bin Wang, Fei Deng, Peifan Jiang"
"Rethinking Tokenization: Crafting Better Tokenizers for Large Language
  Models","  Tokenization significantly influences language models(LMs)' performance. This
paper traces the evolution of tokenizers from word-level to subword-level,
analyzing how they balance tokens and types to enhance model adaptability while
controlling complexity. Despite subword tokenizers like Byte Pair Encoding
(BPE) overcoming many word tokenizer limitations, they encounter difficulties
in handling non-Latin languages and depend heavily on extensive training data
and computational resources to grasp the nuances of multiword expressions
(MWEs). This article argues that tokenizers, more than mere technical tools,
should drawing inspiration from the cognitive science about human language
processing. This study then introduces the ""Principle of Least Effort"" from
cognitive science, that humans naturally seek to reduce cognitive effort, and
discusses the benefits of this principle for tokenizer development. Based on
this principle, the paper proposes that the Less-is-Better (LiB) model could be
a new approach for LLM tokenizer. The LiB model can autonomously learn an
integrated vocabulary consisting of subwords, words, and MWEs, which
effectively reduces both the numbers of tokens and types. Comparative
evaluations show that the LiB tokenizer outperforms existing word and BPE
tokenizers, presenting an innovative method for tokenizer development, and
hinting at the possibility of future cognitive science-based tokenizers being
more efficient.
",2024-03-01T10:03:07Z,http://arxiv.org/abs/2403.00417v1,Jinbiao Yang
"MetaPred: Meta-Learning for Clinical Risk Prediction with Limited
  Patient Electronic Health Records","  In recent years, increasingly augmentation of health data, such as patient
Electronic Health Records (EHR), are becoming readily available. This provides
an unprecedented opportunity for knowledge discovery and data mining algorithms
to dig insights from them, which can, later on, be helpful to the improvement
of the quality of care delivery. Predictive modeling of clinical risk,
including in-hospital mortality, hospital readmission, chronic disease onset,
condition exacerbation, etc., from patient EHR, is one of the health data
analytic problems that attract most of the interests. The reason is not only
because the problem is important in clinical settings, but also there are
challenges working with EHR such as sparsity, irregularity, temporality, etc.
Different from applications in other domains such as computer vision and
natural language processing, the labeled data samples in medicine (patients)
are relatively limited, which creates lots of troubles for effective predictive
model learning, especially for complicated models such as deep learning. In
this paper, we propose MetaPred, a meta-learning for clinical risk prediction
from longitudinal patient EHRs. In particular, in order to predict the target
risk where there are limited data samples, we train a meta-learner from a set
of related risk prediction tasks which learns how a good predictor is learned.
The meta-learned can then be directly used in target risk prediction, and the
limited available samples can be used for further fine-tuning the model
performance. The effectiveness of MetaPred is tested on a real patient EHR
repository from Oregon Health & Science University. We are able to demonstrate
that with CNN and RNN as base predictors, MetaPred can achieve much better
performance for predicting target risk with low resources comparing with the
predictor trained on the limited samples available for this risk.
",2019-05-08T17:07:51Z,http://arxiv.org/abs/1905.03218v1,"Xi Sheryl Zhang, Fengyi Tang, Hiroko Dodge, Jiayu Zhou, Fei Wang"
"On Sensitivity of Deep Learning Based Text Classification Algorithms to
  Practical Input Perturbations","  Text classification is a fundamental Natural Language Processing task that
has a wide variety of applications, where deep learning approaches have
produced state-of-the-art results. While these models have been heavily
criticized for their black-box nature, their robustness to slight perturbations
in input text has been a matter of concern. In this work, we carry out a
data-focused study evaluating the impact of systematic practical perturbations
on the performance of the deep learning based text classification models like
CNN, LSTM, and BERT-based algorithms. The perturbations are induced by the
addition and removal of unwanted tokens like punctuation and stop-words that
are minimally associated with the final performance of the model. We show that
these deep learning approaches including BERT are sensitive to such legitimate
input perturbations on four standard benchmark datasets SST2, TREC-6, BBC News,
and tweet_eval. We observe that BERT is more susceptible to the removal of
tokens as compared to the addition of tokens. Moreover, LSTM is slightly more
sensitive to input perturbations as compared to CNN based model. The work also
serves as a practical guide to assessing the impact of discrepancies in
train-test conditions on the final performance of models.
",2022-01-02T08:33:49Z,http://arxiv.org/abs/2201.00318v2,"Aamir Miyajiwala, Arnav Ladkat, Samiksha Jagadale, Raviraj Joshi"
"LINDA: Unsupervised Learning to Interpolate in Natural Language
  Processing","  Despite the success of mixup in data augmentation, its applicability to
natural language processing (NLP) tasks has been limited due to the discrete
and variable-length nature of natural languages. Recent studies have thus
relied on domain-specific heuristics and manually crafted resources, such as
dictionaries, in order to apply mixup in NLP. In this paper, we instead propose
an unsupervised learning approach to text interpolation for the purpose of data
augmentation, to which we refer as ""Learning to INterpolate for Data
Augmentation"" (LINDA), that does not require any heuristics nor manually
crafted resources but learns to interpolate between any pair of natural
language sentences over a natural language manifold. After empirically
demonstrating the LINDA's interpolation capability, we show that LINDA indeed
allows us to seamlessly apply mixup in NLP and leads to better generalization
in text classification both in-domain and out-of-domain.
",2021-12-28T02:56:41Z,http://arxiv.org/abs/2112.13969v1,"Yekyung Kim, Seohyeong Jeong, Kyunghyun Cho"
"Cross-lingual Transfer of Abstractive Summarizer to Less-resource
  Language","  Automatic text summarization extracts important information from texts and
presents the information in the form of a summary. Abstractive summarization
approaches progressed significantly by switching to deep neural networks, but
results are not yet satisfactory, especially for languages where large training
sets do not exist. In several natural language processing tasks, a
cross-lingual model transfer is successfully applied in less-resource
languages. For summarization, the cross-lingual model transfer was not
attempted due to a non-reusable decoder side of neural models that cannot
correct target language generation. In our work, we use a pre-trained English
summarization model based on deep neural networks and sequence-to-sequence
architecture to summarize Slovene news articles. We address the problem of
inadequate decoder by using an additional language model for the evaluation of
the generated text in target language. We test several cross-lingual
summarization models with different amounts of target data for fine-tuning. We
assess the models with automatic evaluation measures and conduct a small-scale
human evaluation. Automatic evaluation shows that the summaries of our best
cross-lingual model are useful and of quality similar to the model trained only
in the target language. Human evaluation shows that our best model generates
summaries with high accuracy and acceptable readability. However, similar to
other abstractive models, our models are not perfect and may occasionally
produce misleading or absurd content.
",2020-12-08T09:30:38Z,http://arxiv.org/abs/2012.04307v2,"Aleš Žagar, Marko Robnik-Šikonja"
Using NLP on news headlines to predict index trends,"  This paper attempts to provide a state of the art in trend prediction using
news headlines. We present the research done on predicting DJIA trends using
Natural Language Processing. We will explain the different algorithms we have
used as well as the various embedding techniques attempted. We rely on
statistical and deep learning models in order to extract information from the
corpuses.
",2018-06-22T15:37:35Z,http://arxiv.org/abs/1806.09533v1,"Marc Velay, Fabrice Daniel"
Seq2Seq AI Chatbot with Attention Mechanism,"  Intelligent Conversational Agent development using Artificial Intelligence or
Machine Learning technique is an interesting problem in the field of Natural
Language Processing. With the rise of deep learning, these models were quickly
replaced by end to end trainable neural networks.
",2020-06-04T10:54:43Z,http://arxiv.org/abs/2006.02767v1,Abonia Sojasingarayar
"Analyzing Deep Transformer Models for Time Series Forecasting via
  Manifold Learning","  Transformer models have consistently achieved remarkable results in various
domains such as natural language processing and computer vision. However,
despite ongoing research efforts to better understand these models, the field
still lacks a comprehensive understanding. This is particularly true for deep
time series forecasting methods, where analysis and understanding work is
relatively limited. Time series data, unlike image and text information, can be
more challenging to interpret and analyze. To address this, we approach the
problem from a manifold learning perspective, assuming that the latent
representations of time series forecasting models lie next to a low-dimensional
manifold. In our study, we focus on analyzing the geometric features of these
latent data manifolds, including intrinsic dimension and principal curvatures.
Our findings reveal that deep transformer models exhibit similar geometric
behavior across layers, and these geometric features are correlated with model
performance. Additionally, we observe that untrained models initially have
different structures, but they rapidly converge during training. By leveraging
our geometric analysis and differentiable tools, we can potentially design new
and improved deep forecasting neural networks. This approach complements
existing analysis studies and contributes to a better understanding of
transformer models in the context of time series forecasting. Code is released
at https://github.com/azencot-group/GATLM.
",2024-10-17T17:32:35Z,http://arxiv.org/abs/2410.13792v1,"Ilya Kaufman, Omri Azencot"
"How to keep text private? A systematic review of deep learning methods
  for privacy-preserving natural language processing","  Deep learning (DL) models for natural language processing (NLP) tasks often
handle private data, demanding protection against breaches and disclosures.
Data protection laws, such as the European Union's General Data Protection
Regulation (GDPR), thereby enforce the need for privacy. Although many
privacy-preserving NLP methods have been proposed in recent years, no
categories to organize them have been introduced yet, making it hard to follow
the progress of the literature. To close this gap, this article systematically
reviews over sixty DL methods for privacy-preserving NLP published between 2016
and 2020, covering theoretical foundations, privacy-enhancing technologies, and
analysis of their suitability for real-world scenarios. First, we introduce a
novel taxonomy for classifying the existing methods into three categories: data
safeguarding methods, trusted methods, and verification methods. Second, we
present an extensive summary of privacy threats, datasets for applications, and
metrics for privacy evaluation. Third, throughout the review, we describe
privacy issues in the NLP pipeline in a holistic view. Further, we discuss open
challenges in privacy-preserving NLP regarding data traceability, computation
overhead, dataset size, the prevalence of human biases in embeddings, and the
privacy-utility tradeoff. Finally, this review presents future research
directions to guide successive research and development of privacy-preserving
NLP models.
",2022-05-20T11:29:44Z,http://arxiv.org/abs/2205.10095v1,"Samuel Sousa, Roman Kern"
Hindi to English: Transformer-Based Neural Machine Translation,"  Machine Translation (MT) is one of the most prominent tasks in Natural
Language Processing (NLP) which involves the automatic conversion of texts from
one natural language to another while preserving its meaning and fluency.
Although the research in machine translation has been going on since multiple
decades, the newer approach of integrating deep learning techniques in natural
language processing has led to significant improvements in the translation
quality. In this paper, we have developed a Neural Machine Translation (NMT)
system by training the Transformer model to translate texts from Indian
Language Hindi to English. Hindi being a low resource language has made it
difficult for neural networks to understand the language thereby leading to a
slow growth in the development of neural machine translators. Thus, to address
this gap, we implemented back-translation to augment the training data and for
creating the vocabulary, we experimented with both word and subword level
tokenization using Byte Pair Encoding (BPE) thereby ending up training the
Transformer in 10 different configurations. This led us to achieve a
state-of-the-art BLEU score of 24.53 on the test set of IIT Bombay
English-Hindi Corpus in one of the configurations.
",2023-09-23T00:00:09Z,http://arxiv.org/abs/2309.13222v1,"Kavit Gangar, Hardik Ruparel, Shreyas Lele"
Knowledge-based Biomedical Data Science 2019,"  Knowledge-based biomedical data science (KBDS) involves the design and
implementation of computer systems that act as if they knew about biomedicine.
Such systems depend on formally represented knowledge in computer systems,
often in the form of knowledge graphs. Here we survey the progress in the last
year in systems that use formally represented knowledge to address data science
problems in both clinical and biological domains, as well as on approaches for
creating knowledge graphs. Major themes include the relationships between
knowledge graphs and machine learning, the use of natural language processing,
and the expansion of knowledge-based approaches to novel domains, such as
Chinese Traditional Medicine and biodiversity.
",2019-10-08T17:28:16Z,http://arxiv.org/abs/1910.06710v1,"Tiffany J. Callahan, Harrison Pielke-Lombardo, Ignacio J. Tripodi, Lawrence E. Hunter"
Comparative Analysis of Word Embeddings for Capturing Word Similarities,"  Distributed language representation has become the most widely used technique
for language representation in various natural language processing tasks. Most
of the natural language processing models that are based on deep learning
techniques use already pre-trained distributed word representations, commonly
called word embeddings. Determining the most qualitative word embeddings is of
crucial importance for such models. However, selecting the appropriate word
embeddings is a perplexing task since the projected embedding space is not
intuitive to humans. In this paper, we explore different approaches for
creating distributed word representations. We perform an intrinsic evaluation
of several state-of-the-art word embedding methods. Their performance on
capturing word similarities is analysed with existing benchmark datasets for
word pairs similarities. The research in this paper conducts a correlation
analysis between ground truth word similarities and similarities obtained by
different word embedding methods.
",2020-05-08T01:16:03Z,http://arxiv.org/abs/2005.03812v1,"Martina Toshevska, Frosina Stojanovska, Jovan Kalajdjieski"
"Incorporating Voice Instructions in Model-Based Reinforcement Learning
  for Self-Driving Cars","  This paper presents a novel approach that supports natural language voice
instructions to guide deep reinforcement learning (DRL) algorithms when
training self-driving cars. DRL methods are popular approaches for autonomous
vehicle (AV) agents. However, most existing methods are sample- and
time-inefficient and lack a natural communication channel with the human
expert. In this paper, how new human drivers learn from human coaches motivates
us to study new ways of human-in-the-loop learning and a more natural and
approachable training interface for the agents. We propose incorporating
natural language voice instructions (NLI) in model-based deep reinforcement
learning to train self-driving cars. We evaluate the proposed method together
with a few state-of-the-art DRL methods in the CARLA simulator. The results
show that NLI can help ease the training process and significantly boost the
agents' learning speed.
",2022-06-21T10:55:39Z,http://arxiv.org/abs/2206.10249v1,"Mingze Wang, Ziyang Zhang, Grace Hui Yang"
Semi-supervised Classification for Natural Language Processing,"  Semi-supervised classification is an interesting idea where classification
models are learned from both labeled and unlabeled data. It has several
advantages over supervised classification in natural language processing
domain. For instance, supervised classification exploits only labeled data that
are expensive, often difficult to get, inadequate in quantity, and require
human experts for annotation. On the other hand, unlabeled data are inexpensive
and abundant. Despite the fact that many factors limit the wide-spread use of
semi-supervised classification, it has become popular since its level of
performance is empirically as good as supervised classification. This study
explores the possibilities and achievements as well as complexity and
limitations of semi-supervised classification for several natural langue
processing tasks like parsing, biomedical information processing, text
classification, and summarization.
",2014-09-25T15:18:44Z,http://arxiv.org/abs/1409.7612v1,Rushdi Shams
"Deep Learning the EEG Manifold for Phonological Categorization from
  Active Thoughts","  Speech-related Brain Computer Interfaces (BCI) aim primarily at finding an
alternative vocal communication pathway for people with speaking disabilities.
As a step towards full decoding of imagined speech from active thoughts, we
present a BCI system for subject-independent classification of phonological
categories exploiting a novel deep learning based hierarchical feature
extraction scheme. To better capture the complex representation of
high-dimensional electroencephalography (EEG) data, we compute the joint
variability of EEG electrodes into a channel cross-covariance matrix. We then
extract the spatio-temporal information encoded within the matrix using a mixed
deep neural network strategy. Our model framework is composed of a
convolutional neural network (CNN), a long-short term network (LSTM), and a
deep autoencoder. We train the individual networks hierarchically, feeding
their combined outputs in a final gradient boosting classification step. Our
best models achieve an average accuracy of 77.9% across five different binary
classification tasks, providing a significant 22.5% improvement over previous
methods. As we also show visually, our work demonstrates that the speech
imagery EEG possesses significant discriminative information about the intended
articulatory movements responsible for natural speech synthesis.
",2019-04-08T21:11:40Z,http://arxiv.org/abs/1904.04358v1,"Pramit Saha, Muhammad Abdul-Mageed, Sidney Fels"
"Deep Learning Acceleration Techniques for Real Time Mobile Vision
  Applications","  Deep Learning (DL) has become a crucial technology for Artificial
Intelligence (AI). It is a powerful technique to automatically extract
high-level features from complex data which can be exploited for applications
such as computer vision, natural language processing, cybersecurity,
communications, and so on. For the particular case of computer vision, several
algorithms like object detection in real time videos have been proposed and
they work well on Desktop GPUs and distributed computing platforms. However
these algorithms are still heavy for mobile and embedded visual applications.
The rapid spreading of smart portable devices and the emerging 5G network are
introducing new smart multimedia applications in mobile environments. As a
consequence, the possibility of implementing deep neural networks to mobile
environments has attracted a lot of researchers. This paper presents emerging
deep learning acceleration techniques that can enable the delivery of real time
visual recognition into the hands of end users, anytime and anywhere.
",2019-05-09T02:39:37Z,http://arxiv.org/abs/1905.03418v2,Gael Kamdem De Teyou
"Long-range and hierarchical language predictions in brains and
  algorithms","  Deep learning has recently made remarkable progress in natural language
processing. Yet, the resulting algorithms remain far from competing with the
language abilities of the human brain. Predictive coding theory offers a
potential explanation to this discrepancy: while deep language algorithms are
optimized to predict adjacent words, the human brain would be tuned to make
long-range and hierarchical predictions. To test this hypothesis, we analyze
the fMRI brain signals of 304 subjects each listening to 70min of short
stories. After confirming that the activations of deep language algorithms
linearly map onto those of the brain, we show that enhancing these models with
long-range forecast representations improves their brain-mapping. The results
further reveal a hierarchy of predictions in the brain, whereby the
fronto-parietal cortices forecast more abstract and more distant
representations than the temporal cortices. Overall, this study strengthens
predictive coding theory and suggests a critical role of long-range and
hierarchical predictions in natural language processing.
",2021-11-28T20:26:07Z,http://arxiv.org/abs/2111.14232v1,"Charlotte Caucheteux, Alexandre Gramfort, Jean-Remi King"
Learning Latent Representations for Speech Generation and Transformation,"  An ability to model a generative process and learn a latent representation
for speech in an unsupervised fashion will be crucial to process vast
quantities of unlabelled speech data. Recently, deep probabilistic generative
models such as Variational Autoencoders (VAEs) have achieved tremendous success
in modeling natural images. In this paper, we apply a convolutional VAE to
model the generative process of natural speech. We derive latent space
arithmetic operations to disentangle learned latent representations. We
demonstrate the capability of our model to modify the phonetic content or the
speaker identity for speech segments using the derived operations, without the
need for parallel supervisory data.
",2017-04-13T17:41:11Z,http://arxiv.org/abs/1704.04222v2,"Wei-Ning Hsu, Yu Zhang, James Glass"
"Continuous Sign Language Recognition with Adapted Conformer via
  Unsupervised Pretraining","  Conventional Deep Learning frameworks for continuous sign language
recognition (CSLR) are comprised of a single or multi-modal feature extractor,
a sequence-learning module, and a decoder for outputting the glosses. The
sequence learning module is a crucial part wherein transformers have
demonstrated their efficacy in the sequence-to-sequence tasks. Analyzing the
research progress in the field of Natural Language Processing and Speech
Recognition, a rapid introduction of various transformer variants is observed.
However, in the realm of sign language, experimentation in the sequence
learning component is limited. In this work, the state-of-the-art Conformer
model for Speech Recognition is adapted for CSLR and the proposed model is
termed ConSignformer. This marks the first instance of employing Conformer for
a vision-based task. ConSignformer has bimodal pipeline of CNN as feature
extractor and Conformer for sequence learning. For improved context learning we
also introduce Cross-Modal Relative Attention (CMRA). By incorporating CMRA
into the model, it becomes more adept at learning and utilizing complex
relationships within the data. To further enhance the Conformer model,
unsupervised pretraining called Regressional Feature Extraction is conducted on
a curated sign language dataset. The pretrained Conformer is then fine-tuned
for the downstream recognition task. The experimental results confirm the
effectiveness of the adopted pretraining strategy and demonstrate how CMRA
contributes to the recognition process. Remarkably, leveraging a
Conformer-based backbone, our model achieves state-of-the-art performance on
the benchmark datasets: PHOENIX-2014 and PHOENIX-2014T.
",2024-05-20T13:40:52Z,http://arxiv.org/abs/2405.12018v1,"Neena Aloysius, Geetha M, Prema Nedungadi"
"Transgender Community Sentiment Analysis from Social Media Data: A
  Natural Language Processing Approach","  Transgender community is experiencing a huge disparity in mental health
conditions compared with the general population. Interpreting the social medial
data posted by transgender people may help us understand the sentiments of
these sexual minority groups better and apply early interventions. In this
study, we manually categorize 300 social media comments posted by transgender
people to the sentiment of negative, positive, and neutral. 5 machine learning
algorithms and 2 deep neural networks are adopted to build sentiment analysis
classifiers based on the annotated data. Results show that our annotations are
reliable with a high Cohen's Kappa score over 0.8 across all three classes.
LSTM model yields an optimal performance of accuracy over 0.85 and AUC of
0.876. Our next step will focus on using advanced natural language processing
algorithms on a larger annotated dataset.
",2020-10-25T08:13:34Z,http://arxiv.org/abs/2010.13062v2,"Yuqiao Liu, Yudan Wang, Ying Zhao, Zhixiang Li"
"Deep learning applied to EEG data with different montages using spatial
  attention","  The ability of Deep Learning to process and extract relevant information in
complex brain dynamics from raw EEG data has been demonstrated in various
recent works. Deep learning models, however, have also been shown to perform
best on large corpora of data. When processing EEG, a natural approach is to
combine EEG datasets from different experiments to train large deep-learning
models. However, most EEG experiments use custom channel montages, requiring
the data to be transformed into a common space. Previous methods have used the
raw EEG signal to extract features of interest and focused on using a common
feature space across EEG datasets. While this is a sensible approach, it
underexploits the potential richness of EEG raw data. Here, we explore using
spatial attention applied to EEG electrode coordinates to perform channel
harmonization of raw EEG data, allowing us to train deep learning on EEG data
using different montages. We test this model on a gender classification task.
We first show that spatial attention increases model performance. Then, we show
that a deep learning model trained on data using different channel montages
performs significantly better than deep learning models trained on fixed 23-
and 128-channel data montages.
",2023-10-16T16:17:33Z,http://arxiv.org/abs/2310.10550v1,"Dung Truong, Muhammad Abdullah Khalid, Arnaud Delorme"
"Brief Introduction to Contrastive Learning Pretext Tasks for Visual
  Representation","  To improve performance in visual feature representation from photos or videos
for practical applications, we generally require large-scale human-annotated
labeled data while training deep neural networks. However, the cost of
gathering and annotating human-annotated labeled data is expensive. Given that
there is a lot of unlabeled data in the actual world, it is possible to
introduce self-defined pseudo labels as supervisions to prevent this issue.
Self-supervised learning, specifically contrastive learning, is a subset of
unsupervised learning methods that has grown popular in computer vision,
natural language processing, and other domains. The purpose of contrastive
learning is to embed augmented samples from the same sample near to each other
while pushing away those that are not. In the following sections, we will
introduce the regular formulation among different learnings. In the next
sections, we will discuss the regular formulation of various learnings.
Furthermore, we offer some strategies from contrastive learning that have
recently been published and are focused on pretext tasks for visual
representation.
",2022-10-06T18:54:10Z,http://arxiv.org/abs/2210.03163v1,Zhenyuan Lu
"Active$^2$ Learning: Actively reducing redundancies in Active Learning
  methods for Sequence Tagging and Machine Translation","  While deep learning is a powerful tool for natural language processing (NLP)
problems, successful solutions to these problems rely heavily on large amounts
of annotated samples. However, manually annotating data is expensive and
time-consuming. Active Learning (AL) strategies reduce the need for huge
volumes of labeled data by iteratively selecting a small number of examples for
manual annotation based on their estimated utility in training the given model.
In this paper, we argue that since AL strategies choose examples independently,
they may potentially select similar examples, all of which may not contribute
significantly to the learning process. Our proposed approach,
Active$\mathbf{^2}$ Learning (A$\mathbf{^2}$L), actively adapts to the deep
learning model being trained to eliminate further such redundant examples
chosen by an AL strategy. We show that A$\mathbf{^2}$L is widely applicable by
using it in conjunction with several different AL strategies and NLP tasks. We
empirically demonstrate that the proposed approach is further able to reduce
the data requirements of state-of-the-art AL strategies by an absolute
percentage reduction of $\approx\mathbf{3-25\%}$ on multiple NLP tasks while
achieving the same performance with no additional computation overhead.
",2021-03-11T06:27:31Z,http://arxiv.org/abs/2103.06490v2,"Rishi Hazra, Parag Dutta, Shubham Gupta, Mohammed Abdul Qaathir, Ambedkar Dukkipati"
Low Resource Summarization using Pre-trained Language Models,"  With the advent of Deep Learning based Artificial Neural Networks models,
Natural Language Processing (NLP) has witnessed significant improvements in
textual data processing in terms of its efficiency and accuracy. However, the
research is mostly restricted to high-resource languages such as English and
low-resource languages still suffer from a lack of available resources in terms
of training datasets as well as models with even baseline evaluation results.
Considering the limited availability of resources for low-resource languages,
we propose a methodology for adapting self-attentive transformer-based
architecture models (mBERT, mT5) for low-resource summarization, supplemented
by the construction of a new baseline dataset (76.5k article, summary pairs) in
a low-resource language Urdu. Choosing news (a publicly available source) as
the application domain has the potential to make the proposed methodology
useful for reproducing in other languages with limited resources. Our adapted
summarization model \textit{urT5} with up to 44.78\% reduction in size as
compared to \textit{mT5} can capture contextual information of low resource
language effectively with evaluation score (up to 46.35 ROUGE-1, 77 BERTScore)
at par with state-of-the-art models in high resource language English
\textit{(PEGASUS: 47.21, BART: 45.14 on XSUM Dataset)}. The proposed method
provided a baseline approach towards extractive as well as abstractive
summarization with competitive evaluation results in a limited resource setup.
",2023-10-04T13:09:39Z,http://arxiv.org/abs/2310.02790v1,"Mubashir Munaf, Hammad Afzal, Naima Iltaf, Khawir Mahmood"
TextMage: The Automated Bangla Caption Generator Based On Deep Learning,"  Neural Networks and Deep Learning have seen an upsurge of research in the
past decade due to the improved results. Generates text from the given image is
a crucial task that requires the combination of both sectors which are computer
vision and natural language processing in order to understand an image and
represent it using a natural language. However existing works have all been
done on a particular lingual domain and on the same set of data. This leads to
the systems being developed to perform poorly on images that belong to specific
locales' geographical context. TextMage is a system that is capable of
understanding visual scenes that belong to the Bangladeshi geographical context
and use its knowledge to represent what it understands in Bengali. Hence, we
have trained a model on our previously developed and published dataset named
BanglaLekhaImageCaptions. This dataset contains 9,154 images along with two
annotations for each image. In order to access performance, the proposed model
has been implemented and evaluated.
",2020-10-15T23:24:15Z,http://arxiv.org/abs/2010.08066v1,"Abrar Hasin Kamal, Md. Asifuzzaman Jishan, Nafees Mansoor"
Packet2Vec: Utilizing Word2Vec for Feature Extraction in Packet Data,"  One of deep learning's attractive benefits is the ability to automatically
extract relevant features for a target problem from largely raw data, instead
of utilizing human engineered and error prone handcrafted features. While deep
learning has shown success in fields such as image classification and natural
language processing, its application for feature extraction on raw network
packet data for intrusion detection is largely unexplored. In this paper we
modify a Word2Vec approach, used for text processing, and apply it to packet
data for automatic feature extraction. We call this approach Packet2Vec. For
the classification task of benign versus malicious traffic on a 2009 DARPA
network data set, we obtain an area under the curve (AUC) of the receiver
operating characteristic (ROC) between 0.988-0.996 and an AUC of the
Precision/Recall curve between 0.604-0.667.
",2020-04-29T21:03:48Z,http://arxiv.org/abs/2004.14477v1,"Eric L. Goodman, Chase Zimmerman, Corey Hudson"
"Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study
  on Out-of-Distribution Generalisation","  Combining deep learning with symbolic logic reasoning aims to capitalize on
the success of both fields and is drawing increasing attention. Inspired by
DeepLogic, an end-to-end model trained to perform inference on logic programs,
we introduce IMA-GloVe-GA, an iterative neural inference network for multi-step
reasoning expressed in natural language. In our model, reasoning is performed
using an iterative memory neural network based on RNN with a gated attention
mechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES
V1 and CONCEPTRULES V2. Experimental results show DeepLogic with gated
attention can achieve higher test accuracy than DeepLogic and other RNN
baseline models. Our model achieves better out-of-distribution generalisation
than RoBERTa-Large when the rules have been shuffled. Furthermore, to address
the issue of unbalanced distribution of reasoning depths in the current
multi-step reasoning datasets, we develop PARARULE-Plus, a large dataset with
more examples that require deeper reasoning steps. Experimental results show
that the addition of PARARULE-Plus can increase the model's performance on
examples requiring deeper reasoning depths. The source code and data are
available at
https://github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language.
",2022-07-28T10:44:46Z,http://arxiv.org/abs/2207.14000v3,"Qiming Bao, Alex Yuxuan Peng, Tim Hartill, Neset Tan, Zhenyun Deng, Michael Witbrock, Jiamou Liu"
Efficacy of Transformer Networks for Classification of Raw EEG Data,"  With the unprecedented success of transformer networks in natural language
processing (NLP), recently, they have been successfully adapted to areas like
computer vision, generative adversarial networks (GAN), and reinforcement
learning. Classifying electroencephalogram (EEG) data has been challenging and
researchers have been overly dependent on pre-processing and hand-crafted
feature extraction. Despite having achieved automated feature extraction in
several other domains, deep learning has not yet been accomplished for EEG. In
this paper, the efficacy of the transformer network for the classification of
raw EEG data (cleaned and pre-processed) is explored. The performance of
transformer networks was evaluated on a local (age and gender data) and a
public dataset (STEW). First, a classifier using a transformer network is built
to classify the age and gender of a person with raw resting-state EEG data.
Second, the classifier is tuned for mental workload classification with open
access raw multi-tasking mental workload EEG data (STEW). The network achieves
an accuracy comparable to state-of-the-art accuracy on both the local (Age and
Gender dataset; 94.53% (gender) and 87.79% (age)) and the public (STEW dataset;
95.28% (two workload levels) and 88.72% (three workload levels)) dataset. The
accuracy values have been achieved using raw EEG data without feature
extraction. Results indicate that the transformer-based deep learning models
can successfully abate the need for heavy feature-extraction of EEG data for
successful classification.
",2022-02-08T17:12:27Z,http://arxiv.org/abs/2202.05170v1,"Gourav Siddhad, Anmol Gupta, Debi Prosad Dogra, Partha Pratim Roy"
"Personality Trait Detection Using Bagged SVM over BERT Word Embedding
  Ensembles","  Recently, the automatic prediction of personality traits has received
increasing attention and has emerged as a hot topic within the field of
affective computing. In this work, we present a novel deep learning-based
approach for automated personality detection from text. We leverage state of
the art advances in natural language understanding, namely the BERT language
model to extract contextualized word embeddings from textual data for automated
author personality detection. Our primary goal is to develop a computationally
efficient, high-performance personality prediction model which can be easily
used by a large number of people without access to huge computation resources.
Our extensive experiments with this ideology in mind, led us to develop a novel
model which feeds contextualized embeddings along with psycholinguistic
features toa Bagged-SVM classifier for personality trait prediction. Our model
outperforms the previous state of the art by 1.04% and, at the same time is
significantly more computationally efficient to train. We report our results on
the famous gold standard Essays dataset for personality detection.
",2020-10-03T09:25:51Z,http://arxiv.org/abs/2010.01309v1,"Amirmohammad Kazameini, Samin Fatehi, Yash Mehta, Sauleh Eetemadi, Erik Cambria"
"Beqi: Revitalize the Senegalese Wolof Language with a Robust Spelling
  Corrector","  The progress of Natural Language Processing (NLP), although fast in recent
years, is not at the same pace for all languages. African languages in
particular are still behind and lack automatic processing tools. Some of these
tools are very important for the development of these languages but also have
an important role in many NLP applications. This is particularly the case for
automatic spell checkers. Several approaches have been studied to address this
task and the one modeling spelling correction as a translation task from
misspelled (noisy) text to well-spelled (correct) text shows promising results.
However, this approach requires a parallel corpus of noisy data on the one hand
and correct data on the other hand, whereas Wolof is a low-resource language
and does not have such a corpus. In this paper, we present a way to address the
constraint related to the lack of data by generating synthetic data and we
present sequence-to-sequence models using Deep Learning for spelling correction
in Wolof. We evaluated these models in three different scenarios depending on
the subwording method applied to the data and showed that the latter had a
significant impact on the performance of the models, which opens the way for
future research in Wolof spelling correction.
",2023-05-15T10:28:36Z,http://arxiv.org/abs/2305.08518v1,"Derguene Mbaye, Moussa Diallo"
"Outlier Gradient Analysis: Efficiently Identifying Detrimental Training
  Samples for Deep Learning Models","  A core data-centric learning challenge is the identification of training
samples that are detrimental to model performance. Influence functions serve as
a prominent tool for this task and offer a robust framework for assessing
training data influence on model predictions. Despite their widespread use,
their high computational cost associated with calculating the inverse of the
Hessian matrix pose constraints, particularly when analyzing large-sized deep
models. In this paper, we establish a bridge between identifying detrimental
training samples via influence functions and outlier gradient detection. This
transformation not only presents a straightforward and Hessian-free formulation
but also provides insights into the role of the gradient in sample impact.
Through systematic empirical evaluations, we first validate the hypothesis of
our proposed outlier gradient analysis approach on synthetic datasets. We then
demonstrate its effectiveness in detecting mislabeled samples in vision models
and selecting data samples for improving performance of natural language
processing transformer models. We also extend its use to influential sample
identification for fine-tuning Large Language Models.
",2024-05-06T21:34:46Z,http://arxiv.org/abs/2405.03869v4,"Anshuman Chhabra, Bo Li, Jian Chen, Prasant Mohapatra, Hongfu Liu"
"Automated Medical Report Generation for ECG Data: Bridging Medical Text
  and Signal Processing with Deep Learning","  Recent advances in deep learning and natural language generation have
significantly improved image captioning, enabling automated, human-like
descriptions for visual content. In this work, we apply these captioning
techniques to generate clinician-like interpretations of ECG data. This study
leverages existing ECG datasets accompanied by free-text reports authored by
healthcare professionals (HCPs) as training data. These reports, while often
inconsistent, provide a valuable foundation for automated learning. We
introduce an encoder-decoder-based method that uses these reports to train
models to generate detailed descriptions of ECG episodes. This represents a
significant advancement in ECG analysis automation, with potential applications
in zero-shot classification and automated clinical decision support.
  The model is tested on various datasets, including both 1- and 12-lead ECGs.
It significantly outperforms the state-of-the-art reference model by Qiu et
al., achieving a METEOR score of 55.53% compared to 24.51% achieved by the
reference model. Furthermore, several key design choices are discussed,
providing a comprehensive overview of current challenges and innovations in
this domain.
  The source codes for this research are publicly available in our Git
repository https://git.zib.de/ableich/ecg-comment-generation-public
",2024-12-05T11:05:12Z,http://arxiv.org/abs/2412.04067v1,"Amnon Bleich, Antje Linnemann, Bjoern H. Diem, Tim OF Conrad"
"The NLP Cookbook: Modern Recipes for Transformer based Deep Learning
  Architectures","  In recent years, Natural Language Processing (NLP) models have achieved
phenomenal success in linguistic and semantic tasks like text classification,
machine translation, cognitive dialogue systems, information retrieval via
Natural Language Understanding (NLU), and Natural Language Generation (NLG).
This feat is primarily attributed due to the seminal Transformer architecture,
leading to designs such as BERT, GPT (I, II, III), etc. Although these
large-size models have achieved unprecedented performances, they come at high
computational costs. Consequently, some of the recent NLP architectures have
utilized concepts of transfer learning, pruning, quantization, and knowledge
distillation to achieve moderate model sizes while keeping nearly similar
performances as achieved by their predecessors. Additionally, to mitigate the
data size challenge raised by language models from a knowledge extraction
perspective, Knowledge Retrievers have been built to extricate explicit data
documents from a large corpus of databases with greater efficiency and
accuracy. Recent research has also focused on superior inference by providing
efficient attention to longer input sequences. In this paper, we summarize and
examine the current state-of-the-art (SOTA) NLP models that have been employed
for numerous NLP tasks for optimal performance and efficiency. We provide a
detailed understanding and functioning of the different architectures, a
taxonomy of NLP designs, comparative evaluations, and future directions in NLP.
",2021-03-23T22:38:20Z,http://arxiv.org/abs/2104.10640v3,"Sushant Singh, Ausif Mahmood"
"""I'm sorry Dave, I'm afraid I can't do that"": Linguistics, Statistics,
  and Natural Language Processing circa 2001","  A brief, general-audience overview of the history of natural language
processing, focusing on data-driven approaches.Topics include ""Ambiguity and
language analysis"", ""Firth things first"", ""A 'C' change"", and ""The empiricists
strike back"".
",2003-04-21T22:10:21Z,http://arxiv.org/abs/cs/0304027v1,Lillian Lee
"Improving Entity Recognition Using Ensembles of Deep Learning and
  Fine-tuned Large Language Models: A Case Study on Adverse Event Extraction
  from Multiple Sources","  Adverse event (AE) extraction following COVID-19 vaccines from text data is
crucial for monitoring and analyzing the safety profiles of immunizations.
Traditional deep learning models are adept at learning intricate feature
representations and dependencies in sequential data, but often require
extensive labeled data. In contrast, large language models (LLMs) excel in
understanding contextual information, but exhibit unstable performance on named
entity recognition tasks, possibly due to their broad but unspecific training.
This study aims to evaluate the effectiveness of LLMs and traditional deep
learning models in AE extraction, and to assess the impact of ensembling these
models on performance. In this study, we utilized reports and posts from the
VAERS (n=621), Twitter (n=9,133), and Reddit (n=131) as our corpora. Our goal
was to extract three types of entities: ""vaccine"", ""shot"", and ""ae"". We
explored and fine-tuned (except GPT-4) multiple LLMs, including GPT-2, GPT-3.5,
GPT-4, and Llama-2, as well as traditional deep learning models like RNN and
BioBERT. To enhance performance, we created ensembles of the three models with
the best performance. For evaluation, we used strict and relaxed F1 scores to
evaluate the performance for each entity type, and micro-average F1 was used to
assess the overall performance. The ensemble model achieved the highest
performance in ""vaccine"", ""shot"", and ""ae"" with strict F1-scores of 0.878,
0.930, and 0.925, respectively, along with a micro-average score of 0.903. In
conclusion, this study demonstrates the effectiveness and robustness of
ensembling fine-tuned traditional deep learning models and LLMs, for extracting
AE-related information. This study contributes to the advancement of biomedical
natural language processing, providing valuable insights into improving AE
extraction from text data for pharmacovigilance and public health surveillance.
",2024-06-26T03:56:21Z,http://arxiv.org/abs/2406.18049v1,"Yiming Li, Deepthi Viswaroopan, William He, Jianfu Li, Xu Zuo, Hua Xu, Cui Tao"
"Automated Multi-Language to English Machine Translation Using Generative
  Pre-Trained Transformers","  The task of accurate and efficient language translation is an extremely
important information processing task. Machine learning enabled and automated
translation that is accurate and fast is often a large topic of interest in the
machine learning and data science communities. In this study, we examine using
local Generative Pretrained Transformer (GPT) models to perform automated zero
shot black-box, sentence wise, multi-natural-language translation into English
text. We benchmark 16 different open-source GPT models, with no custom
fine-tuning, from the Huggingface LLM repository for translating 50 different
non-English languages into English using translated TED Talk transcripts as the
reference dataset. These GPT model inference calls are performed strictly
locally, on single A100 Nvidia GPUs. Benchmark metrics that are reported are
language translation accuracy, using BLEU, GLEU, METEOR, and chrF text overlap
measures, and wall-clock time for each sentence translation. The best overall
performing GPT model for translating into English text for the BLEU metric is
ReMM-v2-L2-13B with a mean score across all tested languages of $0.152$, for
the GLEU metric is ReMM-v2-L2-13B with a mean score across all tested languages
of $0.256$, for the chrF metric is Llama2-chat-AYT-13B with a mean score across
all tested languages of $0.448$, and for the METEOR metric is ReMM-v2-L2-13B
with a mean score across all tested languages of $0.438$.
",2024-04-23T02:19:35Z,http://arxiv.org/abs/2404.14680v1,"Elijah Pelofske, Vincent Urias, Lorie M. Liebrock"
"Profile Prediction: An Alignment-Based Pre-Training Task for Protein
  Sequence Models","  For protein sequence datasets, unlabeled data has greatly outpaced labeled
data due to the high cost of wet-lab characterization. Recent deep-learning
approaches to protein prediction have shown that pre-training on unlabeled data
can yield useful representations for downstream tasks. However, the optimal
pre-training strategy remains an open question. Instead of strictly borrowing
from natural language processing (NLP) in the form of masked or autoregressive
language modeling, we introduce a new pre-training task: directly predicting
protein profiles derived from multiple sequence alignments. Using a set of
five, standardized downstream tasks for protein models, we demonstrate that our
pre-training task along with a multi-task objective outperforms masked language
modeling alone on all five tasks. Our results suggest that protein sequence
models may benefit from leveraging biologically-inspired inductive biases that
go beyond existing language modeling techniques in NLP.
",2020-12-01T01:01:34Z,http://arxiv.org/abs/2012.00195v1,"Pascal Sturmfels, Jesse Vig, Ali Madani, Nazneen Fatema Rajani"
An Integrated Approach for Video Captioning and Applications,"  Physical computing infrastructure, data gathering, and algorithms have
recently had significant advances to extract information from images and
videos. The growth has been especially outstanding in image captioning and
video captioning. However, most of the advancements in video captioning still
take place in short videos. In this research, we caption longer videos only by
using the keyframes, which are a small subset of the total video frames.
Instead of processing thousands of frames, only a few frames are processed
depending on the number of keyframes. There is a trade-off between the
computation of many frames and the speed of the captioning process. The
approach in this research is to allow the user to specify the trade-off between
execution time and accuracy. In addition, we argue that linking images, videos,
and natural language offers many practical benefits and immediate practical
applications. From the modeling perspective, instead of designing and staging
explicit algorithms to process videos and generate captions in complex
processing pipelines, our contribution lies in designing hybrid deep learning
architectures to apply in long videos by captioning video keyframes. We
consider the technology and the methodology that we have developed as steps
toward the applications discussed in this research.
",2022-01-23T01:06:00Z,http://arxiv.org/abs/2201.09153v1,"Soheyla Amirian, Thiab R. Taha, Khaled Rasheed, Hamid R. Arabnia"
"Reinforcement Learning and Bandits for Speech and Language Processing:
  Tutorial, Review and Outlook","  In recent years, reinforcement learning and bandits have transformed a wide
range of real-world applications including healthcare, finance, recommendation
systems, robotics, and last but not least, the speech and natural language
processing. While most speech and language applications of reinforcement
learning algorithms are centered around improving the training of deep neural
networks with its flexible optimization properties, there are still many
grounds to explore to utilize the benefits of reinforcement learning, such as
its reward-driven adaptability, state representations, temporal structures and
generalizability. In this survey, we present an overview of recent advancements
of reinforcement learning and bandits, and discuss how they can be effectively
employed to solve speech and natural language processing problems with models
that are adaptive, interactive and scalable.
",2022-10-24T21:49:12Z,http://arxiv.org/abs/2210.13623v3,Baihan Lin
A Multimodal Approach for Advanced Pest Detection and Classification,"  This paper presents a novel multi modal deep learning framework for enhanced
agricultural pest detection, combining tiny-BERT's natural language processing
with R-CNN and ResNet-18's image processing. Addressing limitations of
traditional CNN-based visual methods, this approach integrates textual context
for more accurate pest identification. The R-CNN and ResNet-18 integration
tackles deep CNN issues like vanishing gradients, while tiny-BERT ensures
computational efficiency. Employing ensemble learning with linear regression
and random forest models, the framework demonstrates superior discriminate
ability, as shown in ROC and AUC analyses. This multi modal approach, blending
text and image data, significantly boosts pest detection in agriculture. The
study highlights the potential of multi modal deep learning in complex
real-world scenarios, suggesting future expansions in diversity of datasets,
advanced data augmentation, and cross-modal attention mechanisms to enhance
model performance.
",2023-12-18T05:54:20Z,http://arxiv.org/abs/2312.10948v1,"Jinli Duan, Haoyu Ding, Sung Kim"
"Temporal Embeddings and Transformer Models for Narrative Text
  Understanding","  We present two deep learning approaches to narrative text understanding for
character relationship modelling. The temporal evolution of these relations is
described by dynamic word embeddings, that are designed to learn semantic
changes over time. An empirical analysis of the corresponding character
trajectories shows that such approaches are effective in depicting dynamic
evolution. A supervised learning approach based on the state-of-the-art
transformer model BERT is used instead to detect static relations between
characters. The empirical validation shows that such events (e.g., two
characters belonging to the same family) might be spotted with good accuracy,
even when using automatically annotated data. This provides a deeper
understanding of narrative plots based on the identification of key facts.
Standard clustering techniques are finally used for character de-aliasing, a
necessary pre-processing step for both approaches. Overall, deep learning
models appear to be suitable for narrative text understanding, while also
providing a challenging and unexploited benchmark for general natural language
understanding.
",2020-03-19T14:23:12Z,http://arxiv.org/abs/2003.08811v1,"Vani K, Simone Mellace, Alessandro Antonucci"
Deep learning evaluation using deep linguistic processing,"  We discuss problems with the standard approaches to evaluation for tasks like
visual question answering, and argue that artificial data can be used to
address these as a complement to current practice. We demonstrate that with the
help of existing 'deep' linguistic processing technology we are able to create
challenging abstract datasets, which enable us to investigate the language
understanding abilities of multimodal deep learning models in detail, as
compared to a single performance value on a static and monolithic dataset.
",2017-06-05T13:53:56Z,http://arxiv.org/abs/1706.01322v2,"Alexander Kuhnle, Ann Copestake"
"Implications of Deep Circuits in Improving Quality of Quantum Question
  Answering","  Question Answering (QA) has proved to be an arduous challenge in the area of
natural language processing (NLP) and artificial intelligence (AI). Many
attempts have been made to develop complete solutions for QA as well as
improving significant sub-modules of the QA systems to improve the overall
performance through the course of time. Questions are the most important piece
of QA, because knowing the question is equivalent to knowing what counts as an
answer (Harrah in Philos Sci, 1961 [1]). In this work, we have attempted to
understand questions in a better way by using Quantum Machine Learning (QML).
The properties of Quantum Computing (QC) have enabled classically intractable
data processing. So, in this paper, we have performed question classification
on questions from two classes of SelQA (Selection-based Question Answering)
dataset using quantum-based classifier algorithms-quantum support vector
machine (QSVM) and variational quantum classifier (VQC) from Qiskit (Quantum
Information Science toolKIT) for Python. We perform classification with both
classifiers in almost similar environments and study the effects of circuit
depths while comparing the results of both classifiers. We also use these
classification results with our own rule-based QA system and observe
significant performance improvement. Hence, this experiment has helped in
improving the quality of QA in general.
",2023-05-12T10:52:13Z,http://arxiv.org/abs/2305.07374v1,"Pragya Katyayan, Nisheeth Joshi"
Self-Supervised Speech Representation Learning: A Review,"  Although supervised deep learning has revolutionized speech and audio
processing, it has necessitated the building of specialist models for
individual tasks and application scenarios. It is likewise difficult to apply
this to dialects and languages for which only limited labeled data is
available. Self-supervised representation learning methods promise a single
universal model that would benefit a wide variety of tasks and domains. Such
methods have shown success in natural language processing and computer vision
domains, achieving new levels of performance while reducing the number of
labels required for many downstream scenarios. Speech representation learning
is experiencing similar progress in three main categories: generative,
contrastive, and predictive methods. Other approaches rely on multi-modal data
for pre-training, mixing text or visual data streams with speech. Although
self-supervised speech representation is still a nascent research area, it is
closely related to acoustic word embedding and learning with zero lexical
resources, both of which have seen active research for many years. This review
presents approaches for self-supervised speech representation learning and
their connection to other research areas. Since many current methods focus
solely on automatic speech recognition as a downstream task, we review recent
efforts on benchmarking learned representations to extend the application
beyond speech recognition.
",2022-05-21T16:52:57Z,http://arxiv.org/abs/2205.10643v3,"Abdelrahman Mohamed, Hung-yi Lee, Lasse Borgholt, Jakob D. Havtorn, Joakim Edin, Christian Igel, Katrin Kirchhoff, Shang-Wen Li, Karen Livescu, Lars Maaløe, Tara N. Sainath, Shinji Watanabe"
Including Signed Languages in Natural Language Processing,"  Signed languages are the primary means of communication for many deaf and
hard of hearing individuals. Since signed languages exhibit all the fundamental
linguistic properties of natural language, we believe that tools and theories
of Natural Language Processing (NLP) are crucial towards its modeling. However,
existing research in Sign Language Processing (SLP) seldom attempt to explore
and leverage the linguistic organization of signed languages. This position
paper calls on the NLP community to include signed languages as a research area
with high social and scientific impact. We first discuss the linguistic
properties of signed languages to consider during their modeling. Then, we
review the limitations of current SLP models and identify the open challenges
to extend NLP to signed languages. Finally, we urge (1) the adoption of an
efficient tokenization method; (2) the development of linguistically-informed
models; (3) the collection of real-world signed language data; (4) the
inclusion of local signed language communities as an active and leading voice
in the direction of research.
",2021-05-11T17:37:55Z,http://arxiv.org/abs/2105.05222v2,"Kayo Yin, Amit Moryossef, Julie Hochgesang, Yoav Goldberg, Malihe Alikhani"
"A Critical Review of Physics-Informed Machine Learning Applications in
  Subsurface Energy Systems","  Machine learning has emerged as a powerful tool in various fields, including
computer vision, natural language processing, and speech recognition. It can
unravel hidden patterns within large data sets and reveal unparalleled
insights, revolutionizing many industries and disciplines. However, machine and
deep learning models lack interpretability and limited domain-specific
knowledge, especially in applications such as physics and engineering.
Alternatively, physics-informed machine learning (PIML) techniques integrate
physics principles into data-driven models. By combining deep learning with
domain knowledge, PIML improves the generalization of the model, abidance by
the governing physical laws, and interpretability. This paper comprehensively
reviews PIML applications related to subsurface energy systems, mainly in the
oil and gas industry. The review highlights the successful utilization of PIML
for tasks such as seismic applications, reservoir simulation, hydrocarbons
production forecasting, and intelligent decision-making in the exploration and
production stages. Additionally, it demonstrates PIML's capabilities to
revolutionize the oil and gas industry and other emerging areas of interest,
such as carbon and hydrogen storage; and geothermal systems by providing more
accurate and reliable predictions for resource management and operational
efficiency.
",2023-08-06T18:20:24Z,http://arxiv.org/abs/2308.04457v1,"Abdeldjalil Latrach, Mohamed Lamine Malki, Misael Morales, Mohamed Mehana, Minou Rabiei"
A Tour of TensorFlow,"  Deep learning is a branch of artificial intelligence employing deep neural
network architectures that has significantly advanced the state-of-the-art in
computer vision, speech recognition, natural language processing and other
domains. In November 2015, Google released $\textit{TensorFlow}$, an open
source deep learning software library for defining, training and deploying
machine learning models. In this paper, we review TensorFlow and put it in
context of modern deep learning concepts and software. We discuss its basic
computational paradigms and distributed execution model, its programming
interface as well as accompanying visualization toolkits. We then compare
TensorFlow to alternative libraries such as Theano, Torch or Caffe on a
qualitative as well as quantitative basis and finally comment on observed
use-cases of TensorFlow in academia and industry.
",2016-10-01T11:32:03Z,http://arxiv.org/abs/1610.01178v1,Peter Goldsborough
SELFormer: Molecular Representation Learning via SELFIES Language Models,"  Automated computational analysis of the vast chemical space is critical for
numerous fields of research such as drug discovery and material science.
Representation learning techniques have recently been employed with the primary
objective of generating compact and informative numerical expressions of
complex data. One approach to efficiently learn molecular representations is
processing string-based notations of chemicals via natural language processing
(NLP) algorithms. Majority of the methods proposed so far utilize SMILES
notations for this purpose; however, SMILES is associated with numerous
problems related to validity and robustness, which may prevent the model from
effectively uncovering the knowledge hidden in the data. In this study, we
propose SELFormer, a transformer architecture-based chemical language model
that utilizes a 100% valid, compact and expressive notation, SELFIES, as input,
in order to learn flexible and high-quality molecular representations.
SELFormer is pre-trained on two million drug-like compounds and fine-tuned for
diverse molecular property prediction tasks. Our performance evaluation has
revealed that, SELFormer outperforms all competing methods, including graph
learning-based approaches and SMILES-based chemical language models, on
predicting aqueous solubility of molecules and adverse drug reactions. We also
visualized molecular representations learned by SELFormer via dimensionality
reduction, which indicated that even the pre-trained model can discriminate
molecules with differing structural properties. We shared SELFormer as a
programmatic tool, together with its datasets and pre-trained models. Overall,
our research demonstrates the benefit of using the SELFIES notations in the
context of chemical language modeling and opens up new possibilities for the
design and discovery of novel drug candidates with desired features.
",2023-04-10T15:38:25Z,http://arxiv.org/abs/2304.04662v2,"Atakan Yüksel, Erva Ulusoy, Atabey Ünlü, Tunca Doğan"
"A scoping review on multimodal deep learning in biomedical images and
  texts","  Computer-assisted diagnostic and prognostic systems of the future should be
capable of simultaneously processing multimodal data. Multimodal deep learning
(MDL), which involves the integration of multiple sources of data, such as
images and text, has the potential to revolutionize the analysis and
interpretation of biomedical data. However, it only caught researchers'
attention recently. To this end, there is a critical need to conduct a
systematic review on this topic, identify the limitations of current work, and
explore future directions. In this scoping review, we aim to provide a
comprehensive overview of the current state of the field and identify key
concepts, types of studies, and research gaps with a focus on biomedical images
and texts joint learning, mainly because these two were the most commonly
available data types in MDL research. This study reviewed the current uses of
multimodal deep learning on five tasks: (1) Report generation, (2) Visual
question answering, (3) Cross-modal retrieval, (4) Computer-aided diagnosis,
and (5) Semantic segmentation. Our results highlight the diverse applications
and potential of MDL and suggest directions for future research in the field.
We hope our review will facilitate the collaboration of natural language
processing (NLP) and medical imaging communities and support the next
generation of decision-making and computer-assisted diagnostic system
development.
",2023-07-14T14:08:54Z,http://arxiv.org/abs/2307.07362v3,"Zhaoyi Sun, Mingquan Lin, Qingqing Zhu, Qianqian Xie, Fei Wang, Zhiyong Lu, Yifan Peng"
Large language models in bioinformatics: applications and perspectives,"  Large language models (LLMs) are a class of artificial intelligence models
based on deep learning, which have great performance in various tasks,
especially in natural language processing (NLP). Large language models
typically consist of artificial neural networks with numerous parameters,
trained on large amounts of unlabeled input using self-supervised or
semi-supervised learning. However, their potential for solving bioinformatics
problems may even exceed their proficiency in modeling human language. In this
review, we will present a summary of the prominent large language models used
in natural language processing, such as BERT and GPT, and focus on exploring
the applications of large language models at different omics levels in
bioinformatics, mainly including applications of large language models in
genomics, transcriptomics, proteomics, drug discovery and single cell analysis.
Finally, this review summarizes the potential and prospects of large language
models in solving bioinformatic problems.
",2024-01-08T17:26:59Z,http://arxiv.org/abs/2401.04155v1,"Jiajia Liu, Mengyuan Yang, Yankai Yu, Haixia Xu, Kang Li, Xiaobo Zhou"
"GluonCV and GluonNLP: Deep Learning in Computer Vision and Natural
  Language Processing","  We present GluonCV and GluonNLP, the deep learning toolkits for computer
vision and natural language processing based on Apache MXNet (incubating).
These toolkits provide state-of-the-art pre-trained models, training scripts,
and training logs, to facilitate rapid prototyping and promote reproducible
research. We also provide modular APIs with flexible building blocks to enable
efficient customization. Leveraging the MXNet ecosystem, the deep learning
models in GluonCV and GluonNLP can be deployed onto a variety of platforms with
different programming languages. The Apache 2.0 license has been adopted by
GluonCV and GluonNLP to allow for software distribution, modification, and
usage.
",2019-07-09T21:59:44Z,http://arxiv.org/abs/1907.04433v2,"Jian Guo, He He, Tong He, Leonard Lausen, Mu Li, Haibin Lin, Xingjian Shi, Chenguang Wang, Junyuan Xie, Sheng Zha, Aston Zhang, Hang Zhang, Zhi Zhang, Zhongyue Zhang, Shuai Zheng, Yi Zhu"
"Towards a neural architecture of language: Deep learning versus
  logistics of access in neural architectures for compositional processing","  Recently, a number of articles have argued that deep learning models such as
GPT could also capture key aspects of language processing in the human mind and
brain. However, I will argue that these models are not suitable as neural
models of human language. Firstly, because they fail on fundamental boundary
conditions, such as the amount of learning they require. This would in fact
imply that the mechanisms of GPT and brain language processing are
fundamentally different. Secondly, because they do not possess the logistics of
access needed for compositional and productive human language processing.
Neural architectures could possess logistics of access based on small-world
like network structures, in which processing does not consist of symbol
manipulation but of controlling the flow of activation. In this view, two
complementary approaches would be needed to investigate the relation between
brain and cognition. Investigating learning methods could reveal how 'learned
cognition' as found in deep learning could develop in the brain. However,
neural architectures with logistics of access should also be developed to
account for 'productive cognition' as required for natural or artificial human
language processing. Later on, these approaches could perhaps be combined to
see how such architectures could develop by learning and development from a
simpler basis.
",2022-10-19T13:31:26Z,http://arxiv.org/abs/2210.10543v1,Frank van der Velde
Interpreting Deep Neural Networks Through Variable Importance,"  While the success of deep neural networks (DNNs) is well-established across a
variety of domains, our ability to explain and interpret these methods is
limited. Unlike previously proposed local methods which try to explain
particular classification decisions, we focus on global interpretability and
ask a universally applicable question: given a trained model, which features
are the most important? In the context of neural networks, a feature is rarely
important on its own, so our strategy is specifically designed to leverage
partial covariance structures and incorporate variable dependence into feature
ranking. Our methodological contributions in this paper are two-fold. First, we
propose an effect size analogue for DNNs that is appropriate for applications
with highly collinear predictors (ubiquitous in computer vision). Second, we
extend the recently proposed ""RelATive cEntrality"" (RATE) measure (Crawford et
al., 2019) to the Bayesian deep learning setting. RATE applies an information
theoretic criterion to the posterior distribution of effect sizes to assess
feature significance. We apply our framework to three broad application areas:
computer vision, natural language processing, and social science.
",2019-01-28T17:34:06Z,http://arxiv.org/abs/1901.09839v3,"Jonathan Ish-Horowicz, Dana Udwin, Seth Flaxman, Sarah Filippi, Lorin Crawford"
"Harnessing the Power of Beta Scoring in Deep Active Learning for
  Multi-Label Text Classification","  Within the scope of natural language processing, the domain of multi-label
text classification is uniquely challenging due to its expansive and uneven
label distribution. The complexity deepens due to the demand for an extensive
set of annotated data for training an advanced deep learning model, especially
in specialized fields where the labeling task can be labor-intensive and often
requires domain-specific knowledge. Addressing these challenges, our study
introduces a novel deep active learning strategy, capitalizing on the Beta
family of proper scoring rules within the Expected Loss Reduction framework. It
computes the expected increase in scores using the Beta Scoring Rules, which
are then transformed into sample vector representations. These vector
representations guide the diverse selection of informative samples, directly
linking this process to the model's expected proper score. Comprehensive
evaluations across both synthetic and real datasets reveal our method's
capability to often outperform established acquisition techniques in
multi-label text classification, presenting encouraging outcomes across various
architectural and dataset scenarios.
",2024-01-15T00:06:24Z,http://arxiv.org/abs/2401.07395v1,"Wei Tan, Ngoc Dang Nguyen, Lan Du, Wray Buntine"
Morphosyntactic Analysis for CHILDES,"  Language development researchers are interested in comparing the process of
language learning across languages. Unfortunately, it has been difficult to
construct a consistent quantitative framework for such comparisons. However,
recent advances in AI (Artificial Intelligence) and ML (Machine Learning) are
providing new methods for ASR (automatic speech recognition) and NLP (natural
language processing) that can be brought to bear on this problem. Using the
Batchalign2 program (Liu et al., 2023), we have been transcribing and linking
data for the CHILDES database and have applied the UD (Universal Dependencies)
framework to provide a consistent and comparable morphosyntactic analysis for
27 languages. These new resources open possibilities for deeper crosslinguistic
study of language learning.
",2024-07-17T08:11:24Z,http://arxiv.org/abs/2407.12389v1,"Houjun Liu, Brian MacWhinney"
"Can Large Language Models Aid in Annotating Speech Emotional Data?
  Uncovering New Frontiers","  Despite recent advancements in speech emotion recognition (SER) models,
state-of-the-art deep learning (DL) approaches face the challenge of the
limited availability of annotated data. Large language models (LLMs) have
revolutionised our understanding of natural language, introducing emergent
properties that broaden comprehension in language, speech, and vision. This
paper examines the potential of LLMs to annotate abundant speech data, aiming
to enhance the state-of-the-art in SER. We evaluate this capability across
various settings using publicly available speech emotion classification
datasets. Leveraging ChatGPT, we experimentally demonstrate the promising role
of LLMs in speech emotion data annotation. Our evaluation encompasses
single-shot and few-shots scenarios, revealing performance variability in SER.
Notably, we achieve improved results through data augmentation, incorporating
ChatGPT-annotated samples into existing datasets. Our work uncovers new
frontiers in speech emotion classification, highlighting the increasing
significance of LLMs in this field moving forward.
",2023-07-12T11:27:40Z,http://arxiv.org/abs/2307.06090v3,"Siddique Latif, Muhammad Usama, Mohammad Ibrahim Malik, Björn W. Schuller"
"LinkTransformer: A Unified Package for Record Linkage with Transformer
  Language Models","  Linking information across sources is fundamental to a variety of analyses in
social science, business, and government. While large language models (LLMs)
offer enormous promise for improving record linkage in noisy datasets, in many
domains approximate string matching packages in popular softwares such as R and
Stata remain predominant. These packages have clean, simple interfaces and can
be easily extended to a diversity of languages. Our open-source package
LinkTransformer aims to extend the familiarity and ease-of-use of popular
string matching methods to deep learning. It is a general purpose package for
record linkage with transformer LLMs that treats record linkage as a text
retrieval problem. At its core is an off-the-shelf toolkit for applying
transformer models to record linkage with four lines of code. LinkTransformer
contains a rich repository of pre-trained transformer semantic similarity
models for multiple languages and supports easy integration of any transformer
language model from Hugging Face or OpenAI. It supports standard functionality
such as blocking and linking on multiple noisy fields. LinkTransformer APIs
also perform other common text data processing tasks, e.g., aggregation, noisy
de-duplication, and translation-free cross-lingual linkage. Importantly,
LinkTransformer also contains comprehensive tools for efficient model tuning,
to facilitate different levels of customization when off-the-shelf models do
not provide the required accuracy. Finally, to promote reusability,
reproducibility, and extensibility, LinkTransformer makes it easy for users to
contribute their custom-trained models to its model hub. By combining
transformer language models with intuitive APIs that will be familiar to many
users of popular string matching packages, LinkTransformer aims to democratize
the benefits of LLMs among those who may be less familiar with deep learning
frameworks.
",2023-09-02T01:45:27Z,http://arxiv.org/abs/2309.00789v2,"Abhishek Arora, Melissa Dell"
"Nemesyst: A Hybrid Parallelism Deep Learning-Based Framework Applied for
  Internet of Things Enabled Food Retailing Refrigeration Systems","  Deep Learning has attracted considerable attention across multiple
application domains, including computer vision, signal processing and natural
language processing. Although quite a few single node deep learning frameworks
exist, such as tensorflow, pytorch and keras, we still lack a complete
processing structure that can accommodate large scale data processing, version
control, and deployment, all while staying agnostic of any specific single node
framework. To bridge this gap, this paper proposes a new, higher level
framework, i.e. Nemesyst, which uses databases along with model
sequentialisation to allow processes to be fed unique and transformed data at
the point of need. This facilitates near real-time application and makes models
available for further training or use at any node that has access to the
database simultaneously. Nemesyst is well suited as an application framework
for internet of things aggregated control systems, deploying deep learning
techniques to optimise individual machines in massive networks. To demonstrate
this framework, we adopted a case study in a novel domain; deploying deep
learning to optimise the high speed control of electrical power consumed by a
massive internet of things network of retail refrigeration systems in
proportion to load available on the UK National Grid (a demand side response).
The case study demonstrated for the first time in such a setting how deep
learning models, such as Recurrent Neural Networks (vanilla and Long-Short-Term
Memory) and Generative Adversarial Networks paired with Nemesyst, achieve
compelling performance, whilst still being malleable to future adjustments as
both the data and requirements inevitably change over time.
",2019-06-04T17:23:09Z,http://arxiv.org/abs/1906.01600v2,"George Onoufriou, Ronald Bickerton, Simon Pearson, Georgios Leontidis"
"Deep Learning for Quantile Regression under Right Censoring:
  DeepQuantreg","  The computational prediction algorithm of neural network, or deep learning,
has drawn much attention recently in statistics as well as in image recognition
and natural language processing. Particularly in statistical application for
censored survival data, the loss function used for optimization has been mainly
based on the partial likelihood from Cox's model and its variations to utilize
existing neural network library such as Keras, which was built upon the open
source library of TensorFlow. This paper presents a novel application of the
neural network to the quantile regression for survival data with right
censoring, which is adjusted by the inverse of the estimated censoring
distribution in the check function. The main purpose of this work is to show
that the deep learning method could be flexible enough to predict nonlinear
patterns more accurately compared to existing quantile regression methods such
as traditional linear quantile regression and nonparametric quantile regression
with total variation regularization, emphasizing practicality of the method for
censored survival data. Simulation studies were performed to generate nonlinear
censored survival data and compare the deep learning method with existing
quantile regression methods in terms of prediction accuracy. The proposed
method is illustrated with two publicly available breast cancer data sets with
gene signatures. The method has been built into a package and is freely
available at \url{https://github.com/yicjia/DeepQuantreg}.
",2020-07-14T14:31:11Z,http://arxiv.org/abs/2007.07056v2,"Yichen Jia, Jong-Hyeon Jeong"
"Around the GLOBE: Numerical Aggregation Question-Answering on
  Heterogeneous Genealogical Knowledge Graphs with Deep Neural Networks","  One of the key AI tools for textual corpora exploration is natural language
question-answering (QA). Unlike keyword-based search engines, QA algorithms
receive and process natural language questions and produce precise answers to
these questions, rather than long lists of documents that need to be manually
scanned by the users. State-of-the-art QA algorithms based on DNNs were
successfully employed in various domains. However, QA in the genealogical
domain is still underexplored, while researchers in this field (and other
fields in humanities and social sciences) can highly benefit from the ability
to ask questions in natural language, receive concrete answers and gain
insights hidden within large corpora. While some research has been recently
conducted for factual QA in the genealogical domain, to the best of our
knowledge, there is no previous research on the more challenging task of
numerical aggregation QA (i.e., answering questions combining aggregation
functions, e.g., count, average, max). Numerical aggregation QA is critical for
distant reading and analysis for researchers (and the general public)
interested in investigating cultural heritage domains. Therefore, in this
study, we present a new end-to-end methodology for numerical aggregation QA for
genealogical trees that includes: 1) an automatic method for training dataset
generation; 2) a transformer-based table selection method, and 3) an optimized
transformer-based numerical aggregation QA model. The findings indicate that
the proposed architecture, GLOBE, outperforms the state-of-the-art models and
pipelines by achieving 87% accuracy for this task compared to only 21% by
current state-of-the-art models. This study may have practical implications for
genealogical information centers and museums, making genealogical data research
easy and scalable for experts as well as the general public.
",2023-07-30T12:09:00Z,http://arxiv.org/abs/2307.16208v1,"Omri Suissa, Maayan Zhitomirsky-Geffet, Avshalom Elmalech"
"A natural language processing-based approach: mapping human perception
  by understanding deep semantic features in street view images","  In the past decade, using Street View images and machine learning to measure
human perception has become a mainstream research approach in urban science.
However, this approach using only image-shallow information makes it difficult
to comprehensively understand the deep semantic features of human perception of
a scene. In this study, we proposed a new framework based on a pre-train
natural language model to understand the relationship between human perception
and the sense of a scene. Firstly, Place Pulse 2.0 was used as our base
dataset, which contains a variety of human-perceived labels, namely, beautiful,
safe, wealthy, depressing, boring, and lively. An image captioning network was
used to extract the description information of each street view image.
Secondly, a pre-trained BERT model was finetuning and added a regression
function for six human perceptual dimensions. Furthermore, we compared the
performance of five traditional regression methods with our approach and
conducted a migration experiment in Hong Kong. Our results show that human
perception scoring by deep semantic features performed better than previous
studies by machine learning methods with shallow features. The use of deep
scene semantic features provides new ideas for subsequent human perception
research, as well as better explanatory power in the face of spatial
heterogeneity.
",2023-11-29T05:00:43Z,http://arxiv.org/abs/2311.17354v1,"Haoran Ma, Dongdong Wu"
Decoding the Diversity: A Review of the Indic AI Research Landscape,"  This review paper provides a comprehensive overview of large language model
(LLM) research directions within Indic languages. Indic languages are those
spoken in the Indian subcontinent, including India, Pakistan, Bangladesh, Sri
Lanka, Nepal, and Bhutan, among others. These languages have a rich cultural
and linguistic heritage and are spoken by over 1.5 billion people worldwide.
With the tremendous market potential and growing demand for natural language
processing (NLP) based applications in diverse languages, generative
applications for Indic languages pose unique challenges and opportunities for
research. Our paper deep dives into the recent advancements in Indic generative
modeling, contributing with a taxonomy of research directions, tabulating 84
recent publications. Research directions surveyed in this paper include LLM
development, fine-tuning existing LLMs, development of corpora, benchmarking
and evaluation, as well as publications around specific techniques, tools, and
applications. We found that researchers across the publications emphasize the
challenges associated with limited data availability, lack of standardization,
and the peculiar linguistic complexities of Indic languages. This work aims to
serve as a valuable resource for researchers and practitioners working in the
field of NLP, particularly those focused on Indic languages, and contributes to
the development of more accurate and efficient LLM applications for these
languages.
",2024-06-13T19:55:20Z,http://arxiv.org/abs/2406.09559v1,"Sankalp KJ, Vinija Jain, Sreyoshi Bhaduri, Tamoghna Roy, Aman Chadha"
"Deep Learning for Plasma Tomography and Disruption Prediction from
  Bolometer Data","  The use of deep learning is facilitating a wide range of data processing
tasks in many areas. The analysis of fusion data is no exception, since there
is a need to process large amounts of data collected from the diagnostic
systems attached to a fusion device. Fusion data involves images and time
series, and are a natural candidate for the use of convolutional and recurrent
neural networks. In this work, we describe how CNNs can be used to reconstruct
the plasma radiation profile, and we discuss the potential of using RNNs for
disruption prediction based on the same input data. Both approaches have been
applied at JET using data from a multi-channel diagnostic system. Similar
approaches can be applied to other fusion devices and diagnostics.
",2019-10-27T11:37:24Z,http://arxiv.org/abs/1910.13257v1,"Diogo R. Ferreira, Pedro J. Carvalho, Horácio Fernandes"
"Adaptivity of deep ReLU network for learning in Besov and mixed smooth
  Besov spaces: optimal rate and curse of dimensionality","  Deep learning has shown high performances in various types of tasks from
visual recognition to natural language processing, which indicates superior
flexibility and adaptivity of deep learning. To understand this phenomenon
theoretically, we develop a new approximation and estimation error analysis of
deep learning with the ReLU activation for functions in a Besov space and its
variant with mixed smoothness. The Besov space is a considerably general
function space including the Holder space and Sobolev space, and especially can
capture spatial inhomogeneity of smoothness. Through the analysis in the Besov
space, it is shown that deep learning can achieve the minimax optimal rate and
outperform any non-adaptive (linear) estimator such as kernel ridge regression,
which shows that deep learning has higher adaptivity to the spatial
inhomogeneity of the target function than other estimators such as linear ones.
In addition to this, it is shown that deep learning can avoid the curse of
dimensionality if the target function is in a mixed smooth Besov space. We also
show that the dependency of the convergence rate on the dimensionality is tight
due to its minimax optimality. These results support high adaptivity of deep
learning and its superior ability as a feature extractor.
",2018-10-18T13:17:20Z,http://arxiv.org/abs/1810.08033v1,Taiji Suzuki
"Autonomous Droplet Microfluidic Design Framework with Large Language
  Models","  Droplet-based microfluidic devices have substantial promise as cost-effective
alternatives to current assessment tools in biological research. Moreover,
machine learning models that leverage tabular data, including input design
parameters and their corresponding efficiency outputs, are increasingly
utilised to automate the design process of these devices and to predict their
performance. However, these models fail to fully leverage the data presented in
the tables, neglecting crucial contextual information, including column
headings and their associated descriptions. This study presents
MicroFluidic-LLMs, a framework designed for processing and feature extraction,
which effectively captures contextual information from tabular data formats.
MicroFluidic-LLMs overcomes processing challenges by transforming the content
into a linguistic format and leveraging pre-trained large language models
(LLMs) for analysis. We evaluate our MicroFluidic-LLMs framework on 11
prediction tasks, covering aspects such as geometry, flow conditions, regimes,
and performance, utilising a publicly available dataset on flow-focusing
droplet microfluidics. We demonstrate that our MicroFluidic-LLMs framework can
empower deep neural network models to be highly effective and straightforward
while minimising the need for extensive data preprocessing. Moreover, the
exceptional performance of deep neural network models, particularly when
combined with advanced natural language processing models such as DistilBERT
and GPT-2, reduces the mean absolute error in the droplet diameter and
generation rate by nearly 5- and 7-fold, respectively, and enhances the regime
classification accuracy by over 4%, compared with the performance reported in a
previous study. This study lays the foundation for the huge potential
applications of LLMs and machine learning in a wider spectrum of microfluidic
applications.
",2024-11-11T03:20:53Z,http://arxiv.org/abs/2411.06691v1,"Dinh-Nguyen Nguyen, Raymond Kai-Yu Tong, Ngoc-Duy Dinh"
A Survey of Active Learning for Natural Language Processing,"  In this work, we provide a survey of active learning (AL) for its
applications in natural language processing (NLP). In addition to a
fine-grained categorization of query strategies, we also investigate several
other important aspects of applying AL to NLP problems. These include AL for
structured prediction tasks, annotation cost, model learning (especially with
deep neural models), and starting and stopping AL. Finally, we conclude with a
discussion of related topics and future directions.
",2022-10-18T19:14:42Z,http://arxiv.org/abs/2210.10109v2,"Zhisong Zhang, Emma Strubell, Eduard Hovy"
"Process Knowledge-infused Learning for Suicidality Assessment on Social
  Media","  Improving the performance and natural language explanations of deep learning
algorithms is a priority for adoption by humans in the real world. In several
domains, such as healthcare, such technology has significant potential to
reduce the burden on humans by providing quality assistance at scale. However,
current methods rely on the traditional pipeline of predicting labels from
data, thus completely ignoring the process and guidelines used to obtain the
labels. Furthermore, post hoc explanations on the data to label prediction
using explainable AI (XAI) models, while satisfactory to computer scientists,
leave much to be desired to the end-users due to lacking explanations of the
process in terms of human-understandable concepts. We \textit{introduce},
\textit{formalize}, and \textit{develop} a novel Artificial Intelligence (A)
paradigm -- Process Knowledge-infused Learning (PK-iL). PK-iL utilizes a
structured process knowledge that explicitly explains the underlying prediction
process that makes sense to end-users. The qualitative human evaluation
confirms through a annotator agreement of 0.72, that humans are understand
explanations for the predictions. PK-iL also performs competitively with the
state-of-the-art (SOTA) baselines.
",2022-04-26T19:43:41Z,http://arxiv.org/abs/2204.12560v1,"Kaushik Roy, Manas Gaur, Qi Zhang, Amit Sheth"
Natural Language Processing using Hadoop and KOSHIK,"  Natural language processing, as a data analytics related technology, is used
widely in many research areas such as artificial intelligence, human language
processing, and translation. At present, due to explosive growth of data, there
are many challenges for natural language processing. Hadoop is one of the
platforms that can process the large amount of data required for natural
language processing. KOSHIK is one of the natural language processing
architectures, and utilizes Hadoop and contains language processing components
such as Stanford CoreNLP and OpenNLP. This study describes how to build a
KOSHIK platform with the relevant tools, and provides the steps to analyze wiki
data. Finally, it evaluates and discusses the advantages and disadvantages of
the KOSHIK architecture, and gives recommendations on improving the processing
performance.
",2016-08-15T23:09:21Z,http://arxiv.org/abs/1608.04434v1,"Emre Erturk, Hong Shi"
"Measuring Fine-Grained Domain Relevance of Terms: A Hierarchical
  Core-Fringe Approach","  We propose to measure fine-grained domain relevance - the degree that a term
is relevant to a broad (e.g., computer science) or narrow (e.g., deep learning)
domain. Such measurement is crucial for many downstream tasks in natural
language processing. To handle long-tail terms, we build a core-anchored
semantic graph, which uses core terms with rich description information to
bridge the vast remaining fringe terms semantically. To support a fine-grained
domain without relying on a matching corpus for supervision, we develop
hierarchical core-fringe learning, which learns core and fringe terms jointly
in a semi-supervised manner contextualized in the hierarchy of the domain. To
reduce expensive human efforts, we employ automatic annotation and hierarchical
positive-unlabeled learning. Our approach applies to big or small domains,
covers head or tail terms, and requires little human effort. Extensive
experiments demonstrate that our methods outperform strong baselines and even
surpass professional human performance.
",2021-05-27T15:52:34Z,http://arxiv.org/abs/2105.13255v1,"Jie Huang, Kevin Chen-Chuan Chang, Jinjun Xiong, Wen-mei Hwu"
"Generating Feasible and Plausible Counterfactual Explanations for
  Outcome Prediction of Business Processes","  In recent years, various machine and deep learning architectures have been
successfully introduced to the field of predictive process analytics.
Nevertheless, the inherent opacity of these algorithms poses a significant
challenge for human decision-makers, hindering their ability to understand the
reasoning behind the predictions. This growing concern has sparked the
introduction of counterfactual explanations, designed as human-understandable
what if scenarios, to provide clearer insights into the decision-making process
behind undesirable predictions. The generation of counterfactual explanations,
however, encounters specific challenges when dealing with the sequential nature
of the (business) process cases typically used in predictive process analytics.
Our paper tackles this challenge by introducing a data-driven approach,
REVISEDplus, to generate more feasible and plausible counterfactual
explanations. First, we restrict the counterfactual algorithm to generate
counterfactuals that lie within a high-density region of the process data,
ensuring that the proposed counterfactuals are realistic and feasible within
the observed process data distribution. Additionally, we ensure plausibility by
learning sequential patterns between the activities in the process cases,
utilising Declare language templates. Finally, we evaluate the properties that
define the validity of counterfactuals.
",2024-03-14T09:56:35Z,http://arxiv.org/abs/2403.09232v1,"Alexander Stevens, Chun Ouyang, Johannes De Smedt, Catarina Moreira"
"Semantic Similarity Measure of Natural Language Text through Machine
  Learning and a Keyword-Aware Cross-Encoder-Ranking Summarizer -- A Case Study
  Using UCGIS GIS&T Body of Knowledge","  Initiated by the University Consortium of Geographic Information Science
(UCGIS), GIS&T Body of Knowledge (BoK) is a community-driven endeavor to
define, develop, and document geospatial topics related to geographic
information science and technologies (GIS&T). In recent years, GIS&T BoK has
undergone rigorous development in terms of its topic re-organization and
content updating, resulting in a new digital version of the project. While the
BoK topics provide useful materials for researchers and students to learn about
GIS, the semantic relationships among the topics, such as semantic similarity,
should also be identified so that a better and automated topic navigation can
be achieved. Currently, the related topics are either defined manually by
editors or authors, which may result in an incomplete assessment of topic
relationship. To address this challenge, our research evaluates the
effectiveness of multiple natural language processing (NLP) techniques in
extracting semantics from text, including both deep neural networks and
traditional machine learning approaches. Besides, a novel text summarization -
KACERS (Keyword-Aware Cross-Encoder-Ranking Summarizer) - is proposed to
generate a semantic summary of scientific publications. By identifying the
semantic linkages among key topics, this work provides guidance for future
development and content organization of the GIS&T BoK project. It also offers a
new perspective on the use of machine learning techniques for analyzing
scientific publications, and demonstrate the potential of KACERS summarizer in
semantic understanding of long text documents.
",2023-05-17T01:17:57Z,http://arxiv.org/abs/2305.09877v1,"Yuanyuan Tian, Wenwen Li, Sizhe Wang, Zhining Gu"
Pre-trained Language Model Based Active Learning for Sentence Matching,"  Active learning is able to significantly reduce the annotation cost for
data-driven techniques. However, previous active learning approaches for
natural language processing mainly depend on the entropy-based uncertainty
criterion, and ignore the characteristics of natural language. In this paper,
we propose a pre-trained language model based active learning approach for
sentence matching. Differing from previous active learning, it can provide
linguistic criteria to measure instances and help select more efficient
instances for annotation. Experiments demonstrate our approach can achieve
greater accuracy with fewer labeled training instances.
",2020-10-12T08:24:36Z,http://arxiv.org/abs/2010.05522v1,"Guirong Bai, Shizhu He, Kang Liu, Jun Zhao, Zaiqing Nie"
Language Models sounds the Death Knell of Knowledge Graphs,"  Healthcare domain generates a lot of unstructured and semi-structured text.
Natural Language processing (NLP) has been used extensively to process this
data. Deep Learning based NLP especially Large Language Models (LLMs) such as
BERT have found broad acceptance and are used extensively for many
applications. A Language Model is a probability distribution over a word
sequence. Self-supervised Learning on a large corpus of data automatically
generates deep learning-based language models. BioBERT and Med-BERT are
language models pre-trained for the healthcare domain. Healthcare uses typical
NLP tasks such as question answering, information extraction, named entity
recognition, and search to simplify and improve processes. However, to ensure
robust application of the results, NLP practitioners need to normalize and
standardize them. One of the main ways of achieving normalization and
standardization is the use of Knowledge Graphs. A Knowledge Graph captures
concepts and their relationships for a specific domain, but their creation is
time-consuming and requires manual intervention from domain experts, which can
prove expensive. SNOMED CT (Systematized Nomenclature of Medicine -- Clinical
Terms), Unified Medical Language System (UMLS), and Gene Ontology (GO) are
popular ontologies from the healthcare domain. SNOMED CT and UMLS capture
concepts such as disease, symptoms and diagnosis and GO is the world's largest
source of information on the functions of genes. Healthcare has been dealing
with an explosion in information about different types of drugs, diseases, and
procedures. This paper argues that using Knowledge Graphs is not the best
solution for solving problems in this domain. We present experiments using LLMs
for the healthcare domain to demonstrate that language models provide the same
functionality as knowledge graphs, thereby making knowledge graphs redundant.
",2023-01-10T14:20:15Z,http://arxiv.org/abs/2301.03980v1,"Kunal Suri, Atul Singh, Prakhar Mishra, Swapna Sourav Rout, Rajesh Sabapathy"
"Survey of different Large Language Model Architectures: Trends,
  Benchmarks, and Challenges","  Large Language Models (LLMs) represent a class of deep learning models adept
at understanding natural language and generating coherent responses to various
prompts or queries. These models far exceed the complexity of conventional
neural networks, often encompassing dozens of neural network layers and
containing billions to trillions of parameters. They are typically trained on
vast datasets, utilizing architectures based on transformer blocks. Present-day
LLMs are multi-functional, capable of performing a range of tasks from text
generation and language translation to question answering, as well as code
generation and analysis. An advanced subset of these models, known as
Multimodal Large Language Models (MLLMs), extends LLM capabilities to process
and interpret multiple data modalities, including images, audio, and video.
This enhancement empowers MLLMs with capabilities like video editing, image
comprehension, and captioning for visual content. This survey provides a
comprehensive overview of the recent advancements in LLMs. We begin by tracing
the evolution of LLMs and subsequently delve into the advent and nuances of
MLLMs. We analyze emerging state-of-the-art MLLMs, exploring their technical
features, strengths, and limitations. Additionally, we present a comparative
analysis of these models and discuss their challenges, potential limitations,
and prospects for future development.
",2024-12-04T11:14:06Z,http://arxiv.org/abs/2412.03220v1,"Minghao Shao, Abdul Basit, Ramesh Karri, Muhammad Shafique"
Unsupervised Sentiment Analysis of Plastic Surgery Social Media Posts,"  The massive collection of user posts across social media platforms is
primarily untapped for artificial intelligence (AI) use cases based on the
sheer volume and velocity of textual data. Natural language processing (NLP) is
a subfield of AI that leverages bodies of documents, known as corpora, to train
computers in human-like language understanding. Using a word ranking method,
term frequency-inverse document frequency (TF-IDF), to create features across
documents, it is possible to perform unsupervised analytics, machine learning
(ML) that can group the documents without a human manually labeling the data.
For large datasets with thousands of features, t-distributed stochastic
neighbor embedding (t-SNE), k-means clustering and Latent Dirichlet allocation
(LDA) are employed to learn top words and generate topics for a Reddit and
Twitter combined corpus. Using extremely simple deep learning models, this
study demonstrates that the applied results of unsupervised analysis allow a
computer to predict either negative, positive, or neutral user sentiment
towards plastic surgery based on a tweet or subreddit post with almost 90%
accuracy. Furthermore, the model is capable of achieving higher accuracy on the
unsupervised sentiment task than on a rudimentary supervised document
classification task. Therefore, unsupervised learning may be considered a
viable option in labeling social media documents for NLP tasks.
",2023-07-05T20:16:20Z,http://arxiv.org/abs/2307.02640v1,Alexandrea K. Ramnarine
Sexism Detection on a Data Diet,"  There is an increase in the proliferation of online hate commensurate with
the rise in the usage of social media. In response, there is also a significant
advancement in the creation of automated tools aimed at identifying harmful
text content using approaches grounded in Natural Language Processing and Deep
Learning. Although it is known that training Deep Learning models require a
substantial amount of annotated data, recent line of work suggests that models
trained on specific subsets of the data still retain performance comparable to
the model that was trained on the full dataset. In this work, we show how we
can leverage influence scores to estimate the importance of a data point while
training a model and designing a pruning strategy applied to the case of sexism
detection. We evaluate the model performance trained on data pruned with
different pruning strategies on three out-of-domain datasets and find, that in
accordance with other work a large fraction of instances can be removed without
significant performance drop. However, we also discover that the strategies for
pruning data, previously successful in Natural Language Inference tasks, do not
readily apply to the detection of harmful content and instead amplify the
already prevalent class imbalance even more, leading in the worst-case to a
complete absence of the hateful class.
",2024-06-07T12:39:54Z,http://arxiv.org/abs/2406.04892v1,"Rabiraj Bandyopadhyay, Dennis Assenmacher, Jose M. Alonso Moral, Claudia Wagner"
"Probabilistic Generative Transformer Language models for Generative
  Design of Molecules","  Self-supervised neural language models have recently found wide applications
in generative design of organic molecules and protein sequences as well as
representation learning for downstream structure classification and functional
prediction. However, most of the existing deep learning models for molecule
design usually require a big dataset and have a black-box architecture, which
makes it difficult to interpret their design logic. Here we propose Generative
Molecular Transformer (GMTransformer), a probabilistic neural network model for
generative design of molecules. Our model is built on the blank filling
language model originally developed for text processing, which has demonstrated
unique advantages in learning the ""molecules grammars"" with high-quality
generation, interpretability, and data efficiency. Benchmarked on the MOSES
datasets, our models achieve high novelty and Scaf compared to other baselines.
The probabilistic generation steps have the potential in tinkering molecule
design due to their capability of recommending how to modify existing molecules
with explanation, guided by the learned implicit molecule chemistry. The source
code and datasets can be accessed freely at
https://github.com/usccolumbia/GMTransformer
",2022-09-20T01:51:57Z,http://arxiv.org/abs/2209.09406v1,"Lai Wei, Nihang Fu, Yuqi Song, Qian Wang, Jianjun Hu"
HELM: Hierarchical Encoding for mRNA Language Modeling,"  Messenger RNA (mRNA) plays a crucial role in protein synthesis, with its
codon structure directly impacting biological properties. While Language Models
(LMs) have shown promise in analyzing biological sequences, existing approaches
fail to account for the hierarchical nature of mRNA's codon structure. We
introduce Hierarchical Encoding for mRNA Language Modeling (HELM), a novel
pre-training strategy that incorporates codon-level hierarchical structure into
language model training. HELM modulates the loss function based on codon
synonymity, aligning the model's learning process with the biological reality
of mRNA sequences. We evaluate HELM on diverse mRNA datasets and tasks,
demonstrating that HELM outperforms standard language model pre-training as
well as existing foundation model baselines on six diverse downstream property
prediction tasks and an antibody region annotation tasks on average by around
8\%. Additionally, HELM enhances the generative capabilities of language model,
producing diverse mRNA sequences that better align with the underlying true
data distribution compared to non-hierarchical baselines.
",2024-10-16T11:16:47Z,http://arxiv.org/abs/2410.12459v1,"Mehdi Yazdani-Jahromi, Mangal Prakash, Tommaso Mansi, Artem Moskalev, Rui Liao"
"Explaining NonLinear Classification Decisions with Deep Taylor
  Decomposition","  Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard
for various challenging machine learning problems, e.g., image classification,
natural language processing or human action recognition. Although these methods
perform impressively well, they have a significant disadvantage, the lack of
transparency, limiting the interpretability of the solution and thus the scope
of application in practice. Especially DNNs act as black boxes due to their
multilayer nonlinear structure. In this paper we introduce a novel methodology
for interpreting generic multilayer neural networks by decomposing the network
classification decision into contributions of its input elements. Although our
focus is on image classification, the method is applicable to a broad set of
input data, learning tasks and network architectures. Our method is based on
deep Taylor decomposition and efficiently utilizes the structure of the network
by backpropagating the explanations from the output to the input layer. We
evaluate the proposed method empirically on the MNIST and ILSVRC data sets.
",2015-12-08T14:25:29Z,http://arxiv.org/abs/1512.02479v1,"Grégoire Montavon, Sebastian Bach, Alexander Binder, Wojciech Samek, Klaus-Robert Müller"
Deep Leakage from Gradients,"  Exchanging gradients is a widely used method in modern multi-node machine
learning system (e.g., distributed training, collaborative learning). For a
long time, people believed that gradients are safe to share: i.e., the training
data will not be leaked by gradient exchange. However, we show that it is
possible to obtain the private training data from the publicly shared
gradients. We name this leakage as Deep Leakage from Gradient and empirically
validate the effectiveness on both computer vision and natural language
processing tasks. Experimental results show that our attack is much stronger
than previous approaches: the recovery is pixel-wise accurate for images and
token-wise matching for texts. We want to raise people's awareness to rethink
the gradient's safety. Finally, we discuss several possible strategies to
prevent such deep leakage. The most effective defense method is gradient
pruning.
",2019-06-21T03:46:43Z,http://arxiv.org/abs/1906.08935v2,"Ligeng Zhu, Zhijian Liu, Song Han"
"Pre-Learning Environment Representations for Data-Efficient Neural
  Instruction Following","  We consider the problem of learning to map from natural language instructions
to state transitions (actions) in a data-efficient manner. Our method takes
inspiration from the idea that it should be easier to ground language to
concepts that have already been formed through pre-linguistic observation. We
augment a baseline instruction-following learner with an initial
environment-learning phase that uses observations of language-free state
transitions to induce a suitable latent representation of actions before
processing the instruction-following training data. We show that mapping to
pre-learned representations substantially improves performance over systems
whose representations are learned from limited instructional data alone.
",2019-07-23T03:11:07Z,http://arxiv.org/abs/1907.09671v1,"David Gaddy, Dan Klein"
"Who's to say what's funny? A computer using Language Models and Deep
  Learning, That's Who!","  Humor is a defining characteristic of human beings. Our goal is to develop
methods that automatically detect humorous statements and rank them on a
continuous scale. In this paper we report on results using a Language Model
approach, and outline our plans for using methods from Deep Learning.
",2017-05-29T16:20:21Z,http://arxiv.org/abs/1705.10272v1,"Xinru Yan, Ted Pedersen"
Investigating Masking-based Data Generation in Language Models,"  The current era of natural language processing (NLP) has been defined by the
prominence of pre-trained language models since the advent of BERT. A feature
of BERT and models with similar architecture is the objective of masked
language modeling, in which part of the input is intentionally masked and the
model is trained to predict this piece of masked information. Data augmentation
is a data-driven technique widely used in machine learning, including research
areas like computer vision and natural language processing, to improve model
performance by artificially augmenting the training data set by designated
techniques. Masked language models (MLM), an essential training feature of
BERT, have introduced a novel approach to perform effective pre-training on
Transformer based models in natural language processing tasks. Recent studies
have utilized masked language model to generate artificially augmented data for
NLP downstream tasks. The experimental results show that Mask based data
augmentation method provides a simple but efficient approach to improve the
model performance. In this paper, we explore and discuss the broader
utilization of these data augmentation methods based on MLM.
",2023-06-16T16:48:27Z,http://arxiv.org/abs/2307.00008v1,Ed S. Ma
A Deep Neural Network Approach To Parallel Sentence Extraction,"  Parallel sentence extraction is a task addressing the data sparsity problem
found in multilingual natural language processing applications. We propose an
end-to-end deep neural network approach to detect translational equivalence
between sentences in two different languages. In contrast to previous
approaches, which typically rely on multiples models and various word alignment
features, by leveraging continuous vector representation of sentences we remove
the need of any domain specific feature engineering. Using a siamese
bidirectional recurrent neural networks, our results against a strong baseline
based on a state-of-the-art parallel sentence extraction system show a
significant improvement in both the quality of the extracted parallel sentences
and the translation performance of statistical machine translation systems. We
believe this study is the first one to investigate deep learning for the
parallel sentence extraction task.
",2017-09-28T02:09:04Z,http://arxiv.org/abs/1709.09783v1,"Francis Grégoire, Philippe Langlais"
"Detection of Hate Speech using BERT and Hate Speech Word Embedding with
  Deep Model","  The enormous amount of data being generated on the web and social media has
increased the demand for detecting online hate speech. Detecting hate speech
will reduce their negative impact and influence on others. A lot of effort in
the Natural Language Processing (NLP) domain aimed to detect hate speech in
general or detect specific hate speech such as religion, race, gender, or
sexual orientation. Hate communities tend to use abbreviations, intentional
spelling mistakes, and coded words in their communication to evade detection,
adding more challenges to hate speech detection tasks. Thus, word
representation will play an increasingly pivotal role in detecting hate speech.
This paper investigates the feasibility of leveraging domain-specific word
embedding in Bidirectional LSTM based deep model to automatically
detect/classify hate speech. Furthermore, we investigate the use of the
transfer learning language model (BERT) on hate speech problem as a binary
classification task. The experiments showed that domainspecific word embedding
with the Bidirectional LSTM based deep model achieved a 93% f1-score while BERT
achieved up to 96% f1-score on a combined balanced dataset from available hate
speech datasets.
",2021-11-02T11:42:54Z,http://arxiv.org/abs/2111.01515v1,"Hind Saleh, Areej Alhothali, Kawthar Moria"
"Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing?
  A Structured Review","  Advocates for Neuro-Symbolic Artificial Intelligence (NeSy) assert that
combining deep learning with symbolic reasoning will lead to stronger AI than
either paradigm on its own. As successful as deep learning has been, it is
generally accepted that even our best deep learning systems are not very good
at abstract reasoning. And since reasoning is inextricably linked to language,
it makes intuitive sense that Natural Language Processing (NLP), would be a
particularly well-suited candidate for NeSy. We conduct a structured review of
studies implementing NeSy for NLP, with the aim of answering the question of
whether NeSy is indeed meeting its promises: reasoning, out-of-distribution
generalization, interpretability, learning and reasoning from small data, and
transferability to new domains. We examine the impact of knowledge
representation, such as rules and semantic networks, language structure and
relational structure, and whether implicit or explicit reasoning contributes to
higher promise scores. We find that systems where logic is compiled into the
neural network lead to the most NeSy goals being satisfied, while other factors
such as knowledge representation, or type of neural architecture do not exhibit
a clear correlation with goals being met. We find many discrepancies in how
reasoning is defined, specifically in relation to human level reasoning, which
impact decisions about model architectures and drive conclusions which are not
always consistent across studies. Hence we advocate for a more methodical
approach to the application of theories of human reasoning as well as the
development of appropriate benchmarks, which we hope can lead to a better
understanding of progress in the field. We make our data and code available on
github for further analysis.
",2022-02-24T17:13:33Z,http://arxiv.org/abs/2202.12205v2,"Kyle Hamilton, Aparna Nayak, Bojan Božić, Luca Longo"
"A Review of the Trends and Challenges in Adopting Natural Language
  Processing Methods for Education Feedback Analysis","  Artificial Intelligence (AI) is a fast-growing area of study that stretching
its presence to many business and research domains. Machine learning, deep
learning, and natural language processing (NLP) are subsets of AI to tackle
different areas of data processing and modelling. This review article presents
an overview of AI impact on education outlining with current opportunities. In
the education domain, student feedback data is crucial to uncover the merits
and demerits of existing services provided to students. AI can assist in
identifying the areas of improvement in educational infrastructure, learning
management systems, teaching practices and study environment. NLP techniques
play a vital role in analyzing student feedback in textual format. This
research focuses on existing NLP methodologies and applications that could be
adapted to educational domain applications like sentiment annotations, entity
annotations, text summarization, and topic modelling. Trends and challenges in
adopting NLP in education were reviewed and explored. Contextbased challenges
in NLP like sarcasm, domain-specific language, ambiguity, and aspect-based
sentiment analysis are explained with existing methodologies to overcome them.
Research community approaches to extract the semantic meaning of emoticons and
special characters in feedback which conveys user opinion and challenges in
adopting NLP in education are explored.
",2023-01-20T23:38:58Z,http://arxiv.org/abs/2301.08826v1,"Thanveer Shaik, Xiaohui Tao, Yan Li, Christopher Dann, Jacquie Mcdonald, Petrea Redmond, Linda Galligan"
Joint Learning of Set Cardinality and State Distribution,"  We present a novel approach for learning to predict sets using deep learning.
In recent years, deep neural networks have shown remarkable results in computer
vision, natural language processing and other related problems. Despite their
success, traditional architectures suffer from a serious limitation in that
they are built to deal with structured input and output data, i.e. vectors or
matrices. Many real-world problems, however, are naturally described as sets,
rather than vectors. Existing techniques that allow for sequential data, such
as recurrent neural networks, typically heavily depend on the input and output
order and do not guarantee a valid solution. Here, we derive in a principled
way, a mathematical formulation for set prediction where the output is
permutation invariant. In particular, our approach jointly learns both the
cardinality and the state distribution of the target set. We demonstrate the
validity of our method on the task of multi-label image classification and
achieve a new state of the art on the PASCAL VOC and MS COCO datasets.
",2017-09-13T00:33:50Z,http://arxiv.org/abs/1709.04093v2,"S. Hamid Rezatofighi, Anton Milan, Qinfeng Shi, Anthony Dick, Ian Reid"
Deep Learning-based Sentiment Analysis of Olympics Tweets,"  Sentiment analysis (SA), is an approach of natural language processing (NLP)
for determining a text's emotional tone by analyzing subjective information
such as views, feelings, and attitudes toward specific topics, products,
services, events, or experiences. This study attempts to develop an advanced
deep learning (DL) model for SA to understand global audience emotions through
tweets in the context of the Olympic Games. The findings represent global
attitudes around the Olympics and contribute to advancing the SA models. We
have used NLP for tweet pre-processing and sophisticated DL models for arguing
with SA, this research enhances the reliability and accuracy of sentiment
classification. The study focuses on data selection, preprocessing,
visualization, feature extraction, and model building, featuring a baseline
Na\""ive Bayes (NB) model and three advanced DL models: Convolutional Neural
Network (CNN), Bidirectional Long Short-Term Memory (BiLSTM), and Bidirectional
Encoder Representations from Transformers (BERT). The results of the
experiments show that the BERT model can efficiently classify sentiments
related to the Olympics, achieving the highest accuracy of 99.23%.
",2024-07-17T07:55:04Z,http://arxiv.org/abs/2407.12376v1,"Indranil Bandyopadhyay, Rahul Karmakar"
"Learning to Attack: Towards Textual Adversarial Attacking in Real-world
  Situations","  Adversarial attacking aims to fool deep neural networks with adversarial
examples. In the field of natural language processing, various textual
adversarial attack models have been proposed, varying in the accessibility to
the victim model. Among them, the attack models that only require the output of
the victim model are more fit for real-world situations of adversarial
attacking. However, to achieve high attack performance, these models usually
need to query the victim model too many times, which is neither efficient nor
viable in practice. To tackle this problem, we propose a reinforcement learning
based attack model, which can learn from attack history and launch attacks more
efficiently. In experiments, we evaluate our model by attacking several
state-of-the-art models on the benchmark datasets of multiple tasks including
sentiment analysis, text classification and natural language inference.
Experimental results demonstrate that our model consistently achieves both
better attack performance and higher efficiency than recently proposed baseline
methods. We also find our attack model can bring more robustness improvement to
the victim model by adversarial training. All the code and data of this paper
will be made public.
",2020-09-19T09:12:24Z,http://arxiv.org/abs/2009.09192v1,"Yuan Zang, Bairu Hou, Fanchao Qi, Zhiyuan Liu, Xiaojun Meng, Maosong Sun"
DeL-haTE: A Deep Learning Tunable Ensemble for Hate Speech Detection,"  Online hate speech on social media has become a fast-growing problem in
recent times. Nefarious groups have developed large content delivery networks
across several main-stream (Twitter and Facebook) and fringe (Gab, 4chan,
8chan, etc.) outlets to deliver cascades of hate messages directed both at
individuals and communities. Thus addressing these issues has become a top
priority for large-scale social media outlets. Three key challenges in
automated detection and classification of hateful content are the lack of
clearly labeled data, evolving vocabulary and lexicon - hashtags, emojis, etc.
- and the lack of baseline models for fringe outlets such as Gab. In this work,
we propose a novel framework with three major contributions. (a) We engineer an
ensemble of deep learning models that combines the strengths of
state-of-the-art approaches, (b) we incorporate a tuning factor into this
framework that leverages transfer learning to conduct automated hate speech
classification on unlabeled datasets, like Gab, and (c) we develop a weak
supervised learning methodology that allows our framework to train on unlabeled
data. Our ensemble models achieve an 83% hate recall on the HON dataset,
surpassing the performance of the state-of-the-art deep models. We demonstrate
that weak supervised training in combination with classifier tuning
significantly increases model performance on unlabeled data from Gab, achieving
a hate recall of 67%.
",2020-11-03T17:32:50Z,http://arxiv.org/abs/2011.01861v1,"Joshua Melton, Arunkumar Bagavathi, Siddharth Krishnan"
"Analyzing the Generalizability of Deep Contextualized Language
  Representations For Text Classification","  This study evaluates the robustness of two state-of-the-art deep contextual
language representations, ELMo and DistilBERT, on supervised learning of binary
protest news classification and sentiment analysis of product reviews. A
""cross-context"" setting is enabled using test sets that are distinct from the
training data. Specifically, in the news classification task, the models are
developed on local news from India and tested on the local news from China. In
the sentiment analysis task, the models are trained on movie reviews and tested
on customer reviews. This comparison is aimed at exploring the limits of the
representative power of today's Natural Language Processing systems on the path
to the systems that are generalizable to real-life scenarios. The models are
fine-tuned and fed into a Feed-Forward Neural Network and a Bidirectional Long
Short Term Memory network. Multinomial Naive Bayes and Linear Support Vector
Machine are used as traditional baselines. The results show that, in binary
text classification, DistilBERT is significantly better than ELMo on
generalizing to the cross-context setting. ELMo is observed to be significantly
more robust to the cross-context test data than both baselines. On the other
hand, the baselines performed comparably well to ELMo when the training and
test data are subsets of the same corpus (no cross-context). DistilBERT is also
found to be 30% smaller and 83% faster than ELMo. The results suggest that
DistilBERT can transfer generic semantic knowledge to other domains better than
ELMo. DistilBERT is also favorable in incorporating into real-life systems for
it requires a smaller computational training budget. When generalization is not
the utmost preference and test domain is similar to the training domain, the
traditional ML algorithms can still be considered as more economic alternatives
to deep language representations.
",2023-03-22T22:31:09Z,http://arxiv.org/abs/2303.12936v1,Berfu Buyukoz
"On the evolution of research in hypersonics: application of natural
  language processing and machine learning","  Research and development in hypersonics have progressed significantly in
recent years, with various military and commercial applications being
demonstrated increasingly. Public and private organizations in several
countries have been investing in hypersonics, with the aim to overtake their
competitors and secure/improve strategic advantage and deterrence. For these
organizations, being able to identify emerging technologies in a timely and
reliable manner is paramount. Recent advances in information technology have
made it possible to analyze large amounts of data, extract hidden patterns, and
provide decision-makers with new insights. In this study, we focus on
scientific publications about hypersonics within the period of 2000-2020, and
employ natural language processing and machine learning to characterize the
research landscape by identifying 12 key latent research themes and analyzing
their temporal evolution. Our publication similarity analysis revealed patterns
that are indicative of cycles during two decades of research. The study offers
a comprehensive analysis of the research field and the fact that the research
themes are algorithmically extracted removes subjectivity from the exercise and
enables consistent comparisons between topics and between time intervals.
",2022-08-17T19:57:31Z,http://arxiv.org/abs/2208.08507v1,"Ashkan Ebadi, Alain Auger, Yvan Gauthier"
Deep Human Answer Understanding for Natural Reverse QA,"  This study focuses on a reverse question answering (QA) procedure, in which
machines proactively raise questions and humans supply the answers. This
procedure exists in many real human-machine interaction applications. However,
a crucial problem in human-machine interaction is answer understanding. The
existing solutions have relied on mandatory option term selection to avoid
automatic answer understanding. However, these solutions have led to unnatural
human-computer interaction and negatively affected user experience. To this
end, the current study proposes a novel deep answer understanding network,
called AntNet, for reverse QA. The network consists of three new modules,
namely, skeleton attention for questions, relevance-aware representation of
answers, and multi-hop based fusion. As answer understanding for reverse QA has
not been explored, a new data corpus is compiled in this study. Experimental
results indicate that our proposed network is significantly better than
existing methods and those modified from classical natural language processing
deep models. The effectiveness of the three new modules is also verified.
",2019-12-01T13:03:03Z,http://arxiv.org/abs/1912.00398v2,"Rujing Yao, Linlin Hou, Lei Yang, Jie Gui, Qing Yin, Ou Wu"
Survey on reinforcement learning for language processing,"  In recent years some researchers have explored the use of reinforcement
learning (RL) algorithms as key components in the solution of various natural
language processing tasks. For instance, some of these algorithms leveraging
deep neural learning have found their way into conversational systems. This
paper reviews the state of the art of RL methods for their possible use for
different problems of natural language processing, focusing primarily on
conversational systems, mainly due to their growing relevance. We provide
detailed descriptions of the problems as well as discussions of why RL is
well-suited to solve them. Also, we analyze the advantages and limitations of
these methods. Finally, we elaborate on promising research directions in
natural language processing that might benefit from reinforcement learning.
",2021-04-12T15:33:11Z,http://arxiv.org/abs/2104.05565v3,"Victor Uc-Cetina, Nicolas Navarro-Guerrero, Anabel Martin-Gonzalez, Cornelius Weber, Stefan Wermter"
PERLEX: A Bilingual Persian-English Gold Dataset for Relation Extraction,"  Relation extraction is the task of extracting semantic relations between
entities in a sentence. It is an essential part of some natural language
processing tasks such as information extraction, knowledge extraction, and
knowledge base population. The main motivations of this research stem from a
lack of a dataset for relation extraction in the Persian language as well as
the necessity of extracting knowledge from the growing big-data in the Persian
language for different applications. In this paper, we present ""PERLEX"" as the
first Persian dataset for relation extraction, which is an expert-translated
version of the ""Semeval-2010-Task-8"" dataset. Moreover, this paper addresses
Persian relation extraction utilizing state-of-the-art language-agnostic
algorithms. We employ six different models for relation extraction on the
proposed bilingual dataset, including a non-neural model (as the baseline),
three neural models, and two deep learning models fed by multilingual-BERT
contextual word representations. The experiments result in the maximum f-score
77.66% (provided by BERTEM-MTB method) as the state-of-the-art of relation
extraction in the Persian language.
",2020-05-13T21:06:59Z,http://arxiv.org/abs/2005.06588v1,"Majid Asgari-Bidhendi, Mehrdad Nasser, Behrooz Janfada, Behrouz Minaei-Bidgoli"
"A Transfer Learning Method for Goal Recognition Exploiting Cross-Domain
  Spatial Features","  The ability to infer the intentions of others, predict their goals, and
deduce their plans are critical features for intelligent agents. For a long
time, several approaches investigated the use of symbolic representations and
inferences with limited success, principally because it is difficult to capture
the cognitive knowledge behind human decisions explicitly. The trend, nowadays,
is increasingly focusing on learning to infer intentions directly from data,
using deep learning in particular. We are now observing interesting
applications of intent classification in natural language processing, visual
activity recognition, and emerging approaches in other domains. This paper
discusses a novel approach combining few-shot and transfer learning with
cross-domain features, to learn to infer the intent of an agent navigating in
physical environments, executing arbitrary long sequences of actions to achieve
their goals. Experiments in synthetic environments demonstrate improved
performance in terms of learning from few samples and generalizing to unseen
configurations, compared to a deep-learning baseline approach.
",2019-11-22T16:53:19Z,http://arxiv.org/abs/1911.10134v1,"Thibault Duhamel, Mariane Maynard, Froduald Kabanza"
Task Transfer and Domain Adaptation for Zero-Shot Question Answering,"  Pretrained language models have shown success in various areas of natural
language processing, including reading comprehension tasks. However, when
applying machine learning methods to new domains, labeled data may not always
be available. To address this, we use supervised pretraining on source-domain
data to reduce sample complexity on domain-specific downstream tasks. We
evaluate zero-shot performance on domain-specific reading comprehension tasks
by combining task transfer with domain adaptation to fine-tune a pretrained
model with no labelled data from the target task. Our approach outperforms
Domain-Adaptive Pretraining on downstream domain-specific reading comprehension
tasks in 3 out of 4 domains.
",2022-06-14T09:10:48Z,http://arxiv.org/abs/2206.06705v1,"Xiang Pan, Alex Sheng, David Shimshoni, Aditya Singhal, Sara Rosenthal, Avirup Sil"
SHAP values for Explaining CNN-based Text Classification Models,"  Deep neural networks are increasingly used in natural language processing
(NLP) models. However, the need to interpret and explain the results from
complex algorithms are limiting their widespread adoption in regulated
industries such as banking. There has been recent work on interpretability of
machine learning algorithms with structured data. But there are only limited
techniques for NLP applications where the problem is more challenging due to
the size of the vocabulary, high-dimensional nature, and the need to consider
textual coherence and language structure. This paper develops a methodology to
compute SHAP values for local explainability of CNN-based text classification
models. The approach is also extended to compute global scores to assess the
importance of features. The results are illustrated on sentiment analysis of
Amazon Electronic Review data.
",2020-08-26T21:28:41Z,http://arxiv.org/abs/2008.11825v2,"Wei Zhao, Tarun Joshi, Vijayan N. Nair, Agus Sudjianto"
"From Rule-Based Models to Deep Learning Transformers Architectures for
  Natural Language Processing and Sign Language Translation Systems: Survey,
  Taxonomy and Performance Evaluation","  With the growing Deaf and Hard of Hearing population worldwide and the
persistent shortage of certified sign language interpreters, there is a
pressing need for an efficient, signs-driven, integrated end-to-end translation
system, from sign to gloss to text and vice-versa. There has been a wealth of
research on machine translations and related reviews. However, there are few
works on sign language machine translation considering the particularity of the
language being continuous and dynamic. This paper aims to address this void,
providing a retrospective analysis of the temporal evolution of sign language
machine translation algorithms and a taxonomy of the Transformers
architectures, the most used approach in language translation. We also present
the requirements of a real-time Quality-of-Service sign language ma-chine
translation system underpinned by accurate deep learning algorithms. We propose
future research directions for sign language translation systems.
",2024-08-27T07:11:45Z,http://arxiv.org/abs/2408.14825v1,"Nada Shahin, Leila Ismail"
Learning Representations of Graph Data -- A Survey,"  Deep Neural Networks have shown tremendous success in the area of object
recognition, image classification and natural language processing. However,
designing optimal Neural Network architectures that can learn and output
arbitrary graphs is an ongoing research problem. The objective of this survey
is to summarize and discuss the latest advances in methods to Learn
Representations of Graph Data. We start by identifying commonly used types of
graph data and review basics of graph theory. This is followed by a discussion
of the relationships between graph kernel methods and neural networks. Next we
identify the major approaches used for learning representations of graph data
namely: Kernel approaches, Convolutional approaches, Graph neural networks
approaches, Graph embedding approaches and Probabilistic approaches. A variety
of methods under each of the approaches are discussed and the survey is
concluded with a brief discussion of the future of learning representation of
graph data.
",2019-06-07T09:52:53Z,http://arxiv.org/abs/1906.02989v2,Mital Kinderkhedia
"Natural Language Processing to Detect Cognitive Concerns in Electronic
  Health Records Using Deep Learning","  Dementia is under-recognized in the community, under-diagnosed by healthcare
professionals, and under-coded in claims data. Information on cognitive
dysfunction, however, is often found in unstructured clinician notes within
medical records but manual review by experts is time consuming and often prone
to errors. Automated mining of these notes presents a potential opportunity to
label patients with cognitive concerns who could benefit from an evaluation or
be referred to specialist care. In order to identify patients with cognitive
concerns in electronic medical records, we applied natural language processing
(NLP) algorithms and compared model performance to a baseline model that used
structured diagnosis codes and medication data only. An attention-based deep
learning model outperformed the baseline model and other simpler models.
",2020-11-12T16:59:56Z,http://arxiv.org/abs/2011.06489v1,"Zhuoqiao Hong, Colin G. Magdamo, Yi-han Sheu, Prathamesh Mohite, Ayush Noori, Elissa M. Ye, Wendong Ge, Haoqi Sun, Laura Brenner, Gregory Robbins, Shibani Mukerji, Sahar Zafar, Nicole Benson, Lidia Moura, John Hsu, Bradley T. Hyman, Michael B. Westover, Deborah Blacker, Sudeshna Das"
Automatic Code Generation using Pre-Trained Language Models,"  Recent advancements in natural language processing \cite{gpt2} \cite{BERT}
have led to near-human performance in multiple natural language tasks. In this
paper, we seek to understand whether similar techniques can be applied to a
highly structured environment with strict syntax rules. Specifically, we
propose an end-to-end machine learning model for code generation in the Python
language built on-top of pre-trained language models. We demonstrate that a
fine-tuned model can perform well in code generation tasks, achieving a BLEU
score of 0.22, an improvement of 46\% over a reasonable sequence-to-sequence
baseline. All results and related code used for training and data processing
are available on GitHub.
",2021-02-21T07:21:26Z,http://arxiv.org/abs/2102.10535v1,"Luis Perez, Lizi Ottens, Sudharshan Viswanathan"
"PiggyBack: Pretrained Visual Question Answering Environment for Backing
  up Non-deep Learning Professionals","  We propose a PiggyBack, a Visual Question Answering platform that allows
users to apply the state-of-the-art visual-language pretrained models easily.
The PiggyBack supports the full stack of visual question answering tasks,
specifically data processing, model fine-tuning, and result visualisation. We
integrate visual-language models, pretrained by HuggingFace, an open-source API
platform of deep learning technologies; however, it cannot be runnable without
programming skills or deep learning understanding. Hence, our PiggyBack
supports an easy-to-use browser-based user interface with several deep learning
visual language pretrained models for general users and domain experts. The
PiggyBack includes the following benefits: Free availability under the MIT
License, Portability due to web-based and thus runs on almost any platform, A
comprehensive data creation and processing technique, and ease of use on deep
learning-based visual language pretrained models. The demo video is available
on YouTube and can be found at https://youtu.be/iz44RZ1lF4s.
",2022-11-29T05:37:27Z,http://arxiv.org/abs/2211.15940v3,"Zhihao Zhang, Siwen Luo, Junyi Chen, Sijia Lai, Siqu Long, Hyunsuk Chung, Soyeon Caren Han"
Semantic Representation and Inference for NLP,"  Semantic representation and inference is essential for Natural Language
Processing (NLP). The state of the art for semantic representation and
inference is deep learning, and particularly Recurrent Neural Networks (RNNs),
Convolutional Neural Networks (CNNs), and transformer Self-Attention models.
This thesis investigates the use of deep learning for novel semantic
representation and inference, and makes contributions in the following three
areas: creating training data, improving semantic representations and extending
inference learning. In terms of creating training data, we contribute the
largest publicly available dataset of real-life factual claims for the purpose
of automatic claim verification (MultiFC), and we present a novel inference
model composed of multi-scale CNNs with different kernel sizes that learn from
external sources to infer fact checking labels. In terms of improving semantic
representations, we contribute a novel model that captures non-compositional
semantic indicators. By definition, the meaning of a non-compositional phrase
cannot be inferred from the individual meanings of its composing words (e.g.,
hot dog). Motivated by this, we operationalize the compositionality of a phrase
contextually by enriching the phrase representation with external word
embeddings and knowledge graphs. Finally, in terms of inference learning, we
propose a series of novel deep learning architectures that improve inference by
using syntactic dependencies, by ensembling role guided attention heads,
incorporating gating layers, and concatenating multiple heads in novel and
effective ways. This thesis consists of seven publications (five published and
two under review).
",2021-06-15T13:22:48Z,http://arxiv.org/abs/2106.08117v1,Dongsheng Wang
Semantic Adversarial Deep Learning,"  Fueled by massive amounts of data, models produced by machine-learning (ML)
algorithms, especially deep neural networks, are being used in diverse domains
where trustworthiness is a concern, including automotive systems, finance,
health care, natural language processing, and malware detection. Of particular
concern is the use of ML algorithms in cyber-physical systems (CPS), such as
self-driving cars and aviation, where an adversary can cause serious
consequences. However, existing approaches to generating adversarial examples
and devising robust ML algorithms mostly ignore the semantics and context of
the overall system containing the ML component. For example, in an autonomous
vehicle using deep learning for perception, not every adversarial example for
the neural network might lead to a harmful consequence. Moreover, one may want
to prioritize the search for adversarial examples towards those that
significantly modify the desired semantics of the overall system. Along the
same lines, existing algorithms for constructing robust ML algorithms ignore
the specification of the overall system. In this paper, we argue that the
semantics and specification of the overall system has a crucial role to play in
this line of research. We present preliminary research results that support
this claim.
",2018-04-19T09:15:58Z,http://arxiv.org/abs/1804.07045v2,"Tommaso Dreossi, Somesh Jha, Sanjit A. Seshia"
Improving Transparency of Deep Neural Inference Process,"  Deep learning techniques are rapidly advanced recently, and becoming a
necessity component for widespread systems. However, the inference process of
deep learning is black-box, and not very suitable to safety-critical systems
which must exhibit high transparency. In this paper, to address this black-box
limitation, we develop a simple analysis method which consists of 1) structural
feature analysis: lists of the features contributing to inference process, 2)
linguistic feature analysis: lists of the natural language labels describing
the visual attributes for each feature contributing to inference process, and
3) consistency analysis: measuring consistency among input data, inference
(label), and the result of our structural and linguistic feature analysis. Our
analysis is simplified to reflect the actual inference process for high
transparency, whereas it does not include any additional black-box mechanisms
such as LSTM for highly human readable results. We conduct experiments and
discuss the results of our analysis qualitatively and quantitatively, and come
to believe that our work improves the transparency of neural networks.
Evaluated through 12,800 human tasks, 75% workers answer that input data and
result of our feature analysis are consistent, and 70% workers answer that
inference (label) and result of our feature analysis are consistent. In
addition to the evaluation of the proposed analysis, we find that our analysis
also provide suggestions, or possible next actions such as expanding neural
network complexity or collecting training data to improve a neural network.
",2019-03-13T14:11:44Z,http://arxiv.org/abs/1903.05501v1,"Hiroshi Kuwajima, Masayuki Tanaka, Masatoshi Okutomi"
"Mixture of Expert/Imitator Networks: Scalable Semi-supervised Learning
  Framework","  The current success of deep neural networks (DNNs) in an increasingly broad
range of tasks involving artificial intelligence strongly depends on the
quality and quantity of labeled training data. In general, the scarcity of
labeled data, which is often observed in many natural language processing
tasks, is one of the most important issues to be addressed. Semi-supervised
learning (SSL) is a promising approach to overcoming this issue by
incorporating a large amount of unlabeled data. In this paper, we propose a
novel scalable method of SSL for text classification tasks. The unique property
of our method, Mixture of Expert/Imitator Networks, is that imitator networks
learn to ""imitate"" the estimated label distribution of the expert network over
the unlabeled data, which potentially contributes a set of features for the
classification. Our experiments demonstrate that the proposed method
consistently improves the performance of several types of baseline DNNs. We
also demonstrate that our method has the more data, better performance property
with promising scalability to the amount of unlabeled data.
",2018-10-13T02:39:39Z,http://arxiv.org/abs/1810.05788v2,"Shun Kiyono, Jun Suzuki, Kentaro Inui"
BIM: Block-Wise Self-Supervised Learning with Masked Image Modeling,"  Like masked language modeling (MLM) in natural language processing, masked
image modeling (MIM) aims to extract valuable insights from image patches to
enhance the feature extraction capabilities of the underlying deep neural
network (DNN). Contrasted with other training paradigms like supervised
learning and unsupervised contrastive learning, masked image modeling (MIM)
pretraining typically demands significant computational resources in order to
manage large training data batches (e.g., 4096). The significant memory and
computation requirements pose a considerable challenge to its broad adoption.
To mitigate this, we introduce a novel learning framework,
termed~\textit{Block-Wise Masked Image Modeling} (BIM). This framework involves
decomposing the MIM tasks into several sub-tasks with independent computation
patterns, resulting in block-wise back-propagation operations instead of the
traditional end-to-end approach. Our proposed BIM maintains superior
performance compared to conventional MIM while greatly reducing peak memory
consumption. Moreover, BIM naturally enables the concurrent training of
numerous DNN backbones of varying depths. This leads to the creation of
multiple trained DNN backbones, each tailored to different hardware platforms
with distinct computing capabilities. This approach significantly reduces
computational costs in comparison with training each DNN backbone individually.
Our framework offers a promising solution for resource constrained training of
MIM.
",2023-11-28T20:42:30Z,http://arxiv.org/abs/2311.17218v1,"Yixuan Luo, Mengye Ren, Sai Qian Zhang"
SINGA-Easy: An Easy-to-Use Framework for MultiModal Analysis,"  Deep learning has achieved great success in a wide spectrum of multimedia
applications such as image classification, natural language processing and
multimodal data analysis. Recent years have seen the development of many deep
learning frameworks that provide a high-level programming interface for users
to design models, conduct training and deploy inference. However, it remains
challenging to build an efficient end-to-end multimedia application with most
existing frameworks. Specifically, in terms of usability, it is demanding for
non-experts to implement deep learning models, obtain the right settings for
the entire machine learning pipeline, manage models and datasets, and exploit
external data sources all together. Further, in terms of adaptability, elastic
computation solutions are much needed as the actual serving workload fluctuates
constantly, and scaling the hardware resources to handle the fluctuating
workload is typically infeasible. To address these challenges, we introduce
SINGA-Easy, a new deep learning framework that provides distributed
hyper-parameter tuning at the training stage, dynamic computational cost
control at the inference stage, and intuitive user interactions with multimedia
contents facilitated by model explanation. Our experiments on the training and
deployment of multi-modality data analysis applications show that the framework
is both usable and adaptable to dynamic inference loads. We implement
SINGA-Easy on top of Apache SINGA and demonstrate our system with the entire
machine learning life cycle.
",2021-08-03T08:39:54Z,http://arxiv.org/abs/2108.02572v1,"Naili Xing, Sai Ho Yeung, Chenghao Cai, Teck Khim Ng, Wei Wang, Kaiyuan Yang, Nan Yang, Meihui Zhang, Gang Chen, Beng Chin Ooi"
An overview of deep learning in medical imaging focusing on MRI,"  What has happened in machine learning lately, and what does it mean for the
future of medical image analysis? Machine learning has witnessed a tremendous
amount of attention over the last few years. The current boom started around
2009 when so-called deep artificial neural networks began outperforming other
established models on a number of important benchmarks. Deep neural networks
are now the state-of-the-art machine learning models across a variety of areas,
from image analysis to natural language processing, and widely deployed in
academia and industry. These developments have a huge potential for medical
imaging technology, medical data analysis, medical diagnostics and healthcare
in general, slowly being realized. We provide a short overview of recent
advances and some associated challenges in machine learning applied to medical
image processing and image analysis. As this has become a very broad and fast
expanding field we will not survey the entire landscape of applications, but
put particular focus on deep learning in MRI.
  Our aim is threefold: (i) give a brief introduction to deep learning with
pointers to core references; (ii) indicate how deep learning has been applied
to the entire MRI processing chain, from acquisition to image retrieval, from
segmentation to disease prediction; (iii) provide a starting point for people
interested in experimenting and perhaps contributing to the field of machine
learning for medical imaging by pointing out good educational resources,
state-of-the-art open-source code, and interesting sources of data and problems
related medical imaging.
",2018-11-25T16:40:42Z,http://arxiv.org/abs/1811.10052v2,"Alexander Selvikvåg Lundervold, Arvid Lundervold"
"Neural Natural Language Processing for Unstructured Data in Electronic
  Health Records: a Review","  Electronic health records (EHRs), digital collections of patient healthcare
events and observations, are ubiquitous in medicine and critical to healthcare
delivery, operations, and research. Despite this central role, EHRs are
notoriously difficult to process automatically. Well over half of the
information stored within EHRs is in the form of unstructured text (e.g.
provider notes, operation reports) and remains largely untapped for secondary
use. Recently, however, newer neural network and deep learning approaches to
Natural Language Processing (NLP) have made considerable advances,
outperforming traditional statistical and rule-based systems on a variety of
tasks. In this survey paper, we summarize current neural NLP methods for EHR
applications. We focus on a broad scope of tasks, namely, classification and
prediction, word embeddings, extraction, generation, and other topics such as
question answering, phenotyping, knowledge graphs, medical dialogue,
multilinguality, interpretability, etc.
",2021-07-07T01:50:02Z,http://arxiv.org/abs/2107.02975v1,"Irene Li, Jessica Pan, Jeremy Goldwasser, Neha Verma, Wai Pan Wong, Muhammed Yavuz Nuzumlalı, Benjamin Rosand, Yixin Li, Matthew Zhang, David Chang, R. Andrew Taylor, Harlan M. Krumholz, Dragomir Radev"
"The value of text for small business default prediction: A deep learning
  approach","  Compared to consumer lending, Micro, Small and Medium Enterprise (mSME)
credit risk modelling is particularly challenging, as, often, the same sources
of information are not available. Therefore, it is standard policy for a loan
officer to provide a textual loan assessment to mitigate limited data
availability. In turn, this statement is analysed by a credit expert alongside
any available standard credit data. In our paper, we exploit recent advances
from the field of Deep Learning and Natural Language Processing (NLP),
including the BERT (Bidirectional Encoder Representations from Transformers)
model, to extract information from 60 000 textual assessments provided by a
lender. We consider the performance in terms of the AUC (Area Under the
receiver operating characteristic Curve) and Brier Score metrics and find that
the text alone is surprisingly effective for predicting default. However, when
combined with traditional data, it yields no additional predictive capability,
with performance dependent on the text's length. Our proposed deep learning
model does, however, appear to be robust to the quality of the text and
therefore suitable for partly automating the mSME lending process. We also
demonstrate how the content of loan assessments influences performance, leading
us to a series of recommendations on a new strategy for collecting future mSME
loan assessments.
",2020-03-19T18:15:05Z,http://arxiv.org/abs/2003.08964v4,"Matthew Stevenson, Christophe Mues, Cristián Bravo"
"Deep Active Learning for Sequence Labeling Based on Diversity and
  Uncertainty in Gradient","  Recently, several studies have investigated active learning (AL) for natural
language processing tasks to alleviate data dependency. However, for query
selection, most of these studies mainly rely on uncertainty-based sampling,
which generally does not exploit the structural information of the unlabeled
data. This leads to a sampling bias in the batch active learning setting, which
selects several samples at once. In this work, we demonstrate that the amount
of labeled training data can be reduced using active learning when it
incorporates both uncertainty and diversity in the sequence labeling task. We
examined the effects of our sequence-based approach by selecting weighted
diverse in the gradient embedding approach across multiple tasks, datasets,
models, and consistently outperform classic uncertainty-based sampling and
diversity-based sampling.
",2020-11-27T06:03:27Z,http://arxiv.org/abs/2011.13570v1,Yekyung Kim
Neurosymbolic AI for Situated Language Understanding,"  In recent years, data-intensive AI, particularly the domain of natural
language processing and understanding, has seen significant progress driven by
the advent of large datasets and deep neural networks that have sidelined more
classic AI approaches to the field. These systems can apparently demonstrate
sophisticated linguistic understanding or generation capabilities, but often
fail to transfer their skills to situations they have not encountered before.
We argue that computational situated grounding provides a solution to some of
these learning challenges by creating situational representations that both
serve as a formal model of the salient phenomena, and contain rich amounts of
exploitable, task-appropriate data for training new, flexible computational
models. Our model reincorporates some ideas of classic AI into a framework of
neurosymbolic intelligence, using multimodal contextual modeling of interactive
situations, events, and object properties. We discuss how situated grounding
provides diverse data and multiple levels of modeling for a variety of AI
learning challenges, including learning how to interact with object
affordances, learning semantics for novel structures and configurations, and
transferring such learned knowledge to new objects and situations.
",2020-12-05T05:03:28Z,http://arxiv.org/abs/2012.02947v1,"Nikhil Krishnaswamy, James Pustejovsky"
"Fair Differentiable Neural Network Architecture Search for Long-Tailed
  Data with Self-Supervised Learning","  Recent advancements in artificial intelligence (AI) have positioned deep
learning (DL) as a pivotal technology in fields like computer vision, data
mining, and natural language processing. A critical factor in DL performance is
the selection of neural network architecture. Traditional predefined
architectures often fail to adapt to different data distributions, making it
challenging to achieve optimal performance. Neural architecture search (NAS)
offers a solution by automatically designing architectures tailored to specific
datasets. However, the effectiveness of NAS diminishes on long-tailed datasets,
where a few classes have abundant samples, and many have few, leading to biased
models.In this paper, we explore to improve the searching and training
performance of NAS on long-tailed datasets. Specifically, we first discuss the
related works about NAS and the deep learning method for long-tailed
datasets.Then, we focus on an existing work, called SSF-NAS, which integrates
the self-supervised learning and fair differentiable NAS to making NAS achieve
better performance on long-tailed datasets.An detailed description about the
fundamental techniques for SSF-NAS is provided in this paper, including DARTS,
FairDARTS, and Barlow Twins. Finally, we conducted a series of experiments on
the CIFAR10-LT dataset for performance evaluation, where the results are align
with our expectation.
",2024-06-19T12:39:02Z,http://arxiv.org/abs/2406.16949v1,Jiaming Yan
"Causality Extraction from Nuclear Licensee Event Reports Using a Hybrid
  Framework","  Industry-wide nuclear power plant operating experience is a critical source
of raw data for performing parameter estimations in reliability and risk
models. Much operating experience information pertains to failure events and is
stored as reports containing unstructured data, such as narratives. Event
reports are essential for understanding how failures are initiated and
propagated, including the numerous causal relations involved. Causal relation
extraction using deep learning represents a significant frontier in the field
of natural language processing (NLP), and is crucial since it enables the
interpretation of intricate narratives and connections contained within vast
amounts of written information. This paper proposed a hybrid framework for
causality detection and extraction from nuclear licensee event reports. The
main contributions include: (1) we compiled an LER corpus with 20,129 text
samples for causality analysis, (2) developed an interactive tool for labeling
cause effect pairs, (3) built a deep-learning-based approach for causal
relation detection, and (4) developed a knowledge based cause-effect extraction
approach.
",2024-04-08T16:39:34Z,http://arxiv.org/abs/2404.05656v2,"Shahidur Rahoman Sohag, Sai Zhang, Min Xian, Shoukun Sun, Fei Xu, Zhegang Ma"
"Grassmannian Learning: Embedding Geometry Awareness in Shallow and Deep
  Learning","  Modern machine learning algorithms have been adopted in a range of
signal-processing applications spanning computer vision, natural language
processing, and artificial intelligence. Many relevant problems involve
subspace-structured features, orthogonality constrained or low-rank constrained
objective functions, or subspace distances. These mathematical characteristics
are expressed naturally using the Grassmann manifold. Unfortunately, this fact
is not yet explored in many traditional learning algorithms. In the last few
years, there have been growing interests in studying Grassmann manifold to
tackle new learning problems. Such attempts have been reassured by substantial
performance improvements in both classic learning and learning using deep
neural networks. We term the former as shallow and the latter deep Grassmannian
learning. The aim of this paper is to introduce the emerging area of
Grassmannian learning by surveying common mathematical problems and primary
solution approaches, and overviewing various applications. We hope to inspire
practitioners in different fields to adopt the powerful tool of Grassmannian
learning in their research.
",2018-08-07T06:54:06Z,http://arxiv.org/abs/1808.02229v2,"Jiayao Zhang, Guangxu Zhu, Robert W. Heath Jr., Kaibin Huang"
"Striking a Balance between Classical and Deep Learning Approaches in
  Natural Language Processing Pedagogy","  While deep learning approaches represent the state-of-the-art of natural
language processing (NLP) today, classical algorithms and approaches still find
a place in NLP textbooks and courses of recent years. This paper discusses the
perspectives of conveners of two introductory NLP courses taught in Australia
and India, and examines how classical and deep learning approaches can be
balanced within the lecture plan and assessments of the courses. We also draw
parallels with the objects-first and objects-later debate in CS1 education. We
observe that teaching classical approaches adds value to student learning by
building an intuitive understanding of NLP problems, potential solutions, and
even deep learning models themselves. Despite classical approaches not being
state-of-the-art, the paper makes a case for their inclusion in NLP courses
today.
",2024-05-16T07:14:13Z,http://arxiv.org/abs/2405.09854v2,"Aditya Joshi, Jake Renzella, Pushpak Bhattacharyya, Saurav Jha, Xiangyu Zhang"
Deep Latent Defence,"  Deep learning methods have shown state of the art performance in a range of
tasks from computer vision to natural language processing. However, it is well
known that such systems are vulnerable to attackers who craft inputs in order
to cause misclassification. The level of perturbation an attacker needs to
introduce in order to cause such a misclassification can be extremely small,
and often imperceptible. This is of significant security concern, particularly
where misclassification can cause harm to humans.
  We thus propose Deep Latent Defence, an architecture which seeks to combine
adversarial training with a detection system. At its core Deep Latent Defence
has a adversarially trained neural network. A series of encoders take the
intermediate layer representation of data as it passes though the network and
project it to a latent space which we use for detecting adversarial samples via
a $k$-nn classifier. We present results using both grey and white box
attackers, as well as an adaptive $L_{\infty}$ bounded attack which was
constructed specifically to try and evade our defence. We find that even under
the strongest attacker model that we have investigated our defence is able to
offer significant defensive benefits.
",2019-10-09T12:00:52Z,http://arxiv.org/abs/1910.03916v2,"Giulio Zizzo, Chris Hankin, Sergio Maffeis, Kevin Jones"
"Audacity of huge: overcoming challenges of data scarcity and data
  quality for machine learning in computational materials discovery","  Machine learning (ML)-accelerated discovery requires large amounts of
high-fidelity data to reveal predictive structure-property relationships. For
many properties of interest in materials discovery, the challenging nature and
high cost of data generation has resulted in a data landscape that is both
scarcely populated and of dubious quality. Data-driven techniques starting to
overcome these limitations include the use of consensus across functionals in
density functional theory, the development of new functionals or accelerated
electronic structure theories, and the detection of where computationally
demanding methods are most necessary. When properties cannot be reliably
simulated, large experimental data sets can be used to train ML models. In the
absence of manual curation, increasingly sophisticated natural language
processing and automated image analysis are making it possible to learn
structure-property relationships from the literature. Models trained on these
data sets will improve as they incorporate community feedback.
",2021-11-02T21:43:58Z,http://arxiv.org/abs/2111.01905v1,"Aditya Nandy, Chenru Duan, Heather J. Kulik"
A Survey on Few-Shot Class-Incremental Learning,"  Large deep learning models are impressive, but they struggle when real-time
data is not available. Few-shot class-incremental learning (FSCIL) poses a
significant challenge for deep neural networks to learn new tasks from just a
few labeled samples without forgetting the previously learned ones. This setup
easily leads to catastrophic forgetting and overfitting problems, severely
affecting model performance. Studying FSCIL helps overcome deep learning model
limitations on data volume and acquisition time, while improving practicality
and adaptability of machine learning models. This paper provides a
comprehensive survey on FSCIL. Unlike previous surveys, we aim to synthesize
few-shot learning and incremental learning, focusing on introducing FSCIL from
two perspectives, while reviewing over 30 theoretical research studies and more
than 20 applied research studies. From the theoretical perspective, we provide
a novel categorization approach that divides the field into five subcategories,
including traditional machine learning methods, meta-learning based methods,
feature and feature space-based methods, replay-based methods, and dynamic
network structure-based methods. We also evaluate the performance of recent
theoretical research on benchmark datasets of FSCIL. From the application
perspective, FSCIL has achieved impressive achievements in various fields of
computer vision such as image classification, object detection, and image
segmentation, as well as in natural language processing and graph. We summarize
the important applications. Finally, we point out potential future research
directions, including applications, problem setups, and theory development.
Overall, this paper offers a comprehensive analysis of the latest advances in
FSCIL from a methodological, performance, and application perspective.
",2023-04-17T10:15:08Z,http://arxiv.org/abs/2304.08130v2,"Songsong Tian, Lusi Li, Weijun Li, Hang Ran, Xin Ning, Prayag Tiwari"
"Reducing Labeling Costs in Sentiment Analysis via Semi-Supervised
  Learning","  Labeling datasets is a noteworthy challenge in machine learning, both in
terms of cost and time. This research, however, leverages an efficient answer.
By exploring label propagation in semi-supervised learning, we can
significantly reduce the number of labels required compared to traditional
methods. We employ a transductive label propagation method based on the
manifold assumption for text classification. Our approach utilizes a
graph-based method to generate pseudo-labels for unlabeled data for the text
classification task, which are then used to train deep neural networks. By
extending labels based on cosine proximity within a nearest neighbor graph from
network embeddings, we combine unlabeled data into supervised learning, thereby
reducing labeling costs. Based on previous successes in other domains, this
study builds and evaluates this approach's effectiveness in sentiment analysis,
presenting insights into semi-supervised learning.
",2024-10-15T07:25:33Z,http://arxiv.org/abs/2410.11355v1,"Minoo Jafarlou, Mario M. Kubek"
"Hindi/Bengali Sentiment Analysis Using Transfer Learning and Joint Dual
  Input Learning with Self Attention","  Sentiment Analysis typically refers to using natural language processing,
text analysis and computational linguistics to extract affect and emotion based
information from text data. Our work explores how we can effectively use deep
neural networks in transfer learning and joint dual input learning settings to
effectively classify sentiments and detect hate speech in Hindi and Bengali
data. We start by training Word2Vec word embeddings for Hindi \textbf{HASOC
dataset} and Bengali hate speech and then train LSTM and subsequently, employ
parameter sharing based transfer learning to Bengali sentiment classifiers by
reusing and fine-tuning the trained weights of Hindi classifiers with both
classifier being used as baseline in our study. Finally, we use BiLSTM with
self attention in joint dual input learning setting where we train a single
neural network on Hindi and Bengali dataset simultaneously using their
respective embeddings.
",2022-02-11T05:36:11Z,http://arxiv.org/abs/2202.05457v1,"Shahrukh Khan, Mahnoor Shahid"
ChatGPT as your Personal Data Scientist,"  The rise of big data has amplified the need for efficient, user-friendly
automated machine learning (AutoML) tools. However, the intricacy of
understanding domain-specific data and defining prediction tasks necessitates
human intervention making the process time-consuming while preventing full
automation. Instead, envision an intelligent agent capable of assisting users
in conducting AutoML tasks through intuitive, natural conversations without
requiring in-depth knowledge of the underlying machine learning (ML) processes.
This agent's key challenge is to accurately comprehend the user's prediction
goals and, consequently, formulate precise ML tasks, adjust data sets and model
parameters accordingly, and articulate results effectively. In this paper, we
take a pioneering step towards this ambitious goal by introducing a
ChatGPT-based conversational data-science framework to act as a ""personal data
scientist"". Precisely, we utilize Large Language Models (ChatGPT) to build a
natural interface between the users and the ML models (Scikit-Learn), which in
turn, allows us to approach this ambitious problem with a realistic solution.
  Our model pivots around four dialogue states: Data Visualization, Task
Formulation, Prediction Engineering, and Result Summary and Recommendation.
Each state marks a unique conversation phase, impacting the overall user-system
interaction. Multiple LLM instances, serving as ""micro-agents"", ensure a
cohesive conversation flow, granting us granular control over the
conversation's progression. In summary, we developed an end-to-end system that
not only proves the viability of the novel concept of conversational data
science but also underscores the potency of LLMs in solving complex tasks.
Interestingly, its development spotlighted several critical weaknesses in the
current LLMs (ChatGPT) and highlighted substantial opportunities for
improvement.
",2023-05-23T04:00:16Z,http://arxiv.org/abs/2305.13657v1,"Md Mahadi Hassan, Alex Knipper, Shubhra Kanti Karmaker Santu"
"TCM-SD: A Benchmark for Probing Syndrome Differentiation via Natural
  Language Processing","  Traditional Chinese Medicine (TCM) is a natural, safe, and effective therapy
that has spread and been applied worldwide. The unique TCM diagnosis and
treatment system requires a comprehensive analysis of a patient's symptoms
hidden in the clinical record written in free text. Prior studies have shown
that this system can be informationized and intelligentized with the aid of
artificial intelligence (AI) technology, such as natural language processing
(NLP). However, existing datasets are not of sufficient quality nor quantity to
support the further development of data-driven AI technology in TCM. Therefore,
in this paper, we focus on the core task of the TCM diagnosis and treatment
system -- syndrome differentiation (SD) -- and we introduce the first public
large-scale dataset for SD, called TCM-SD. Our dataset contains 54,152
real-world clinical records covering 148 syndromes. Furthermore, we collect a
large-scale unlabelled textual corpus in the field of TCM and propose a
domain-specific pre-trained language model, called ZY-BERT. We conducted
experiments using deep neural networks to establish a strong performance
baseline, reveal various challenges in SD, and prove the potential of
domain-specific pre-trained language model. Our study and analysis reveal
opportunities for incorporating computer science and linguistics knowledge to
explore the empirical validity of TCM theories.
",2022-03-21T09:59:54Z,http://arxiv.org/abs/2203.10839v2,"Mucheng Ren, Heyan Huang, Yuxiang Zhou, Qianwen Cao, Yuan Bu, Yang Gao"
"The Unreasonable Effectiveness of Deep Learning in Artificial
  Intelligence","  Deep learning networks have been trained to recognize speech, caption
photographs and translate text between languages at high levels of performance.
Although applications of deep learning networks to real world problems have
become ubiquitous, our understanding of why they are so effective is lacking.
These empirical results should not be possible according to sample complexity
in statistics and non-convex optimization theory. However, paradoxes in the
training and effectiveness of deep learning networks are being investigated and
insights are being found in the geometry of high-dimensional spaces. A
mathematical theory of deep learning would illuminate how they function, allow
us to assess the strengths and weaknesses of different network architectures
and lead to major improvements. Deep learning has provided natural ways for
humans to communicate with digital devices and is foundational for building
artificial general intelligence. Deep learning was inspired by the architecture
of the cerebral cortex and insights into autonomy and general intelligence may
be found in other brain regions that are essential for planning and survival,
but major breakthroughs will be needed to achieve these goals.
",2020-02-12T05:25:15Z,http://arxiv.org/abs/2002.04806v1,Terrence J. Sejnowski
Active Learning for Classifying 2D Grid-Based Level Completability,"  Determining the completability of levels generated by procedural generators
such as machine learning models can be challenging, as it can involve the use
of solver agents that often require a significant amount of time to analyze and
solve levels. Active learning is not yet widely adopted in game evaluations,
although it has been used successfully in natural language processing, image
and speech recognition, and computer vision, where the availability of labeled
data is limited or expensive. In this paper, we propose the use of active
learning for learning level completability classification. Through an active
learning approach, we train deep-learning models to classify the completability
of generated levels for Super Mario Bros., Kid Icarus, and a Zelda-like game.
We compare active learning for querying levels to label with completability
against random queries. Our results show using an active learning approach to
label levels results in better classifier performance with the same amount of
labeled data.
",2023-09-08T14:56:22Z,http://arxiv.org/abs/2309.04367v1,"Mahsa Bazzaz, Seth Cooper"
Deep Learning for Digital Text Analytics: Sentiment Analysis,"  In today's scenario, imagining a world without negativity is something very
unrealistic, as bad NEWS spreads more virally than good ones. Though it seems
impractical in real life, this could be implemented by building a system using
Machine Learning and Natural Language Processing techniques in identifying the
news datum with negative shade and filter them by taking only the news with
positive shade (good news) to the end user. In this work, around two lakhs
datum have been trained and tested using a combination of rule-based and data
driven approaches. VADER along with a filtration method has been used as an
annotating tool followed by statistical Machine Learning approach that have
used Document Term Matrix (representation) and Support Vector Machine
(classification). Deep Learning algorithms then came into picture to make this
system reliable (Doc2Vec) which finally ended up with Convolutional Neural
Network(CNN) that yielded better results than the other experimented modules.
It showed up a training accuracy of 96%, while a test accuracy of (internal and
external news datum) above 85% was obtained.
",2018-04-10T18:10:33Z,http://arxiv.org/abs/1804.03673v1,"Reshma U, Barathi Ganesh H B, Mandar Kale, Prachi Mankame, Gouri Kulkarni"
"Comparative Study of Sentiment Analysis for Multi-Sourced Social Media
  Platforms","  There is a vast amount of data generated every second due to the rapidly
growing technology in the current world. This area of research attempts to
determine the feelings or opinions of people on social media posts. The dataset
we used was a multi-source dataset from the comment section of various social
networking sites like Twitter, Reddit, etc. Natural Language Processing
Techniques were employed to perform sentiment analysis on the obtained dataset.
In this paper, we provide a comparative analysis using techniques of
lexicon-based, machine learning and deep learning approaches. The Machine
Learning algorithm used in this work is Naive Bayes, the Lexicon-based approach
used in this work is TextBlob, and the deep-learning algorithm used in this
work is LSTM.
",2022-12-09T06:33:49Z,http://arxiv.org/abs/2212.04688v1,"Keshav Kapur, Rajitha Harikrishnan"
"Learn to Explain: Multimodal Reasoning via Thought Chains for Science
  Question Answering","  When answering a question, humans utilize the information available across
different modalities to synthesize a consistent and complete chain of thought
(CoT). This process is normally a black box in the case of deep learning models
like large-scale language models. Recently, science question benchmarks have
been used to diagnose the multi-hop reasoning ability and interpretability of
an AI system. However, existing datasets fail to provide annotations for the
answers, or are restricted to the textual-only modality, small scales, and
limited domain diversity. To this end, we present Science Question Answering
(ScienceQA), a new benchmark that consists of ~21k multimodal multiple choice
questions with a diverse set of science topics and annotations of their answers
with corresponding lectures and explanations. We further design language models
to learn to generate lectures and explanations as the chain of thought (CoT) to
mimic the multi-hop reasoning process when answering ScienceQA questions.
ScienceQA demonstrates the utility of CoT in language models, as CoT improves
the question answering performance by 1.20% in few-shot GPT-3 and 3.99% in
fine-tuned UnifiedQA. We also explore the upper bound for models to leverage
explanations by feeding those in the input; we observe that it improves the
few-shot performance of GPT-3 by 18.96%. Our analysis further shows that
language models, similar to humans, benefit from explanations to learn from
fewer data and achieve the same performance with just 40% of the data. The data
and code are available at https://scienceqa.github.io.
",2022-09-20T07:04:24Z,http://arxiv.org/abs/2209.09513v2,"Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan"
"Autonomous development and learning in artificial intelligence and
  robotics: Scaling up deep learning to human--like learning","  Autonomous lifelong development and learning is a fundamental capability of
humans, differentiating them from current deep learning systems. However, other
branches of artificial intelligence have designed crucial ingredients towards
autonomous learning: curiosity and intrinsic motivation, social learning and
natural interaction with peers, and embodiment. These mechanisms guide
exploration and autonomous choice of goals, and integrating them with deep
learning opens stimulating perspectives. Deep learning (DL) approaches made
great advances in artificial intelligence, but are still far away from human
learning. As argued convincingly by Lake et al., differences include human
capabilities to learn causal models of the world from very little data,
leveraging compositional representations and priors like intuitive physics and
psychology. However, there are other fundamental differences between current DL
systems and human learning, as well as technical ingredients to fill this gap,
that are either superficially, or not adequately, discussed by Lake et al.
These fundamental mechanisms relate to autonomous development and learning.
They are bound to play a central role in artificial intelligence in the future.
Current DL systems require engineers to manually specify a task-specific
objective function for every new task, and learn through off-line processing of
large training databases. On the contrary, humans learn autonomously open-ended
repertoires of skills, deciding for themselves which goals to pursue or value,
and which skills to explore, driven by intrinsic motivation/curiosity and
social learning through natural interaction with peers. Such learning processes
are incremental, online, and progressive. Human child development involves a
progressive increase of complexity in a curriculum of learning where skills are
explored, acquired, and built on each other, through particular ordering and
timing. Finally, human learning happens in the physical world, and through
bodily and physical experimentation, under severe constraints on energy, time,
and computational resources. In the two last decades, the field of
Developmental and Cognitive Robotics (Cangelosi and Schlesinger, 2015, Asada et
al., 2009), in strong interaction with developmental psychology and
neuroscience, has achieved significant advances in computational
",2017-12-05T14:03:56Z,http://arxiv.org/abs/1712.01626v1,Pierre-Yves Oudeyer
"Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in
  Low-Resource Languages","  Large pre-trained language models (PLMs) are at the forefront of advances in
Natural Language Processing. One widespread use case of PLMs is ""prompting"" -
or in-context learning - where a user provides a description of a task and some
completed examples of the task to a PLM as context before prompting the PLM to
perform the task on a new example. Only the largest, most capable PLMs are able
to perform in-context learning effectively, and these models are typically
trained with a predominantly English corpus, leaving all other languages
behind. The data limitations in most languages preclude the training of
language-specific PLMs capable of prompting. Albeit the surge in work of
prompting settings, it is still unclear how PLMs should be adapted
cross-lingually specifically for prompting. We evaluate the possible methods to
adapt LLaMa, a 7B parameter open-source PLM mainly trained in English, for
prompting in low-resource languages, namely for Kinyarwanda, Hausa, and
Luganda. We consider three methods: few-shot prompting (prompt),
language-adaptive fine-tuning (LAFT), and neural machine translation
(translate), and evaluate on abstractive summarization, multi-class topic
classification, and named-entity recognition. Although LAFT carries the
greatest compute cost and intuitively should lead to the best results, our
experiments exhibit that LAFT is only occasionally the optimal choice for
adapting PLMs for prompting. Rather, the translate and prompt settings are a
compute-efficient and cost-effective method of few-shot prompting for the
selected low-resource languages. We find that the results are task and language
dependent but find that the prompting method is the best on average across all
tasks and languages. Results show that the prompt setting performs better than
both translating and LAFT with statistical significance for all shots when
aggregated across all tasks and languages.
",2024-03-09T21:36:13Z,http://arxiv.org/abs/2403.06018v1,Christopher Toukmaji
"Digestion Algorithm in Hierarchical Symbolic Forests: A Fast Text
  Normalization Algorithm and Semantic Parsing Framework for Specific Scenarios
  and Lightweight Deployment","  Text Normalization and Semantic Parsing have numerous applications in natural
language processing, such as natural language programming, paraphrasing, data
augmentation, constructing expert systems, text matching, and more. Despite the
prominent achievements of deep learning in Large Language Models (LLMs), the
interpretability of neural network architectures is still poor, which affects
their credibility and hence limits the deployments of risk-sensitive scenarios.
In certain scenario-specific domains with scarce data, rapidly obtaining a
large number of supervised learning labels is challenging, and the workload of
manually labeling data would be enormous. Catastrophic forgetting in neural
networks further leads to low data utilization rates. In situations where swift
responses are vital, the density of the model makes local deployment difficult
and the response time long, which is not conducive to local applications of
these fields. Inspired by the multiplication rule, a principle of combinatorial
mathematics, and human thinking patterns, a multilayer framework along with its
algorithm, the Digestion Algorithm in Hierarchical Symbolic Forests (DAHSF), is
proposed to address these above issues, combining text normalization and
semantic parsing workflows. The Chinese Scripting Language ""Fire Bunny
Intelligent Development Platform V2.0"" is an important test and application of
the technology discussed in this paper. DAHSF can run locally in
scenario-specific domains on little datasets, with model size and memory usage
optimized by at least two orders of magnitude, thus improving the execution
speed, and possessing a promising optimization outlook.
",2024-12-18T17:05:49Z,http://arxiv.org/abs/2412.14054v1,Kevin You
GenAug: Data Augmentation for Finetuning Text Generators,"  In this paper, we investigate data augmentation for text generation, which we
call GenAug. Text generation and language modeling are important tasks within
natural language processing, and are especially challenging for low-data
regimes. We propose and evaluate various augmentation methods, including some
that incorporate external knowledge, for finetuning GPT-2 on a subset of Yelp
Reviews. We also examine the relationship between the amount of augmentation
and the quality of the generated text. We utilize several metrics that evaluate
important aspects of the generated text including its diversity and fluency.
Our experiments demonstrate that insertion of character-level synthetic noise
and keyword replacement with hypernyms are effective augmentation methods, and
that the quality of generations improves to a peak at approximately three times
the amount of original data.
",2020-10-05T05:46:39Z,http://arxiv.org/abs/2010.01794v2,"Steven Y. Feng, Varun Gangal, Dongyeop Kang, Teruko Mitamura, Eduard Hovy"
"One Single Deep Bidirectional LSTM Network for Word Sense Disambiguation
  of Text Data","  Due to recent technical and scientific advances, we have a wealth of
information hidden in unstructured text data such as offline/online narratives,
research articles, and clinical reports. To mine these data properly,
attributable to their innate ambiguity, a Word Sense Disambiguation (WSD)
algorithm can avoid numbers of difficulties in Natural Language Processing
(NLP) pipeline. However, considering a large number of ambiguous words in one
language or technical domain, we may encounter limiting constraints for proper
deployment of existing WSD models. This paper attempts to address the problem
of one-classifier-per-one-word WSD algorithms by proposing a single
Bidirectional Long Short-Term Memory (BLSTM) network which by considering
senses and context sequences works on all ambiguous words collectively.
Evaluated on SensEval-3 benchmark, we show the result of our model is
comparable with top-performing WSD algorithms. We also discuss how applying
additional modifications alleviates the model fault and the need for more
training data.
",2018-02-25T18:51:53Z,http://arxiv.org/abs/1802.09059v1,"Ahmad Pesaranghader, Ali Pesaranghader, Stan Matwin, Marina Sokolova"
A Comprehensive Review on Summarizing Financial News Using Deep Learning,"  Investors make investment decisions depending on several factors such as
fundamental analysis, technical analysis, and quantitative analysis. Another
factor on which investors can make investment decisions is through sentiment
analysis of news headlines, the sole purpose of this study. Natural Language
Processing techniques are typically used to deal with such a large amount of
data and get valuable information out of it. NLP algorithms convert raw text
into numerical representations that machines can easily understand and
interpret. This conversion can be done using various embedding techniques. In
this research, embedding techniques used are BoW, TF-IDF, Word2Vec, BERT,
GloVe, and FastText, and then fed to deep learning models such as RNN and LSTM.
This work aims to evaluate these model's performance to choose the robust model
in identifying the significant factors influencing the prediction. During this
research, it was expected that Deep Leaming would be applied to get the desired
results or achieve better accuracy than the state-of-the-art. The models are
compared to check their outputs to know which one has performed better.
",2021-09-21T12:00:31Z,http://arxiv.org/abs/2109.10118v1,"Saurabh Kamal, Sahil Sharma"
"Feature-rich multiplex lexical networks reveal mental strategies of
  early language learning","  Knowledge in the human mind exhibits a dualistic vector/network nature.
Modelling words as vectors is key to natural language processing, whereas
networks of word associations can map the nature of semantic memory. We
reconcile these paradigms - fragmented across linguistics, psychology and
computer science - by introducing FEature-Rich MUltiplex LEXical (FERMULEX)
networks. This novel framework merges structural similarities in networks and
vector features of words, which can be combined or explored independently.
Similarities model heterogenous word associations across
semantic/syntactic/phonological aspects of knowledge. Words are enriched with
multi-dimensional feature embeddings including frequency, age of acquisition,
length and polysemy. These aspects enable unprecedented explorations of
cognitive knowledge. Through CHILDES data, we use FERMULEX networks to model
normative language acquisition by 1000 toddlers between 18 and 30 months.
Similarities and embeddings capture word homophily via conformity, which
measures assortative mixing via distance and features. Conformity unearths a
language kernel of frequent/polysemous/short nouns and verbs key for basic
sentence production, supporting recent evidence of children's syntactic
constructs emerging at 30 months. This kernel is invisible to network
core-detection and feature-only clustering: It emerges from the dual
vector/network nature of words. Our quantitative analysis reveals two key
strategies in early word learning. Modelling word acquisition as random walks
on FERMULEX topology, we highlight non-uniform filling of communicative
developmental inventories (CDIs). Conformity-based walkers lead to accurate
(75%), precise (55%) and partially well-recalled (34%) predictions of early
word learning in CDIs, providing quantitative support to previous empirical
findings and developmental theories.
",2022-01-13T16:44:51Z,http://arxiv.org/abs/2201.05061v1,"Salvatore Citraro, Michael S. Vitevitch, Massimo Stella, Giulio Rossetti"
"Complex-Valued Neural Networks for Data-Driven Signal Processing and
  Signal Understanding","  Complex-valued neural networks have emerged boasting superior modeling
performance for many tasks across the signal processing, sensing, and
communications arenas. However, developing complex-valued models currently
demands development of basic deep learning operations, such as linear or
convolution layers, as modern deep learning frameworks like PyTorch and Tensor
flow do not adequately support complex-valued neural networks. This paper
overviews a package built on PyTorch with the intention of implementing
light-weight interfaces for common complex-valued neural network operations and
architectures. Similar to natural language understanding (NLU), which as
recently made tremendous leaps towards text-based intelligence, RF Signal
Understanding (RFSU) is a promising field extending conventional signal
processing algorithms using a hybrid approach of signal mechanics-based insight
with data-driven modeling power. Notably, we include efficient implementations
for linear, convolution, and attention modules in addition to activation
functions and normalization layers such as batchnorm and layernorm.
Additionally, we include efficient implementations of manifold-based
complex-valued neural network layers that have shown tremendous promise but
remain relatively unexplored in many research contexts. Although there is an
emphasis on 1-D data tensors, due to a focus on signal processing,
communications, and radar data, many of the routines are implemented for 2-D
and 3-D data as well. Specifically, the proposed approach offers a useful set
of tools and documentation for data-driven signal processing research and
practical implementation.
",2023-09-14T16:55:28Z,http://arxiv.org/abs/2309.07948v1,Josiah W. Smith
"An ensemble deep learning technique for detecting suicidal ideation from
  posts in social media platforms","  Suicidal ideation detection from social media is an evolving research with
great challenges. Many of the people who have the tendency to suicide share
their thoughts and opinions through social media platforms. As part of many
researches it is observed that the publicly available posts from social media
contain valuable criteria to effectively detect individuals with suicidal
thoughts. The most difficult part to prevent suicide is to detect and
understand the complex risk factors and warning signs that may lead to suicide.
This can be achieved by identifying the sudden changes in a user behavior
automatically. Natural language processing techniques can be used to collect
behavioral and textual features from social media interactions and these
features can be passed to a specially designed framework to detect anomalies in
human interactions that are indicators of suicidal intentions. We can achieve
fast detection of suicidal ideation using deep learning and/or machine learning
based classification approaches. For such a purpose, we can employ the
combination of LSTM and CNN models to detect such emotions from posts of the
users. In order to improve the accuracy, some approaches like using more data
for training, using attention model to improve the efficiency of existing
models etc. could be done. This paper proposes a LSTM-Attention-CNN combined
model to analyze social media submissions to detect any underlying suicidal
intentions. During evaluations, the proposed model demonstrated an accuracy of
90.3 percent and an F1-score of 92.6 percent, which is greater than the
baseline models.
",2021-12-17T15:34:03Z,http://arxiv.org/abs/2112.10609v1,"Shini Renjith, Annie Abraham, Surya B. Jyothi, Lekshmi Chandran, Jincy Thomson"
Deep Joint Entity Disambiguation with Local Neural Attention,"  We propose a novel deep learning model for joint document-level entity
disambiguation, which leverages learned neural representations. Key components
are entity embeddings, a neural attention mechanism over local context windows,
and a differentiable joint inference stage for disambiguation. Our approach
thereby combines benefits of deep learning with more traditional approaches
such as graphical models and probabilistic mention-entity maps. Extensive
experiments show that we are able to obtain competitive or state-of-the-art
accuracy at moderate computational costs.
",2017-04-17T10:18:32Z,http://arxiv.org/abs/1704.04920v3,"Octavian-Eugen Ganea, Thomas Hofmann"
"Automated Utterance Labeling of Conversations Using Natural Language
  Processing","  Conversational data is essential in psychology because it can help
researchers understand individuals cognitive processes, emotions, and
behaviors. Utterance labelling is a common strategy for analyzing this type of
data. The development of NLP algorithms allows researchers to automate this
task. However, psychological conversational data present some challenges to NLP
researchers, including multilabel classification, a large number of classes,
and limited available data. This study explored how automated labels generated
by NLP methods are comparable to human labels in the context of conversations
on adulthood transition. We proposed strategies to handle three common
challenges raised in psychological studies. Our findings showed that the deep
learning method with domain adaptation (RoBERTa-CON) outperformed all other
machine learning methods; and the hierarchical labelling system that we
proposed was shown to help researchers strategically analyze conversational
data. Our Python code and NLP model are available at
https://github.com/mlaricheva/automated_labeling.
",2022-08-12T23:03:45Z,http://arxiv.org/abs/2208.06525v1,"Maria Laricheva, Chiyu Zhang, Yan Liu, Guanyu Chen, Terence Tracey, Richard Young, Giuseppe Carenini"
"UIT-ViIC: A Dataset for the First Evaluation on Vietnamese Image
  Captioning","  Image Captioning, the task of automatic generation of image captions, has
attracted attentions from researchers in many fields of computer science, being
computer vision, natural language processing and machine learning in recent
years. This paper contributes to research on Image Captioning task in terms of
extending dataset to a different language - Vietnamese. So far, there is no
existed Image Captioning dataset for Vietnamese language, so this is the
foremost fundamental step for developing Vietnamese Image Captioning. In this
scope, we first build a dataset which contains manually written captions for
images from Microsoft COCO dataset relating to sports played with balls, we
called this dataset UIT-ViIC. UIT-ViIC consists of 19,250 Vietnamese captions
for 3,850 images. Following that, we evaluate our dataset on deep neural
network models and do comparisons with English dataset and two Vietnamese
datasets built by different methods. UIT-ViIC is published on our lab website
for research purposes.
",2020-02-01T09:26:07Z,http://arxiv.org/abs/2002.00175v1,"Quan Hoang Lam, Quang Duy Le, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen"
"One Model to Rule them all: Multitask and Multilingual Modelling for
  Lexical Analysis","  When learning a new skill, you take advantage of your preexisting skills and
knowledge. For instance, if you are a skilled violinist, you will likely have
an easier time learning to play cello. Similarly, when learning a new language
you take advantage of the languages you already speak. For instance, if your
native language is Norwegian and you decide to learn Dutch, the lexical overlap
between these two languages will likely benefit your rate of language
acquisition. This thesis deals with the intersection of learning multiple tasks
and learning multiple languages in the context of Natural Language Processing
(NLP), which can be defined as the study of computational processing of human
language. Although these two types of learning may seem different on the
surface, we will see that they share many similarities.
  The traditional approach in NLP is to consider a single task for a single
language at a time. However, recent advances allow for broadening this
approach, by considering data for multiple tasks and languages simultaneously.
This is an important approach to explore further as the key to improving the
reliability of NLP, especially for low-resource languages, is to take advantage
of all relevant data whenever possible. In doing so, the hope is that in the
long term, low-resource languages can benefit from the advances made in NLP
which are currently to a large extent reserved for high-resource languages.
This, in turn, may then have positive consequences for, e.g., language
preservation, as speakers of minority languages will have a lower degree of
pressure to using high-resource languages. In the short term, answering the
specific research questions posed should be of use to NLP researchers working
towards the same goal.
",2017-11-03T10:53:05Z,http://arxiv.org/abs/1711.01100v1,Johannes Bjerva
An Overview of Multi-Task Learning in Deep Neural Networks,"  Multi-task learning (MTL) has led to successes in many applications of
machine learning, from natural language processing and speech recognition to
computer vision and drug discovery. This article aims to give a general
overview of MTL, particularly in deep neural networks. It introduces the two
most common methods for MTL in Deep Learning, gives an overview of the
literature, and discusses recent advances. In particular, it seeks to help ML
practitioners apply MTL by shedding light on how MTL works and providing
guidelines for choosing appropriate auxiliary tasks.
",2017-06-15T21:38:12Z,http://arxiv.org/abs/1706.05098v1,Sebastian Ruder
"Aquila: A Hierarchically Aligned Visual-Language Model for Enhanced
  Remote Sensing Image Comprehension","  Recently, large vision language models (VLMs) have made significant strides
in visual language capabilities through visual instruction tuning, showing
great promise in the field of remote sensing image interpretation. However,
existing remote sensing vision language models (RSVLMs) often fall short in
capturing the complex characteristics of remote sensing scenes, as they
typically rely on low resolution, single scale visual features and simplistic
methods to map visual features to language features. In this paper, we present
Aquila, an advanced visual language foundation model designed to enable richer
visual feature representation and more precise visual-language feature
alignment for remote sensing images. Our approach introduces a learnable
Hierarchical Spatial Feature Integration (SFI) module that supports high
resolution image inputs and aggregates multi scale visual features, allowing
for the detailed representation of complex visual information. Additionally,
the SFI module is repeatedly integrated into the layers of the large language
model (LLM) to achieve deep visual language feature alignment, without
compromising the model's performance in natural language processing tasks.
These innovations, capturing detailed visual effects through higher resolution
and multi scale input, and enhancing feature alignment significantly improve
the model's ability to learn from image text data. We validate the
effectiveness of Aquila through extensive quantitative experiments and
qualitative analyses, demonstrating its superior performance.
",2024-11-09T05:31:56Z,http://arxiv.org/abs/2411.06074v1,"Kaixuan Lu, Ruiqian Zhang, Xiao Huang, Yuxing Xie"
"FollowNet: Robot Navigation by Following Natural Language Directions
  with Deep Reinforcement Learning","  Understanding and following directions provided by humans can enable robots
to navigate effectively in unknown situations. We present FollowNet, an
end-to-end differentiable neural architecture for learning multi-modal
navigation policies. FollowNet maps natural language instructions as well as
visual and depth inputs to locomotion primitives. FollowNet processes
instructions using an attention mechanism conditioned on its visual and depth
input to focus on the relevant parts of the command while performing the
navigation task. Deep reinforcement learning (RL) a sparse reward learns
simultaneously the state representation, the attention function, and control
policies. We evaluate our agent on a dataset of complex natural language
directions that guide the agent through a rich and realistic dataset of
simulated homes. We show that the FollowNet agent learns to execute previously
unseen instructions described with a similar vocabulary, and successfully
navigates along paths not encountered during training. The agent shows 30%
improvement over a baseline model without the attention mechanism, with 52%
success rate at novel instructions.
",2018-05-16T06:29:18Z,http://arxiv.org/abs/1805.06150v1,"Pararth Shah, Marek Fiser, Aleksandra Faust, J. Chase Kew, Dilek Hakkani-Tur"
"Combat COVID-19 Infodemic Using Explainable Natural Language Processing
  Models","  Misinformation of COVID-19 is prevalent on social media as the pandemic
unfolds, and the associated risks are extremely high. Thus, it is critical to
detect and combat such misinformation. Recently, deep learning models using
natural language processing techniques, such as BERT (Bidirectional Encoder
Representations from Transformers), have achieved great successes in detecting
misinformation. In this paper, we proposed an explainable natural language
processing model based on DistilBERT and SHAP (Shapley Additive exPlanations)
to combat misinformation about COVID-19 due to their efficiency and
effectiveness. First, we collected a dataset of 984 claims about COVID-19 with
fact checking. By augmenting the data using back-translation, we doubled the
sample size of the dataset and the DistilBERT model was able to obtain good
performance (accuracy: 0.972; areas under the curve: 0.993) in detecting
misinformation about COVID-19. Our model was also tested on a larger dataset
for AAAI2021 - COVID-19 Fake News Detection Shared Task and obtained good
performance (accuracy: 0.938; areas under the curve: 0.985). The performance on
both datasets was better than traditional machine learning models. Second, in
order to boost public trust in model prediction, we employed SHAP to improve
model explainability, which was further evaluated using a between-subjects
experiment with three conditions, i.e., text (T), text+SHAP explanation (TSE),
and text+SHAP explanation+source and evidence (TSESE). The participants were
significantly more likely to trust and share information related to COVID-19 in
the TSE and TSESE conditions than in the T condition. Our results provided good
implications in detecting misinformation about COVID-19 and improving public
trust.
",2021-03-01T04:28:39Z,http://arxiv.org/abs/2103.00747v1,"Jackie Ayoub, X. Jessie Yang, Feng Zhou"
"A Deep Causal Inference Approach to Measuring the Effects of Forming
  Group Loans in Online Non-profit Microfinance Platform","  Kiva is an online non-profit crowdsouring microfinance platform that raises
funds for the poor in the third world. The borrowers on Kiva are small business
owners and individuals in urgent need of money. To raise funds as fast as
possible, they have the option to form groups and post loan requests in the
name of their groups. While it is generally believed that group loans pose less
risk for investors than individual loans do, we study whether this is the case
in a philanthropic online marketplace. In particular, we measure the effect of
group loans on funding time while controlling for the loan sizes and other
factors. Because loan descriptions (in the form of texts) play an important
role in lenders' decision process on Kiva, we make use of this information
through deep learning in natural language processing. In this aspect, this is
the first paper that uses one of the most advanced deep learning techniques to
deal with unstructured data in a way that can take advantage of its superior
prediction power to answer causal questions. We find that on average, forming
group loans speeds up the funding time by about 3.3 days.
",2017-06-08T23:43:12Z,http://arxiv.org/abs/1706.02795v1,"Thai T. Pham, Yuanyuan Shen"
"Deep Generative Modeling-based Data Augmentation with Demonstration
  using the BFBT Benchmark Void Fraction Datasets","  Deep learning (DL) has achieved remarkable successes in many disciplines such
as computer vision and natural language processing due to the availability of
``big data''. However, such success cannot be easily replicated in many nuclear
engineering problems because of the limited amount of training data, especially
when the data comes from high-cost experiments. To overcome such a data
scarcity issue, this paper explores the applications of deep generative models
(DGMs) that have been widely used for image data generation to scientific data
augmentation. DGMs, such as generative adversarial networks (GANs), normalizing
flows (NFs), variational autoencoders (VAEs), and conditional VAEs (CVAEs), can
be trained to learn the underlying probabilistic distribution of the training
dataset. Once trained, they can be used to generate synthetic data that are
similar to the training data and significantly expand the dataset size. By
employing DGMs to augment TRACE simulated data of the steady-state void
fractions based on the NUPEC Boiling Water Reactor Full-size Fine-mesh Bundle
Test (BFBT) benchmark, this study demonstrates that VAEs, CVAEs, and GANs have
comparable generative performance with similar errors in the synthetic data,
with CVAEs achieving the smallest errors. The findings shows that DGMs have a
great potential to augment scientific data in nuclear engineering, which proves
effective for expanding the training dataset and enabling other DL models to be
trained more accurately.
",2023-08-19T22:19:41Z,http://arxiv.org/abs/2308.10120v1,"Farah Alsafadi, Xu Wu"
DeepSI: Interactive Deep Learning for Semantic Interaction,"  In this paper, we design novel interactive deep learning methods to improve
semantic interactions in visual analytics applications. The ability of semantic
interaction to infer analysts' precise intents during sensemaking is dependent
on the quality of the underlying data representation. We propose the
$\text{DeepSI}_{\text{finetune}}$ framework that integrates deep learning into
the human-in-the-loop interactive sensemaking pipeline, with two important
properties. First, deep learning extracts meaningful representations from raw
data, which improves semantic interaction inference. Second, semantic
interactions are exploited to fine-tune the deep learning representations,
which then further improves semantic interaction inference. This feedback loop
between human interaction and deep learning enables efficient learning of user-
and task-specific representations. To evaluate the advantage of embedding the
deep learning within the semantic interaction loop, we compare
$\text{DeepSI}_{\text{finetune}}$ against a state-of-the-art but more basic use
of deep learning as only a feature extractor pre-processed outside of the
interactive loop. Results of two complementary studies, a human-centered
qualitative case study and an algorithm-centered simulation-based quantitative
experiment, show that $\text{DeepSI}_{\text{finetune}}$ more accurately
captures users' complex mental models with fewer interactions.
",2023-05-26T18:05:57Z,http://arxiv.org/abs/2305.18357v1,"Yali Bian, Chris North"
A Survey on Extraction of Causal Relations from Natural Language Text,"  As an essential component of human cognition, cause-effect relations appear
frequently in text, and curating cause-effect relations from text helps in
building causal networks for predictive tasks. Existing causality extraction
techniques include knowledge-based, statistical machine learning(ML)-based, and
deep learning-based approaches. Each method has its advantages and weaknesses.
For example, knowledge-based methods are understandable but require extensive
manual domain knowledge and have poor cross-domain applicability. Statistical
machine learning methods are more automated because of natural language
processing (NLP) toolkits. However, feature engineering is labor-intensive, and
toolkits may lead to error propagation. In the past few years, deep learning
techniques attract substantial attention from NLP researchers because of its'
powerful representation learning ability and the rapid increase in
computational resources. Their limitations include high computational costs and
a lack of adequate annotated training data. In this paper, we conduct a
comprehensive survey of causality extraction. We initially introduce primary
forms existing in the causality extraction: explicit intra-sentential
causality, implicit causality, and inter-sentential causality. Next, we list
benchmark datasets and modeling assessment methods for causal relation
extraction. Then, we present a structured overview of the three techniques with
their representative systems. Lastly, we highlight existing open challenges
with their potential directions.
",2021-01-16T10:49:39Z,http://arxiv.org/abs/2101.06426v2,"Jie Yang, Soyeon Caren Han, Josiah Poon"
"Where's the Question? A Multi-channel Deep Convolutional Neural Network
  for Question Identification in Textual Data","  In most clinical practice settings, there is no rigorous reviewing of the
clinical documentation, resulting in inaccurate information captured in the
patient medical records. The gold standard in clinical data capturing is
achieved via ""expert-review"", where clinicians can have a dialogue with a
domain expert (reviewers) and ask them questions about data entry rules.
Automatically identifying ""real questions"" in these dialogues could uncover
ambiguities or common problems in data capturing in a given clinical setting.
  In this study, we proposed a novel multi-channel deep convolutional neural
network architecture, namely Quest-CNN, for the purpose of separating real
questions that expect an answer (information or help) about an issue from
sentences that are not questions, as well as from questions referring to an
issue mentioned in a nearby sentence (e.g., can you clarify this?), which we
will refer as ""c-questions"". We conducted a comprehensive performance
comparison analysis of the proposed multi-channel deep convolutional neural
network against other deep neural networks. Furthermore, we evaluated the
performance of traditional rule-based and learning-based methods for detecting
question sentences. The proposed Quest-CNN achieved the best F1 score both on a
dataset of data entry-review dialogue in a dialysis care setting, and on a
general domain dataset.
",2020-10-15T15:11:22Z,http://arxiv.org/abs/2010.07816v1,"George Michalopoulos, Helen Chen, Alexander Wong"
"HEIDL: Learning Linguistic Expressions with Deep Learning and
  Human-in-the-Loop","  While the role of humans is increasingly recognized in machine learning
community, representation of and interaction with models in current
human-in-the-loop machine learning (HITL-ML) approaches are too low-level and
far-removed from human's conceptual models. We demonstrate HEIDL, a prototype
HITL-ML system that exposes the machine-learned model through high-level,
explainable linguistic expressions formed of predicates representing semantic
structure of text. In HEIDL, human's role is elevated from simply evaluating
model predictions to interpreting and even updating the model logic directly by
enabling interaction with rule predicates themselves. Raising the currency of
interaction to such semantic levels calls for new interaction paradigms between
humans and machines that result in improved productivity for text analytics
model development process. Moreover, by involving humans in the process, the
human-machine co-created models generalize better to unseen data as domain
experts are able to instill their expertise by extrapolating from what has been
learned by automated algorithms from few labelled data.
",2019-07-25T16:45:06Z,http://arxiv.org/abs/1907.11184v1,"Yiwei Yang, Eser Kandogan, Yunyao Li, Walter S. Lasecki, Prithviraj Sen"
Lexical semantics enhanced neural word embeddings,"  Current breakthroughs in natural language processing have benefited
dramatically from neural language models, through which distributional
semantics can leverage neural data representations to facilitate downstream
applications. Since neural embeddings use context prediction on word
co-occurrences to yield dense vectors, they are inevitably prone to capture
more semantic association than semantic similarity. To improve vector space
models in deriving semantic similarity, we post-process neural word embeddings
through deep metric learning, through which we can inject lexical-semantic
relations, including syn/antonymy and hypo/hypernymy, into a distributional
space. We introduce hierarchy-fitting, a novel semantic specialization approach
to modelling semantic similarity nuances inherently stored in the IS-A
hierarchies. Hierarchy-fitting attains state-of-the-art results on the common-
and rare-word benchmark datasets for deriving semantic similarity from neural
word embeddings. It also incorporates an asymmetric distance function to
specialize hypernymy's directionality explicitly, through which it
significantly improves vanilla embeddings in multiple evaluation tasks of
detecting hypernymy and directionality without negative impacts on semantic
similarity judgement. The results demonstrate the efficacy of hierarchy-fitting
in specializing neural embeddings with semantic relations in late fusion,
potentially expanding its applicability to aggregating heterogeneous data and
various knowledge resources for learning multimodal semantic spaces.
",2022-10-03T08:10:23Z,http://arxiv.org/abs/2210.00754v1,"Dongqiang Yang, Ning Li, Li Zou, Hongwei Ma"
"Unsupervised Cross-Domain Prerequisite Chain Learning using Variational
  Graph Autoencoders","  Learning prerequisite chains is an essential task for efficiently acquiring
knowledge in both known and unknown domains. For example, one may be an expert
in the natural language processing (NLP) domain but want to determine the best
order to learn new concepts in an unfamiliar Computer Vision domain (CV). Both
domains share some common concepts, such as machine learning basics and deep
learning models. In this paper, we propose unsupervised cross-domain concept
prerequisite chain learning using an optimized variational graph autoencoder.
Our model learns to transfer concept prerequisite relations from an
information-rich domain (source domain) to an information-poor domain (target
domain), substantially surpassing other baseline models. Also, we expand an
existing dataset by introducing two new domains: CV and Bioinformatics (BIO).
The annotated data and resources, as well as the code, will be made publicly
available.
",2021-05-07T21:02:41Z,http://arxiv.org/abs/2105.03505v3,"Irene Li, Vanessa Yan, Tianxiao Li, Rihao Qu, Dragomir Radev"
Curriculum Learning with Adam: The Devil Is in the Wrong Details,"  Curriculum learning (CL) posits that machine learning models -- similar to
humans -- may learn more efficiently from data that match their current
learning progress. However, CL methods are still poorly understood and, in
particular for natural language processing (NLP), have achieved only limited
success. In this paper, we explore why. Starting from an attempt to replicate
and extend a number of recent curriculum methods, we find that their results
are surprisingly brittle when applied to NLP. A deep dive into the
(in)effectiveness of the curricula in some scenarios shows us why: when
curricula are employed in combination with the popular Adam optimisation
algorithm, they oftentimes learn to adapt to suboptimally chosen optimisation
parameters for this algorithm. We present a number of different case studies
with different common hand-crafted and automated CL approaches to illustrate
this phenomenon, and we find that none of them outperforms optimisation with
only Adam with well-chosen hyperparameters. As such, our results contribute to
understanding why CL methods work, but at the same time urge caution when
claiming positive results.
",2023-08-23T15:39:42Z,http://arxiv.org/abs/2308.12202v1,"Lucas Weber, Jaap Jumelet, Paul Michel, Elia Bruni, Dieuwke Hupkes"
"Breaking Language Barriers: A Question Answering Dataset for Hindi and
  Marathi","  The recent advances in deep-learning have led to the development of highly
sophisticated systems with an unquenchable appetite for data. On the other
hand, building good deep-learning models for low-resource languages remains a
challenging task. This paper focuses on developing a Question Answering dataset
for two such languages- Hindi and Marathi. Despite Hindi being the 3rd most
spoken language worldwide, with 345 million speakers, and Marathi being the
11th most spoken language globally, with 83.2 million speakers, both languages
face limited resources for building efficient Question Answering systems. To
tackle the challenge of data scarcity, we have developed a novel approach for
translating the SQuAD 2.0 dataset into Hindi and Marathi. We release the
largest Question-Answering dataset available for these languages, with each
dataset containing 28,000 samples. We evaluate the dataset on various
architectures and release the best-performing models for both Hindi and
Marathi, which will facilitate further research in these languages. Leveraging
similarity tools, our method holds the potential to create datasets in diverse
languages, thereby enhancing the understanding of natural language across
varied linguistic contexts. Our fine-tuned models, code, and dataset will be
made publicly available.
",2023-08-19T00:39:21Z,http://arxiv.org/abs/2308.09862v3,"Maithili Sabane, Onkar Litake, Aman Chadha"
"Learning to Selectively Transfer: Reinforced Transfer Learning for Deep
  Text Matching","  Deep text matching approaches have been widely studied for many applications
including question answering and information retrieval systems. To deal with a
domain that has insufficient labeled data, these approaches can be used in a
Transfer Learning (TL) setting to leverage labeled data from a resource-rich
source domain. To achieve better performance, source domain data selection is
essential in this process to prevent the ""negative transfer"" problem. However,
the emerging deep transfer models do not fit well with most existing data
selection methods, because the data selection policy and the transfer learning
model are not jointly trained, leading to sub-optimal training efficiency.
  In this paper, we propose a novel reinforced data selector to select
high-quality source domain data to help the TL model. Specifically, the data
selector ""acts"" on the source domain data to find a subset for optimization of
the TL model, and the performance of the TL model can provide ""rewards"" in turn
to update the selector. We build the reinforced data selector based on the
actor-critic framework and integrate it to a DNN based transfer learning model,
resulting in a Reinforced Transfer Learning (RTL) method. We perform a thorough
experimental evaluation on two major tasks for text matching, namely,
paraphrase identification and natural language inference. Experimental results
show the proposed RTL can significantly improve the performance of the TL
model. We further investigate different settings of states, rewards, and policy
optimization methods to examine the robustness of our method. Last, we conduct
a case study on the selected data and find our method is able to select source
domain data whose Wasserstein distance is close to the target domain data. This
is reasonable and intuitive as such source domain data can provide more
transferability power to the model.
",2018-12-30T15:39:57Z,http://arxiv.org/abs/1812.11561v1,"Chen Qu, Feng Ji, Minghui Qiu, Liu Yang, Zhiyu Min, Haiqing Chen, Jun Huang, W. Bruce Croft"
Multi-Task Learning for Argumentation Mining,"  Multi-task learning has recently become a very active field in deep learning
research. In contrast to learning a single task in isolation, multiple tasks
are learned at the same time, thereby utilizing the training signal of related
tasks to improve the performance on the respective machine learning tasks.
Related work shows various successes in different domains when applying this
paradigm and this thesis extends the existing empirical results by evaluating
multi-task learning in four different scenarios: argumentation mining,
epistemic segmentation, argumentation component segmentation, and
grapheme-to-phoneme conversion. We show that multi-task learning can, indeed,
improve the performance compared to single-task learning in all these
scenarios, but may also hurt the performance. Therefore, we investigate the
reasons for successful and less successful applications of this paradigm and
find that dataset properties such as entropy or the size of the label inventory
are good indicators for a potential multi-task learning success and that
multi-task learning is particularly useful if the task at hand suffers from
data sparsity, i.e. a lack of training data. Moreover, multi-task learning is
particularly effective for long input sequences in our experiments. We have
observed this trend in all evaluated scenarios. Finally, we develop a highly
configurable and extensible sequence tagging framework which supports
multi-task learning to conduct our empirical experiments and to aid future
research regarding the multi-task learning paradigm and natural language
processing.
",2019-04-23T05:58:54Z,http://arxiv.org/abs/1904.10162v1,Tobias Kahse
"Deep Learning on FPGAs: Past, Present, and Future","  The rapid growth of data size and accessibility in recent years has
instigated a shift of philosophy in algorithm design for artificial
intelligence. Instead of engineering algorithms by hand, the ability to learn
composable systems automatically from massive amounts of data has led to
ground-breaking performance in important domains such as computer vision,
speech recognition, and natural language processing. The most popular class of
techniques used in these domains is called deep learning, and is seeing
significant attention from industry. However, these models require incredible
amounts of data and compute power to train, and are limited by the need for
better hardware acceleration to accommodate scaling beyond current data and
model sizes. While the current solution has been to use clusters of graphics
processing units (GPU) as general purpose processors (GPGPU), the use of field
programmable gate arrays (FPGA) provide an interesting alternative. Current
trends in design tools for FPGAs have made them more compatible with the
high-level software practices typically practiced in the deep learning
community, making FPGAs more accessible to those who build and deploy models.
Since FPGA architectures are flexible, this could also allow researchers the
ability to explore model-level optimizations beyond what is possible on fixed
architectures such as GPUs. As well, FPGAs tend to provide high performance per
watt of power consumption, which is of particular importance for application
scientists interested in large scale server-based deployment or
resource-limited embedded applications. This review takes a look at deep
learning and FPGAs from a hardware acceleration perspective, identifying trends
and innovations that make these technologies a natural fit, and motivates a
discussion on how FPGAs may best serve the needs of the deep learning community
moving forward.
",2016-02-13T03:50:37Z,http://arxiv.org/abs/1602.04283v1,"Griffin Lacey, Graham W. Taylor, Shawki Areibi"
Teaching Data Science,"  We describe an introductory data science course, entitled Introduction to
Data Science, offered at the University of Illinois at Urbana-Champaign. The
course introduced general programming concepts by using the Python programming
language with an emphasis on data preparation, processing, and presentation.
The course had no prerequisites, and students were not expected to have any
programming experience. This introductory course was designed to cover a wide
range of topics, from the nature of data, to storage, to visualization, to
probability and statistical analysis, to cloud and high performance computing,
without becoming overly focused on any one subject. We conclude this article
with a discussion of lessons learned and our plans to develop new data science
courses.
",2016-04-25T18:26:51Z,http://arxiv.org/abs/1604.07397v1,"Robert J. Brunner, Edward J. Kim"
"Learning Universal Graph Neural Network Embeddings With Aid Of Transfer
  Learning","  Learning powerful data embeddings has become a center piece in machine
learning, especially in natural language processing and computer vision
domains. The crux of these embeddings is that they are pretrained on huge
corpus of data in a unsupervised fashion, sometimes aided with transfer
learning. However currently in the graph learning domain, embeddings learned
through existing graph neural networks (GNNs) are task dependent and thus
cannot be shared across different datasets. In this paper, we present a first
powerful and theoretically guaranteed graph neural network that is designed to
learn task-independent graph embeddings, thereafter referred to as deep
universal graph embedding (DUGNN). Our DUGNN model incorporates a novel graph
neural network (as a universal graph encoder) and leverages rich Graph Kernels
(as a multi-task graph decoder) for both unsupervised learning and
(task-specific) adaptive supervised learning. By learning task-independent
graph embeddings across diverse datasets, DUGNN also reaps the benefits of
transfer learning. Through extensive experiments and ablation studies, we show
that the proposed DUGNN model consistently outperforms both the existing
state-of-art GNN models and Graph Kernels by an increased accuracy of 3% - 8%
on graph classification benchmark datasets.
",2019-09-22T20:21:15Z,http://arxiv.org/abs/1909.10086v3,"Saurabh Verma, Zhi-Li Zhang"
"TransPolymer: a Transformer-based language model for polymer property
  predictions","  Accurate and efficient prediction of polymer properties is of great
significance in polymer design. Conventionally, expensive and time-consuming
experiments or simulations are required to evaluate polymer functions.
Recently, Transformer models, equipped with self-attention mechanisms, have
exhibited superior performance in natural language processing. However, such
methods have not been investigated in polymer sciences. Herein, we report
TransPolymer, a Transformer-based language model for polymer property
prediction. Our proposed polymer tokenizer with chemical awareness enables
learning representations from polymer sequences. Rigorous experiments on ten
polymer property prediction benchmarks demonstrate the superior performance of
TransPolymer. Moreover, we show that TransPolymer benefits from pretraining on
large unlabeled dataset via Masked Language Modeling. Experimental results
further manifest the important role of self-attention in modeling polymer
sequences. We highlight this model as a promising computational tool for
promoting rational polymer design and understanding structure-property
relationships from a data science view.
",2022-09-03T01:29:59Z,http://arxiv.org/abs/2209.01307v4,"Changwen Xu, Yuyang Wang, Amir Barati Farimani"
Deep Regression Unlearning,"  With the introduction of data protection and privacy regulations, it has
become crucial to remove the lineage of data on demand from a machine learning
(ML) model. In the last few years, there have been notable developments in
machine unlearning to remove the information of certain training data
efficiently and effectively from ML models. In this work, we explore unlearning
for the regression problem, particularly in deep learning models. Unlearning in
classification and simple linear regression has been considerably investigated.
However, unlearning in deep regression models largely remains an untouched
problem till now. In this work, we introduce deep regression unlearning methods
that generalize well and are robust to privacy attacks. We propose the
Blindspot unlearning method which uses a novel weight optimization process. A
randomly initialized model, partially exposed to the retain samples and a copy
of the original model are used together to selectively imprint knowledge about
the data that we wish to keep and scrub off the information of the data we wish
to forget. We also propose a Gaussian fine tuning method for regression
unlearning. The existing unlearning metrics for classification are not directly
applicable to regression unlearning. Therefore, we adapt these metrics for the
regression setting. We conduct regression unlearning experiments for computer
vision, natural language processing and forecasting applications. Our methods
show excellent performance for all these datasets across all the metrics.
Source code: https://github.com/ayu987/deep-regression-unlearning
",2022-10-15T05:00:20Z,http://arxiv.org/abs/2210.08196v2,"Ayush K Tarun, Vikram S Chundawat, Murari Mandal, Mohan Kankanhalli"
DNF-Net: A Neural Architecture for Tabular Data,"  A challenging open question in deep learning is how to handle tabular data.
Unlike domains such as image and natural language processing, where deep
architectures prevail, there is still no widely accepted neural architecture
that dominates tabular data. As a step toward bridging this gap, we present
DNF-Net a novel generic architecture whose inductive bias elicits models whose
structure corresponds to logical Boolean formulas in disjunctive normal form
(DNF) over affine soft-threshold decision terms. In addition, DNF-Net promotes
localized decisions that are taken over small subsets of the features. We
present an extensive empirical study showing that DNF-Nets significantly and
consistently outperform FCNs over tabular data. With relatively few
hyperparameters, DNF-Nets open the door to practical end-to-end handling of
tabular data using neural networks. We present ablation studies, which justify
the design choices of DNF-Net including the three inductive bias elements,
namely, Boolean formulation, locality, and feature selection.
",2020-06-11T14:21:45Z,http://arxiv.org/abs/2006.06465v1,"Ami Abutbul, Gal Elidan, Liran Katzir, Ran El-Yaniv"
Delving into Deep Imbalanced Regression,"  Real-world data often exhibit imbalanced distributions, where certain target
values have significantly fewer observations. Existing techniques for dealing
with imbalanced data focus on targets with categorical indices, i.e., different
classes. However, many tasks involve continuous targets, where hard boundaries
between classes do not exist. We define Deep Imbalanced Regression (DIR) as
learning from such imbalanced data with continuous targets, dealing with
potential missing data for certain target values, and generalizing to the
entire target range. Motivated by the intrinsic difference between categorical
and continuous label space, we propose distribution smoothing for both labels
and features, which explicitly acknowledges the effects of nearby targets, and
calibrates both label and learned feature distributions. We curate and
benchmark large-scale DIR datasets from common real-world tasks in computer
vision, natural language processing, and healthcare domains. Extensive
experiments verify the superior performance of our strategies. Our work fills
the gap in benchmarks and techniques for practical imbalanced regression
problems. Code and data are available at
https://github.com/YyzHarry/imbalanced-regression.
",2021-02-18T18:56:03Z,http://arxiv.org/abs/2102.09554v2,"Yuzhe Yang, Kaiwen Zha, Ying-Cong Chen, Hao Wang, Dina Katabi"
"Convolutional Neural Network (CNN) to reduce construction loss in JPEG
  compression caused by Discrete Fourier Transform (DFT)","  In recent decades, digital image processing has gained enormous popularity.
Consequently, a number of data compression strategies have been put forth, with
the goal of minimizing the amount of information required to represent images.
Among them, JPEG compression is one of the most popular methods that has been
widely applied in multimedia and digital applications. The periodic nature of
DFT makes it impossible to meet the periodic condition of an image's opposing
edges without producing severe artifacts, which lowers the image's perceptual
visual quality. On the other hand, deep learning has recently achieved
outstanding results for applications like speech recognition, image reduction,
and natural language processing. Convolutional Neural Networks (CNN) have
received more attention than most other types of deep neural networks. The use
of convolution in feature extraction results in a less redundant feature map
and a smaller dataset, both of which are crucial for image compression. In this
work, an effective image compression method is purposed using autoencoders. The
study's findings revealed a number of important trends that suggested better
reconstruction along with good compression can be achieved using autoencoders.
",2022-08-26T12:46:16Z,http://arxiv.org/abs/2209.03475v2,Suman Kunwar
Adaptive Transfer Learning of Multi-View Time Series Classification,"  Time Series Classification (TSC) has been an important and challenging task
in data mining, especially on multivariate time series and multi-view time
series data sets. Meanwhile, transfer learning has been widely applied in
computer vision and natural language processing applications to improve deep
neural network's generalization capabilities. However, very few previous works
applied transfer learning framework to time series mining problems.
Particularly, the technique of measuring similarities between source domain and
target domain based on dynamic representation such as density estimation with
importance sampling has never been combined with transfer learning framework.
In this paper, we first proposed a general adaptive transfer learning framework
for multi-view time series data, which shows strong ability in storing
inter-view importance value in the process of knowledge transfer. Next, we
represented inter-view importance through some time series similarity
measurements and approximated the posterior distribution in latent space for
the importance sampling via density estimation techniques. We then computed the
matrix norm of sampled importance value, which controls the degree of knowledge
transfer in pre-training process. We further evaluated our work, applied it to
many other time series classification tasks, and observed that our architecture
maintained desirable generalization ability. Finally, we concluded that our
framework could be adapted with deep learning techniques to receive significant
model performance improvements.
",2019-10-14T21:46:03Z,http://arxiv.org/abs/1910.07632v1,"Donglin Zhan, Shiyu Yi, Dongli Xu, Xiao Yu, Denglin Jiang, Siqi Yu, Haoting Zhang, Wenfang Shangguan, Weihua Zhang"
Optimization Methods in Deep Learning: A Comprehensive Overview,"  In recent years, deep learning has achieved remarkable success in various
fields such as image recognition, natural language processing, and speech
recognition. The effectiveness of deep learning largely depends on the
optimization methods used to train deep neural networks. In this paper, we
provide an overview of first-order optimization methods such as Stochastic
Gradient Descent, Adagrad, Adadelta, and RMSprop, as well as recent
momentum-based and adaptive gradient methods such as Nesterov accelerated
gradient, Adam, Nadam, AdaMax, and AMSGrad. We also discuss the challenges
associated with optimization in deep learning and explore techniques for
addressing these challenges, including weight initialization, batch
normalization, and layer normalization. Finally, we provide recommendations for
selecting optimization methods for different deep learning tasks and datasets.
This paper serves as a comprehensive guide to optimization methods in deep
learning and can be used as a reference for researchers and practitioners in
the field.
",2023-02-19T13:01:53Z,http://arxiv.org/abs/2302.09566v2,David Shulman
PreTraM: Self-Supervised Pre-training via Connecting Trajectory and Map,"  Deep learning has recently achieved significant progress in trajectory
forecasting. However, the scarcity of trajectory data inhibits the data-hungry
deep-learning models from learning good representations. While mature
representation learning methods exist in computer vision and natural language
processing, these pre-training methods require large-scale data. It is hard to
replicate these approaches in trajectory forecasting due to the lack of
adequate trajectory data (e.g., 34K samples in the nuScenes dataset). To work
around the scarcity of trajectory data, we resort to another data modality
closely related to trajectories-HD-maps, which is abundantly provided in
existing datasets. In this paper, we propose PreTraM, a self-supervised
pre-training scheme via connecting trajectories and maps for trajectory
forecasting. Specifically, PreTraM consists of two parts: 1) Trajectory-Map
Contrastive Learning, where we project trajectories and maps to a shared
embedding space with cross-modal contrastive learning, and 2) Map Contrastive
Learning, where we enhance map representation with contrastive learning on
large quantities of HD-maps. On top of popular baselines such as AgentFormer
and Trajectron++, PreTraM boosts their performance by 5.5% and 6.9% relatively
in FDE-10 on the challenging nuScenes dataset. We show that PreTraM improves
data efficiency and scales well with model size.
",2022-04-21T23:01:21Z,http://arxiv.org/abs/2204.10435v1,"Chenfeng Xu, Tian Li, Chen Tang, Lingfeng Sun, Kurt Keutzer, Masayoshi Tomizuka, Alireza Fathi, Wei Zhan"
Privacy Guarantees for De-identifying Text Transformations,"  Machine Learning approaches to Natural Language Processing tasks benefit from
a comprehensive collection of real-life user data. At the same time, there is a
clear need for protecting the privacy of the users whose data is collected and
processed. For text collections, such as, e.g., transcripts of voice
interactions or patient records, replacing sensitive parts with benign
alternatives can provide de-identification. However, how much privacy is
actually guaranteed by such text transformations, and are the resulting texts
still useful for machine learning? In this paper, we derive formal privacy
guarantees for general text transformation-based de-identification methods on
the basis of Differential Privacy. We also measure the effect that different
ways of masking private information in dialog transcripts have on a subsequent
machine learning task. To this end, we formulate different masking strategies
and compare their privacy-utility trade-offs. In particular, we compare a
simple redact approach with more sophisticated word-by-word replacement using
deep learning models on multiple natural language understanding tasks like
named entity recognition, intent detection, and dialog act classification. We
find that only word-by-word replacement is robust against performance drops in
various tasks.
",2020-08-07T12:06:42Z,http://arxiv.org/abs/2008.03101v2,"David Ifeoluwa Adelani, Ali Davody, Thomas Kleinbauer, Dietrich Klakow"
Labeled Data Generation with Inexact Supervision,"  The recent advanced deep learning techniques have shown the promising results
in various domains such as computer vision and natural language processing. The
success of deep neural networks in supervised learning heavily relies on a
large amount of labeled data. However, obtaining labeled data with target
labels is often challenging due to various reasons such as cost of labeling and
privacy issues, which challenges existing deep models. In spite of that, it is
relatively easy to obtain data with \textit{inexact supervision}, i.e., having
labels/tags related to the target task. For example, social media platforms are
overwhelmed with billions of posts and images with self-customized tags, which
are not the exact labels for target classification tasks but are usually
related to the target labels. It is promising to leverage these tags (inexact
supervision) and their relations with target classes to generate labeled data
to facilitate the downstream classification tasks. However, the work on this is
rather limited. Therefore, we study a novel problem of labeled data generation
with inexact supervision. We propose a novel generative framework named as
ADDES which can synthesize high-quality labeled data for target classification
tasks by learning from data with inexact supervision and the relations between
inexact supervision and target classes. Experimental results on image and text
datasets demonstrate the effectiveness of the proposed ADDES for generating
realistic labeled data from inexact supervision to facilitate the target
classification task.
",2021-06-08T22:22:26Z,http://arxiv.org/abs/2106.04716v1,"Enyan Dai, Kai Shu, Yiwei Sun, Suhang Wang"
"DARVIZ: Deep Abstract Representation, Visualization, and Verification of
  Deep Learning Models","  Traditional software engineering programming paradigms are mostly object or
procedure oriented, driven by deterministic algorithms. With the advent of deep
learning and cognitive sciences there is an emerging trend for data-driven
programming, creating a shift in the programming paradigm among the software
engineering communities. Visualizing and interpreting the execution of a
current large scale data-driven software development is challenging. Further,
for deep learning development there are many libraries in multiple programming
languages such as TensorFlow (Python), CAFFE (C++), Theano (Python), Torch
(Lua), and Deeplearning4j (Java), driving a huge need for interoperability
across libraries.
",2017-08-16T14:46:27Z,http://arxiv.org/abs/1708.04915v1,"Anush Sankaran, Rahul Aralikatte, Senthil Mani, Shreya Khare, Naveen Panwar, Neelamadhav Gantayat"
Deep Reinforcement Learning: An Overview,"  We give an overview of recent exciting achievements of deep reinforcement
learning (RL). We discuss six core elements, six important mechanisms, and
twelve applications. We start with background of machine learning, deep
learning and reinforcement learning. Next we discuss core RL elements,
including value function, in particular, Deep Q-Network (DQN), policy, reward,
model, planning, and exploration. After that, we discuss important mechanisms
for RL, including attention and memory, unsupervised learning, transfer
learning, multi-agent RL, hierarchical RL, and learning to learn. Then we
discuss various applications of RL, including games, in particular, AlphaGo,
robotics, natural language processing, including dialogue systems, machine
translation, and text generation, computer vision, neural architecture design,
business management, finance, healthcare, Industry 4.0, smart grid, intelligent
transportation systems, and computer systems. We mention topics not reviewed
yet, and list a collection of RL resources. After presenting a brief summary,
we close with discussions.
  Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant
update.
",2017-01-25T11:52:11Z,http://arxiv.org/abs/1701.07274v6,Yuxi Li
"Pretraining with Artificial Language: Studying Transferable Knowledge in
  Language Models","  We investigate what kind of structural knowledge learned in neural network
encoders is transferable to processing natural language. We design artificial
languages with structural properties that mimic natural language, pretrain
encoders on the data, and see how much performance the encoder exhibits on
downstream tasks in natural language. Our experimental results show that
pretraining with an artificial language with a nesting dependency structure
provides some knowledge transferable to natural language. A follow-up probing
analysis indicates that its success in the transfer is related to the amount of
encoded contextual information and what is transferred is the knowledge of
position-aware context dependence of language. Our results provide insights
into how neural network encoders process human languages and the source of
cross-lingual transferability of recent multilingual language models.
",2022-03-19T13:29:48Z,http://arxiv.org/abs/2203.10326v2,"Ryokan Ri, Yoshimasa Tsuruoka"
BADM: Batch ADMM for Deep Learning,"  Stochastic gradient descent-based algorithms are widely used for training
deep neural networks but often suffer from slow convergence. To address the
challenge, we leverage the framework of the alternating direction method of
multipliers (ADMM) to develop a novel data-driven algorithm, called batch ADMM
(BADM). The fundamental idea of the proposed algorithm is to split the training
data into batches, which is further divided into sub-batches where primal and
dual variables are updated to generate global parameters through aggregation.
We evaluate the performance of BADM across various deep learning tasks,
including graph modelling, computer vision, image generation, and natural
language processing. Extensive numerical experiments demonstrate that BADM
achieves faster convergence and superior testing accuracy compared to other
state-of-the-art optimizers.
",2024-06-30T20:47:15Z,http://arxiv.org/abs/2407.01640v1,"Ouya Wang, Shenglong Zhou, Geoffrey Ye Li"
Self-supervised Learning: Generative or Contrastive,"  Deep supervised learning has achieved great success in the last decade.
However, its deficiencies of dependence on manual labels and vulnerability to
attacks have driven people to explore a better solution. As an alternative,
self-supervised learning attracts many researchers for its soaring performance
on representation learning in the last several years. Self-supervised
representation learning leverages input data itself as supervision and benefits
almost all types of downstream tasks. In this survey, we take a look into new
self-supervised learning methods for representation in computer vision, natural
language processing, and graph learning. We comprehensively review the existing
empirical methods and summarize them into three main categories according to
their objectives: generative, contrastive, and generative-contrastive
(adversarial). We further investigate related theoretical analysis work to
provide deeper thoughts on how self-supervised learning works. Finally, we
briefly discuss open problems and future directions for self-supervised
learning. An outline slide for the survey is provided.
",2020-06-15T08:40:03Z,http://arxiv.org/abs/2006.08218v5,"Xiao Liu, Fanjin Zhang, Zhenyu Hou, Zhaoyu Wang, Li Mian, Jing Zhang, Jie Tang"
Towards Deep Conversational Recommendations,"  There has been growing interest in using neural networks and deep learning
techniques to create dialogue systems. Conversational recommendation is an
interesting setting for the scientific exploration of dialogue with natural
language as the associated discourse involves goal-driven dialogue that often
transforms naturally into more free-form chat. This paper provides two
contributions. First, until now there has been no publicly available
large-scale dataset consisting of real-world dialogues centered around
recommendations. To address this issue and to facilitate our exploration here,
we have collected ReDial, a dataset consisting of over 10,000 conversations
centered around the theme of providing movie recommendations. We make this data
available to the community for further research. Second, we use this dataset to
explore multiple facets of conversational recommendations. In particular we
explore new neural architectures, mechanisms, and methods suitable for
composing conversational recommendation systems. Our dataset allows us to
systematically probe model sub-components addressing different parts of the
overall problem domain ranging from: sentiment analysis and cold-start
recommendation generation to detailed aspects of how natural language is used
in this setting in the real world. We combine such sub-components into a
full-blown dialogue system and examine its behavior.
",2018-12-18T19:34:32Z,http://arxiv.org/abs/1812.07617v2,"Raymond Li, Samira Kahou, Hannes Schulz, Vincent Michalski, Laurent Charlin, Chris Pal"
Grounded Language Acquisition From Object and Action Imagery,"  Deep learning approaches to natural language processing have made great
strides in recent years. While these models produce symbols that convey vast
amounts of diverse knowledge, it is unclear how such symbols are grounded in
data from the world. In this paper, we explore the development of a private
language for visual data representation by training emergent language (EL)
encoders/decoders in both i) a traditional referential game environment and ii)
a contrastive learning environment utilizing a within-class matching training
paradigm. An additional classification layer utilizing neural machine
translation and random forest classification was used to transform symbolic
representations (sequences of integer symbols) to class labels. These methods
were applied in two experiments focusing on object recognition and action
recognition. For object recognition, a set of sketches produced by human
participants from real imagery was used (Sketchy dataset) and for action
recognition, 2D trajectories were generated from 3D motion capture systems
(MOVI dataset). In order to interpret the symbols produced for data in each
experiment, gradient-weighted class activation mapping (Grad-CAM) methods were
used to identify pixel regions indicating semantic features which contribute
evidence towards symbols in learned languages. Additionally, a t-distributed
stochastic neighbor embedding (t-SNE) method was used to investigate embeddings
learned by CNN feature extractors.
",2023-09-12T15:52:08Z,http://arxiv.org/abs/2309.06335v1,"James Robert Kubricht, Zhaoyuan Yang, Jianwei Qiu, Peter Henry Tu"
Multimodal Recommender Systems in the Prediction of Disease Comorbidity,"  While deep-learning based recommender systems utilizing collaborative
filtering have been commonly used for recommendation in other domains, their
application in the medical domain have been limited. In addition to modeling
user-item interactions, we show that deep-learning based recommender systems
can be used to model subject-disease code interactions. Two novel applications
of deep learning-based recommender systems using Neural Collaborative Filtering
(NCF) and Deep Hybrid Filtering (DHF) were utilized for disease diagnosis based
on known past patient comorbidities. Two datasets, one incorporating all
subject-disease code pairs present in the MIMIC-III database, and the other
incorporating the top 50 most commonly occurring diseases, were used for
prediction. Accuracy and Hit Ratio@10 were utilized as metrics to estimate
model performance. The performance of the NCF model making use of the reduced
""top 50"" ICD-9 code dataset was found to be lower (accuracy of ~80% and hit
ratio@10 of 35%) as compared to the performance of the NCF model trained on all
ICD-9 codes (accuracy of ~90% and hit ratio@10 of ~80%). Reasons for the
superior performance of the sparser dataset with all ICD codes can be mainly
attributed to the higher volume of data and the robustness of deep-learning
based recommender systems with modeling sparse data. Additionally, results from
the DHF models reflect better performance than the NCF models, with a better
accuracy of 94.4% and hit ratio@10 of 85.36%, reflecting the importance of the
incorporation of clinical note information. Additionally, compared to
literature reports utilizing primarily natural language processing-based
predictions for the task of ICD-9 code co-occurrence, the novel deep
learning-based recommender systems approach performed better. Overall, the deep
learning-based recommender systems have shown promise in predicting disease
comorbidity.
",2023-08-30T01:40:45Z,http://arxiv.org/abs/2309.08613v1,Aashish Cheruvu
TookaBERT: A Step Forward for Persian NLU,"  The field of natural language processing (NLP) has seen remarkable
advancements, thanks to the power of deep learning and foundation models.
Language models, and specifically BERT, have been key players in this progress.
In this study, we trained and introduced two new BERT models using Persian
data. We put our models to the test, comparing them to seven existing models
across 14 diverse Persian natural language understanding (NLU) tasks. The
results speak for themselves: our larger model outperforms the competition,
showing an average improvement of at least +2.8 points. This highlights the
effectiveness and potential of our new BERT models for Persian NLU tasks.
",2024-07-23T11:12:47Z,http://arxiv.org/abs/2407.16382v1,"MohammadAli SadraeiJavaheri, Ali Moghaddaszadeh, Milad Molazadeh, Fariba Naeiji, Farnaz Aghababaloo, Hamideh Rafiee, Zahra Amirmahani, Tohid Abedini, Fatemeh Zahra Sheikhi, Amirmohammad Salehoof"
Bayesian Neural Networks: An Introduction and Survey,"  Neural Networks (NNs) have provided state-of-the-art results for many
challenging machine learning tasks such as detection, regression and
classification across the domains of computer vision, speech recognition and
natural language processing. Despite their success, they are often implemented
in a frequentist scheme, meaning they are unable to reason about uncertainty in
their predictions. This article introduces Bayesian Neural Networks (BNNs) and
the seminal research regarding their implementation. Different approximate
inference methods are compared, and used to highlight where future research can
improve on current methods.
",2020-06-22T06:30:15Z,http://arxiv.org/abs/2006.12024v1,"Ethan Goan, Clinton Fookes"
ChatDev: Communicative Agents for Software Development,"  Software development is a complex task that necessitates cooperation among
multiple members with diverse skills. Numerous studies used deep learning to
improve specific phases in a waterfall model, such as design, coding, and
testing. However, the deep learning model in each phase requires unique
designs, leading to technical inconsistencies across various phases, which
results in a fragmented and ineffective development process. In this paper, we
introduce ChatDev, a chat-powered software development framework in which
specialized agents driven by large language models (LLMs) are guided in what to
communicate (via chat chain) and how to communicate (via communicative
dehallucination). These agents actively contribute to the design, coding, and
testing phases through unified language-based communication, with solutions
derived from their multi-turn dialogues. We found their utilization of natural
language is advantageous for system design, and communicating in programming
language proves helpful in debugging. This paradigm demonstrates how linguistic
communication facilitates multi-agent collaboration, establishing language as a
unifying bridge for autonomous task-solving among LLM agents. The code and data
are available at https://github.com/OpenBMB/ChatDev.
",2023-07-16T02:11:34Z,http://arxiv.org/abs/2307.07924v5,"Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, Maosong Sun"
"Detection of Increased Time Intervals of Anti-Vaccine Tweets for
  COVID-19 Vaccine with BERT Model","  The most effective of the solutions against Covid-19 is the various vaccines
developed. Distrust of vaccines can hinder the rapid and effective use of this
remedy. One of the means of expressing the thoughts of society is social media.
Determining the time intervals during which anti-vaccination increases in
social media can help institutions determine the strategy to be used in
combating anti-vaccination. Recording and tracking every tweet entered with
human labor would be inefficient, so various automation solutions are needed.
In this study, The Bidirectional Encoder Representations from Transformers
(BERT) model, which is a deep learning-based natural language processing (NLP)
model, was used. In a dataset of 1506 tweets divided into four different
categories as news, irrelevant, anti-vaccine, and vaccine supporters, the model
was trained with a learning rate of 5e-6 for 25 epochs. To determine the
intervals in which anti-vaccine tweets are concentrated, the categories to
which 652840 tweets belong were determined by using the trained model. The
change of the determined categories overtime was visualized and the events that
could cause the change were determined. As a result of model training, in the
test dataset, the f-score of 0.81 and AUC values for different classes were
obtained as 0.99,0.91, 0.92, 0.92, respectively. In this model, unlike the
studies in the literature, an auxiliary system is designed that provides data
that institutions can use when determining their strategy by measuring and
visualizing the frequency of anti-vaccine tweets in a time interval, different
from detecting and censoring such tweets.
",2022-01-12T18:30:23Z,http://arxiv.org/abs/2202.00477v1,"Ülkü Tuncer Küçüktaş, Fatih Uysal, Fırat Hardalaç, İsmail Biri"
"Psychomatics -- A Multidisciplinary Framework for Understanding
  Artificial Minds","  Although LLMs and other artificial intelligence systems demonstrate cognitive
skills similar to humans, like concept learning and language acquisition, the
way they process information fundamentally differs from biological cognition.
To better understand these differences this paper introduces Psychomatics, a
multidisciplinary framework bridging cognitive science, linguistics, and
computer science. It aims to better understand the high-level functioning of
LLMs, focusing specifically on how LLMs acquire, learn, remember, and use
information to produce their outputs. To achieve this goal, Psychomatics will
rely on a comparative methodology, starting from a theory-driven research
question - is the process of language development and use different in humans
and LLMs? - drawing parallels between LLMs and biological systems. Our analysis
shows how LLMs can map and manipulate complex linguistic patterns in their
training data. Moreover, LLMs can follow Grice's Cooperative Principle to
provide relevant and informative responses. However, human cognition draws from
multiple sources of meaning, including experiential, emotional, and imaginative
facets, which transcend mere language processing and are rooted in our social
and developmental trajectories. Moreover, current LLMs lack physical
embodiment, reducing their ability to make sense of the intricate interplay
between perception, action, and cognition that shapes human understanding and
expression. Ultimately, Psychomatics holds the potential to yield
transformative insights into the nature of language, cognition, and
intelligence, both artificial and biological. Moreover, by drawing parallels
between LLMs and human cognitive processes, Psychomatics can inform the
development of more robust and human-like AI systems.
",2024-07-23T12:53:41Z,http://arxiv.org/abs/2407.16444v1,"Giuseppe Riva, Fabrizia Mantovani, Brenda K. Wiederhold, Antonella Marchetti, Andrea Gaggioli"
Few-shot Learning for Slot Tagging with Attentive Relational Network,"  Metric-based learning is a well-known family of methods for few-shot
learning, especially in computer vision. Recently, they have been used in many
natural language processing applications but not for slot tagging. In this
paper, we explore metric-based learning methods in the slot tagging task and
propose a novel metric-based learning architecture - Attentive Relational
Network. Our proposed method extends relation networks, making them more
suitable for natural language processing applications in general, by leveraging
pretrained contextual embeddings such as ELMO and BERT and by using attention
mechanism. The results on SNIPS data show that our proposed method outperforms
other state-of-the-art metric-based learning methods.
",2021-03-03T11:24:24Z,http://arxiv.org/abs/2103.02333v1,"Cennet Oguz, Ngoc Thang Vu"
"Rethinking Sparse Lexical Representations for Image Retrieval in the Age
  of Rising Multi-Modal Large Language Models","  In this paper, we rethink sparse lexical representations for image retrieval.
By utilizing multi-modal large language models (M-LLMs) that support visual
prompting, we can extract image features and convert them into textual data,
enabling us to utilize efficient sparse retrieval algorithms employed in
natural language processing for image retrieval tasks. To assist the LLM in
extracting image features, we apply data augmentation techniques for key
expansion and analyze the impact with a metric for relevance between images and
textual data. We empirically show the superior precision and recall performance
of our image retrieval method compared to conventional vision-language
model-based methods on the MS-COCO, PASCAL VOC, and NUS-WIDE datasets in a
keyword-based image retrieval scenario, where keywords serve as search queries.
We also demonstrate that the retrieval performance can be improved by
iteratively incorporating keywords into search queries.
",2024-08-29T06:54:03Z,http://arxiv.org/abs/2408.16296v1,"Kengo Nakata, Daisuke Miyashita, Youyang Ng, Yasuto Hoshi, Jun Deguchi"
"Entangling Solid Solutions: Machine Learning of Tensor Networks for
  Materials Property Prediction","  Progress in the application of machine learning techniques to the prediction
of solid-state and molecular materials properties has been greatly facilitated
by the development state-of-the-art feature representations and novel deep
learning architectures. A large class of atomic structure representations based
on expansions of smoothed atomic densities have been shown to correspond to
specific choices of basis sets in an abstract many-body Hilbert space.
Concurrently, tensor network structures, conventionally the purview of quantum
many-body physics and quantum information, have been successfully applied in
supervised and unsupervised learning tasks in computer vision and natural
language processing. In this work, we argue that architectures based on tensor
networks are well-suited to machine learning on Hilbert-space representations
of atomic structures. This is demonstrated on supervised learning tasks
involving widely available datasets of density functional theory calculations
of metal and semiconductor alloys. In particular, we show that certain standard
tensor network topologies exhibit strong generalizability even on small
training datasets while being parametrically efficient. We further relate this
generalizability to the presence of complex entanglement in the trained tensor
networks. We also discuss connections to learning with generalized structural
kernels and related strategies for compressing large input feature spaces.
",2022-03-17T21:02:58Z,http://arxiv.org/abs/2203.09613v1,"David E. Sommer, Scott T. Dunham"
"Large-scale representation learning from visually grounded untranscribed
  speech","  Systems that can associate images with their spoken audio captions are an
important step towards visually grounded language learning. We describe a
scalable method to automatically generate diverse audio for image captioning
datasets. This supports pretraining deep networks for encoding both audio and
images, which we do via a dual encoder that learns to align latent
representations from both modalities. We show that a masked margin softmax loss
for such models is superior to the standard triplet loss. We fine-tune these
models on the Flickr8k Audio Captions Corpus and obtain state-of-the-art
results---improving recall in the top 10 from 29.6% to 49.5%. We also obtain
human ratings on retrieval outputs to better assess the impact of incidentally
matching image-caption pairs that were not associated in the data, finding that
automatic evaluation substantially underestimates the quality of the retrieved
results.
",2019-09-19T02:50:23Z,http://arxiv.org/abs/1909.08782v1,"Gabriel Ilharco, Yuan Zhang, Jason Baldridge"
"Perceiving the World: Question-guided Reinforcement Learning for
  Text-based Games","  Text-based games provide an interactive way to study natural language
processing. While deep reinforcement learning has shown effectiveness in
developing the game playing agent, the low sample efficiency and the large
action space remain to be the two major challenges that hinder the DRL from
being applied in the real world. In this paper, we address the challenges by
introducing world-perceiving modules, which automatically decompose tasks and
prune actions by answering questions about the environment. We then propose a
two-phase training framework to decouple language learning from reinforcement
learning, which further improves the sample efficiency. The experimental
results show that the proposed method significantly improves the performance
and sample efficiency. Besides, it shows robustness against compound error and
limited pre-training data.
",2022-03-20T04:23:57Z,http://arxiv.org/abs/2204.09597v2,"Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, Chengqi Zhang"
"A Decade of In-text Citation Analysis based on Natural Language
  Processing and Machine Learning Techniques: An overview of empirical studies","  Citation analysis is one of the most frequently used methods in research
evaluation. We are seeing significant growth in citation analysis through
bibliometric metadata, primarily due to the availability of citation databases
such as the Web of Science, Scopus, Google Scholar, Microsoft Academic, and
Dimensions. Due to better access to full-text publication corpora in recent
years, information scientists have gone far beyond traditional bibliometrics by
tapping into advancements in full-text data processing techniques to measure
the impact of scientific publications in contextual terms. This has led to
technical developments in citation context and content analysis, citation
classifications, citation sentiment analysis, citation summarisation, and
citation-based recommendation. This article aims to narratively review the
studies on these developments. Its primary focus is on publications that have
used natural language processing and machine learning techniques to analyse
citations.
",2020-08-29T17:27:08Z,http://arxiv.org/abs/2008.13020v1,"Sehrish Iqbal, Saeed-Ul Hassan, Naif Radi Aljohani, Salem Alelyani, Raheel Nawaz, Lutz Bornmann"
"Language and Intelligence, Artificial vs. Natural or What Can and What
  Cannot AI Do with NL?","  In this talk, I argue that there are certain pragmatic features of natural
language (that I will call 'productivity' and 'malleability', on top of
syntactical generativity and semantical compositionality), which are not only
hard, but even impossible to capture in an artificial language used by an AI
system, and the reason for this is to be found in certain deep, metaphysical
differences between artificial and natural intelligence, accounting for the
differences in their respective processes of concept-formation.
",2022-08-31T10:11:50Z,http://arxiv.org/abs/2209.12829v1,Gyula Klima
"Medical code prediction with multi-view convolution and
  description-regularized label-dependent attention","  A ubiquitous task in processing electronic medical data is the assignment of
standardized codes representing diagnoses and/or procedures to free-text
documents such as medical reports. This is a difficult natural language
processing task that requires parsing long, heterogeneous documents and
selecting a set of appropriate codes from tens of thousands of
possibilities---many of which have very few positive training samples. We
present a deep learning system that advances the state of the art for the
MIMIC-III dataset, achieving a new best micro F1-measure of 55.85\%,
significantly outperforming the previous best result (Mullenbach et al. 2018).
We achieve this through a number of enhancements, including two major novel
contributions: multi-view convolutional channels, which effectively learn to
adjust kernel sizes throughout the input; and attention regularization,
mediated by natural-language code descriptions, which helps overcome sparsity
for thousands of uncommon codes. These and other modifications are selected to
address difficulties inherent to both automated coding specifically and deep
learning generally. Finally, we investigate our accuracy results in detail to
individually measure the impact of these contributions and point the way
towards future algorithmic improvements.
",2018-11-05T00:54:03Z,http://arxiv.org/abs/1811.01468v1,"Najmeh Sadoughi, Greg P. Finley, James Fone, Vignesh Murali, Maxim Korenevski, Slava Baryshnikov, Nico Axtmann, Mark Miller, David Suendermann-Oeft"
A Multi-cascaded Deep Model for Bilingual SMS Classification,"  Most studies on text classification are focused on the English language.
However, short texts such as SMS are influenced by regional languages. This
makes the automatic text classification task challenging due to the
multilingual, informal, and noisy nature of language in the text. In this work,
we propose a novel multi-cascaded deep learning model called McM for bilingual
SMS classification. McM exploits $n$-gram level information as well as
long-term dependencies of text for learning. Our approach aims to learn a model
without any code-switching indication, lexical normalization, language
translation, or language transliteration. The model relies entirely upon the
text as no external knowledge base is utilized for learning. For this purpose,
a 12 class bilingual text dataset is developed from SMS feedbacks of citizens
on public services containing mixed Roman Urdu and English languages. Our model
achieves high accuracy for classification on this dataset and outperforms the
previous model for multilingual text classification, highlighting language
independence of McM.
",2019-11-29T11:35:13Z,http://arxiv.org/abs/1911.13066v1,"Muhammad Haroon Shakeel, Asim Karim, Imdadullah Khan"
"Deep Learning Transformer Architecture for Named Entity Recognition on
  Low Resourced Languages: State of the art results","  This paper reports on the evaluation of Deep Learning (DL) transformer
architecture models for Named-Entity Recognition (NER) on ten low-resourced
South African (SA) languages. In addition, these DL transformer models were
compared to other Neural Network and Machine Learning (ML) NER models. The
findings show that transformer models substantially improve performance when
applying discrete fine-tuning parameters per language. Furthermore, fine-tuned
transformer models outperform other neural network and machine learning models
on NER with the low-resourced SA languages. For example, the transformer models
obtained the highest F-scores for six of the ten SA languages and the highest
average F-score surpassing the Conditional Random Fields ML model. Practical
implications include developing high-performance NER capability with less
effort and resource costs, potentially improving downstream NLP tasks such as
Machine Translation (MT). Therefore, the application of DL transformer
architecture models for NLP NER sequence tagging tasks on low-resourced SA
languages is viable. Additional research could evaluate the more recent
transformer architecture models on other Natural Language Processing tasks and
applications, such as Phrase chunking, MT, and Part-of-Speech tagging.
",2021-11-01T11:02:01Z,http://arxiv.org/abs/2111.00830v2,Ridewaan Hanslo
Helix: Holistic Optimization for Accelerating Iterative Machine Learning,"  Machine learning workflow development is a process of trial-and-error:
developers iterate on workflows by testing out small modifications until the
desired accuracy is achieved. Unfortunately, existing machine learning systems
focus narrowly on model training---a small fraction of the overall development
time---and neglect to address iterative development. We propose Helix, a
machine learning system that optimizes the execution across
iterations---intelligently caching and reusing, or recomputing intermediates as
appropriate. Helix captures a wide variety of application needs within its
Scala DSL, with succinct syntax defining unified processes for data
preprocessing, model specification, and learning. We demonstrate that the reuse
problem can be cast as a Max-Flow problem, while the caching problem is
NP-Hard. We develop effective lightweight heuristics for the latter. Empirical
evaluation shows that Helix is not only able to handle a wide variety of use
cases in one unified workflow but also much faster, providing run time
reductions of up to 19x over state-of-the-art systems, such as DeepDive or
KeystoneML, on four real-world applications in natural language processing,
computer vision, social and natural sciences.
",2018-12-14T02:32:45Z,http://arxiv.org/abs/1812.05762v1,"Doris Xin, Stephen Macke, Litian Ma, Jialin Liu, Shuchen Song, Aditya Parameswaran"
"Adaptive Feature Fusion: Enhancing Generalization in Deep Learning
  Models","  In recent years, deep learning models have demonstrated remarkable success in
various domains, such as computer vision, natural language processing, and
speech recognition. However, the generalization capabilities of these models
can be negatively impacted by the limitations of their feature fusion
techniques. This paper introduces an innovative approach, Adaptive Feature
Fusion (AFF), to enhance the generalization of deep learning models by
dynamically adapting the fusion process of feature representations.
  The proposed AFF framework is designed to incorporate fusion layers into
existing deep learning architectures, enabling seamless integration and
improved performance. By leveraging a combination of data-driven and
model-based fusion strategies, AFF is able to adaptively fuse features based on
the underlying data characteristics and model requirements. This paper presents
a detailed description of the AFF framework, including the design and
implementation of fusion layers for various architectures.
  Extensive experiments are conducted on multiple benchmark datasets, with the
results demonstrating the superiority of the AFF approach in comparison to
traditional feature fusion techniques. The analysis showcases the effectiveness
of AFF in enhancing generalization capabilities, leading to improved
performance across different tasks and applications.
  Finally, the paper discusses various real-world use cases where AFF can be
employed, providing insights into its practical applicability. The conclusion
highlights the potential for future research directions, including the
exploration of advanced fusion strategies and the extension of AFF to other
machine learning paradigms.
",2023-04-04T21:41:38Z,http://arxiv.org/abs/2304.03290v1,Neelesh Mungoli
Data Readiness for Natural Language Processing,"  This document concerns data readiness in the context of machine learning and
Natural Language Processing. It describes how an organization may proceed to
identify, make available, validate, and prepare data to facilitate automated
analysis methods. The contents of the document is based on the practical
challenges and frequently asked questions we have encountered in our work as an
applied research institute with helping organizations and companies, both in
the public and private sectors, to use data in their business processes.
",2020-09-04T07:53:43Z,http://arxiv.org/abs/2009.02043v2,"Fredrik Olsson, Magnus Sahlgren"
"Learning What Makes a Difference from Counterfactual Examples and
  Gradient Supervision","  One of the primary challenges limiting the applicability of deep learning is
its susceptibility to learning spurious correlations rather than the underlying
mechanisms of the task of interest. The resulting failure to generalise cannot
be addressed by simply using more data from the same distribution. We propose
an auxiliary training objective that improves the generalization capabilities
of neural networks by leveraging an overlooked supervisory signal found in
existing datasets. We use pairs of minimally-different examples with different
labels, a.k.a counterfactual or contrasting examples, which provide a signal
indicative of the underlying causal structure of the task. We show that such
pairs can be identified in a number of existing datasets in computer vision
(visual question answering, multi-label image classification) and natural
language processing (sentiment analysis, natural language inference). The new
training objective orients the gradient of a model's decision function with
pairs of counterfactual examples. Models trained with this technique
demonstrate improved performance on out-of-distribution test sets.
",2020-04-20T02:47:49Z,http://arxiv.org/abs/2004.09034v1,"Damien Teney, Ehsan Abbasnedjad, Anton van den Hengel"
"Underwater-Art: Expanding Information Perspectives With Text Templates
  For Underwater Acoustic Target Recognition","  Underwater acoustic target recognition is an intractable task due to the
complex acoustic source characteristics and sound propagation patterns. Limited
by insufficient data and narrow information perspective, recognition models
based on deep learning seem far from satisfactory in practical underwater
scenarios. Although underwater acoustic signals are severely influenced by
distance, channel depth, or other factors, annotations of relevant information
are often non-uniform, incomplete, and hard to use. In our work, we propose to
implement Underwater Acoustic Recognition based on Templates made up of rich
relevant information (hereinafter called ""UART""). We design templates to
integrate relevant information from different perspectives into descriptive
natural language. UART adopts an audio-spectrogram-text tri-modal contrastive
learning framework, which endows UART with the ability to guide the learning of
acoustic representations by descriptive natural language. Our experiments
reveal that UART has better recognition capability and generalization
performance than traditional paradigms. Furthermore, the pre-trained UART model
could provide superior prior knowledge for the recognition model in the
scenario without any auxiliary annotation.
",2023-05-31T07:28:37Z,http://arxiv.org/abs/2305.19612v2,"Yuan Xie, Jiawei Ren, Ji Xu"
EZLearn: Exploiting Organic Supervision in Large-Scale Data Annotation,"  Many real-world applications require automated data annotation, such as
identifying tissue origins based on gene expressions and classifying images
into semantic categories. Annotation classes are often numerous and subject to
changes over time, and annotating examples has become the major bottleneck for
supervised learning methods. In science and other high-value domains, large
repositories of data samples are often available, together with two sources of
organic supervision: a lexicon for the annotation classes, and text
descriptions that accompany some data samples. Distant supervision has emerged
as a promising paradigm for exploiting such indirect supervision by
automatically annotating examples where the text description contains a class
mention in the lexicon. However, due to linguistic variations and ambiguities,
such training data is inherently noisy, which limits the accuracy of this
approach. In this paper, we introduce an auxiliary natural language processing
system for the text modality, and incorporate co-training to reduce noise and
augment signal in distant supervision. Without using any manually labeled data,
our EZLearn system learned to accurately annotate data samples in functional
genomics and scientific figure comprehension, substantially outperforming
state-of-the-art supervised methods trained on tens of thousands of annotated
examples.
",2017-09-25T17:10:46Z,http://arxiv.org/abs/1709.08600v3,"Maxim Grechkin, Hoifung Poon, Bill Howe"
"Learning User Preferences and Understanding Calendar Contexts for Event
  Scheduling","  With online calendar services gaining popularity worldwide, calendar data has
become one of the richest context sources for understanding human behavior.
However, event scheduling is still time-consuming even with the development of
online calendars. Although machine learning based event scheduling models have
automated scheduling processes to some extent, they often fail to understand
subtle user preferences and complex calendar contexts with event titles written
in natural language. In this paper, we propose Neural Event Scheduling
Assistant (NESA) which learns user preferences and understands calendar
contexts, directly from raw online calendars for fully automated and highly
effective event scheduling. We leverage over 593K calendar events for NESA to
learn scheduling personal events, and we further utilize NESA for
multi-attendee event scheduling. NESA successfully incorporates deep neural
networks such as Bidirectional Long Short-Term Memory, Convolutional Neural
Network, and Highway Network for learning the preferences of each user and
understanding calendar context based on natural languages. The experimental
results show that NESA significantly outperforms previous baseline models in
terms of various evaluation metrics on both personal and multi-attendee event
scheduling tasks. Our qualitative analysis demonstrates the effectiveness of
each layer in NESA and learned user preferences.
",2018-09-05T04:15:13Z,http://arxiv.org/abs/1809.01316v3,"Donghyeon Kim, Jinhyuk Lee, Donghee Choi, Jaehoon Choi, Jaewoo Kang"
"Translating synthetic natural language to database queries: a polyglot
  deep learning framework","  The number of databases as well as their size and complexity is increasing.
This creates a barrier to use especially for non-experts, who have to come to
grips with the nature of the data, the way it has been represented in the
database, and the specific query languages or user interfaces by which data are
accessed. These difficulties worsen in research settings, where it is common to
work with many different databases. One approach to improving this situation is
to allow users to pose their queries in natural language.
  In this work we describe a machine learning framework, Polyglotter, that in a
general way supports the mapping of natural language searches to database
queries. Importantly, it does not require the creation of manually annotated
data for training and therefore can be applied easily to multiple domains. The
framework is polyglot in the sense that it supports multiple different database
engines that are accessed with a variety of query languages, including SQL and
Cypher. Furthermore Polyglotter also supports multi-class queries.
  Our results indicate that our framework performs well on both synthetic and
real databases, and may provide opportunities for database maintainers to
improve accessibility to their resources.
",2021-04-14T17:43:51Z,http://arxiv.org/abs/2104.07010v1,"Adrián Bazaga, Nupur Gunwant, Gos Micklem"
"Colloquial Persian POS (CPPOS) Corpus: A Novel Corpus for Colloquial
  Persian Part of Speech Tagging","  Introduction: Part-of-Speech (POS) Tagging, the process of classifying words
into their respective parts of speech (e.g., verb or noun), is essential in
various natural language processing applications. POS tagging is a crucial
preprocessing task for applications like machine translation, question
answering, sentiment analysis, etc. However, existing corpora for POS tagging
in Persian mainly consist of formal texts, such as daily news and newspapers.
As a result, smart POS tools, machine learning models, and deep learning models
trained on these corpora may not perform optimally for processing colloquial
text in social network analysis. Method: This paper introduces a novel corpus,
""Colloquial Persian POS"" (CPPOS), specifically designed to support colloquial
Persian text. The corpus includes formal and informal text collected from
various domains such as political, social, and commercial on Telegram, Twitter,
and Instagram more than 520K labeled tokens. After collecting posts from these
social platforms for one year, special preprocessing steps were conducted,
including normalization, sentence tokenizing, and word tokenizing for social
text. The tokens and sentences were then manually annotated and verified by a
team of linguistic experts. This study also defines a POS tagging guideline for
annotating the data and conducting the annotation process. Results: To evaluate
the quality of CPPOS, various deep learning models, such as the RNN family,
were trained using the constructed corpus. A comparison with another well-known
Persian POS corpus named ""Bijankhan"" and the Persian Hazm POS tool trained on
Bijankhan revealed that our model trained on CPPOS outperforms them. With the
new corpus and the BiLSTM deep neural model, we achieved a 14% improvement over
the previous dataset.
",2023-10-01T05:06:33Z,http://arxiv.org/abs/2310.00572v1,"Leyla Rabiei, Farzaneh Rahmani, Mohammad Khansari, Zeinab Rajabi, Moein Salimi"
ModelHub.AI: Dissemination Platform for Deep Learning Models,"  Recent advances in artificial intelligence research have led to a profusion
of studies that apply deep learning to problems in image analysis and natural
language processing among others. Additionally, the availability of open-source
computational frameworks has lowered the barriers to implementing
state-of-the-art methods across multiple domains. Albeit leading to major
performance breakthroughs in some tasks, effective dissemination of deep
learning algorithms remains challenging, inhibiting reproducibility and
benchmarking studies, impeding further validation, and ultimately hindering
their effectiveness in the cumulative scientific progress. In developing a
platform for sharing research outputs, we present ModelHub.AI
(www.modelhub.ai), a community-driven container-based software engine and
platform for the structured dissemination of deep learning models. For
contributors, the engine controls data flow throughout the inference cycle,
while the contributor-facing standard template exposes model-specific functions
including inference, as well as pre- and post-processing. Python and RESTful
Application programming interfaces (APIs) enable users to interact with models
hosted on ModelHub.AI and allows both researchers and developers to utilize
models out-of-the-box. ModelHub.AI is domain-, data-, and framework-agnostic,
catering to different workflows and contributors' preferences.
",2019-11-26T22:48:11Z,http://arxiv.org/abs/1911.13218v1,"Ahmed Hosny, Michael Schwier, Christoph Berger, Evin P Örnek, Mehmet Turan, Phi V Tran, Leon Weninger, Fabian Isensee, Klaus H Maier-Hein, Richard McKinley, Michael T Lu, Udo Hoffmann, Bjoern Menze, Spyridon Bakas, Andriy Fedorov, Hugo JWL Aerts"
Who wrote this book? A challenge for e-commerce,"  Modern e-commerce catalogs contain millions of references, associated with
textual and visual information that is of paramount importance for the products
to be found via search or browsing. Of particular significance is the book
category, where the author name(s) field poses a significant challenge. Indeed,
books written by a given author (such as F. Scott Fitzgerald) might be listed
with different authors' names in a catalog due to abbreviations and spelling
variants and mistakes, among others. To solve this problem at scale, we design
a composite system involving open data sources for books as well as machine
learning components leveraging deep learning-based techniques for natural
language processing. In particular, we use Siamese neural networks for an
approximate match with known author names, and direct correction of the
provided author's name using sequence-to-sequence learning with neural
networks. We evaluate this approach on product data from the e-commerce website
Rakuten France, and find that the top proposal of the system is the normalized
author name with 72% accuracy.
",2019-04-19T10:13:07Z,http://arxiv.org/abs/1905.01973v1,"Béranger Dumont, Simona Maggio, Ghiles Sidi Said, Quoc-Tien Au"
"Integration of Neural Network-Based Symbolic Regression in Deep Learning
  for Scientific Discovery","  Symbolic regression is a powerful technique that can discover analytical
equations that describe data, which can lead to explainable models and
generalizability outside of the training data set. In contrast, neural networks
have achieved amazing levels of accuracy on image recognition and natural
language processing tasks, but are often seen as black-box models that are
difficult to interpret and typically extrapolate poorly. Here we use a neural
network-based architecture for symbolic regression called the Equation Learner
(EQL) network and integrate it with other deep learning architectures such that
the whole system can be trained end-to-end through backpropagation. To
demonstrate the power of such systems, we study their performance on several
substantially different tasks. First, we show that the neural network can
perform symbolic regression and learn the form of several functions. Next, we
present an MNIST arithmetic task where a separate part of the neural network
extracts the digits. Finally, we demonstrate prediction of dynamical systems
where an unknown parameter is extracted through an encoder. We find that the
EQL-based architecture can extrapolate quite well outside of the training data
set compared to a standard neural network-based architecture, paving the way
for deep learning to be applied in scientific exploration and discovery.
",2019-12-10T17:07:52Z,http://arxiv.org/abs/1912.04825v2,"Samuel Kim, Peter Y. Lu, Srijon Mukherjee, Michael Gilbert, Li Jing, Vladimir Čeperić, Marin Soljačić"
"On Efficient Training of Large-Scale Deep Learning Models: A Literature
  Review","  The field of deep learning has witnessed significant progress, particularly
in computer vision (CV), natural language processing (NLP), and speech. The use
of large-scale models trained on vast amounts of data holds immense promise for
practical applications, enhancing industrial productivity and facilitating
social development. With the increasing demands on computational capacity,
though numerous studies have explored the efficient training, a comprehensive
summarization on acceleration techniques of training deep learning models is
still much anticipated. In this survey, we present a detailed review for
training acceleration. We consider the fundamental update formulation and split
its basic components into five main perspectives: (1) data-centric: including
dataset regularization, data sampling, and data-centric curriculum learning
techniques, which can significantly reduce the computational complexity of the
data samples; (2) model-centric, including acceleration of basic modules,
compression training, model initialization and model-centric curriculum
learning techniques, which focus on accelerating the training via reducing the
calculations on parameters; (3) optimization-centric, including the selection
of learning rate, the employment of large batchsize, the designs of efficient
objectives, and model average techniques, which pay attention to the training
policy and improving the generality for the large-scale models; (4) budgeted
training, including some distinctive acceleration methods on source-constrained
situations; (5) system-centric, including some efficient open-source
distributed libraries/systems which provide adequate hardware support for the
implementation of acceleration algorithms. By presenting this comprehensive
taxonomy, our survey presents a comprehensive review to understand the general
mechanisms within each component and their joint interaction.
",2023-04-07T11:13:23Z,http://arxiv.org/abs/2304.03589v1,"Li Shen, Yan Sun, Zhiyuan Yu, Liang Ding, Xinmei Tian, Dacheng Tao"
An Inference Approach To Question Answering Over Knowledge Graphs,"  Knowledge Graphs (KG) act as a great tool for holding distilled information
from large natural language text corpora. The problem of natural language
querying over knowledge graphs is essential for the human consumption of this
information. This problem is typically addressed by converting the natural
language query to a structured query and then firing the structured query on
the KG. Direct answering models over knowledge graphs in literature are very
few. The query conversion models and direct models both require specific
training data pertaining to the domain of the knowledge graph. In this work, we
convert the problem of natural language querying over knowledge graphs to an
inference problem over premise-hypothesis pairs. Using trained deep learning
models for the converted proxy inferencing problem, we provide the solution for
the original natural language querying problem. Our method achieves over 90%
accuracy on MetaQA dataset, beating the existing state-of-the-art. We also
propose a model for inferencing called Hierarchical Recurrent Path
Encoder(HRPE). The inferencing models can be fine-tuned to be used across
domains with less training data. Our approach does not require large
domain-specific training data for querying on new knowledge graphs from
different domains.
",2021-12-21T10:07:55Z,http://arxiv.org/abs/2112.11070v1,"Aayushee Gupta, K. M. Annervaz, Ambedkar Dukkipati, Shubhashis Sengupta"
"An Efficient Architecture for Predicting the Case of Characters using
  Sequence Models","  The dearth of clean textual data often acts as a bottleneck in several
natural language processing applications. The data available often lacks proper
case (uppercase or lowercase) information. This often comes up when text is
obtained from social media, messaging applications and other online platforms.
This paper attempts to solve this problem by restoring the correct case of
characters, commonly known as Truecasing. Doing so improves the accuracy of
several processing tasks further down in the NLP pipeline. Our proposed
architecture uses a combination of convolutional neural networks (CNN),
bi-directional long short-term memory networks (LSTM) and conditional random
fields (CRF), which work at a character level without any explicit feature
engineering. In this study we compare our approach to previous statistical and
deep learning based approaches. Our method shows an increment of 0.83 in F1
score over the current state of the art. Since truecasing acts as a
preprocessing step in several applications, every increment in the F1 score
leads to a significant improvement in the language processing tasks.
",2020-01-30T06:54:39Z,http://arxiv.org/abs/2002.00738v1,"Gopi Ramena, Divija Nagaraju, Sukumar Moharana, Debi Prasanna Mohanty, Naresh Purre"
Deep Neural Networks with Short Circuits for Improved Gradient Learning,"  Deep neural networks have achieved great success both in computer vision and
natural language processing tasks. However, mostly state-of-art methods highly
rely on external training or computing to improve the performance. To alleviate
the external reliance, we proposed a gradient enhancement approach, conducted
by the short circuit neural connections, to improve the gradient learning of
deep neural networks. The proposed short circuit is a unidirectional connection
that single back propagates the sensitive from the deep layer to the shallows.
Moreover, the short circuit formulates to be a gradient truncation of its
crossing layers which can plug into the backbone deep neural networks without
introducing external training parameters. Extensive experiments demonstrate
deep neural networks with our short circuit gain a large margin over the
baselines on both computer vision and natural language processing tasks.
",2020-09-23T15:51:37Z,http://arxiv.org/abs/2009.11719v1,"Ming Yan, Xueli Xiao, Joey Tianyi Zhou, Yi Pan"
"Graph Neural Network contextual embedding for Deep Learning on Tabular
  Data","  All industries are trying to leverage Artificial Intelligence (AI) based on
their existing big data which is available in so called tabular form, where
each record is composed of a number of heterogeneous continuous and categorical
columns also known as features. Deep Learning (DL) has constituted a major
breakthrough for AI in fields related to human skills like natural language
processing, but its applicability to tabular data has been more challenging.
More classical Machine Learning (ML) models like tree-based ensemble ones
usually perform better. This paper presents a novel DL model using Graph Neural
Network (GNN) more specifically Interaction Network (IN), for contextual
embedding and modelling interactions among tabular features. Its results
outperform those of a recently published survey with DL benchmark based on five
public datasets, also achieving competitive results when compared to
boosted-tree solutions.
",2023-03-11T17:13:24Z,http://arxiv.org/abs/2303.06455v2,"Mario Villaizán-Vallelado, Matteo Salvatori, Belén Carro Martinez, Antonio Javier Sanchez Esguevillas"
Posing Fair Generalization Tasks for Natural Language Inference,"  Deep learning models for semantics are generally evaluated using naturalistic
corpora. Adversarial methods, in which models are evaluated on new examples
with known semantic properties, have begun to reveal that good performance at
these naturalistic tasks can hide serious shortcomings. However, we should
insist that these evaluations be fair -that the models are given data
sufficient to support the requisite kinds of generalization. In this paper, we
define and motivate a formal notion of fairness in this sense. We then apply
these ideas to natural language inference by constructing very challenging but
provably fair artificial datasets and showing that standard neural models fail
to generalize in the required ways; only task-specific models that jointly
compose the premise and hypothesis are able to achieve high performance, and
even these models do not solve the task perfectly.
",2019-11-03T02:47:51Z,http://arxiv.org/abs/1911.00811v1,"Atticus Geiger, Ignacio Cases, Lauri Karttunen, Chris Potts"
"Active$^2$ Learning: Actively reducing redundancies in Active Learning
  methods for Sequence Tagging and Machine Translation","  While deep learning is a powerful tool for natural language processing (NLP)
problems, successful solutions to these problems rely heavily on large amounts
of annotated samples. However, manually annotating data is expensive and
time-consuming. Active Learning (AL) strategies reduce the need for huge
volumes of labeled data by iteratively selecting a small number of examples for
manual annotation based on their estimated utility in training the given model.
In this paper, we argue that since AL strategies choose examples independently,
they may potentially select similar examples, all of which may not contribute
significantly to the learning process. Our proposed approach,
Active$\mathbf{^2}$ Learning (A$\mathbf{^2}$L), actively adapts to the deep
learning model being trained to eliminate such redundant examples chosen by an
AL strategy. We show that A$\mathbf{^2}$L is widely applicable by using it in
conjunction with several different AL strategies and NLP tasks. We empirically
demonstrate that the proposed approach is further able to reduce the data
requirements of state-of-the-art AL strategies by $\approx \mathbf{3-25\%}$ on
an absolute scale on multiple NLP tasks while achieving the same performance
with virtually no additional computation overhead.
",2019-11-01T07:31:02Z,http://arxiv.org/abs/1911.00234v4,"Rishi Hazra, Parag Dutta, Shubham Gupta, Mohammed Abdul Qaathir, Ambedkar Dukkipati"
Data Agnostic RoBERTa-based Natural Language to SQL Query Generation,"  Relational databases are among the most widely used architectures to store
massive amounts of data in the modern world. However, there is a barrier
between these databases and the average user. The user often lacks the
knowledge of a query language such as SQL required to interact with the
database. The NL2SQL task aims at finding deep learning approaches to solve
this problem by converting natural language questions into valid SQL queries.
Given the sensitive nature of some databases and the growing need for data
privacy, we have presented an approach with data privacy at its core. We have
passed RoBERTa embeddings and data-agnostic knowledge vectors into LSTM based
submodels to predict the final query. Although we have not achieved state of
the art results, we have eliminated the need for the table data, right from the
training of the model, and have achieved a test set execution accuracy of
76.7%. By eliminating the table data dependency while training we have created
a model capable of zero shot learning based on the natural language question
and table schema alone.
",2020-10-11T13:18:46Z,http://arxiv.org/abs/2010.05243v3,"Debaditya Pal, Harsh Sharma, Kaustubh Chaudhari"
RF Signal Transformation and Classification using Deep Neural Networks,"  Deep neural networks (DNNs) designed for computer vision and natural language
processing tasks cannot be directly applied to the radio frequency (RF)
datasets. To address this challenge, we propose to convert the raw RF data to
data types that are suitable for off-the-shelf DNNs by introducing a
convolutional transform technique. In addition, we propose a simple 5-layer
convolutional neural network architecture (CONV-5) that can operate with raw RF
I/Q data without any transformation. Further, we put forward an RF dataset,
referred to as RF1024, to facilitate future RF research. RF1024 consists of 8
different RF modulation classes with each class having 1000/200 training/test
samples. Each sample of the RF1024 dataset contains 1024 complex I/Q values.
Lastly, the experiments are performed on the RadioML2016 and RF1024 datasets to
demonstrate the improved classification performance.
",2022-04-06T05:01:59Z,http://arxiv.org/abs/2204.03564v1,"Umar Khalid, Nazmul Karim, Nazanin Rahnavard"
Rock Guitar Tablature Generation via Natural Language Processing,"  Deep learning has recently empowered and democratized generative modeling of
images and text, with additional concurrent works exploring the possibility of
generating more complex forms of data, such as audio. However, the high
dimensionality, long-range dependencies, and lack of standardized datasets
currently makes generative modeling of audio and music very challenging. We
propose to model music as a series of discrete notes upon which we can use
autoregressive natural language processing techniques for successful generative
modeling. While previous works used similar pipelines on data such as sheet
music and MIDI, we aim to extend such approaches to the under-studied medium of
guitar tablature. Specifically, we develop the first work to our knowledge that
models one specific genre as guitar tablature: heavy rock. Unlike other works
in guitar tablature generation, we have a freely available public demo at
https://huggingface.co/spaces/josuelmet/Metal_Music_Interpolator
",2023-01-12T21:12:08Z,http://arxiv.org/abs/2301.05295v2,Josue Casco-Rodriguez
"Towards Coinductive Models for Natural Language Understanding. Bringing
  together Deep Learning and Deep Semantics","  This article contains a proposal to add coinduction to the computational
apparatus of natural language understanding. This, we argue, will provide a
basis for more realistic, computationally sound, and scalable models of natural
language dialogue, syntax and semantics. Given that the bottom up, inductively
constructed, semantic and syntactic structures are brittle, and seemingly
incapable of adequately representing the meaning of longer sentences or
realistic dialogues, natural language understanding is in need of a new
foundation. Coinduction, which uses top down constraints, has been successfully
used in the design of operating systems and programming languages. Moreover,
implicitly it has been present in text mining, machine translation, and in some
attempts to model intensionality and modalities, which provides evidence that
it works. This article shows high level formalizations of some of such uses.
  Since coinduction and induction can coexist, they can provide a common
language and a conceptual model for research in natural language understanding.
In particular, such an opportunity seems to be emerging in research on
compositionality. This article shows several examples of the joint appearance
of induction and coinduction in natural language processing. We argue that the
known individual limitations of induction and coinduction can be overcome in
empirical settings by a combination of the the two methods. We see an open
problem in providing a theory of their joint use.
",2020-12-09T03:10:36Z,http://arxiv.org/abs/2012.05715v1,Wlodek W. Zadrozny
"Genetic Bottleneck and the Emergence of High Intelligence by Scaling-out
  and High Throughput","  We study the biological evolution of low-latency natural neural networks for
short-term survival, and its parallels in the development of low latency
high-performance Central Processing Unit in computer design and architecture.
The necessity of accurate high-quality display of motion picture led to the
special processing units known as the GPU, just as how special visual cortex
regions of animals produced such low-latency computational capacity. The human
brain, especially considered as nothing but a scaled-up version of a primate
brain evolved in response to genomic bottleneck, producing a brain that is
trainable and prunable by society, and as a further extension, invents
language, writing and storage of narratives displaced in time and space. We
conclude that this modern digital invention of social media and the archived
collective common corpus has further evolved from just simple CPU-based
low-latency fast retrieval to high-throughput parallel processing of data using
GPUs to train Attention based Deep Learning Neural Networks producing
Generative AI with aspects like toxicity, bias, memorization, hallucination,
with intriguing close parallels in humans and their society. We show how this
paves the way for constructive approaches to eliminating such drawbacks from
human society and its proxy and collective large-scale mirror, the Generative
AI of the LLMs.
",2024-05-29T14:35:39Z,http://arxiv.org/abs/2407.08743v1,"Arifa Khan, Saravanan P, Venkatesan S. K."
On Inductive Biases for Machine Learning in Data Constrained Settings,"  Learning with limited data is one of the biggest problems of machine
learning. Current approaches to this issue consist in learning general
representations from huge amounts of data before fine-tuning the model on a
small dataset of interest. While such technique, coined transfer learning, is
very effective in domains such as computer vision or natural langage
processing, it does not yet solve common problems of deep learning such as
model interpretability or the overall need for data. This thesis explores a
different answer to the problem of learning expressive models in data
constrained settings: instead of relying on big datasets to learn neural
networks, we will replace some modules by known functions reflecting the
structure of the data. Very often, these functions will be drawn from the rich
literature of kernel methods. Indeed, many kernels can reflect the underlying
structure of the data, thus sparing learning parameters to some extent. Our
approach falls under the hood of ""inductive biases"", which can be defined as
hypothesis on the data at hand restricting the space of models to explore
during learning. We demonstrate the effectiveness of this approach in the
context of sequences, such as sentences in natural language or protein
sequences, and graphs, such as molecules. We also highlight the relationship
between our work and recent advances in deep learning. Additionally, we study
convex machine learning models. Here, rather than proposing new models, we
wonder which proportion of the samples in a dataset is really needed to learn a
""good"" model. More precisely, we study the problem of safe sample screening,
i.e, executing simple tests to discard uninformative samples from a dataset
even before fitting a machine learning model, without affecting the optimal
model. Such techniques can be used to prune datasets or mine for rare samples.
",2023-02-21T14:22:01Z,http://arxiv.org/abs/2302.10692v1,Grégoire Mialon
"Diversified Ensemble of Independent Sub-Networks for Robust
  Self-Supervised Representation Learning","  Ensembling a neural network is a widely recognized approach to enhance model
performance, estimate uncertainty, and improve robustness in deep supervised
learning. However, deep ensembles often come with high computational costs and
memory demands. In addition, the efficiency of a deep ensemble is related to
diversity among the ensemble members which is challenging for large,
over-parameterized deep neural networks. Moreover, ensemble learning has not
yet seen such widespread adoption, and it remains a challenging endeavor for
self-supervised or unsupervised representation learning. Motivated by these
challenges, we present a novel self-supervised training regime that leverages
an ensemble of independent sub-networks, complemented by a new loss function
designed to encourage diversity. Our method efficiently builds a sub-model
ensemble with high diversity, leading to well-calibrated estimates of model
uncertainty, all achieved with minimal computational overhead compared to
traditional deep self-supervised ensembles. To evaluate the effectiveness of
our approach, we conducted extensive experiments across various tasks,
including in-distribution generalization, out-of-distribution detection,
dataset corruption, and semi-supervised settings. The results demonstrate that
our method significantly improves prediction reliability. Our approach not only
achieves excellent accuracy but also enhances calibration, surpassing baseline
performance across a wide range of self-supervised architectures in computer
vision, natural language processing, and genomics data.
",2023-08-28T16:58:44Z,http://arxiv.org/abs/2308.14705v2,"Amirhossein Vahidi, Lisa Wimmer, Hüseyin Anil Gündüz, Bernd Bischl, Eyke Hüllermeier, Mina Rezaei"
Offensive Language Analysis using Deep Learning Architecture,"  SemEval-2019 Task 6 (Zampieri et al., 2019b) requires us to identify and
categorise offensive language in social media. In this paper we will describe
the process we took to tackle this challenge. Our process is heavily inspired
by Sosa (2017) where he proposed CNN-LSTM and LSTM-CNN models to conduct
twitter sentiment analysis. We decided to follow his approach as well as
further his work by testing out different variations of RNN models with CNN.
Specifically, we have divided the challenge into two parts: data processing and
sampling and choosing the optimal deep learning architecture. In preprocessing,
we experimented with two techniques, SMOTE and Class Weights to counter the
imbalance between classes. Once we are happy with the quality of our input
data, we proceed to choosing the optimal deep learning architecture for this
task. Given the quality and quantity of data we have been given, we found that
the addition of CNN layer provides very little to no additional improvement to
our model's performance and sometimes even lead to a decrease in our F1-score.
In the end, the deep learning architecture that gives us the highest macro
F1-score is a simple BiLSTM-CNN.
",2019-03-12T09:36:25Z,http://arxiv.org/abs/1903.05280v3,Ryan Ong
Optimizing Federated Learning by Entropy-Based Client Selection,"  Deep learning is an emerging field revolutionizing various industries,
including natural language processing, computer vision, and many more. These
domains typically require an extensive amount of data for optimal performance,
potentially utilizing huge centralized data repositories. However, such
centralization could raise privacy issues concerning the storage of sensitive
data. To address this issue, federated learning was developed. It is a newly
distributed learning technique that enables to collaboratively train a deep
learning model on decentralized devices, referred to as clients, without
compromising their data privacy. Traditional federated learning methods often
suffer from severe performance degradation when the data distribution among
clients differs significantly. This becomes especially problematic in the case
of label distribution skew, where the distribution of labels varies across
clients. To address this, a novel method called FedEntOpt is proposed.
FedEntOpt is designed to mitigate performance issues caused by label
distribution skew by maximizing the entropy of the global label distribution of
the selected client subset in each federated learning round. This ensures that
the aggregated model parameters from the clients were exhibited to data from
all available labels, which improves the accuracy of the global model.
Extensive experiments on several benchmark datasets show that the proposed
method outperforms several state-of-the-art algorithms by up to 6% in
classification accuracy, demonstrating robust and superior performance,
particularly under low participation rates. In addition, it offers the
flexibility to be combined with them, enhancing their performance by over 40%.
",2024-11-02T13:31:36Z,http://arxiv.org/abs/2411.01240v1,"Andreas Lutz, Gabriele Steidl, Karsten Müller, Wojciech Samek"
PATO: Policy Assisted TeleOperation for Scalable Robot Data Collection,"  Large-scale data is an essential component of machine learning as
demonstrated in recent advances in natural language processing and computer
vision research. However, collecting large-scale robotic data is much more
expensive and slower as each operator can control only a single robot at a
time. To make this costly data collection process efficient and scalable, we
propose Policy Assisted TeleOperation (PATO), a system which automates part of
the demonstration collection process using a learned assistive policy. PATO
autonomously executes repetitive behaviors in data collection and asks for
human input only when it is uncertain about which subtask or behavior to
execute. We conduct teleoperation user studies both with a real robot and a
simulated robot fleet and demonstrate that our assisted teleoperation system
reduces human operators' mental load while improving data collection
efficiency. Further, it enables a single operator to control multiple robots in
parallel, which is a first step towards scalable robotic data collection. For
code and video results, see https://clvrai.com/pato
",2022-12-09T07:38:09Z,http://arxiv.org/abs/2212.04708v2,"Shivin Dass, Karl Pertsch, Hejia Zhang, Youngwoon Lee, Joseph J. Lim, Stefanos Nikolaidis"
"Semi-Supervised Natural Language Approach for Fine-Grained
  Classification of Medical Reports","  Although machine learning has become a powerful tool to augment doctors in
clinical analysis, the immense amount of labeled data that is necessary to
train supervised learning approaches burdens each development task as time and
resource intensive. The vast majority of dense clinical information is stored
in written reports, detailing pertinent patient information. The challenge with
utilizing natural language data for standard model development is due to the
complex nature of the modality. In this research, a model pipeline was
developed to utilize an unsupervised approach to train an encoder-language
model, a recurrent network, to generate document encodings; which then can be
used as features passed into a decoder-classifier model that requires
magnitudes less labeled data than previous approaches to differentiate between
fine-grained disease classes accurately. The language model was trained on
unlabeled radiology reports from the Massachusetts General Hospital Radiology
Department (n=218,159) and terminated with a loss of 1.62. The classification
models were trained on three labeled datasets of head CT studies of reported
patients, presenting large vessel occlusion (n=1403), acute ischemic strokes
(n=331), and intracranial hemorrhage (n=4350), to identify a variety of
different findings directly from the radiology report data; resulting in AUCs
of 0.98, 0.95, and 0.99, respectively, for the large vessel occlusion, acute
ischemic stroke, and intracranial hemorrhage datasets. The output encodings are
able to be used in conjunction with imaging data, to create models that can
process a multitude of different modalities. The ability to automatically
extract relevant features from textual data allows for faster model development
and integration of textual modality, overall, allowing clinical reports to
become a more viable input for more encompassing and accurate deep learning
models.
",2019-10-29T23:25:59Z,http://arxiv.org/abs/1910.13573v2,"Neil Deshmukh, Selin Gumustop, Romane Gauriau, Varun Buch, Bradley Wright, Christopher Bridge, Ram Naidu, Katherine Andriole, Bernardo Bizzo"
"LINX: A Language Driven Generative System for Goal-Oriented Automated
  Data Exploration","  Data exploration is a challenging process in which users examine a dataset by
iteratively employing a series of queries. While in some cases the user
explores a new dataset to become familiar with it, more often, the exploration
process is conducted with a specific analysis goal or question in mind. To
assist users in exploring a new dataset, Automated Data Exploration (ADE)
systems have been devised in previous work. These systems aim to auto-generate
a full exploration session, containing a sequence of queries that showcase
interesting elements of the data. However, existing ADE systems are often
constrained by a predefined objective function, thus always generating the same
session for a given dataset. Therefore, their effectiveness in goal-oriented
exploration, in which users need to answer specific questions about the data,
are extremely limited.
  To this end, this paper presents LINX, a generative system augmented with a
natural language interface for goal-oriented ADE. Given an input dataset and an
analytical goal described in natural language, LINX generates a personalized
exploratory session that is relevant to the user's goal. LINX utilizes a Large
Language Model (LLM) to interpret the input analysis goal, and then derive a
set of specifications for the desired output exploration session. These
specifications are then transferred to a novel, modular ADE engine based on
Constrained Deep Reinforcement Learning (CDRL), which can adapt its output
according to the specified instructions.
  To validate LINX's effectiveness, we introduce a new benchmark dataset for
goal-oriented exploration and conduct an extensive user study. Our analysis
underscores LINX's superior capability in producing exploratory notebooks that
are significantly more relevant and beneficial than those generated by existing
solutions, including ChatGPT, goal-agnostic ADE, and commercial systems.
",2024-06-07T17:37:05Z,http://arxiv.org/abs/2406.05107v1,"Tavor Lipman, Tova Milo, Amit Somech, Tomer Wolfson, Oz Zafar"
"PID Control-Based Self-Healing to Improve the Robustness of Large
  Language Models","  Despite the effectiveness of deep neural networks in numerous natural
language processing applications, recent findings have exposed the
vulnerability of these language models when minor perturbations are introduced.
While appearing semantically indistinguishable to humans, these perturbations
can significantly reduce the performance of well-trained language models,
raising concerns about the reliability of deploying them in safe-critical
situations. In this work, we construct a computationally efficient self-healing
process to correct undesired model behavior during online inference when
perturbations are applied to input data. This is formulated as a trajectory
optimization problem in which the internal states of the neural network layers
are automatically corrected using a PID (Proportional-Integral-Derivative)
control mechanism. The P controller targets immediate state adjustments, while
the I and D controllers consider past states and future dynamical trends,
respectively. We leverage the geometrical properties of the training data to
design effective linear PID controllers. This approach reduces the
computational cost to that of using just the P controller, instead of the full
PID control. Further, we introduce an analytical method for approximating the
optimal control solutions, enhancing the real-time inference capabilities of
this controlled system. Moreover, we conduct a theoretical error analysis of
the analytic solution in a simplified setting. The proposed PID control-based
self-healing is a low cost framework that improves the robustness of
pre-trained large language models, whether standard or robustly trained,
against a wide range of perturbations. A detailed implementation can be found
in:https://github.com/zhuotongchen/PID-Control-Based-Self-Healing-to-Improve-the-Robustness-of-Large-Language-Models.
",2024-03-31T23:46:51Z,http://arxiv.org/abs/2404.00828v1,"Zhuotong Chen, Zihu Wang, Yifan Yang, Qianxiao Li, Zheng Zhang"
"Beyond Text: A Deep Dive into Large Language Models' Ability on
  Understanding Graph Data","  Large language models (LLMs) have achieved impressive performance on many
natural language processing tasks. However, their capabilities on
graph-structured data remain relatively unexplored. In this paper, we conduct a
series of experiments benchmarking leading LLMs on diverse graph prediction
tasks spanning node, edge, and graph levels. We aim to assess whether LLMs can
effectively process graph data and leverage topological structures to enhance
performance, compared to specialized graph neural networks. Through varied
prompt formatting and task/dataset selection, we analyze how well LLMs can
interpret and utilize graph structures. By comparing LLMs' performance with
specialized graph models, we offer insights into the strengths and limitations
of employing LLMs for graph analytics. Our findings provide insights into LLMs'
capabilities and suggest avenues for further exploration in applying them to
graph analytics.
",2023-10-07T23:25:22Z,http://arxiv.org/abs/2310.04944v1,"Yuntong Hu, Zheng Zhang, Liang Zhao"
Deep Image-to-Recipe Translation,"  The modern saying, ""You Are What You Eat"" resonates on a profound level,
reflecting the intricate connection between our identities and the food we
consume. Our project, Deep Image-to-Recipe Translation, is an intersection of
computer vision and natural language generation that aims to bridge the gap
between cherished food memories and the art of culinary creation. Our primary
objective involves predicting ingredients from a given food image. For this
task, we first develop a custom convolutional network and then compare its
performance to a model that leverages transfer learning. We pursue an
additional goal of generating a comprehensive set of recipe steps from a list
of ingredients. We frame this process as a sequence-to-sequence task and
develop a recurrent neural network that utilizes pre-trained word embeddings.
We address several challenges of deep learning including imbalanced datasets,
data cleaning, overfitting, and hyperparameter selection. Our approach
emphasizes the importance of metrics such as Intersection over Union (IoU) and
F1 score in scenarios where accuracy alone might be misleading. For our recipe
prediction model, we employ perplexity, a commonly used and important metric
for language models. We find that transfer learning via pre-trained ResNet-50
weights and GloVe embeddings provide an exceptional boost to model performance,
especially when considering training resource constraints. Although we have
made progress on the image-to-recipe translation, there is an opportunity for
future exploration with advancements in model architectures, dataset
scalability, and enhanced user interaction.
",2024-07-01T02:33:07Z,http://arxiv.org/abs/2407.00911v1,"Jiangqin Ma, Bilal Mawji, Franz Williams"
"Language acquisition: do children and language models follow similar
  learning stages?","  During language acquisition, children follow a typical sequence of learning
stages, whereby they first learn to categorize phonemes before they develop
their lexicon and eventually master increasingly complex syntactic structures.
However, the computational principles that lead to this learning trajectory
remain largely unknown. To investigate this, we here compare the learning
trajectories of deep language models to those of children. Specifically, we
test whether, during its training, GPT-2 exhibits stages of language
acquisition comparable to those observed in children aged between 18 months and
6 years. For this, we train 48 GPT-2 models from scratch and evaluate their
syntactic and semantic abilities at each training step, using 96 probes curated
from the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these
evaluations with the behavior of 54 children during language production. Our
analyses reveal three main findings. First, similarly to children, the language
models tend to learn linguistic skills in a systematic order. Second, this
learning scheme is parallel: the language tasks that are learned last improve
from the very first training steps. Third, some - but not all - learning stages
are shared between children and these language models. Overall, these results
shed new light on the principles of language acquisition, and highlight
important divergences in how humans and modern algorithms learn to process
natural language.
",2023-06-06T11:08:20Z,http://arxiv.org/abs/2306.03586v1,"Linnea Evanson, Yair Lakretz, Jean-Rémi King"
"Symmetric Kernels with Non-Symmetric Data: A Data-Agnostic Learnability
  Bound","  Kernel ridge regression (KRR) and Gaussian processes (GPs) are fundamental
tools in statistics and machine learning with recent applications to highly
over-parameterized deep neural networks. The ability of these tools to learn a
target function is directly related to the eigenvalues of their kernel sampled
on the input data. Targets having support on higher eigenvalues are more
learnable. While kernels are often highly symmetric objects, the data is often
not. Thus kernel symmetry seems to have little to no bearing on the above
eigenvalues or learnability, making spectral analysis on real-world data
challenging. Here, we show that contrary to this common lure, one may use
eigenvalues and eigenfunctions associated with highly idealized data-measures
to bound learnability on realistic data. As a demonstration, we give a
theoretical lower bound on the sample complexity of copying heads for kernels
associated with generic transformers acting on natural language.
",2024-06-04T18:00:00Z,http://arxiv.org/abs/2406.02663v1,"Itay Lavie, Zohar Ringel"
"Data Augmentation with In-Context Learning and Comparative Evaluation in
  Math Word Problem Solving","  Math Word Problem (MWP) solving presents a challenging task in Natural
Language Processing (NLP). This study aims to provide MWP solvers with a more
diverse training set, ultimately improving their ability to solve various math
problems. We propose several methods for data augmentation by modifying the
problem texts and equations, such as synonym replacement, rule-based: question
replacement, and rule based: reversing question methodologies over two English
MWP datasets. This study extends by introducing a new in-context learning
augmentation method, employing the Llama-7b language model. This approach
involves instruction-based prompting for rephrasing the math problem texts.
Performance evaluations are conducted on 9 baseline models, revealing that
augmentation methods outperform baseline models. Moreover, concatenating
examples generated by various augmentation methods further improves
performance.
",2024-04-05T07:57:03Z,http://arxiv.org/abs/2404.03938v1,"Gulsum Yigit, Mehmet Fatih Amasyali"
Supervised learning model for parsing Arabic language,"  Parsing the Arabic language is a difficult task given the specificities of
this language and given the scarcity of digital resources (grammars and
annotated corpora). In this paper, we suggest a method for Arabic parsing based
on supervised machine learning. We used the SVMs algorithm to select the
syntactic labels of the sentence. Furthermore, we evaluated our parser
following the cross validation method by using the Penn Arabic Treebank. The
obtained results are very encouraging.
",2014-10-31T15:53:49Z,http://arxiv.org/abs/1410.8783v1,"Nabil Khoufi, Chafik Aloulou, Lamia Hadrich Belguith"
"Multi-Attribute Relation Extraction (MARE) -- Simplifying the
  Application of Relation Extraction","  Natural language understanding's relation extraction makes innovative and
encouraging novel business concepts possible and facilitates new digitilized
decision-making processes. Current approaches allow the extraction of relations
with a fixed number of entities as attributes. Extracting relations with an
arbitrary amount of attributes requires complex systems and costly
relation-trigger annotations to assist these systems. We introduce
multi-attribute relation extraction (MARE) as an assumption-less problem
formulation with two approaches, facilitating an explicit mapping from business
use cases to the data annotations. Avoiding elaborated annotation constraints
simplifies the application of relation extraction approaches. The evaluation
compares our models to current state-of-the-art event extraction and binary
relation extraction methods. Our approaches show improvement compared to these
on the extraction of general multi-attribute relations.
",2021-11-17T11:06:39Z,http://arxiv.org/abs/2111.09035v1,"Lars Klöser, Philipp Kohl, Bodo Kraft, Albert Zündorf"
"Deep Learning-based Spatio Temporal Facial Feature Visual Speech
  Recognition","  In low-resource computing contexts, such as smartphones and other tiny
devices, Both deep learning and machine learning are being used in a lot of
identification systems. as authentication techniques. The transparent,
contactless, and non-invasive nature of these face recognition technologies
driven by AI has led to their meteoric rise in popularity in recent years.
While they are mostly successful, there are still methods to get inside without
permission by utilising things like pictures, masks, glasses, etc. In this
research, we present an alternate authentication process that makes use of both
facial recognition and the individual's distinctive temporal facial feature
motions while they speak a password. Because the suggested methodology allows
for a password to be specified in any language, it is not limited by language.
The suggested model attained an accuracy of 96.1% when tested on the
industry-standard MIRACL-VC1 dataset, demonstrating its efficacy as a reliable
and powerful solution. In addition to being data-efficient, the suggested
technique shows promising outcomes with as little as 10 positive video examples
for training the model. The effectiveness of the network's training is further
proved via comparisons with other combined facial recognition and lip reading
models.
",2023-04-30T18:52:29Z,http://arxiv.org/abs/2305.00552v1,"Pangoth Santhosh Kumar, Garika Akshay"
"A survey on natural language processing (nlp) and applications in
  insurance","  Text is the most widely used means of communication today. This data is
abundant but nevertheless complex to exploit within algorithms. For years,
scientists have been trying to implement different techniques that enable
computers to replicate some mechanisms of human reading. During the past five
years, research disrupted the capacity of the algorithms to unleash the value
of text data. It brings today, many opportunities for the insurance
industry.Understanding those methods and, above all, knowing how to apply them
is a major challenge and key to unleash the value of text data that have been
stored for many years. Processing language with computer brings many new
opportunities especially in the insurance sector where reports are central in
the information used by insurers. SCOR's Data Analytics team has been working
on the implementation of innovative tools or products that enable the use of
the latest research on text analysis. Understanding text mining techniques in
insurance enhances the monitoring of the underwritten risks and many processes
that finally benefit policyholders.This article proposes to explain
opportunities that Natural Language Processing (NLP) are providing to
insurance. It details different methods used today in practice traces back the
story of them. We also illustrate the implementation of certain methods using
open source libraries and python codes that we have developed to facilitate the
use of these techniques.After giving a general overview on the evolution of
text mining during the past few years,we share about how to conduct a full
study with text mining and share some examples to serve those models into
insurance products or services. Finally, we explained in more details every
step that composes a Natural Language Processing study to ensure the reader can
have a deep understanding on the implementation.
",2020-10-01T14:56:18Z,http://arxiv.org/abs/2010.00462v1,"Antoine Ly, Benno Uthayasooriyar, Tingting Wang"
Adaptive Prompt Learning-based Few-Shot Sentiment Analysis,"  In the field of natural language processing, sentiment analysis via deep
learning has a excellent performance by using large labeled datasets.
Meanwhile, labeled data are insufficient in many sentiment analysis, and
obtaining these data is time-consuming and laborious. Prompt learning devotes
to resolving the data deficiency by reformulating downstream tasks with the
help of prompt. In this way, the appropriate prompt is very important for the
performance of the model. This paper proposes an adaptive prompting(AP)
construction strategy using seq2seq-attention structure to acquire the semantic
information of the input sequence. Then dynamically construct adaptive prompt
which can not only improve the quality of the prompt, but also can effectively
generalize to other fields by pre-trained prompt which is constructed by
existing public labeled data. The experimental results on FewCLUE datasets
demonstrate that the proposed method AP can effectively construct appropriate
adaptive prompt regardless of the quality of hand-crafted prompt and outperform
the state-of-the-art baselines.
",2022-05-15T08:34:48Z,http://arxiv.org/abs/2205.07220v1,"Pengfei Zhang, Tingting Chai, Yongdong Xu"
"Colorful Cutout: Enhancing Image Data Augmentation with Curriculum
  Learning","  Data augmentation is one of the regularization strategies for the training of
deep learning models, which enhances generalizability and prevents overfitting,
leading to performance improvement. Although researchers have proposed various
data augmentation techniques, they often lack consideration for the difficulty
of augmented data. Recently, another line of research suggests incorporating
the concept of curriculum learning with data augmentation in the field of
natural language processing. In this study, we adopt curriculum data
augmentation for image data augmentation and propose colorful cutout, which
gradually increases the noise and difficulty introduced in the augmented image.
Our experimental results highlight the possibility of curriculum data
augmentation for image data. We publicly released our source code to improve
the reproducibility of our study.
",2024-03-29T06:53:52Z,http://arxiv.org/abs/2403.20012v1,"Juhwan Choi, YoungBin Kim"
"Neural Skill Transfer from Supervised Language Tasks to Reading
  Comprehension","  Reading comprehension is a challenging task in natural language processing
and requires a set of skills to be solved. While current approaches focus on
solving the task as a whole, in this paper, we propose to use a neural network
`skill' transfer approach. We transfer knowledge from several lower-level
language tasks (skills) including textual entailment, named entity recognition,
paraphrase detection and question type classification into the reading
comprehension model.
  We conduct an empirical evaluation and show that transferring language skill
knowledge leads to significant improvements for the task with much fewer steps
compared to the baseline model. We also show that the skill transfer approach
is effective even with small amounts of training data. Another finding of this
work is that using token-wise deep label supervision for text classification
improves the performance of transfer learning.
",2017-11-10T10:13:51Z,http://arxiv.org/abs/1711.03754v1,"Todor Mihaylov, Zornitsa Kozareva, Anette Frank"
"Leveraging Dependency Grammar for Fine-Grained Offensive Language
  Detection using Graph Convolutional Networks","  The last few years have witnessed an exponential rise in the propagation of
offensive text on social media. Identification of this text with high precision
is crucial for the well-being of society. Most of the existing approaches tend
to give high toxicity scores to innocuous statements (e.g., ""I am a gay man"").
These false positives result from over-generalization on the training data
where specific terms in the statement may have been used in a pejorative sense
(e.g., ""gay""). Emphasis on such words alone can lead to discrimination against
the classes these systems are designed to protect. In this paper, we address
the problem of offensive language detection on Twitter, while also detecting
the type and the target of the offence. We propose a novel approach called
SyLSTM, which integrates syntactic features in the form of the dependency parse
tree of a sentence and semantic features in the form of word embeddings into a
deep learning architecture using a Graph Convolutional Network. Results show
that the proposed approach significantly outperforms the state-of-the-art BERT
model with orders of magnitude fewer number of parameters.
",2022-05-26T05:27:50Z,http://arxiv.org/abs/2205.13164v1,"Divyam Goel, Raksha Sharma"
A Survey on Large Language Models from Concept to Implementation,"  Recent advancements in Large Language Models (LLMs), particularly those built
on Transformer architectures, have significantly broadened the scope of natural
language processing (NLP) applications, transcending their initial use in
chatbot technology. This paper investigates the multifaceted applications of
these models, with an emphasis on the GPT series. This exploration focuses on
the transformative impact of artificial intelligence (AI) driven tools in
revolutionizing traditional tasks like coding and problem-solving, while also
paving new paths in research and development across diverse industries. From
code interpretation and image captioning to facilitating the construction of
interactive systems and advancing computational domains, Transformer models
exemplify a synergy of deep learning, data analysis, and neural network design.
This survey provides an in-depth look at the latest research in Transformer
models, highlighting their versatility and the potential they hold for
transforming diverse application sectors, thereby offering readers a
comprehensive understanding of the current and future landscape of
Transformer-based LLMs in practical applications.
",2024-03-27T19:35:41Z,http://arxiv.org/abs/2403.18969v2,"Chen Wang, Jin Zhao, Jiaqi Gong"
"Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and
  Metrics for Open Domain Question Answering in the Era of Large Language
  Models","  Open Domain Question Answering (ODQA) within natural language processing
involves building systems that answer factual questions using large-scale
knowledge corpora. Recent advances stem from the confluence of several factors,
such as large-scale training datasets, deep learning techniques, and the rise
of large language models. High-quality datasets are used to train models on
realistic scenarios and enable the evaluation of the system on potentially
unseen data. Standardized metrics facilitate comparisons between different ODQA
systems, allowing researchers to objectively track advancements in the field.
Our study presents a thorough examination of the current landscape of ODQA
benchmarking by reviewing 52 datasets and 20 evaluation techniques across
textual and multimodal modalities. We introduce a novel taxonomy for ODQA
datasets that incorporates both the modality and difficulty of the question
types. Additionally, we present a structured organization of ODQA evaluation
metrics along with a critical analysis of their inherent trade-offs. Our study
aims to empower researchers by providing a framework for the robust evaluation
of modern question-answering systems. We conclude by identifying the current
challenges and outlining promising avenues for future research and development.
",2024-06-19T05:43:02Z,http://arxiv.org/abs/2406.13232v1,"Akchay Srivastava, Atif Memon"
"MT-GBM: A Multi-Task Gradient Boosting Machine with Shared Decision
  Trees","  Despite the success of deep learning in computer vision and natural language
processing, Gradient Boosted Decision Tree (GBDT) is yet one of the most
powerful tools for applications with tabular data such as e-commerce and
FinTech. However, applying GBDT to multi-task learning is still a challenge.
Unlike deep models that can jointly learn a shared latent representation across
multiple tasks, GBDT can hardly learn a shared tree structure. In this paper,
we propose Multi-task Gradient Boosting Machine (MT-GBM), a GBDT-based method
for multi-task learning. The MT-GBM can find the shared tree structures and
split branches according to multi-task losses. First, it assigns multiple
outputs to each leaf node. Next, it computes the gradient corresponding to each
output (task). Then, we also propose an algorithm to combine the gradients of
all tasks and update the tree. Finally, we apply MT-GBM to LightGBM.
Experiments show that our MT-GBM improves the performance of the main task
significantly, which means the proposed MT-GBM is efficient and effective.
",2022-01-17T06:43:14Z,http://arxiv.org/abs/2201.06239v2,"ZhenZhe Ying, Zhuoer Xu, Zhifeng Li, Weiqiang Wang, Changhua Meng"
"A New Era in Computational Pathology: A Survey on Foundation and
  Vision-Language Models","  Recent advances in deep learning have completely transformed the domain of
computational pathology (CPath). More specifically, it has altered the
diagnostic workflow of pathologists by integrating foundation models (FMs) and
vision-language models (VLMs) in their assessment and decision-making process.
The limitations of existing deep learning approaches in CPath can be overcome
by FMs through learning a representation space that can be adapted to a wide
variety of downstream tasks without explicit supervision. Deploying VLMs allow
pathology reports written in natural language be used as rich semantic
information sources to improve existing models as well as generate predictions
in natural language form. In this survey, a holistic and systematic overview of
recent innovations in FMs and VLMs in CPath is presented. Furthermore, the
tools, datasets and training schemes for these models are summarized in
addition to categorizing them into distinct groups. This extensive survey
highlights the current trends in CPath and its possible revolution through the
use of FMs and VLMs in the future.
",2024-08-23T16:33:57Z,http://arxiv.org/abs/2408.14496v3,"Dibaloke Chanda, Milan Aryal, Nasim Yahya Soltani, Masoud Ganji"
"ActKnow: Active External Knowledge Infusion Learning for Question
  Answering in Low Data Regime","  Deep learning models have set benchmark results in various Natural Language
Processing tasks. However, these models require an enormous amount of training
data, which is infeasible in many practical problems. While various techniques
like domain adaptation, fewshot learning techniques address this problem, we
introduce a new technique of actively infusing external knowledge into learning
to solve low data regime problems. We propose a technique called ActKnow that
actively infuses knowledge from Knowledge Graphs (KG) based ""on-demand"" into
learning for Question Answering (QA). By infusing world knowledge from
Concept-Net, we show significant improvements on the ARC Challenge-set
benchmark over purely text-based transformer models like RoBERTa in the low
data regime. For example, by using only 20% training examples, we demonstrate a
4% improvement in the accuracy for both ARC-challenge and OpenBookQA,
respectively.
",2021-12-17T10:39:41Z,http://arxiv.org/abs/2112.09423v1,"K. M. Annervaz, Pritam Kumar Nath, Ambedkar Dukkipati"
"TabDeco: A Comprehensive Contrastive Framework for Decoupled
  Representations in Tabular Data","  Representation learning is a fundamental aspect of modern artificial
intelligence, driving substantial improvements across diverse applications.
While selfsupervised contrastive learning has led to significant advancements
in fields like computer vision and natural language processing, its adaptation
to tabular data presents unique challenges. Traditional approaches often
prioritize optimizing model architecture and loss functions but may overlook
the crucial task of constructing meaningful positive and negative sample pairs
from various perspectives like feature interactions, instance-level patterns
and batch-specific contexts. To address these challenges, we introduce TabDeco,
a novel method that leverages attention-based encoding strategies across both
rows and columns and employs contrastive learning framework to effectively
disentangle feature representations at multiple levels, including features,
instances and data batches. With the innovative feature decoupling hierarchies,
TabDeco consistently surpasses existing deep learning methods and leading
gradient boosting algorithms, including XG-Boost, CatBoost, and LightGBM,
across various benchmark tasks, underscoring its effectiveness in advancing
tabular data representation learning.
",2024-11-17T18:42:46Z,http://arxiv.org/abs/2411.11148v1,"Suiyao Chen, Jing Wu, Yunxiao Wang, Cheng Ji, Tianpei Xie, Daniel Cociorva, Michael Sharps, Cecile Levasseur, Hakan Brunzell"
"Exploiting Synchronized Lyrics And Vocal Features For Music Emotion
  Detection","  One of the key points in music recommendation is authoring engaging playlists
according to sentiment and emotions. While previous works were mostly based on
audio for music discovery and playlists generation, we take advantage of our
synchronized lyrics dataset to combine text representations and music features
in a novel way; we therefore introduce the Synchronized Lyrics Emotion Dataset.
Unlike other approaches that randomly exploited the audio samples and the whole
text, our data is split according to the temporal information provided by the
synchronization between lyrics and audio. This work shows a comparison between
text-based and audio-based deep learning classification models using different
techniques from Natural Language Processing and Music Information Retrieval
domains. From the experiments on audio we conclude that using vocals only,
instead of the whole audio data improves the overall performances of the audio
classifier. In the lyrics experiments we exploit the state-of-the-art word
representations applied to the main Deep Learning architectures available in
literature. In our benchmarks the results show how the Bilinear LSTM classifier
with Attention based on fastText word embedding performs better than the CNN
applied on audio.
",2019-01-15T14:10:25Z,http://arxiv.org/abs/1901.04831v1,"Loreto Parisi, Simone Francia, Silvio Olivastri, Maria Stella Tavella"
Building Tamil Treebanks,"  Treebanks are important linguistic resources, which are structured and
annotated corpora with rich linguistic annotations. These resources are used in
Natural Language Processing (NLP) applications, supporting linguistic analyses,
and are essential for training and evaluating various computational models.
This paper discusses the creation of Tamil treebanks using three distinct
approaches: manual annotation, computational grammars, and machine learning
techniques. Manual annotation, though time-consuming and requiring linguistic
expertise, ensures high-quality and rich syntactic and semantic information.
Computational deep grammars, such as Lexical Functional Grammar (LFG), offer
deep linguistic analyses but necessitate significant knowledge of the
formalism. Machine learning approaches, utilising off-the-shelf frameworks and
tools like Stanza, UDpipe, and UUParser, facilitate the automated annotation of
large datasets but depend on the availability of quality annotated data,
cross-linguistic training resources, and computational power. The paper
discusses the challenges encountered in building Tamil treebanks, including
issues with Internet data, the need for comprehensive linguistic analysis, and
the difficulty of finding skilled annotators. Despite these challenges, the
development of Tamil treebanks is essential for advancing linguistic research
and improving NLP tools for Tamil.
",2024-09-23T01:58:50Z,http://arxiv.org/abs/2409.14657v1,Kengatharaiyer Sarveswaran
"A Non-Technical Survey on Deep Convolutional Neural Network
  Architectures","  Artificial neural networks have recently shown great results in many
disciplines and a variety of applications, including natural language
understanding, speech processing, games and image data generation. One
particular application in which the strong performance of artificial neural
networks was demonstrated is the recognition of objects in images, where deep
convolutional neural networks are commonly applied. In this survey, we give a
comprehensive introduction to this topic (object recognition with deep
convolutional neural networks), with a strong focus on the evolution of network
architectures. Therefore, we aim to compress the most important concepts in
this field in a simple and non-technical manner to allow for future researchers
to have a quick general understanding.
  This work is structured as follows:
  1. We will explain the basic ideas of (convolutional) neural networks and
deep learning and examine their usage for three object recognition tasks: image
classification, object localization and object detection.
  2. We give a review on the evolution of deep convolutional neural networks by
providing an extensive overview of the most important network architectures
presented in chronological order of their appearances.
",2018-03-06T11:40:46Z,http://arxiv.org/abs/1803.02129v1,"Felix Altenberger, Claus Lenz"
Reversible Architectures for Arbitrarily Deep Residual Neural Networks,"  Recently, deep residual networks have been successfully applied in many
computer vision and natural language processing tasks, pushing the
state-of-the-art performance with deeper and wider architectures. In this work,
we interpret deep residual networks as ordinary differential equations (ODEs),
which have long been studied in mathematics and physics with rich theoretical
and empirical success. From this interpretation, we develop a theoretical
framework on stability and reversibility of deep neural networks, and derive
three reversible neural network architectures that can go arbitrarily deep in
theory. The reversibility property allows a memory-efficient implementation,
which does not need to store the activations for most hidden layers. Together
with the stability of our architectures, this enables training deeper networks
using only modest computational resources. We provide both theoretical analyses
and empirical results. Experimental results demonstrate the efficacy of our
architectures against several strong baselines on CIFAR-10, CIFAR-100 and
STL-10 with superior or on-par state-of-the-art performance. Furthermore, we
show our architectures yield superior results when trained using fewer training
data.
",2017-09-12T05:41:13Z,http://arxiv.org/abs/1709.03698v2,"Bo Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David Begert, Elliot Holtham"
"Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence
  Learning","  We present Deep Voice 3, a fully-convolutional attention-based neural
text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural
speech synthesis systems in naturalness while training ten times faster. We
scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more
than eight hundred hours of audio from over two thousand speakers. In addition,
we identify common error modes of attention-based speech synthesis networks,
demonstrate how to mitigate them, and compare several different waveform
synthesis methods. We also describe how to scale inference to ten million
queries per day on one single-GPU server.
",2017-10-20T18:17:23Z,http://arxiv.org/abs/1710.07654v3,"Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang, Jonathan Raiman, John Miller"
"Leveraging Natural Language Processing to Augment Structured Social
  Determinants of Health Data in the Electronic Health Record","  Objective: Social determinants of health (SDOH) impact health outcomes and
are documented in the electronic health record (EHR) through structured data
and unstructured clinical notes. However, clinical notes often contain more
comprehensive SDOH information, detailing aspects such as status, severity, and
temporality. This work has two primary objectives: i) develop a natural
language processing (NLP) information extraction model to capture detailed SDOH
information and ii) evaluate the information gain achieved by applying the SDOH
extractor to clinical narratives and combining the extracted representations
with existing structured data.
  Materials and Methods: We developed a novel SDOH extractor using a deep
learning entity and relation extraction architecture to characterize SDOH
across various dimensions. In an EHR case study, we applied the SDOH extractor
to a large clinical data set with 225,089 patients and 430,406 notes with
social history sections and compared the extracted SDOH information with
existing structured data.
  Results: The SDOH extractor achieved 0.86 F1 on a withheld test set. In the
EHR case study, we found extracted SDOH information complements existing
structured data with 32% of homeless patients, 19% of current tobacco users,
and 10% of drug users only having these health risk factors documented in the
clinical narrative.
  Conclusions: Utilizing EHR data to identify SDOH health risk factors and
social needs may improve patient care and outcomes. Semantic representations of
text-encoded SDOH information can augment existing structured data, and this
more comprehensive SDOH representation can assist health systems in identifying
and addressing these social needs.
",2022-12-14T22:51:49Z,http://arxiv.org/abs/2212.07538v2,"Kevin Lybarger, Nicholas J Dobbins, Ritche Long, Angad Singh, Patrick Wedgeworth, Ozlem Ozuner, Meliha Yetisgen"
"Listening to Chaotic Whispers: A Deep Learning Framework for
  News-oriented Stock Trend Prediction","  Stock trend prediction plays a critical role in seeking maximized profit from
stock investment. However, precise trend prediction is very difficult since the
highly volatile and non-stationary nature of stock market. Exploding
information on Internet together with advancing development of natural language
processing and text mining techniques have enable investors to unveil market
trends and volatility from online content. Unfortunately, the quality,
trustworthiness and comprehensiveness of online content related to stock market
varies drastically, and a large portion consists of the low-quality news,
comments, or even rumors. To address this challenge, we imitate the learning
process of human beings facing such chaotic online news, driven by three
principles: sequential content dependency, diverse influence, and effective and
efficient learning. In this paper, to capture the first two principles, we
designed a Hybrid Attention Networks to predict the stock trend based on the
sequence of recent related news. Moreover, we apply the self-paced learning
mechanism to imitate the third principle. Extensive experiments on real-world
stock market data demonstrate the effectiveness of our approach.
",2017-12-06T11:33:21Z,http://arxiv.org/abs/1712.02136v3,"Ziniu Hu, Weiqing Liu, Jiang Bian, Xuanzhe Liu, Tie-Yan Liu"
Foundation Model for Composite Materials and Microstructural Analysis,"  The rapid advancement of machine learning has unlocked numerous opportunities
for materials science, particularly in accelerating the design and analysis of
materials. However, a significant challenge lies in the scarcity and high cost
of obtaining high-quality materials datasets. In other fields, such as natural
language processing, foundation models pre-trained on large datasets have
achieved exceptional success in transfer learning, effectively leveraging
latent features to achieve high performance on tasks with limited data. Despite
this progress, the concept of foundation models remains underexplored in
materials science. Here, we present a foundation model specifically designed
for composite materials. Our model is pre-trained on a dataset of short-fiber
composites to learn robust latent features. During transfer learning, the MMAE
accurately predicts homogenized stiffness, with an R2 score reaching as high as
0.959 and consistently exceeding 0.91, even when trained on limited data. These
findings validate the feasibility and effectiveness of foundation models in
composite materials. We anticipate extending this approach to more complex
three-dimensional composite materials, polycrystalline materials, and beyond.
Moreover, this framework enables high-accuracy predictions even when
experimental data are scarce, paving the way for more efficient and
cost-effective materials design and analysis.
",2024-11-10T19:06:25Z,http://arxiv.org/abs/2411.06565v1,"Ting-Ju Wei, Chuin-Shan, Chen"
"Local Interpretations for Explainable Natural Language Processing: A
  Survey","  As the use of deep learning techniques has grown across various fields over
the past decade, complaints about the opaqueness of the black-box models have
increased, resulting in an increased focus on transparency in deep learning
models. This work investigates various methods to improve the interpretability
of deep neural networks for Natural Language Processing (NLP) tasks, including
machine translation and sentiment analysis. We provide a comprehensive
discussion on the definition of the term interpretability and its various
aspects at the beginning of this work. The methods collected and summarised in
this survey are only associated with local interpretation and are specifically
divided into three categories: 1) interpreting the model's predictions through
related input features; 2) interpreting through natural language explanation;
3) probing the hidden states of models and word representations.
",2021-03-20T02:28:33Z,http://arxiv.org/abs/2103.11072v3,"Siwen Luo, Hamish Ivison, Caren Han, Josiah Poon"
"CodeGRU: Context-aware Deep Learning with Gated Recurrent Unit for
  Source Code Modeling","  Recently deep learning based Natural Language Processing (NLP) models have
shown great potential in the modeling of source code. However, a major
limitation of these approaches is that they take source code as simple tokens
of text and ignore its contextual, syntactical and structural dependencies. In
this work, we present CodeGRU, a gated recurrent unit based source code
language model that is capable of capturing source code's contextual,
syntactical and structural dependencies. We introduce a novel approach which
can capture the source code context by leveraging the source code token types.
Further, we adopt a novel approach which can learn variable size context by
taking into account source code's syntax, and structural information. We
evaluate CodeGRU with real-world data set and it shows that CodeGRU outperforms
the state-of-the-art language models and help reduce the vocabulary size up to
24.93\%. Unlike previous works, we tested CodeGRU with an independent test set
which suggests that our methodology does not requisite the source code comes
from the same domain as training data while providing suggestions. We further
evaluate CodeGRU with two software engineering applications: source code
suggestion, and source code completion. Our experiment confirms that the source
code's contextual information can be vital and can help improve the software
language models. The extensive evaluation of CodeGRU shows that it outperforms
the state-of-the-art models. The results further suggest that the proposed
approach can help reduce the vocabulary size and is of practical use for
software developers.
",2019-03-03T11:44:08Z,http://arxiv.org/abs/1903.00884v2,"Yasir Hussain, Zhiqiu Huang, Yu Zhou, Senzhang Wang"
"NLP4PBM: A Systematic Review on Process Extraction using Natural
  Language Processing with Rule-based, Machine and Deep Learning Methods","  This literature review studies the field of automated process extraction,
i.e., transforming textual descriptions into structured processes using Natural
Language Processing (NLP). We found that Machine Learning (ML) / Deep Learning
(DL) methods are being increasingly used for the NLP component. In some cases,
they were chosen for their suitability towards process extraction, and results
show that they can outperform classic rule-based methods. We also found a
paucity of gold-standard, scalable annotated datasets, which currently hinders
objective evaluations as well as the training or fine-tuning of ML / DL
methods. Finally, we discuss preliminary work on the application of LLMs for
automated process extraction, as well as promising developments in this field.
",2024-09-10T15:16:02Z,http://arxiv.org/abs/2409.13738v1,"William Van Woensel, Soroor Motie"
"Time Majority Voting, a PC-based EEG Classifier for Non-expert Users","  Using Machine Learning and Deep Learning to predict cognitive tasks from
electroencephalography (EEG) signals is a rapidly advancing field in
Brain-Computer Interfaces (BCI). In contrast to the fields of computer vision
and natural language processing, the data amount of these trials is still
rather tiny. Developing a PC-based machine learning technique to increase the
participation of non-expert end-users could help solve this data collection
issue. We created a novel algorithm for machine learning called Time Majority
Voting (TMV). In our experiment, TMV performed better than cutting-edge
algorithms. It can operate efficiently on personal computers for classification
tasks involving the BCI. These interpretable data also assisted end-users and
researchers in comprehending EEG tests better.
",2022-07-26T05:43:54Z,http://arxiv.org/abs/2207.12662v1,"Guangyao Dou, Zheng Zhou, Xiaodong Qu"
Named entity recognition in resumes,"  Named entity recognition (NER) is used to extract information from various
documents and texts such as names and dates. It is important to extract
education and work experience information from resumes in order to filter them.
Considering the fact that all information in a resume has to be entered to the
companys system manually, automatizing this process will save time of the
companies. In this study, a deep learning-based semi-automatic named entity
recognition system has been implemented with a focus on resumes in the field of
IT. Firstly, resumes of employees from five different IT related fields has
been annotated. Six transformer based pre-trained models have been adapted to
named entity recognition problem using the annotated data. These models have
been selected among popular models in the natural language processing field.
The obtained system can recognize eight different entity types which are city,
date, degree, diploma major, job title, language, country and skill. Models
used in the experiments are compared using micro, macro and weighted F1 scores
and the performance of the methods was evaluated. Taking these scores into
account for test set the best micro and weighted F1 score is obtained by
RoBERTa and the best macro F1 score is obtained by Electra model.
",2023-06-22T17:30:37Z,http://arxiv.org/abs/2306.13062v1,"Ege Kesim, Aysu Deliahmetoglu"
Multilingual Entity Linking Using Dense Retrieval,"  Entity linking (EL) is the computational process of connecting textual
mentions to corresponding entities. Like many areas of natural language
processing, the EL field has greatly benefited from deep learning, leading to
significant performance improvements. However, present-day approaches are
expensive to train and rely on diverse data sources, complicating their
reproducibility. In this thesis, we develop multiple systems that are fast to
train, demonstrating that competitive entity linking can be achieved without a
large GPU cluster. Moreover, we train on a publicly available dataset, ensuring
reproducibility and accessibility. Our models are evaluated for 9 languages
giving an accurate overview of their strengths. Furthermore, we offer
a~detailed analysis of bi-encoder training hyperparameters, a popular approach
in EL, to guide their informed selection. Overall, our work shows that building
competitive neural network based EL systems that operate in multiple languages
is possible even with limited resources, thus making EL more approachable.
",2024-05-13T18:57:27Z,http://arxiv.org/abs/2406.16892v1,Dominik Farhan
On Learning the Structure of Clusters in Graphs,"  Graph clustering is a fundamental problem in unsupervised learning, with
numerous applications in computer science and in analysing real-world data. In
many real-world applications, we find that the clusters have a significant
high-level structure. This is often overlooked in the design and analysis of
graph clustering algorithms which make strong simplifying assumptions about the
structure of the graph. This thesis addresses the natural question of whether
the structure of clusters can be learned efficiently and describes four new
algorithmic results for learning such structure in graphs and hypergraphs.
  All of the presented theoretical results are extensively evaluated on both
synthetic and real-word datasets of different domains, including image
classification and segmentation, migration networks, co-authorship networks,
and natural language processing. These experimental results demonstrate that
the newly developed algorithms are practical, effective, and immediately
applicable for learning the structure of clusters in real-world data.
",2022-12-29T15:26:19Z,http://arxiv.org/abs/2212.14345v1,Peter Macgregor
Probing Pretrained Models of Source Code,"  Deep learning models are widely used for solving challenging code processing
tasks, such as code generation or code summarization. Traditionally, a specific
model architecture was carefully built to solve a particular code processing
task. However, recently general pretrained models such as CodeBERT or CodeT5
have been shown to outperform task-specific models in many applications. While
pretrained models are known to learn complex patterns from data, they may fail
to understand some properties of source code. To test diverse aspects of code
understanding, we introduce a set of diagnosting probing tasks. We show that
pretrained models of code indeed contain information about code syntactic
structure and correctness, the notions of identifiers, data flow and
namespaces, and natural language naming. We also investigate how probing
results are affected by using code-specific pretraining objectives, varying the
model size, or finetuning.
",2022-02-16T10:26:14Z,http://arxiv.org/abs/2202.08975v3,"Sergey Troshin, Nadezhda Chirkova"
Beyond Language Models: Byte Models are Digital World Simulators,"  Traditional deep learning often overlooks bytes, the basic units of the
digital world, where all forms of information and operations are encoded and
manipulated in binary format. Inspired by the success of next token prediction
in natural language processing, we introduce bGPT, a model with next byte
prediction to simulate the digital world. bGPT matches specialized models in
performance across various modalities, including text, audio, and images, and
offers new possibilities for predicting, simulating, and diagnosing algorithm
or hardware behaviour. It has almost flawlessly replicated the process of
converting symbolic music data, achieving a low error rate of 0.0011 bits per
byte in converting ABC notation to MIDI format. In addition, bGPT demonstrates
exceptional capabilities in simulating CPU behaviour, with an accuracy
exceeding 99.99% in executing various operations. Leveraging next byte
prediction, models like bGPT can directly learn from vast binary data,
effectively simulating the intricate patterns of the digital world.
",2024-02-29T13:38:07Z,http://arxiv.org/abs/2402.19155v1,"Shangda Wu, Xu Tan, Zili Wang, Rui Wang, Xiaobing Li, Maosong Sun"
"Addressing the Selection Bias in Voice Assistance: Training Voice
  Assistance Model in Python with Equal Data Selection","  In recent times, voice assistants have become a part of our day-to-day lives,
allowing information retrieval by voice synthesis, voice recognition, and
natural language processing. These voice assistants can be found in many
modern-day devices such as Apple, Amazon, Google, and Samsung. This project is
primarily focused on Virtual Assistance in Natural Language Processing. Natural
Language Processing is a form of AI that helps machines understand people and
create feedback loops. This project will use deep learning to create a Voice
Recognizer and use Commonvoice and data collected from the local community for
model training using Google Colaboratory. After recognizing a command, the AI
assistant will be able to perform the most suitable actions and then give a
response.
  The motivation for this project comes from the race and gender bias that
exists in many virtual assistants. The computer industry is primarily dominated
by the male gender, and because of this, many of the products produced do not
regard women. This bias has an impact on natural language processing. This
project will be utilizing various open-source projects to implement machine
learning algorithms and train the assistant algorithm to recognize different
types of voices, accents, and dialects. Through this project, the goal to use
voice data from underrepresented groups to build a voice assistant that can
recognize voices regardless of gender, race, or accent. Increasing the
representation of women in the computer industry is important for the future of
the industry. By representing women in the initial study of voice assistants,
it can be shown that females play a vital role in the development of this
technology. In line with related work, this project will use first-hand data
from the college population and middle-aged adults to train voice assistant to
combat gender bias.
",2022-12-20T21:26:05Z,http://arxiv.org/abs/2301.00646v1,"Kashav Piya, Srijal Shrestha, Cameran Frank, Estephanos Jebessa, Tauheed Khan Mohd"
"Network Representation Learning: From Traditional Feature Learning to
  Deep Learning","  Network representation learning (NRL) is an effective graph analytics
technique and promotes users to deeply understand the hidden characteristics of
graph data. It has been successfully applied in many real-world tasks related
to network science, such as social network data processing, biological
information processing, and recommender systems. Deep Learning is a powerful
tool to learn data features. However, it is non-trivial to generalize deep
learning to graph-structured data since it is different from the regular data
such as pictures having spatial information and sounds having temporal
information. Recently, researchers proposed many deep learning-based methods in
the area of NRL. In this survey, we investigate classical NRL from traditional
feature learning method to the deep learning-based model, analyze relationships
between them, and summarize the latest progress. Finally, we discuss open
issues considering NRL and point out the future directions in this field.
",2021-03-07T12:31:33Z,http://arxiv.org/abs/2103.04339v1,"Ke Sun, Lei Wang, Bo Xu, Wenhong Zhao, Shyh Wei Teng, Feng Xia"
"News-Driven Stock Price Forecasting in Indian Markets: A Comparative
  Study of Advanced Deep Learning Models","  Forecasting stock market prices remains a complex challenge for traders,
analysts, and engineers due to the multitude of factors that influence price
movements. Recent advancements in artificial intelligence (AI) and natural
language processing (NLP) have significantly enhanced stock price prediction
capabilities. AI's ability to process vast and intricate data sets has led to
more sophisticated forecasts. However, achieving consistently high accuracy in
stock price forecasting remains elusive. In this paper, we leverage 30 years of
historical data from national banks in India, sourced from the National Stock
Exchange, to forecast stock prices. Our approach utilizes state-of-the-art deep
learning models, including multivariate multi-step Long Short-Term Memory
(LSTM), Facebook Prophet with LightGBM optimized through Optuna, and Seasonal
Auto-Regressive Integrated Moving Average (SARIMA). We further integrate
sentiment analysis from tweets and reliable financial sources such as Business
Standard and Reuters, acknowledging their crucial influence on stock price
fluctuations.
",2024-10-14T15:30:06Z,http://arxiv.org/abs/2411.05788v1,"Kaushal Attaluri, Mukesh Tripathi, Srinithi Reddy, Shivendra"
"The Effect of Normalization for Bi-directional Amharic-English Neural
  Machine Translation","  Machine translation (MT) is one of the main tasks in natural language
processing whose objective is to translate texts automatically from one natural
language to another. Nowadays, using deep neural networks for MT tasks has
received great attention. These networks require lots of data to learn abstract
representations of the input and store it in continuous vectors. This paper
presents the first relatively large-scale Amharic-English parallel sentence
dataset. Using these compiled data, we build bi-directional Amharic-English
translation models by fine-tuning the existing Facebook M2M100 pre-trained
model achieving a BLEU score of 37.79 in Amharic-English 32.74 in
English-Amharic translation. Additionally, we explore the effects of Amharic
homophone normalization on the machine translation task. The results show that
the normalization of Amharic homophone characters increases the performance of
Amharic-English machine translation in both directions.
",2022-10-27T07:18:53Z,http://arxiv.org/abs/2210.15224v1,"Tadesse Destaw Belay, Atnafu Lambebo Tonja, Olga Kolesnikova, Seid Muhie Yimam, Abinew Ali Ayele, Silesh Bogale Haile, Grigori Sidorov, Alexander Gelbukh"
"BaitBuster-Bangla: A Comprehensive Dataset for Clickbait Detection in
  Bangla with Multi-Feature and Multi-Modal Analysis","  This study presents a large multi-modal Bangla YouTube clickbait dataset
consisting of 253,070 data points collected through an automated process using
the YouTube API and Python web automation frameworks. The dataset contains 18
diverse features categorized into metadata, primary content, engagement
statistics, and labels for individual videos from 58 Bangla YouTube channels. A
rigorous preprocessing step has been applied to denoise, deduplicate, and
remove bias from the features, ensuring unbiased and reliable analysis. As the
largest and most robust clickbait corpus in Bangla to date, this dataset
provides significant value for natural language processing and data science
researchers seeking to advance modeling of clickbait phenomena in low-resource
languages. Its multi-modal nature allows for comprehensive analyses of
clickbait across content, user interactions, and linguistic dimensions to
develop more sophisticated detection methods with cross-linguistic
applications.
",2023-10-13T13:25:16Z,http://arxiv.org/abs/2310.11465v1,"Abdullah Al Imran, Md Sakib Hossain Shovon, M. F. Mridha"
"Pre-Trained Neural Language Models for Automatic Mobile App User
  Feedback Answer Generation","  Studies show that developers' answers to the mobile app users' feedbacks on
app stores can increase the apps' star rating. To help app developers generate
answers that are related to the users' issues, recent studies develop models to
generate the answers automatically. Aims: The app response generation models
use deep neural networks and require training data. Pre-Trained neural language
Models (PTM) used in Natural Language Processing (NLP) take advantage of the
information they learned from a large corpora in an unsupervised manner, and
can reduce the amount of required training data. In this paper, we evaluate
PTMs to generate replies to the mobile app user feedbacks. Method: We train a
Transformer model from scratch and fine-tune two PTMs to evaluate the generated
responses, which are compared to RRGEN, a current app response model. We also
evaluate the models with different portions of the training data. Results: The
results on a large dataset evaluated by automatic metrics show that PTMs obtain
lower scores than the baselines. However, our human evaluation confirms that
PTMs can generate more relevant and meaningful responses to the posted
feedbacks. Moreover, the performance of PTMs has less drop compared to other
models when the amount of training data is reduced to 1/3. Conclusion: PTMs are
useful in generating responses to app reviews and are more robust models to the
amount of training data provided. However, the prediction time is 19X than
RRGEN. This study can provide new avenues for research in adapting the PTMs for
analyzing mobile app user feedbacks. Index Terms-mobile app user feedback
analysis, neural pre-trained language models, automatic answer generation
",2022-02-04T18:26:55Z,http://arxiv.org/abs/2202.02294v1,"Yue Cao, Fatemeh H. Fard"
LSTMs Exploit Linguistic Attributes of Data,"  While recurrent neural networks have found success in a variety of natural
language processing applications, they are general models of sequential data.
We investigate how the properties of natural language data affect an LSTM's
ability to learn a nonlinguistic task: recalling elements from its input. We
find that models trained on natural language data are able to recall tokens
from much longer sequences than models trained on non-language sequential data.
Furthermore, we show that the LSTM learns to solve the memorization task by
explicitly using a subset of its neurons to count timesteps in the input. We
hypothesize that the patterns and structure in natural language data enable
LSTMs to learn by providing approximate ways of reducing loss, but
understanding the effect of different training data on the learnability of
LSTMs remains an open question.
",2018-05-29T18:44:31Z,http://arxiv.org/abs/1805.11653v2,"Nelson F. Liu, Omer Levy, Roy Schwartz, Chenhao Tan, Noah A. Smith"
"Paraphrase Identification with Deep Learning: A Review of Datasets and
  Methods","  The rapid progress of Natural Language Processing (NLP) technologies has led
to the widespread availability and effectiveness of text generation tools such
as ChatGPT and Claude. While highly useful, these technologies also pose
significant risks to the credibility of various media forms if they are
employed for paraphrased plagiarism -- one of the most subtle forms of content
misuse in scientific literature and general text media. Although automated
methods for paraphrase identification have been developed, detecting this type
of plagiarism remains challenging due to the inconsistent nature of the
datasets used to train these methods. In this article, we examine traditional
and contemporary approaches to paraphrase identification, investigating how the
under-representation of certain paraphrase types in popular datasets, including
those used to train Large Language Models (LLMs), affects the ability to detect
plagiarism. We introduce and validate a new refined typology for paraphrases
(ReParaphrased, REfined PARAPHRASE typology definitions) to better understand
the disparities in paraphrase type representation. Lastly, we propose new
directions for future research and dataset development to enhance AI-based
paraphrase detection.
",2022-12-13T23:06:20Z,http://arxiv.org/abs/2212.06933v3,"Chao Zhou, Cheng Qiu, Lizhen Liang, Daniel E. Acuna"
Answer ranking in Community Question Answering: a deep learning approach,"  Community Question Answering is the field of computational linguistics that
deals with problems derived from the questions and answers posted to websites
such as Quora or Stack Overflow. Among some of these problems we find the issue
of ranking the multiple answers posted in reply to each question by how
informative they are in the attempt to solve the original question. This work
tries to advance the state of the art on answer ranking for community Question
Answering by proceeding with a deep learning approach. We started off by
creating a large data set of questions and answers posted to the Stack Overflow
website.
  We then leveraged the natural language processing capabilities of dense
embeddings and LSTM networks to produce a prediction for the accepted answer
attribute, and present the answers in a ranked form ordered by how likely they
are to be marked as accepted by the question asker. We also produced a set of
numerical features to assist with the answer ranking task. These numerical
features were either extracted from metadata found in the Stack Overflow posts
or derived from the questions and answers texts. We compared the performance of
our deep learning models against a set of forest and boosted trees ensemble
methods and found that our models could not improve the best baseline results.
We speculate that this lack of performance improvement versus the baseline
models may be caused by the large number of out of vocabulary words present in
the programming code snippets found in the questions and answers text. We
conclude that while a deep learning approach may be helpful in answer ranking
problems new methods should be developed to assist with the large number of out
of vocabulary words present in the programming code snippets
",2022-10-16T18:47:41Z,http://arxiv.org/abs/2212.01218v1,Lucas Valentin
Commit2Vec: Learning Distributed Representations of Code Changes,"  Deep learning methods, which have found successful applications in fields
like image classification and natural language processing, have recently been
applied to source code analysis too, due to the enormous amount of freely
available source code (e.g., from open-source software repositories).
  In this work, we elaborate upon a state-of-the-art approach to the
representation of source code that uses information about its syntactic
structure, and we adapt it to represent source changes (i.e., commits). We use
this representation to classify security-relevant commits.
  Because our method uses transfer learning (that is, we train a network on a
""pretext task"" for which abundant labeled data is available, and then we use
such network for the target task of commit classification, for which fewer
labeled instances are available), we studied the impact of pre-training the
network using two different pretext tasks versus a randomly initialized model.
  Our results indicate that representations that leverage the structural
information obtained through code syntax outperform token-based
representations. Furthermore, the performance metrics obtained when
pre-training on a loosely related pretext task with a very large dataset
($>10^6$ samples) were surpassed when pretraining on a smaller dataset ($>10^4$
samples) but for a pretext task that is more closely related to the target
task.
",2019-11-18T13:23:57Z,http://arxiv.org/abs/1911.07605v4,"Rocìo Cabrera Lozoya, Arnaud Baumann, Antonino Sabetta, Michele Bezzi"
"DIA-MOLE: An Unsupervised Learning Approach to Adaptive Dialogue Models
  for Spoken Dialogue Systems","  The DIAlogue MOdel Learning Environment supports an engineering-oriented
approach towards dialogue modelling for a spoken-language interface. Major
steps towards dialogue models is to know about the basic units that are used to
construct a dialogue model and possible sequences. In difference to many other
approaches a set of dialogue acts is not predefined by any theory or manually
during the engineering process, but is learned from data that are available in
an avised spoken dialogue system. The architecture is outlined and the approach
is applied to the domain of appointment scheduling. Even though based on a word
correctness of about 70% predictability of dialogue acts in DIA-MOLE turns out
to be comparable to human-assigned dialogue acts.
",1997-08-18T15:44:33Z,http://arxiv.org/abs/cmp-lg/9708009v1,Jens-Uwe Moeller
Review of Deep Learning,"  In recent years, China, the United States and other countries, Google and
other high-tech companies have increased investment in artificial intelligence.
Deep learning is one of the current artificial intelligence research's key
areas. This paper analyzes and summarizes the latest progress and future
research directions of deep learning. Firstly, three basic models of deep
learning are outlined, including multilayer perceptrons, convolutional neural
networks, and recurrent neural networks. On this basis, we further analyze the
emerging new models of convolution neural networks and recurrent neural
networks. This paper then summarizes deep learning's applications in many areas
of artificial intelligence, including speech processing, computer vision,
natural language processing and so on. Finally, this paper discusses the
existing problems of deep learning and gives the corresponding possible
solutions.
",2018-04-05T02:23:59Z,http://arxiv.org/abs/1804.01653v2,"Rong Zhang, Weiping Li, Tong Mo"
"A Survey of Active Learning for Text Classification using Deep Neural
  Networks","  Natural language processing (NLP) and neural networks (NNs) have both
undergone significant changes in recent years. For active learning (AL)
purposes, NNs are, however, less commonly used -- despite their current
popularity. By using the superior text classification performance of NNs for
AL, we can either increase a model's performance using the same amount of data
or reduce the data and therefore the required annotation efforts while keeping
the same performance. We review AL for text classification using deep neural
networks (DNNs) and elaborate on two main causes which used to hinder the
adoption: (a) the inability of NNs to provide reliable uncertainty estimates,
on which the most commonly used query strategies rely, and (b) the challenge of
training DNNs on small data. To investigate the former, we construct a taxonomy
of query strategies, which distinguishes between data-based, model-based, and
prediction-based instance selection, and investigate the prevalence of these
classes in recent research. Moreover, we review recent NN-based advances in NLP
like word embeddings or language models in the context of (D)NNs, survey the
current state-of-the-art at the intersection of AL, text classification, and
DNNs and relate recent advances in NLP to AL. Finally, we analyze recent work
in AL for text classification, connect the respective query strategies to the
taxonomy, and outline commonalities and shortcomings. As a result, we highlight
gaps in current research and present open research questions.
",2020-08-17T12:53:20Z,http://arxiv.org/abs/2008.07267v1,"Christopher Schröder, Andreas Niekler"
"Learning Neural Models for Natural Language Processing in the Face of
  Distributional Shift","  The dominating NLP paradigm of training a strong neural predictor to perform
one task on a specific dataset has led to state-of-the-art performance in a
variety of applications (eg. sentiment classification, span-prediction based
question answering or machine translation). However, it builds upon the
assumption that the data distribution is stationary, ie. that the data is
sampled from a fixed distribution both at training and test time. This way of
training is inconsistent with how we as humans are able to learn from and
operate within a constantly changing stream of information. Moreover, it is
ill-adapted to real-world use cases where the data distribution is expected to
shift over the course of a model's lifetime.
  The first goal of this thesis is to characterize the different forms this
shift can take in the context of natural language processing, and propose
benchmarks and evaluation metrics to measure its effect on current deep
learning architectures. We then proceed to take steps to mitigate the effect of
distributional shift on NLP models. To this end, we develop methods based on
parametric reformulations of the distributionally robust optimization
framework. Empirically, we demonstrate that these approaches yield more robust
models as demonstrated on a selection of realistic problems. In the third and
final part of this thesis, we explore ways of efficiently adapting existing
models to new domains or tasks. Our contribution to this topic takes
inspiration from information geometry to derive a new gradient update rule
which alleviate catastrophic forgetting issues during adaptation.
",2021-09-03T14:29:20Z,http://arxiv.org/abs/2109.01558v1,Paul Michel
Visualizing RNN States with Predictive Semantic Encodings,"  Recurrent Neural Networks are an effective and prevalent tool used to model
sequential data such as natural language text. However, their deep nature and
massive number of parameters pose a challenge for those intending to study
precisely how they work. We present a visual technique that gives a high level
intuition behind the semantics of the hidden states within Recurrent Neural
Networks. This semantic encoding allows for hidden states to be compared
throughout the model independent of their internal details. The proposed
technique is displayed in a proof of concept visualization tool which is
demonstrated to visualize the natural language processing task of language
modelling.
",2019-08-01T19:24:59Z,http://arxiv.org/abs/1908.00588v1,"Lindsey Sawatzky, Steven Bergner, Fred Popowich"
Learning a Deep Generative Model like a Program: the Free Category Prior,"  Humans surpass the cognitive abilities of most other animals in our ability
to ""chunk"" concepts into words, and then combine the words to combine the
concepts. In this process, we make ""infinite use of finite means"", enabling us
to learn new concepts quickly and nest concepts within each-other. While
program induction and synthesis remain at the heart of foundational theories of
artificial intelligence, only recently has the community moved forward in
attempting to use program learning as a benchmark task itself. The cognitive
science community has thus often assumed that if the brain has simulation and
reasoning capabilities equivalent to a universal computer, then it must employ
a serialized, symbolic representation. Here we confront that assumption, and
provide a counterexample in which compositionality is expressed via network
structure: the free category prior over programs. We show how our formalism
allows neural networks to serve as primitives in probabilistic programs. We
learn both program structure and model parameters end-to-end.
",2020-11-22T17:16:17Z,http://arxiv.org/abs/2011.11063v1,Eli Sennesh
"torchdistill Meets Hugging Face Libraries for Reproducible, Coding-Free
  Deep Learning Studies: A Case Study on NLP","  Reproducibility in scientific work has been becoming increasingly important
in research communities such as machine learning, natural language processing,
and computer vision communities due to the rapid development of the research
domains supported by recent advances in deep learning. In this work, we present
a significantly upgraded version of torchdistill, a modular-driven coding-free
deep learning framework significantly upgraded from the initial release, which
supports only image classification and object detection tasks for reproducible
knowledge distillation experiments. To demonstrate that the upgraded framework
can support more tasks with third-party libraries, we reproduce the GLUE
benchmark results of BERT models using a script based on the upgraded
torchdistill, harmonizing with various Hugging Face libraries. All the 27
fine-tuned BERT models and configurations to reproduce the results are
published at Hugging Face, and the model weights have already been widely used
in research communities. We also reimplement popular small-sized models and new
knowledge distillation methods and perform additional experiments for computer
vision tasks.
",2023-10-26T17:57:15Z,http://arxiv.org/abs/2310.17644v1,Yoshitomo Matsubara
"Are Words Enough? On the semantic conditioning of affective music
  generation","  Music has been commonly recognized as a means of expressing emotions. In this
sense, an intense debate emerges from the need to verbalize musical emotions.
This concern seems highly relevant today, considering the exponential growth of
natural language processing using deep learning models where it is possible to
prompt semantic propositions to generate music automatically. This scoping
review aims to analyze and discuss the possibilities of music generation
conditioned by emotions. To address this topic, we propose a historical
perspective that encompasses the different disciplines and methods contributing
to this topic. In detail, we review two main paradigms adopted in automatic
music generation: rules-based and machine-learning models. Of note are the deep
learning architectures that aim to generate high-fidelity music from textual
descriptions. These models raise fundamental questions about the expressivity
of music, including whether emotions can be represented with words or expressed
through them. We conclude that overcoming the limitation and ambiguity of
language to express emotions through music, some of the use of deep learning
with natural language has the potential to impact the creative industries by
providing powerful tools to prompt and generate new musical works.
",2023-11-07T00:19:09Z,http://arxiv.org/abs/2311.03624v1,"Jorge Forero, Gilberto Bernardes, Mónica Mendes"
"Learning to Infer from Unlabeled Data: A Semi-supervised Learning
  Approach for Robust Natural Language Inference","  Natural Language Inference (NLI) or Recognizing Textual Entailment (RTE) aims
at predicting the relation between a pair of sentences (premise and hypothesis)
as entailment, contradiction or semantic independence. Although deep learning
models have shown promising performance for NLI in recent years, they rely on
large scale expensive human-annotated datasets. Semi-supervised learning (SSL)
is a popular technique for reducing the reliance on human annotation by
leveraging unlabeled data for training. However, despite its substantial
success on single sentence classification tasks where the challenge in making
use of unlabeled data is to assign ""good enough"" pseudo-labels, for NLI tasks,
the nature of unlabeled data is more complex: one of the sentences in the pair
(usually the hypothesis) along with the class label are missing from the data
and require human annotations, which makes SSL for NLI more challenging. In
this paper, we propose a novel way to incorporate unlabeled data in SSL for NLI
where we use a conditional language model, BART to generate the hypotheses for
the unlabeled sentences (used as premises). Our experiments show that our SSL
framework successfully exploits unlabeled data and substantially improves the
performance of four NLI datasets in low-resource settings. We release our code
at: https://github.com/msadat3/SSL_for_NLI.
",2022-11-05T20:34:08Z,http://arxiv.org/abs/2211.02971v1,"Mobashir Sadat, Cornelia Caragea"
"XLDA: Cross-Lingual Data Augmentation for Natural Language Inference and
  Question Answering","  While natural language processing systems often focus on a single language,
multilingual transfer learning has the potential to improve performance,
especially for low-resource languages. We introduce XLDA, cross-lingual data
augmentation, a method that replaces a segment of the input text with its
translation in another language. XLDA enhances performance of all 14 tested
languages of the cross-lingual natural language inference (XNLI) benchmark.
With improvements of up to $4.8\%$, training with XLDA achieves
state-of-the-art performance for Greek, Turkish, and Urdu. XLDA is in contrast
to, and performs markedly better than, a more naive approach that aggregates
examples in various languages in a way that each example is solely in one
language. On the SQuAD question answering task, we see that XLDA provides a
$1.0\%$ performance increase on the English evaluation set. Comprehensive
experiments suggest that most languages are effective as cross-lingual
augmentors, that XLDA is robust to a wide range of translation quality, and
that XLDA is even more effective for randomly initialized models than for
pretrained models.
",2019-05-27T19:44:33Z,http://arxiv.org/abs/1905.11471v1,"Jasdeep Singh, Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, Richard Socher"
"The Bottleneck Simulator: A Model-based Deep Reinforcement Learning
  Approach","  Deep reinforcement learning has recently shown many impressive successes.
However, one major obstacle towards applying such methods to real-world
problems is their lack of data-efficiency. To this end, we propose the
Bottleneck Simulator: a model-based reinforcement learning method which
combines a learned, factorized transition model of the environment with rollout
simulations to learn an effective policy from few examples. The learned
transition model employs an abstract, discrete (bottleneck) state, which
increases sample efficiency by reducing the number of model parameters and by
exploiting structural properties of the environment. We provide a mathematical
analysis of the Bottleneck Simulator in terms of fixed points of the learned
policy, which reveals how performance is affected by four distinct sources of
error: an error related to the abstract space structure, an error related to
the transition model estimation variance, an error related to the transition
model estimation bias, and an error related to the transition model class bias.
Finally, we evaluate the Bottleneck Simulator on two natural language
processing tasks: a text adventure game and a real-world, complex dialogue
response selection task. On both tasks, the Bottleneck Simulator yields
excellent performance beating competing approaches.
",2018-07-12T16:59:28Z,http://arxiv.org/abs/1807.04723v1,"Iulian Vlad Serban, Chinnadhurai Sankar, Michael Pieper, Joelle Pineau, Yoshua Bengio"
"What is not where: the challenge of integrating spatial representations
  into deep learning architectures","  This paper examines to what degree current deep learning architectures for
image caption generation capture spatial language. On the basis of the
evaluation of examples of generated captions from the literature we argue that
systems capture what objects are in the image data but not where these objects
are located: the captions generated by these systems are the output of a
language model conditioned on the output of an object detector that cannot
capture fine-grained location information. Although language models provide
useful knowledge for image captions, we argue that deep learning image
captioning architectures should also model geometric relations between objects.
",2018-07-21T11:55:17Z,http://arxiv.org/abs/1807.08133v1,"John D. Kelleher, Simon Dobnik"
"AfroLM: A Self-Active Learning-based Multilingual Pretrained Language
  Model for 23 African Languages","  In recent years, multilingual pre-trained language models have gained
prominence due to their remarkable performance on numerous downstream Natural
Language Processing tasks (NLP). However, pre-training these large multilingual
language models requires a lot of training data, which is not available for
African Languages. Active learning is a semi-supervised learning algorithm, in
which a model consistently and dynamically learns to identify the most
beneficial samples to train itself on, in order to achieve better optimization
and performance on downstream tasks. Furthermore, active learning effectively
and practically addresses real-world data scarcity. Despite all its benefits,
active learning, in the context of NLP and especially multilingual language
models pretraining, has received little consideration. In this paper, we
present AfroLM, a multilingual language model pretrained from scratch on 23
African languages (the largest effort to date) using our novel self-active
learning framework. Pretrained on a dataset significantly (14x) smaller than
existing baselines, AfroLM outperforms many multilingual pretrained language
models (AfriBERTa, XLMR-base, mBERT) on various NLP downstream tasks (NER, text
classification, and sentiment analysis). Additional out-of-domain sentiment
analysis experiments show that \textbf{AfroLM} is able to generalize well
across various domains. We release the code source, and our datasets used in
our framework at https://github.com/bonaventuredossou/MLM_AL.
",2022-11-07T02:15:25Z,http://arxiv.org/abs/2211.03263v2,"Bonaventure F. P. Dossou, Atnafu Lambebo Tonja, Oreen Yousuf, Salomey Osei, Abigail Oppong, Iyanuoluwa Shode, Oluwabusayo Olufunke Awoyomi, Chris Chinenye Emezue"
"Audio-visual Generalised Zero-shot Learning with Cross-modal Attention
  and Language","  Learning to classify video data from classes not included in the training
data, i.e. video-based zero-shot learning, is challenging. We conjecture that
the natural alignment between the audio and visual modalities in video data
provides a rich training signal for learning discriminative multi-modal
representations. Focusing on the relatively underexplored task of audio-visual
zero-shot learning, we propose to learn multi-modal representations from
audio-visual data using cross-modal attention and exploit textual label
embeddings for transferring knowledge from seen classes to unseen classes.
Taking this one step further, in our generalised audio-visual zero-shot
learning setting, we include all the training classes in the test-time search
space which act as distractors and increase the difficulty while making the
setting more realistic. Due to the lack of a unified benchmark in this domain,
we introduce a (generalised) zero-shot learning benchmark on three audio-visual
datasets of varying sizes and difficulty, VGGSound, UCF, and ActivityNet,
ensuring that the unseen test classes do not appear in the dataset used for
supervised training of the backbone deep models. Comparing multiple relevant
and recent methods, we demonstrate that our proposed AVCA model achieves
state-of-the-art performance on all three datasets. Code and data are available
at \url{https://github.com/ExplainableML/AVCA-GZSL}.
",2022-03-07T18:52:13Z,http://arxiv.org/abs/2203.03598v2,"Otniel-Bogdan Mercea, Lukas Riesch, A. Sophia Koepke, Zeynep Akata"
"Boardwalk Empire: How Generative AI is Revolutionizing Economic
  Paradigms","  The relentless pursuit of technological advancements has ushered in a new era
where artificial intelligence (AI) is not only a powerful tool but also a
critical economic driver. At the forefront of this transformation is Generative
AI, which is catalyzing a paradigm shift across industries. Deep generative
models, an integration of generative and deep learning techniques, excel in
creating new data beyond analyzing existing ones, revolutionizing sectors from
production and manufacturing to finance. By automating design, optimization,
and innovation cycles, Generative AI is reshaping core industrial processes. In
the financial sector, it is transforming risk assessment, trading strategies,
and forecasting, demonstrating its profound impact. This paper explores the
sweeping changes driven by deep learning models like Large Language Models
(LLMs), highlighting their potential to foster innovative business models,
disruptive technologies, and novel economic landscapes. As we stand at the
threshold of an AI-driven economic era, Generative AI is emerging as a pivotal
force, driving innovation, disruption, and economic evolution on a global
scale.
",2024-10-19T20:57:16Z,http://arxiv.org/abs/2410.15212v2,"Subramanyam Sahoo, Kamlesh Dutta"
"TabLLM: Few-shot Classification of Tabular Data with Large Language
  Models","  We study the application of large language models to zero-shot and few-shot
classification of tabular data. We prompt the large language model with a
serialization of the tabular data to a natural-language string, together with a
short description of the classification problem. In the few-shot setting, we
fine-tune the large language model using some labeled examples. We evaluate
several serialization methods including templates, table-to-text models, and
large language models. Despite its simplicity, we find that this technique
outperforms prior deep-learning-based tabular classification methods on several
benchmark datasets. In most cases, even zero-shot classification obtains
non-trivial performance, illustrating the method's ability to exploit prior
knowledge encoded in large language models. Unlike many deep learning methods
for tabular datasets, this approach is also competitive with strong traditional
baselines like gradient-boosted trees, especially in the very-few-shot setting.
",2022-10-19T17:08:13Z,http://arxiv.org/abs/2210.10723v2,"Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, David Sontag"
Machine Learning for Synthetic Data Generation: A Review,"  Machine learning heavily relies on data, but real-world applications often
encounter various data-related issues. These include data of poor quality,
insufficient data points leading to under-fitting of machine learning models,
and difficulties in data access due to concerns surrounding privacy, safety,
and regulations. In light of these challenges, the concept of synthetic data
generation emerges as a promising alternative that allows for data sharing and
utilization in ways that real-world data cannot facilitate. This paper presents
a comprehensive systematic review of existing studies that employ machine
learning models for the purpose of generating synthetic data. The review
encompasses various perspectives, starting with the applications of synthetic
data generation, spanning computer vision, speech, natural language processing,
healthcare, and business domains. Additionally, it explores different machine
learning methods, with particular emphasis on neural network architectures and
deep generative models. The paper also addresses the crucial aspects of privacy
and fairness concerns related to synthetic data generation. Furthermore, this
study identifies the challenges and opportunities prevalent in this emerging
field, shedding light on the potential avenues for future research. By delving
into the intricacies of synthetic data generation, this paper aims to
contribute to the advancement of knowledge and inspire further exploration in
synthetic data generation.
",2023-02-08T13:59:31Z,http://arxiv.org/abs/2302.04062v9,"Yingzhou Lu, Minjie Shen, Huazheng Wang, Xiao Wang, Capucine van Rechem, Tianfan Fu, Wenqi Wei"
DELTA: A DEep learning based Language Technology plAtform,"  In this paper we present DELTA, a deep learning based language technology
platform. DELTA is an end-to-end platform designed to solve industry level
natural language and speech processing problems. It integrates most popular
neural network models for training as well as comprehensive deployment tools
for production. DELTA aims to provide easy and fast experiences for using,
deploying, and developing natural language processing and speech models for
both academia and industry use cases. We demonstrate the reliable performance
with DELTA on several natural language processing and speech tasks, including
text classification, named entity recognition, natural language inference,
speech recognition, speaker verification, etc. DELTA has been used for
developing several state-of-the-art algorithms for publications and delivering
real production to serve millions of users.
",2019-08-02T01:13:50Z,http://arxiv.org/abs/1908.01853v1,"Kun Han, Junwen Chen, Hui Zhang, Haiyang Xu, Yiping Peng, Yun Wang, Ning Ding, Hui Deng, Yonghu Gao, Tingwei Guo, Yi Zhang, Yahao He, Baochang Ma, Yulong Zhou, Kangli Zhang, Chao Liu, Ying Lyu, Chenxi Wang, Cheng Gong, Yunbo Wang, Wei Zou, Hui Song, Xiangang Li"
"A Review of Hybrid and Ensemble in Deep Learning for Natural Language
  Processing","  This review presents a comprehensive exploration of hybrid and ensemble deep
learning models within Natural Language Processing (NLP), shedding light on
their transformative potential across diverse tasks such as Sentiment Analysis,
Named Entity Recognition, Machine Translation, Question Answering, Text
Classification, Generation, Speech Recognition, Summarization, and Language
Modeling. The paper systematically introduces each task, delineates key
architectures from Recurrent Neural Networks (RNNs) to Transformer-based models
like BERT, and evaluates their performance, challenges, and computational
demands. The adaptability of ensemble techniques is emphasized, highlighting
their capacity to enhance various NLP applications. Challenges in
implementation, including computational overhead, overfitting, and model
interpretation complexities, are addressed alongside the trade-off between
interpretability and performance. Serving as a concise yet invaluable guide,
this review synthesizes insights into tasks, architectures, and challenges,
offering a holistic perspective for researchers and practitioners aiming to
advance language-driven applications through ensemble deep learning in NLP.
",2023-12-09T14:49:34Z,http://arxiv.org/abs/2312.05589v2,"Jianguo Jia, Wen Liang, Youzhi Liang"
"Improving the Performance of English-Tamil Statistical Machine
  Translation System using Source-Side Pre-Processing","  Machine Translation is one of the major oldest and the most active research
area in Natural Language Processing. Currently, Statistical Machine Translation
(SMT) dominates the Machine Translation research. Statistical Machine
Translation is an approach to Machine Translation which uses models to learn
translation patterns directly from data, and generalize them to translate a new
unseen text. The SMT approach is largely language independent, i.e. the models
can be applied to any language pair. Statistical Machine Translation (SMT)
attempts to generate translations using statistical methods based on bilingual
text corpora. Where such corpora are available, excellent results can be
attained translating similar texts, but such corpora are still not available
for many language pairs. Statistical Machine Translation systems, in general,
have difficulty in handling the morphology on the source or the target side
especially for morphologically rich languages. Errors in morphology or syntax
in the target language can have severe consequences on meaning of the sentence.
They change the grammatical function of words or the understanding of the
sentence through the incorrect tense information in verb. Baseline SMT also
known as Phrase Based Statistical Machine Translation (PBSMT) system does not
use any linguistic information and it only operates on surface word form.
Recent researches shown that adding linguistic information helps to improve the
accuracy of the translation with less amount of bilingual corpora. Adding
linguistic information can be done using the Factored Statistical Machine
Translation system through pre-processing steps. This paper investigates about
how English side pre-processing is used to improve the accuracy of
English-Tamil SMT system.
",2014-09-29T04:54:03Z,http://arxiv.org/abs/1409.8581v1,"M. Anand Kumar, V. Dhanalakshmi, K. P. Soman, V. Sharmiladevi"
"Injecting structural hints: Using language models to study inductive
  biases in language learning","  Both humans and large language models are able to learn language without
explicit structural supervision. What inductive biases make this learning
possible? We address this fundamental cognitive question by leveraging
transformer language models: we inject inductive bias into language models by
pretraining on formally-structured data, and then evaluate the biased learners'
ability to learn typologically-diverse natural languages. Our experimental
setup creates a testbed for hypotheses about inductive bias in human language
learning. We investigate the effect of injecting models with three types of
inductive bias: 1) recursive, hierarchical processing, 2) crossing token-token
relationships that can't be modeled by context-free grammars, and 3) a Zipfian
power-law vocabulary distribution. We show that non-context-free relationships
form the best inductive biases. Our study leverages the capabilities of
transformer models to run controlled language learning experiments that are not
possible to run on humans, and surfaces hypotheses about the structures that
facilitate language learning in both humans and machines.
",2023-04-25T18:00:08Z,http://arxiv.org/abs/2304.13060v2,"Isabel Papadimitriou, Dan Jurafsky"
"Deep learning for language understanding of mental health concepts
  derived from Cognitive Behavioural Therapy","  In recent years, we have seen deep learning and distributed representations
of words and sentences make impact on a number of natural language processing
tasks, such as similarity, entailment and sentiment analysis. Here we introduce
a new task: understanding of mental health concepts derived from Cognitive
Behavioural Therapy (CBT). We define a mental health ontology based on the CBT
principles, annotate a large corpus where this phenomena is exhibited and
perform understanding using deep learning and distributed representations. Our
results show that the performance of deep learning models combined with word
embeddings or sentence embeddings significantly outperform non-deep-learning
models in this difficult task. This understanding module will be an essential
component of a statistical dialogue system delivering therapy.
",2018-09-03T16:17:11Z,http://arxiv.org/abs/1809.00640v1,"Lina Rojas-Barahona, Bo-Hsiang Tseng, Yinpei Dai, Clare Mansfield, Osman Ramadan, Stefan Ultes, Michael Crawford, Milica Gasic"
Graph Neural Networks for Text Classification: A Survey,"  Text Classification is the most essential and fundamental problem in Natural
Language Processing. While numerous recent text classification models applied
the sequential deep learning technique, graph neural network-based models can
directly deal with complex structured text data and exploit global information.
Many real text classification applications can be naturally cast into a graph,
which captures words, documents, and corpus global features. In this survey, we
bring the coverage of methods up to 2023, including corpus-level and
document-level graph neural networks. We discuss each of these methods in
detail, dealing with the graph construction mechanisms and the graph-based
learning process. As well as the technological survey, we look at issues behind
and future directions addressed in text classification using graph neural
networks. We also cover datasets, evaluation metrics, and experiment design and
present a summary of published performance on the publicly available
benchmarks. Note that we present a comprehensive comparison between different
techniques and identify the pros and cons of various evaluation metrics in this
survey.
",2023-04-23T04:21:50Z,http://arxiv.org/abs/2304.11534v3,"Kunze Wang, Yihao Ding, Soyeon Caren Han"
"SNOBERT: A Benchmark for clinical notes entity linking in the SNOMED CT
  clinical terminology","  The extraction and analysis of insights from medical data, primarily stored
in free-text formats by healthcare workers, presents significant challenges due
to its unstructured nature. Medical coding, a crucial process in healthcare,
remains minimally automated due to the complexity of medical ontologies and
restricted access to medical texts for training Natural Language Processing
models. In this paper, we proposed a method, ""SNOBERT,"" of linking text spans
in clinical notes to specific concepts in the SNOMED CT using BERT-based
models. The method consists of two stages: candidate selection and candidate
matching. The models were trained on one of the largest publicly available
dataset of labeled clinical notes. SNOBERT outperforms other classical methods
based on deep learning, as confirmed by the results of a challenge in which it
was applied.
",2024-05-25T08:00:44Z,http://arxiv.org/abs/2405.16115v1,"Mikhail Kulyabin, Gleb Sokolov, Aleksandr Galaida, Andreas Maier, Tomas Arias-Vergara"
"Advancements and Challenges in Bangla Question Answering Models: A
  Comprehensive Review","  The domain of Natural Language Processing (NLP) has experienced notable
progress in the evolution of Bangla Question Answering (QA) systems. This paper
presents a comprehensive review of seven research articles that contribute to
the progress in this domain. These research studies explore different aspects
of creating question-answering systems for the Bangla language. They cover
areas like collecting data, preparing it for analysis, designing models,
conducting experiments, and interpreting results. The papers introduce
innovative methods like using LSTM-based models with attention mechanisms,
context-based QA systems, and deep learning techniques based on prior
knowledge. However, despite the progress made, several challenges remain,
including the lack of well-annotated data, the absence of high-quality reading
comprehension datasets, and difficulties in understanding the meaning of words
in context. Bangla QA models' precision and applicability are constrained by
these challenges. This review emphasizes the significance of these research
contributions by highlighting the developments achieved in creating Bangla QA
systems as well as the ongoing effort required to get past roadblocks and
improve the performance of these systems for actual language comprehension
tasks.
",2024-12-16T14:42:26Z,http://arxiv.org/abs/2412.11823v1,"Md Iftekhar Islam Tashik, Abdullah Khondoker, Enam Ahmed Taufik, Antara Firoz Parsa, S M Ishtiak Mahmud"
"Who Wrote it and Why? Prompting Large-Language Models for Authorship
  Verification","  Authorship verification (AV) is a fundamental task in natural language
processing (NLP) and computational linguistics, with applications in forensic
analysis, plagiarism detection, and identification of deceptive content.
Existing AV techniques, including traditional stylometric and deep learning
approaches, face limitations in terms of data requirements and lack of
explainability. To address these limitations, this paper proposes PromptAV, a
novel technique that leverages Large-Language Models (LLMs) for AV by providing
step-by-step stylometric explanation prompts. PromptAV outperforms
state-of-the-art baselines, operates effectively with limited training data,
and enhances interpretability through intuitive explanations, showcasing its
potential as an effective and interpretable solution for the AV task.
",2023-10-12T08:24:15Z,http://arxiv.org/abs/2310.08123v1,"Chia-Yu Hung, Zhiqiang Hu, Yujia Hu, Roy Ka-Wei Lee"
A Comprehensive Survey on Graph Neural Networks,"  Deep learning has revolutionized many machine learning tasks in recent years,
ranging from image classification and video processing to speech recognition
and natural language understanding. The data in these tasks are typically
represented in the Euclidean space. However, there is an increasing number of
applications where data are generated from non-Euclidean domains and are
represented as graphs with complex relationships and interdependency between
objects. The complexity of graph data has imposed significant challenges on
existing machine learning algorithms. Recently, many studies on extending deep
learning approaches for graph data have emerged. In this survey, we provide a
comprehensive overview of graph neural networks (GNNs) in data mining and
machine learning fields. We propose a new taxonomy to divide the
state-of-the-art graph neural networks into four categories, namely recurrent
graph neural networks, convolutional graph neural networks, graph autoencoders,
and spatial-temporal graph neural networks. We further discuss the applications
of graph neural networks across various domains and summarize the open source
codes, benchmark data sets, and model evaluation of graph neural networks.
Finally, we propose potential research directions in this rapidly growing
field.
",2019-01-03T03:20:55Z,http://arxiv.org/abs/1901.00596v4,"Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, Philip S. Yu"
"Instance-based Inductive Deep Transfer Learning by Cross-Dataset
  Querying with Locality Sensitive Hashing","  Supervised learning models are typically trained on a single dataset and the
performance of these models rely heavily on the size of the dataset, i.e.,
amount of data available with the ground truth. Learning algorithms try to
generalize solely based on the data that is presented with during the training.
In this work, we propose an inductive transfer learning method that can augment
learning models by infusing similar instances from different learning tasks in
the Natural Language Processing (NLP) domain. We propose to use instance
representations from a source dataset, \textit{without inheriting anything}
from the source learning model. Representations of the instances of
\textit{source} \& \textit{target} datasets are learned, retrieval of relevant
source instances is performed using soft-attention mechanism and
\textit{locality sensitive hashing}, and then, augmented into the model during
training on the target dataset. Our approach simultaneously exploits the local
\textit{instance level information} as well as the macro statistical viewpoint
of the dataset. Using this approach we have shown significant improvements for
three major news classification datasets over the baseline. Experimental
evaluations also show that the proposed approach reduces dependency on labeled
data by a significant margin for comparable performance. With our proposed
cross dataset learning procedure we show that one can achieve
competitive/better performance than learning from a single dataset.
",2018-02-16T13:59:15Z,http://arxiv.org/abs/1802.05934v1,"Somnath Basu Roy Chowdhury, K M Annervaz, Ambedkar Dukkipati"
"Interpretable Sentence Representation with Variational Autoencoders and
  Attention","  In this thesis, we develop methods to enhance the interpretability of recent
representation learning techniques in natural language processing (NLP) while
accounting for the unavailability of annotated data. We choose to leverage
Variational Autoencoders (VAEs) due to their efficiency in relating
observations to latent generative factors and their effectiveness in
data-efficient learning and interpretable representation learning. As a first
contribution, we identify and remove unnecessary components in the functioning
scheme of semi-supervised VAEs making them faster, smaller and easier to
design. Our second and main contribution is to use VAEs and Transformers to
build two models with inductive bias to separate information in latent
representations into understandable concepts without annotated data. The first
model, Attention-Driven VAE (ADVAE), is able to separately represent and
control information about syntactic roles in sentences. The second model,
QKVAE, uses separate latent variables to form keys and values for its
Transformer decoder and is able to separate syntactic and semantic information
in its neural representations. In transfer experiments, QKVAE has competitive
performance compared to supervised models and equivalent performance to a
supervised model using 50K annotated samples. Additionally, QKVAE displays
improved syntactic role disentanglement capabilities compared to ADVAE.
Overall, we demonstrate that it is possible to enhance the interpretability of
state-of-the-art deep learning architectures for language modeling with
unannotated data in situations where text data is abundant but annotations are
scarce.
",2023-05-04T13:16:15Z,http://arxiv.org/abs/2305.02810v1,Ghazi Felhi
Semi-Automated Construction of Food Composition Knowledge Base,"  A food composition knowledge base, which stores the essential phyto-, micro-,
and macro-nutrients of foods is useful for both research and industrial
applications. Although many existing knowledge bases attempt to curate such
information, they are often limited by time-consuming manual curation
processes. Outside of the food science domain, natural language processing
methods that utilize pre-trained language models have recently shown promising
results for extracting knowledge from unstructured text. In this work, we
propose a semi-automated framework for constructing a knowledge base of food
composition from the scientific literature available online. To this end, we
utilize a pre-trained BioBERT language model in an active learning setup that
allows the optimal use of limited training data. Our work demonstrates how
human-in-the-loop models are a step toward AI-assisted food systems that scale
well to the ever-increasing big data.
",2023-01-24T22:08:49Z,http://arxiv.org/abs/2301.11322v1,"Jason Youn, Fangzhou Li, Ilias Tagkopoulos"
The Janus System: Multi-paradigm Programming in Prolog and Python,"  Python and Prolog express different programming paradigms, with different
strengths. Python is wildly popular because it is well-structured, easy to use,
and mixes well with thousands of scientific and machine learning programs
written in C. Prolog's logic-based approach provides powerful reasoning
capabilities, especially when combined with constraint evaluation,
probabilistic reasoning, well-founded negation, and other advances. Both
languages have commonalities as well: both are usually written in C, both are
dynamically typed, and both use data structures based on a small number of
recursive types.
  This paper describes the design and implementation of Janus, a system that
tightly combines Prolog and Python into a single process. Janus bi-translates
data structures and offers performance of many hundreds of thousands of
round-trip inter-language calls per second. Although Janus is still new, it has
been used in commercial applications including natural language processing,
visual query answering and robotic automation. Janus was developed for XSB, but
porting Janus code to a second Prolog has been straightforward, indicating that
Janus is a tool that other Prologs may easily adopt.
",2023-08-30T09:07:05Z,http://arxiv.org/abs/2308.15893v1,"Theresa Swift, Carl Andersen"
"Uncertainty-Informed Screening for Safer Solvents Used in the Synthesis
  of Perovskite via Language Models","  The challenge of accurately predicting toxicity of industrial solvents used
in perovskite synthesis is a necessary undertaking but is limited by a lack of
a targeted and structured toxicity data. This paper presents a novel framework
that combines an automated data extraction using language models, and an
uncertainty-informed prediction model to fill data gaps and improve prediction
confidence. First, we have utilized and compared two approaches to
automatically extract relevant data from a corpus of scientific literature on
solvents used in perovskite synthesis: smaller bidirectional language models
like BERT and ELMo are used for their repeatability and deterministic outputs,
while autoregressive large language model (LLM) such as GPT-3.5 is used to
leverage its larger training corpus and better response generation. Our novel
'prompting and verification' technique integrated with an LLM aims at targeted
extraction and refinement, thereby reducing hallucination and improving the
quality of the extracted data using the LLM. Next, the extracted data is fed
into our pre-trained multi-task binary classification deep learning to predict
the ED nature of extracted solvents. We have used a Shannon entropy-based
uncertainty quantification utilizing the class probabilities obtained from the
classification model to quantify uncertainty and identify data gaps in our
predictions. This approach leads to the curation of a structured dataset for
solvents used in perovskite synthesis and their uncertainty-informed virtual
toxicity assessment. Additionally, chord diagrams have been used to visualize
solvent interactions and prioritize those with potential hazards, revealing
that 70% of the solvent interactions were primarily associated with two
specific perovskites.
",2024-09-30T17:13:40Z,http://arxiv.org/abs/2409.20512v1,"Arpan Mukherjee, Deepesh Giri, Krishna Rajan"
"Long-range gene expression prediction with token alignment of large
  language model","  Gene expression is a cellular process that plays a fundamental role in human
phenotypical variations and diseases. Despite advances of deep learning models
for gene expression prediction, recent benchmarks have revealed their inability
to learn distal regulatory grammar. Here, we address this challenge by
leveraging a pretrained large language model to enhance gene expression
prediction. We introduce Genetic sequence Token Alignment (GTA), which aligns
genetic sequence features with natural language tokens, allowing for symbolic
reasoning of genomic sequence features via the frozen language model. This
cross-modal adaptation learns the regulatory grammar and allows us to further
incorporate gene-specific human annotations as prompts, enabling in-context
learning that is not possible with existing models. Trained on lymphoblastoid
cells, GTA was evaluated on cells from the Geuvadis consortium and outperforms
state-of-the-art models such as Enformer, achieving a Spearman correlation of
0.65, a 10\% improvement. Additionally, GTA offers improved interpretation of
long-range interactions through the identification of the most meaningful
sections of the input genetic context. GTA represents a powerful and novel
cross-modal approach to gene expression prediction by utilizing a pretrained
language model, in a paradigm shift from conventional gene expression models
trained only on sequence data.
",2024-10-02T02:42:29Z,http://arxiv.org/abs/2410.01858v1,"Edouardo Honig, Huixin Zhan, Ying Nian Wu, Zijun Frank Zhang"
A Web Scraping Methodology for Bypassing Twitter API Restrictions,"  Retrieving information from social networks is the first and primordial step
many data analysis fields such as Natural Language Processing, Sentiment
Analysis and Machine Learning. Important data science tasks relay on historical
data gathering for further predictive results. Most of the recent works use
Twitter API, a public platform for collecting public streams of information,
which allows querying chronological tweets for no more than three weeks old. In
this paper, we present a new methodology for collecting historical tweets
within any date range using web scraping techniques bypassing for Twitter API
restrictions.
",2018-03-27T03:23:19Z,http://arxiv.org/abs/1803.09875v1,"A. Hernandez-Suarez, G. Sanchez-Perez, K. Toscano-Medina, V. Martinez-Hernandez, V. Sanchez, H. Perez-Meana"
"Direct Feedback Alignment Scales to Modern Deep Learning Tasks and
  Architectures","  Despite being the workhorse of deep learning, the backpropagation algorithm
is no panacea. It enforces sequential layer updates, thus preventing efficient
parallelization of the training process. Furthermore, its biological
plausibility is being challenged. Alternative schemes have been devised; yet,
under the constraint of synaptic asymmetry, none have scaled to modern deep
learning tasks and architectures. Here, we challenge this perspective, and
study the applicability of Direct Feedback Alignment to neural view synthesis,
recommender systems, geometric learning, and natural language processing. In
contrast with previous studies limited to computer vision tasks, our findings
show that it successfully trains a large range of state-of-the-art deep
learning architectures, with performance close to fine-tuned backpropagation.
At variance with common beliefs, our work supports that challenging tasks can
be tackled in the absence of weight transport.
",2020-06-23T10:17:49Z,http://arxiv.org/abs/2006.12878v2,"Julien Launay, Iacopo Poli, François Boniface, Florent Krzakala"
"Predictive Insights into LGBTQ+ Minority Stress: A Transductive
  Exploration of Social Media Discourse","  Individuals who identify as sexual and gender minorities, including lesbian,
gay, bisexual, transgender, queer, and others (LGBTQ+) are more likely to
experience poorer health than their heterosexual and cisgender counterparts.
One primary source that drives these health disparities is minority stress
(i.e., chronic and social stressors unique to LGBTQ+ communities' experiences
adapting to the dominant culture). This stress is frequently expressed in
LGBTQ+ users' posts on social media platforms. However, these expressions are
not just straightforward manifestations of minority stress. They involve
linguistic complexity (e.g., idiom or lexical diversity), rendering them
challenging for many traditional natural language processing methods to detect.
In this work, we designed a hybrid model using Graph Neural Networks (GNN) and
Bidirectional Encoder Representations from Transformers (BERT), a pre-trained
deep language model to improve the classification performance of minority
stress detection. We experimented with our model on a benchmark social media
dataset for minority stress detection (LGBTQ+ MiSSoM+). The dataset is
comprised of 5,789 human-annotated Reddit posts from LGBTQ+ subreddits. Our
approach enables the extraction of hidden linguistic nuances through
pretraining on a vast amount of raw data, while also engaging in transductive
learning to jointly develop representations for both labeled training data and
unlabeled test data. The RoBERTa-GCN model achieved an accuracy of 0.86 and an
F1 score of 0.86, surpassing the performance of other baseline models in
predicting LGBTQ+ minority stress. Improved prediction of minority stress
expressions on social media could lead to digital health interventions to
improve the wellbeing of LGBTQ+ people-a community with high rates of
stress-sensitive health problems.
",2024-11-20T18:35:41Z,http://arxiv.org/abs/2411.13534v1,"S. Chapagain, Y. Zhao, T. K. Rohleen, S. M. Hamdi, S. F. Boubrahimi, R. E. Flinn, E. M. Lund, D. Klooster, J. R. Scheer, C. J. Cascalheira"
Machine Learning Technique Based Fake News Detection,"  False news has received attention from both the general public and the
scholarly world. Such false information has the ability to affect public
perception, giving nefarious groups the chance to influence the results of
public events like elections. Anyone can share fake news or facts about anyone
or anything for their personal gain or to cause someone trouble. Also,
information varies depending on the part of the world it is shared on. Thus, in
this paper, we have trained a model to classify fake and true news by utilizing
the 1876 news data from our collected dataset. We have preprocessed the data to
get clean and filtered texts by following the Natural Language Processing
approaches. Our research conducts 3 popular Machine Learning (Stochastic
gradient descent, Na\""ive Bayes, Logistic Regression,) and 2 Deep Learning
(Long-Short Term Memory, ASGD Weight-Dropped LSTM, or AWD-LSTM) algorithms.
After we have found our best Naive Bayes classifier with 56% accuracy and an
F1-macro score of an average of 32%.
",2023-09-18T19:26:54Z,http://arxiv.org/abs/2309.13069v1,"Biplob Kumar Sutradhar, Md. Zonaid, Nushrat Jahan Ria, Sheak Rashed Haider Noori"
"Deep learning for temporal data representation in electronic health
  records: A systematic review of challenges and methodologies","  Objective: Temporal electronic health records (EHRs) can be a wealth of
information for secondary uses, such as clinical events prediction or chronic
disease management. However, challenges exist for temporal data representation.
We therefore sought to identify these challenges and evaluate novel
methodologies for addressing them through a systematic examination of deep
learning solutions.
  Methods: We searched five databases (PubMed, EMBASE, the Institute of
Electrical and Electronics Engineers [IEEE] Xplore Digital Library, the
Association for Computing Machinery [ACM] digital library, and Web of Science)
complemented with hand-searching in several prestigious computer science
conference proceedings. We sought articles that reported deep learning
methodologies on temporal data representation in structured EHR data from
January 1, 2010, to August 30, 2020. We summarized and analyzed the selected
articles from three perspectives: nature of time series, methodology, and model
implementation.
  Results: We included 98 articles related to temporal data representation
using deep learning. Four major challenges were identified, including data
irregularity, data heterogeneity, data sparsity, and model opacity. We then
studied how deep learning techniques were applied to address these challenges.
Finally, we discuss some open challenges arising from deep learning.
  Conclusion: Temporal EHR data present several major challenges for clinical
prediction modeling and data utilization. To some extent, current deep learning
solutions can address these challenges. Future studies can consider designing
comprehensive and integrated solutions. Moreover, researchers should
incorporate additional clinical domain knowledge into study designs and enhance
the interpretability of the model to facilitate its implementation in clinical
practice.
",2021-07-21T09:00:40Z,http://arxiv.org/abs/2107.09951v1,"Feng Xie, Han Yuan, Yilin Ning, Marcus Eng Hock Ong, Mengling Feng, Wynne Hsu, Bibhas Chakraborty, Nan Liu"
Decoding EEG Brain Activity for Multi-Modal Natural Language Processing,"  Until recently, human behavioral data from reading has mainly been of
interest to researchers to understand human cognition. However, these human
language processing signals can also be beneficial in machine learning-based
natural language processing tasks. Using EEG brain activity to this purpose is
largely unexplored as of yet. In this paper, we present the first large-scale
study of systematically analyzing the potential of EEG brain activity data for
improving natural language processing tasks, with a special focus on which
features of the signal are most beneficial. We present a multi-modal machine
learning architecture that learns jointly from textual input as well as from
EEG features. We find that filtering the EEG signals into frequency bands is
more beneficial than using the broadband signal. Moreover, for a range of word
embedding types, EEG data improves binary and ternary sentiment classification
and outperforms multiple baselines. For more complex tasks such as relation
detection, further research is needed. Finally, EEG data shows to be
particularly promising when limited training data is available.
",2021-02-17T09:44:21Z,http://arxiv.org/abs/2102.08655v2,"Nora Hollenstein, Cedric Renggli, Benjamin Glaus, Maria Barrett, Marius Troendle, Nicolas Langer, Ce Zhang"
"Transfer Learning in the Field of Renewable Energies -- A Transfer
  Learning Framework Providing Power Forecasts Throughout the Lifecycle of Wind
  Farms After Initial Connection to the Electrical Grid","  In recent years, transfer learning gained particular interest in the field of
vision and natural language processing. In the research field of vision, e.g.,
deep neural networks and transfer learning techniques achieve almost perfect
classification scores within minutes. Nonetheless, these techniques are not yet
widely applied in other domains. Therefore, this article identifies critical
challenges and shows potential solutions for power forecasts in the field of
renewable energies. It proposes a framework utilizing transfer learning
techniques in wind power forecasts with limited or no historical data. On the
one hand, this allows evaluating the applicability of transfer learning in the
field of renewable energy. On the other hand, by developing automatic
procedures, we assure that the proposed methods provide a framework that
applies to domains in organic computing as well.
",2019-06-03T09:46:09Z,http://arxiv.org/abs/1906.01168v1,Jens Schreiber
GERNERMED++: Transfer Learning in German Medical NLP,"  We present a statistical model for German medical natural language processing
trained for named entity recognition (NER) as an open, publicly available
model. The work serves as a refined successor to our first GERNERMED model
which is substantially outperformed by our work. We demonstrate the
effectiveness of combining multiple techniques in order to achieve strong
results in entity recognition performance by the means of transfer-learning on
pretrained deep language models (LM), word-alignment and neural machine
translation. Due to the sparse situation on open, public medical entity
recognition models for German texts, this work offers benefits to the German
research community on medical NLP as a baseline model. Since our model is based
on public English data, its weights are provided without legal restrictions on
usage and distribution. The sample code and the statistical model is available
at: https://github.com/frankkramer-lab/GERNERMED-pp
",2022-06-29T09:53:10Z,http://arxiv.org/abs/2206.14504v2,"Johann Frei, Ludwig Frei-Stuber, Frank Kramer"
"Recurrent Neural Networks with Mixed Hierarchical Structures and EM
  Algorithm for Natural Language Processing","  How to obtain hierarchical representations with an increasing level of
abstraction becomes one of the key issues of learning with deep neural
networks. A variety of RNN models have recently been proposed to incorporate
both explicit and implicit hierarchical information in modeling languages in
the literature. In this paper, we propose a novel approach called the latent
indicator layer to identify and learn implicit hierarchical information (e.g.,
phrases), and further develop an EM algorithm to handle the latent indicator
layer in training. The latent indicator layer further simplifies a text's
hierarchical structure, which allows us to seamlessly integrate different
levels of attention mechanisms into the structure. We called the resulting
architecture as the EM-HRNN model. Furthermore, we develop two bootstrap
strategies to effectively and efficiently train the EM-HRNN model on long text
documents. Simulation studies and real data applications demonstrate that the
EM-HRNN model with bootstrap training outperforms other RNN-based models in
document classification tasks. The performance of the EM-HRNN model is
comparable to a Transformer-based method called Bert-base, though the former is
much smaller model and does not require pre-training.
",2022-01-21T23:08:33Z,http://arxiv.org/abs/2201.08919v1,"Zhaoxin Luo, Michael Zhu"
"Deep Neural Networks based Modrec: Some Results with Inter-Symbol
  Interference and Adversarial Examples","  Recent successes and advances in Deep Neural Networks (DNN) in machine vision
and Natural Language Processing (NLP) have motivated their use in traditional
signal processing and communications systems. In this paper, we present results
of such applications to the problem of automatic modulation recognition.
Variations in wireless communication channels are represented by statistical
channel models and their parameterization will increase with the advent of 5G.
In this paper, we report effect of simple two path channel model on our naive
deep neural network based implementation. We also report impact of adversarial
perturbation to the input signal.
",2018-11-14T22:36:47Z,http://arxiv.org/abs/1811.06103v1,"S. Asim Ahmed, Subhashish Chakravarty, Michael Newhouse"
An Overview of Neural Network Compression,"  Overparameterized networks trained to convergence have shown impressive
performance in domains such as computer vision and natural language processing.
Pushing state of the art on salient tasks within these domains corresponds to
these models becoming larger and more difficult for machine learning
practitioners to use given the increasing memory and storage requirements, not
to mention the larger carbon footprint. Thus, in recent years there has been a
resurgence in model compression techniques, particularly for deep convolutional
neural networks and self-attention based networks such as the Transformer.
  Hence, this paper provides a timely overview of both old and current
compression techniques for deep neural networks, including pruning,
quantization, tensor decomposition, knowledge distillation and combinations
thereof.
  We assume a basic familiarity with deep learning architectures\footnote{For
an introduction to deep learning, see ~\citet{goodfellow2016deep}}, namely,
Recurrent Neural
Networks~\citep[(RNNs)][]{rumelhart1985learning,hochreiter1997long},
Convolutional Neural Networks~\citep{fukushima1980neocognitron}~\footnote{For
an up to date overview see~\citet{khan2019survey}} and Self-Attention based
networks~\citep{vaswani2017attention}\footnote{For a general overview of
self-attention networks, see ~\citet{chaudhari2019attentive}.},\footnote{For
more detail and their use in natural language processing,
see~\citet{hu2019introductory}}. Most of the papers discussed are proposed in
the context of at least one of these DNN architectures.
",2020-06-05T20:28:56Z,http://arxiv.org/abs/2006.03669v2,James O' Neill
Lambek pregroups are Frobenius spiders in preorders,"  ""Spider"" is a nickname of special Frobenius algebras, a fundamental structure
from mathematics, physics, and computer science. Pregroups are a fundamental
structure from linguistics. Pregroups and spiders have been used together in
natural language processing: one for syntax, the other for semantics. It turns
out that pregroups themselves can be characterized as pointed spiders in the
category of preordered relations, where they naturally arise from grammars. The
other way around, preordered spider algebras in general can be characterized as
unions of pregroups. This extends the characterization of relational spider
algebras as disjoint unions of groups. The compositional framework that emerged
with the results suggests new ways to understand and apply the basis structures
in machine learning and data analysis.
",2021-05-07T02:42:03Z,http://arxiv.org/abs/2105.03038v4,Dusko Pavlovic
Costs to Consider in Adopting NLP for Your Business,"  Recent advances in Natural Language Processing (NLP) have largely pushed deep
transformer-based models as the go-to state-of-the-art technique without much
regard to the production and utilization cost. Companies planning to adopt
these methods into their business face difficulties because of the lack of
machine, data, and human resources to build them. We compare both the
performance and the cost of classical learning algorithms to the latest ones in
common sequence and text labeling tasks. In our industrial datasets, we find
that classical models often perform on par with deep neural ones despite the
lower cost. We show the trade-off between performance gain and the cost across
the models to give more insights for AI-pivoting business. Further, we call for
more research into low-cost models, especially for under-resourced languages.
",2020-12-16T13:57:31Z,http://arxiv.org/abs/2012.08958v2,"Made Nindyatama Nityasya, Haryo Akbarianto Wibowo, Radityo Eko Prasojo, Alham Fikri Aji"
"A Deep Learning Pipeline for Patient Diagnosis Prediction Using
  Electronic Health Records","  Augmentation of disease diagnosis and decision-making in healthcare with
machine learning algorithms is gaining much impetus in recent years. In
particular, in the current epidemiological situation caused by COVID-19
pandemic, swift and accurate prediction of disease diagnosis with machine
learning algorithms could facilitate identification and care of vulnerable
clusters of population, such as those having multi-morbidity conditions. In
order to build a useful disease diagnosis prediction system, advancement in
both data representation and development of machine learning architectures are
imperative. First, with respect to data collection and representation, we face
severe problems due to multitude of formats and lack of coherency prevalent in
Electronic Health Records (EHRs). This causes hindrance in extraction of
valuable information contained in EHRs. Currently, no universal global data
standard has been established. As a useful solution, we develop and publish a
Python package to transform public health dataset into an easy to access
universal format. This data transformation to an international health data
format facilitates researchers to easily combine EHR datasets with clinical
datasets of diverse formats. Second, machine learning algorithms that predict
multiple disease diagnosis categories simultaneously remain underdeveloped. We
propose two novel model architectures in this regard. First, DeepObserver,
which uses structured numerical data to predict the diagnosis categories and
second, ClinicalBERT_Multi, that incorporates rich information available in
clinical notes via natural language processing methods and also provides
interpretable visualizations to medical practitioners. We show that both models
can predict multiple diagnoses simultaneously with high accuracy.
",2020-06-23T14:58:58Z,http://arxiv.org/abs/2006.16926v1,"Leopold Franz, Yash Raj Shrestha, Bibek Paudel"
"Enhancing Source Code Representations for Deep Learning with Static
  Analysis","  Deep learning techniques applied to program analysis tasks such as code
classification, summarization, and bug detection have seen widespread interest.
Traditional approaches, however, treat programming source code as natural
language text, which may neglect significant structural or semantic details.
Additionally, most current methods of representing source code focus solely on
the code, without considering beneficial additional context. This paper
explores the integration of static analysis and additional context such as bug
reports and design patterns into source code representations for deep learning
models. We use the Abstract Syntax Tree-based Neural Network (ASTNN) method and
augment it with additional context information obtained from bug reports and
design patterns, creating an enriched source code representation that
significantly enhances the performance of common software engineering tasks
such as code classification and code clone detection. Utilizing existing
open-source code data, our approach improves the representation and processing
of source code, thereby improving task performance.
",2024-02-14T20:17:04Z,http://arxiv.org/abs/2402.09557v1,"Xueting Guan, Christoph Treude"
"Performance Modelling of Deep Learning on Intel Many Integrated Core
  Architectures","  Many complex problems, such as natural language processing or visual object
detection, are solved using deep learning. However, efficient training of
complex deep convolutional neural networks for large data sets is
computationally demanding and requires parallel computing resources. In this
paper, we present two parameterized performance models for estimation of
execution time of training convolutional neural networks on the Intel many
integrated core architecture. While for the first performance model we
minimally use measurement techniques for parameter value estimation, in the
second model we estimate more parameters based on measurements. We evaluate the
prediction accuracy of performance models in the context of training three
different convolutional neural network architectures on the Intel Xeon Phi. The
achieved average performance prediction accuracy is about 15% for the first
model and 11% for second model.
",2019-06-04T10:14:50Z,http://arxiv.org/abs/1906.01992v1,"Andre Viebke, Sabri Pllana, Suejb Memeti, Joanna Kolodziej"
Deep Learning,"  Deep learning (DL) is a high dimensional data reduction technique for
constructing high-dimensional predictors in input-output models. DL is a form
of machine learning that uses hierarchical layers of latent features. In this
article, we review the state-of-the-art of deep learning from a modeling and
algorithmic perspective. We provide a list of successful areas of applications
in Artificial Intelligence (AI), Image Processing, Robotics and Automation.
Deep learning is predictive in its nature rather then inferential and can be
viewed as a black-box methodology for high-dimensional function estimation.
",2018-07-20T18:20:34Z,http://arxiv.org/abs/1807.07987v2,"Nicholas G. Polson, Vadim O. Sokolov"
Automatic Question-Answering Using A Deep Similarity Neural Network,"  Automatic question-answering is a classical problem in natural language
processing, which aims at designing systems that can automatically answer a
question, in the same way as human does. In this work, we propose a deep
learning based model for automatic question-answering. First the questions and
answers are embedded using neural probabilistic modeling. Then a deep
similarity neural network is trained to find the similarity score of a pair of
answer and question. Then for each question, the best answer is found as the
one with the highest similarity score. We first train this model on a
large-scale public question-answering database, and then fine-tune it to
transfer to the customer-care chat data. We have also tested our framework on a
public question-answering database and achieved very good performance.
",2017-08-05T05:50:44Z,http://arxiv.org/abs/1708.01713v1,"Shervin Minaee, Zhu Liu"
XNLI: Evaluating Cross-lingual Sentence Representations,"  State-of-the-art natural language processing systems rely on supervision in
the form of annotated data to learn competent models. These models are
generally trained on data in a single language (usually English), and cannot be
directly used beyond that language. Since collecting data in every language is
not realistic, there has been a growing interest in cross-lingual language
understanding (XLU) and low-resource cross-language transfer. In this work, we
construct an evaluation set for XLU by extending the development and test sets
of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 15
languages, including low-resource languages such as Swahili and Urdu. We hope
that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence
understanding by providing an informative standard evaluation task. In
addition, we provide several baselines for multilingual sentence understanding,
including two based on machine translation systems, and two that use parallel
data to train aligned multilingual bag-of-words and LSTM encoders. We find that
XNLI represents a practical and challenging evaluation suite, and that directly
translating the test data yields the best performance among available
baselines.
",2018-09-13T16:39:53Z,http://arxiv.org/abs/1809.05053v1,"Alexis Conneau, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel R. Bowman, Holger Schwenk, Veselin Stoyanov"
Using Domain Knowledge for Low Resource Named Entity Recognition,"  In recent years, named entity recognition has always been a popular research
in the field of natural language processing, while traditional deep learning
methods require a large amount of labeled data for model training, which makes
them not suitable for areas where labeling resources are scarce. In addition,
the existing cross-domain knowledge transfer methods need to adjust the entity
labels for different fields, so as to increase the training cost. To solve
these problems, enlightened by a processing method of Chinese named entity
recognition, we propose to use domain knowledge to improve the performance of
named entity recognition in areas with low resources. The domain knowledge
mainly applied by us is domain dictionary and domain labeled data. We use
dictionary information for each word to strengthen its word embedding and
domain labeled data to reinforce the recognition effect. The proposed model
avoids large-scale data adjustments in different domains while handling named
entities recognition with low resources. Experiments demonstrate the
effectiveness of our method, which has achieved impressive results on the data
set in the field of scientific and technological equipment, and the F1 score
has been significantly improved compared with many other baseline methods.
",2022-03-28T13:26:47Z,http://arxiv.org/abs/2203.14738v1,Yuan Shi
Sparsity-based Defense against Adversarial Attacks on Linear Classifiers,"  Deep neural networks represent the state of the art in machine learning in a
growing number of fields, including vision, speech and natural language
processing. However, recent work raises important questions about the
robustness of such architectures, by showing that it is possible to induce
classification errors through tiny, almost imperceptible, perturbations.
Vulnerability to such ""adversarial attacks"", or ""adversarial examples"", has
been conjectured to be due to the excessive linearity of deep networks. In this
paper, we study this phenomenon in the setting of a linear classifier, and show
that it is possible to exploit sparsity in natural data to combat
$\ell_{\infty}$-bounded adversarial perturbations. Specifically, we demonstrate
the efficacy of a sparsifying front end via an ensemble averaged analysis, and
experimental results for the MNIST handwritten digit database. To the best of
our knowledge, this is the first work to show that sparsity provides a
theoretically rigorous framework for defense against adversarial attacks.
",2018-01-15T08:18:33Z,http://arxiv.org/abs/1801.04695v3,"Zhinus Marzi, Soorya Gopalakrishnan, Upamanyu Madhow, Ramtin Pedarsani"
"Machine Learning in Python: Main developments and technology trends in
  data science, machine learning, and artificial intelligence","  Smarter applications are making better use of the insights gleaned from data,
having an impact on every industry and research discipline. At the core of this
revolution lies the tools and the methods that are driving it, from processing
the massive piles of data generated each day to learning from and taking useful
action. Deep neural networks, along with advancements in classical ML and
scalable general-purpose GPU computing, have become critical components of
artificial intelligence, enabling many of these astounding breakthroughs and
lowering the barrier to adoption. Python continues to be the most preferred
language for scientific computing, data science, and machine learning, boosting
both performance and productivity by enabling the use of low-level libraries
and clean high-level APIs. This survey offers insight into the field of machine
learning with Python, taking a tour through important topics to identify some
of the core hardware and software paradigms that have enabled it. We cover
widely-used libraries and concepts, collected together for holistic comparison,
with the goal of educating the reader and driving the field of Python machine
learning forward.
",2020-02-12T05:20:59Z,http://arxiv.org/abs/2002.04803v2,"Sebastian Raschka, Joshua Patterson, Corey Nolet"
FarsTail: A Persian Natural Language Inference Dataset,"  Natural language inference (NLI) is known as one of the central tasks in
natural language processing (NLP) which encapsulates many fundamental aspects
of language understanding. With the considerable achievements of data-hungry
deep learning methods in NLP tasks, a great amount of effort has been devoted
to develop more diverse datasets for different languages. In this paper, we
present a new dataset for the NLI task in the Persian language, also known as
Farsi, which is one of the dominant languages in the Middle East. This dataset,
named FarsTail, includes 10,367 samples which are provided in both the Persian
language as well as the indexed format to be useful for non-Persian
researchers. The samples are generated from 3,539 multiple-choice questions
with the least amount of annotator interventions in a way similar to the
SciTail dataset. A carefully designed multi-step process is adopted to ensure
the quality of the dataset. We also present the results of traditional and
state-of-the-art methods on FarsTail including different embedding methods such
as word2vec, fastText, ELMo, BERT, and LASER, as well as different modeling
approaches such as DecompAtt, ESIM, HBMP, and ULMFiT to provide a solid
baseline for the future research. The best obtained test accuracy is 83.38%
which shows that there is a big room for improving the current methods to be
useful for real-world NLP applications in different languages. We also
investigate the extent to which the models exploit superficial clues, also
known as dataset biases, in FarsTail, and partition the test set into easy and
hard subsets according to the success of biased models. The dataset is
available at https://github.com/dml-qom/FarsTail
",2020-09-18T13:04:04Z,http://arxiv.org/abs/2009.08820v2,"Hossein Amirkhani, Mohammad AzariJafari, Zohreh Pourjafari, Soroush Faridan-Jahromi, Zeinab Kouhkan, Azadeh Amirak"
"Transparent but Powerful: Explainability, Accuracy, and Generalizability
  in ADHD Detection from Social Media Data","  Attention-deficit/hyperactivity disorder (ADHD) is a prevalent mental health
condition affecting both children and adults, yet it remains severely
underdiagnosed. Recent advances in artificial intelligence, particularly in
Natural Language Processing (NLP) and Machine Learning (ML), offer promising
solutions for scalable and non-invasive ADHD screening methods using social
media data. This paper presents a comprehensive study on ADHD detection,
leveraging both shallow machine learning models and deep learning approaches,
including BiLSTM and transformer-based models, to analyze linguistic patterns
in ADHD-related social media text. Our results highlight the trade-offs between
interpretability and performance across different models, with BiLSTM offering
a balance of transparency and accuracy. Additionally, we assess the
generalizability of these models using cross-platform data from Reddit and
Twitter, uncovering key linguistic features associated with ADHD that could
contribute to more effective digital screening tools.
",2024-11-23T15:26:01Z,http://arxiv.org/abs/2411.15586v1,"D. Wiechmann, E. Kempa, E. Kerz, Y. Qiao"
The Transformer Network for the Traveling Salesman Problem,"  The Traveling Salesman Problem (TSP) is the most popular and most studied
combinatorial problem, starting with von Neumann in 1951. It has driven the
discovery of several optimization techniques such as cutting planes,
branch-and-bound, local search, Lagrangian relaxation, and simulated annealing.
The last five years have seen the emergence of promising techniques where
(graph) neural networks have been capable to learn new combinatorial
algorithms. The main question is whether deep learning can learn better
heuristics from data, i.e. replacing human-engineered heuristics? This is
appealing because developing algorithms to tackle efficiently NP-hard problems
may require years of research, and many industry problems are combinatorial by
nature. In this work, we propose to adapt the recent successful Transformer
architecture originally developed for natural language processing to the
combinatorial TSP. Training is done by reinforcement learning, hence without
TSP training solutions, and decoding uses beam search. We report improved
performances over recent learned heuristics with an optimal gap of 0.004% for
TSP50 and 0.39% for TSP100.
",2021-03-04T13:20:06Z,http://arxiv.org/abs/2103.03012v1,"Xavier Bresson, Thomas Laurent"
"Watch and learn -- a generalized approach for transferrable learning in
  deep neural networks via physical principles","  Transfer learning refers to the use of knowledge gained while solving a
machine learning task and applying it to the solution of a closely related
problem. Such an approach has enabled scientific breakthroughs in computer
vision and natural language processing where the weights learned in
state-of-the-art models can be used to initialize models for other tasks which
dramatically improve their performance and save computational time. Here we
demonstrate an unsupervised learning approach augmented with basic physical
principles that achieves fully transferrable learning for problems in
statistical physics across different physical regimes. By coupling a sequence
model based on a recurrent neural network to an extensive deep neural network,
we are able to learn the equilibrium probability distributions and
inter-particle interaction models of classical statistical mechanical systems.
Our approach, distribution-consistent learning, DCL, is a general strategy that
works for a variety of canonical statistical mechanical models (Ising and
Potts) as well as disordered (spin-glass) interaction potentials. Using data
collected from a single set of observation conditions, DCL successfully
extrapolates across all temperatures, thermodynamic phases, and can be applied
to different length-scales. This constitutes a fully transferrable
physics-based learning in a generalizable approach.
",2020-03-03T18:37:23Z,http://arxiv.org/abs/2003.02647v1,"Kyle Sprague, Juan Carrasquilla, Steve Whitelam, Isaac Tamblyn"
Entangled dual-comb spectroscopy,"Optical frequency combs have emerged as a cornerstone for a wide range of
areas, including spectroscopy, ranging, optical clocks, time and frequency
transfer, waveform synthesis, and communications. However, quantum mechanical
fluctuations of the optical carrier impose fundamental performance limits on
the precision of traditional classical laser frequency combs, particularly in
their use for interferometry and spectroscopy. Entanglement, as a
quintessential quantum resource, allows for surpassing the fundamental limits
of classical systems. Here, we introduce and experimentally demonstrate
entangled dual-comb spectroscopy (EDCS) that surmounts the fundamental limits
of classical DCS. EDCS builds on tailored entangled spectral structures of the
frequency combs, enabling simultaneous detection of all comb lines below the
standard quantum limit of classical DCS. Applying EDCS in gas detection, we
achieve a 2.6 dB enhancement in signal-to-noise ratio and a 1.7-fold reduction
in integration time over classical DCS, rendering EDCS particularly suited for
dynamic chemical and biological sensing, where fast, precise measurements
subject to power constraints are required. EDCS represents a new paradigm for
quantum frequency combs, underscoring their prospects in a plethora of
applications in precision metrology, spectroscopy, and timekeeping.",2024-12-27T18:57:59Z,http://arxiv.org/abs/2412.19800v1,"Abdulkarim Hariri, Shuai Liu, Haowei Shi, Quntao Zhuang, Xudong Fan, Zheshen Zhang"
"Generalized Grade-of-Membership Estimation for High-dimensional Locally
  Dependent Data","This work focuses on the mixed membership models for multivariate categorical
data widely used for analyzing survey responses and population genetics data.
These grade of membership (GoM) models offer rich modeling power but present
significant estimation challenges for high-dimensional polytomous data. Popular
existing approaches, such as Bayesian MCMC inference, are not scalable and lack
theoretical guarantees in high-dimensional settings. To address this, we first
observe that data from this model can be reformulated as a three-way
(quasi-)tensor, with many subjects responding to many items with varying
numbers of categories. We introduce a novel and simple approach that flattens
the three-way quasi-tensor into a ""fat"" matrix, and then perform a singular
value decomposition of it to estimate parameters by exploiting the singular
subspace geometry. Our fast spectral method can accommodate a broad range of
data distributions with arbitrarily locally dependent noise, which we formalize
as the generalized-GoM models. We establish finite-sample entrywise error
bounds for the generalized-GoM model parameters. This is supported by a new
sharp two-to-infinity singular subspace perturbation theory for locally
dependent and flexibly distributed noise, a contribution of independent
interest. Simulations and applications to data in political surveys, population
genetics, and single-cell sequencing demonstrate our method's superior
performance.",2024-12-27T18:51:15Z,http://arxiv.org/abs/2412.19796v1,"Ling Chen, Chengzhu Huang, Yuqi Gu"
"g-factor theory of Si/SiGe quantum dots: spin-valley and giant
  renormalization effects","Understanding the $g$-factor physics of Si/SiGe quantum dots is crucial for
realizing high-quality spin qubits. While previous work has explained some
aspects of $g$-factor physics in idealized geometries, the results do not
extend to general cases and they miss several important features. Here, we
construct a theory that gives $g$ in terms of readily computable matrix
elements, and can be applied to all Si/SiGe heterostructures of current
interest. As a concrete example, which currently has no $g$-factor
understanding, we study the so-called Wiggle Well structure, containing Ge
concentration oscillations inside the quantum well. Here we find a significant
renormalization of the $g$-factor compared to conventional Si/SiGe quantum
wells. We also uncover a giant $g$-factor suppression of order
$\mathcal{O}(1)$, which arises due to spin-valley coupling, and occurs at
locations of low valley splitting. Our work therefore opens up new avenues for
$g$-factor engineering in Si/SiGe quantum dots.",2024-12-27T18:50:38Z,http://arxiv.org/abs/2412.19795v1,"Benjamin D. Woods, Merritt P. Losert, Robert Joynt, Mark Friesen"
InfAlign: Inference-aware language model alignment,"Language model alignment has become a critical step in training modern
generative language models. The goal of alignment is to finetune a reference
model such that the win rate of a sample from the aligned model over a sample
from the reference model is high, subject to a KL divergence constraint. Today,
we are increasingly using inference-time algorithms (e.g., Best-of-N,
controlled decoding, tree search) to decode from language models rather than
standard sampling. However, the alignment objective does not capture such
inference-time decoding procedures. We show that the existing alignment
framework is sub-optimal in view of such inference-time methods. We then modify
the alignment objective and propose a framework for inference-aware alignment
(IAPO). We prove that for any inference-time decoding algorithm, the optimal
solution that optimizes the inference-time win rate of the aligned policy
against the reference policy is the solution to the typical RLHF problem with a
transformation of the reward. This motivates us to provide the KL-regularized
calibrate-and-transform RL (CTRL) algorithm to solve this problem, which
involves a reward calibration step and a KL-regularized reward maximization
step with a transformation of the calibrated reward. We particularize our study
to two important inference-time strategies: best-of-N sampling and best-of-N
jailbreaking, where N responses are sampled from the model and the one with the
highest or lowest reward is selected. We propose specific transformations for
these strategies and demonstrate that our framework offers significant
improvements over existing state-of-the-art methods for language model
alignment. Empirically, we outperform baselines that are designed without
taking inference-time decoding into consideration by 8-12% and 4-9% on
inference-time win rates over the Anthropic helpfulness and harmlessness dialog
benchmark datasets.",2024-12-27T18:45:36Z,http://arxiv.org/abs/2412.19792v1,"Ananth Balashankar, Ziteng Sun, Jonathan Berant, Jacob Eisenstein, Michael Collins, Adrian Hutter, Jong Lee, Chirag Nagpal, Flavien Prost, Aradhana Sinha, and Ananda Theertha Suresh, Ahmad Beirami"
"Data-driven analysis of anomalous transport and three-wave-coupling
  effects in E x B plasma discharges","Collisionless cross-field electron transport in an E x B configuration
relevant for electric propulsion is studied using data from a (z, {\theta})
full-PIC simulation. Higher-order spectral analysis shows that transport is
dominated by the in-phase interaction of the oscillations of the azimuthal
electric field and the electron density associated to the first electron
cyclotron drift instability (ECDI) mode. A secondary contribution emanates from
a lower-frequency mode, not predicted by linear ECDI theory, while higher modes
have a minor direct impact on transport. However, a bicoherence analysis
reveals that strong phase couplings exist among the ECDI modes, and a sparse
symbolic regression spectral model, based on the three-wave coupling equations,
suggests an inverse energy cascade as the most likely explanation, thus
suggesting that higher modes contribute indirectly to transport by quadratic
power transfer to the first mode. This work provides new insights into the
dynamics of anomalous plasma transport in E x B sources and the underlying
processes governing energy distribution across different scales, and supports
the validity of weak turbulence theory to examine their behavior.",2024-12-27T18:43:24Z,http://arxiv.org/abs/2412.19789v1,"Borja Bayón-Buján, Enrique Bello-Benítez, Jiewei Zhou, Mario Merino"
Can AI Help with Your Personal Finances?,"In recent years, Large Language Models (LLMs) have emerged as a
transformative development in artificial intelligence (AI), drawing significant
attention from industry and academia. Trained on vast datasets, these
sophisticated AI systems exhibit impressive natural language processing and
content generation capabilities. This paper explores the potential of LLMs to
address key challenges in personal finance, focusing on the United States. We
evaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini,
Anthropic's Claude, and Meta's Llama, to assess their effectiveness in
providing accurate financial advice on topics such as mortgages, taxes, loans,
and investments. Our findings show that while these models achieve an average
accuracy rate of approximately 70%, they also display notable limitations in
certain areas. Specifically, LLMs struggle to provide accurate responses for
complex financial queries, with performance varying significantly across
different topics. Despite these limitations, the analysis reveals notable
improvements in newer versions of these models, highlighting their growing
utility for individuals and financial advisors. As these AI systems continue to
evolve, their potential for advancing AI-driven applications in personal
finance becomes increasingly promising.",2024-12-27T18:25:27Z,http://arxiv.org/abs/2412.19784v1,"Oudom Hean, Utsha Saha, Binita Saha"
"Machine Learning for Sentiment Analysis of Imported Food in Trinidad and
  Tobago","This research investigates the performance of various machine learning
algorithms (CNN, LSTM, VADER, and RoBERTa) for sentiment analysis of Twitter
data related to imported food items in Trinidad and Tobago. The study addresses
three primary research questions: the comparative accuracy and efficiency of
the algorithms, the optimal configurations for each model, and the potential
applications of the optimized models in a live system for monitoring public
sentiment and its impact on the import bill. The dataset comprises tweets from
2018 to 2024, divided into imbalanced, balanced, and temporal subsets to assess
the impact of data balancing and the COVID-19 pandemic on sentiment trends. Ten
experiments were conducted to evaluate the models under various configurations.
Results indicated that VADER outperformed the other models in both multi-class
and binary sentiment classifications. The study highlights significant changes
in sentiment trends pre- and post-COVID-19, with implications for import
policies.",2024-12-27T18:25:08Z,http://arxiv.org/abs/2412.19781v1,"Cassandra Daniels, Koffka Khan"
Tensor Network Estimation of Distribution Algorithms,"Tensor networks are a tool first employed in the context of many-body quantum
physics that now have a wide range of uses across the computational sciences,
from numerical methods to machine learning. Methods integrating tensor networks
into evolutionary optimization algorithms have appeared in the recent
literature. In essence, these methods can be understood as replacing the
traditional crossover operation of a genetic algorithm with a tensor
network-based generative model. We investigate these methods from the point of
view that they are Estimation of Distribution Algorithms (EDAs). We find that
optimization performance of these methods is not related to the power of the
generative model in a straightforward way. Generative models that are better
(in the sense that they better model the distribution from which their training
data is drawn) do not necessarily result in better performance of the
optimization algorithm they form a part of. This raises the question of how
best to incorporate powerful generative models into optimization routines. In
light of this we find that adding an explicit mutation operator to the output
of the generative model often improves optimization performance.",2024-12-27T18:22:47Z,http://arxiv.org/abs/2412.19780v1,"John Gardiner, Javier Lopez-Piqueres"
"Symbolic Approximations to Ricci-flat Metrics Via Extrinsic Symmetries
  of Calabi-Yau Hypersurfaces","Ever since Yau's non-constructive existence proof of Ricci-flat metrics on
Calabi-Yau manifolds, finding their explicit construction remains a major
obstacle to development of both string theory and algebraic geometry. Recent
computational approaches employ machine learning to create novel neural
representations for approximating these metrics, offering high accuracy but
limited interpretability. In this paper, we analyse machine learning
approximations to flat metrics of Fermat Calabi-Yau n-folds and some of their
one-parameter deformations in three dimensions in order to discover their new
properties. We formalise cases in which the flat metric has more symmetries
than the underlying manifold, and prove that these symmetries imply that the
flat metric admits a surprisingly compact representation for certain choices of
complex structure moduli. We show that such symmetries uniquely determine the
flat metric on certain loci, for which we present an analytic form. We also
incorporate our theoretical results into neural networks to achieve
state-of-the-art reductions in Ricci curvature for multiple Calabi-Yau
manifolds. We conclude by distilling the ML models to obtain for the first time
closed form expressions for Kahler metrics with near-zero scalar curvature.",2024-12-27T18:19:26Z,http://arxiv.org/abs/2412.19778v1,"Viktor Mirjanić, Challenger Mishra"
"On the numerical solution of Lasserre relaxations of unconstrained
  binary quadratic optimization problem","The aim of this paper is to solve linear semidefinite programs arising from
higher-order Lasserre relaxations of unconstrained binary quadratic
optimization problems. For this we use an interior point method with a
preconditioned conjugate gradient method solving the linear systems. The
preconditioner utilizes the low-rank structure of the solution of the
relaxations. In order to fully exploit this, we need to re-write the moment
relaxations. To treat the arising linear equality constraints we use an
$\ell_1$-penalty approach within the interior-point solver. The efficiency of
this approach is demonstrated by numerical experiments with the MAXCUT and
other randomly generated problems and a comparison with a state-of-the-art
semidefinite solver and the ADMM method. We further propose a hybrid
ADMM-interior-point method that proves to be efficient for certain problem
classes. As a by-product, we observe that the second-order relaxation is often
high enough to deliver a globally optimal solution of the original problem.",2024-12-27T18:17:45Z,http://arxiv.org/abs/2412.19776v1,"Soodeh Habibi, Michal Kocvara, Michael Stingl"
"Analysis of Premature Death Rates in Texas Counties: The Impact of Air
  Quality, Socioeconomic Factors, and COPD Prevalence","Understanding factors contributing to premature mortality is critical for
public health planning. This study examines the relationships between premature
death rates and multiple risk factors across several Texas counties, utilizing
EPA air quality data, Census information, and county health records from recent
years. We analyze the impact of air quality (PM2.5 levels), socioeconomic
factors (median household income), and health conditions (COPD prevalence)
through statistical analysis and modeling techniques. Results reveal COPD
prevalence as a strong predictor of premature death rates, with higher
prevalence associated with a substantial increase in years of potential life
lost. While socioeconomic factors show a significant negative correlation, air
quality demonstrates more complex indirect relationships. These findings
emphasize the need for integrated public health interventions that prioritize
key health conditions while addressing underlying socioeconomic disparities.",2024-12-27T18:12:04Z,http://arxiv.org/abs/2412.19774v1,"Richard Rich, Ernesto Diaz"
"On the uplift of 4D wormholes in Braneworld models and their 5D
  structure","Recent developments in the consistent embedding of general 4D static and
spherically-symmetric spacetimes in arbitrary single-brane braneworld models
[Phys.Rev.D 109 (2024) 4, L041501] initiated the program of studying the bulk
structure of braneworld wormholes. In this article, adopting a completely
generic approach, we derive the general conditions that the metric functions of
any braneworld spacetime must satisfy to describe a wormhole structure in the
bulk. Particular emphasis is placed on clarifying the proper uplift of 4D
wormholes, expressed in terms of various radial coordinates on the brane, and
we demonstrate the important role of the circumferential radius metric function
for the embedding. Additionally, the flare-out conditions for braneworld
wormholes are presented for the first time and are found to differ from the
case of flat extra dimensions. To illustrate the method, we first perform the
uplift into the Randall-Sundrum II braneworld model for three well-known 4D
wormhole spacetimes; the effective braneworld wormhole solutions of
Casadio-Fabbri-Mazzacurati and Bronnikov-Kim, and the Simpson-Visser spacetime.
Subsequently, we study their bulk features by means of curvature invariants,
flare-out conditions, energy conditions and embedding diagrams. Our analysis
reveals that the assumption of a warped extra dimension has non-trivial
implications for the structure of 5D wormholes.",2024-12-27T18:12:03Z,http://arxiv.org/abs/2412.19773v1,"Thomas Pappas, Theodoros Nakas"
Direct estimates of irreversibility from time series,"The arrow of time can be quantified through the Kullback-Leibler divergence
($D_{KL}$) between the distributions of forward and reverse trajectories in a
system. Many approaches to estimate this rely on specific models, but the use
of incorrect models can introduce uncontrolled errors. Here, we describe a
model-free method that uses trajectory data directly to estimate the evidence
for irreversibility over finite windows of time. To do this we build on
previous work to identify and correct for errors that arise from limited sample
size. Importantly, our approach accurately recovers $D_{KL} = 0$ in systems
that adhere to detailed balance, and the correct nonzero $D_{KL}$ for data
generated by well understood models of nonequilibrium systems. We apply our
method to trajectories of neural activity in the retina as it responds to
naturalistic inputs, and find evidence of irreversibility in single neurons,
emphasizing the non-Markovian character of these data. These results open new
avenues for investigating how the brain represents the arrow of time.",2024-12-27T18:10:53Z,http://arxiv.org/abs/2412.19772v1,"Trevor GrandPre, Gianluca Teza, William Bialek"
Generative Video Propagation,"Large-scale video generation models have the inherent ability to
realistically model natural scenes. In this paper, we demonstrate that through
a careful design of a generative video propagation framework, various video
tasks can be addressed in a unified way by leveraging the generative power of
such models. Specifically, our framework, GenProp, encodes the original video
with a selective content encoder and propagates the changes made to the first
frame using an image-to-video generation model. We propose a data generation
scheme to cover multiple video tasks based on instance-level video segmentation
datasets. Our model is trained by incorporating a mask prediction decoder head
and optimizing a region-aware loss to aid the encoder to preserve the original
content while the generation model propagates the modified region. This novel
design opens up new possibilities: In editing scenarios, GenProp allows
substantial changes to an object's shape; for insertion, the inserted objects
can exhibit independent motion; for removal, GenProp effectively removes
effects like shadows and reflections from the whole video; for tracking,
GenProp is capable of tracking objects and their associated effects together.
Experiment results demonstrate the leading performance of our model in various
video tasks, and we further provide in-depth analyses of the proposed
framework.",2024-12-27T17:42:29Z,http://arxiv.org/abs/2412.19761v1,"Shaoteng Liu, Tianyu Wang, Jui-Hsien Wang, Qing Liu, Zhifei Zhang, Joon-Young Lee, Yijun Li, Bei Yu, Zhe Lin, Soo Ye Kim, Jiaya Jia"
"Enhancing Cognitive Diagnosis by Modeling Learner Cognitive Structure
  State","Cognitive diagnosis represents a fundamental research area within intelligent
education, with the objective of measuring the cognitive status of individuals.
Theoretically, an individual's cognitive state is essentially equivalent to
their cognitive structure state. Cognitive structure state comprises two key
components: knowledge state (KS) and knowledge structure state (KUS). The
knowledge state reflects the learner's mastery of individual concepts, a widely
studied focus within cognitive diagnosis. In contrast, the knowledge structure
state-representing the learner's understanding of the relationships between
concepts-remains inadequately modeled. A learner's cognitive structure is
essential for promoting meaningful learning and shaping academic performance.
Although various methods have been proposed, most focus on assessing KS and
fail to assess KUS. To bridge this gap, we propose an innovative and effective
framework-CSCD (Cognitive Structure State-based Cognitive Diagnosis)-which
introduces a novel framework to modeling learners' cognitive structures in
diagnostic assessments, thereby offering new insights into cognitive structure
modeling. Specifically, we employ an edge-feature-based graph attention network
to represent the learner's cognitive structure state, effectively integrating
KS and KUS. Extensive experiments conducted on real datasets demonstrate the
superior performance of this framework in terms of diagnostic accuracy and
interpretability.",2024-12-27T17:41:39Z,http://arxiv.org/abs/2412.19759v1,"Zhifu Chen, Hengnian Gu, Jin Peng Zhou, Dongdai Zhou"
"""Did my figure do justice to the answer?"" : Towards Multimodal Short
  Answer Grading with Feedback (MMSAF)","Personalized feedback plays a vital role in a student's learning process.
While existing systems are adept at providing feedback over MCQ-based
evaluation, this work focuses more on subjective and open-ended questions,
which is similar to the problem of Automatic Short Answer Grading (ASAG) with
feedback. Additionally, we introduce the Multimodal Short Answer grading with
Feedback (MMSAF) problem over the traditional ASAG feedback problem to address
the scenario where the student answer and reference answer might contain
images. Moreover, we introduce the MMSAF dataset with 2197 data points along
with an automated framework for generating such data sets. Our evaluations on
existing LLMs over this dataset achieved an overall accuracy of 55\% on Level
of Correctness labels, 75\% on Image Relevance labels and a score of 4.27 out
of 5 in correctness level of LLM generated feedback as rated by experts. As per
experts, Pixtral achieved a rating of above 4 out of all metrics, indicating
that it is more aligned to human judgement, and that it is the best solution
for assisting students.",2024-12-27T17:33:39Z,http://arxiv.org/abs/2412.19755v1,"Pritam Sil, Bhaskaran Raman, Pushpak Bhattacharyya"
Complement or substitute? How AI increases the demand for human skills,"The question of whether AI substitutes or complements human work is central
to debates on the future of work. This paper examines the impact of AI on skill
demand and compensation in the U.S. economy, analysing 12 million online job
vacancies from 2018 to 2023. It investigates internal effects (within-job
substitution and complementation) and external effects (across occupations,
industries, and regions). Our findings reveal a significant increase in demand
for AI-complementary skills, such as digital literacy, teamwork, and
resilience, alongside rising wage premiums for these skills in AI roles like
Data Scientist. Conversely, substitute skills, including customer service and
text review, have declined in both demand and value within AI-related
positions. Examining external effects, we find a notable rise in demand for
complementary skills in non-AI roles linked to the growth of AI-related jobs in
specific industries or regions. At the same time, there is a moderate decline
in non-AI roles requiring substitute skills. Overall, AI's complementary effect
is up to 50% larger than its substitution effect, resulting in net positive
demand for skills. These results, replicated for the UK and Australia,
highlight AI's transformative impact on workforce skill requirements. They
suggest reskilling efforts should prioritise not only technical AI skills but
also complementary skills like ethics and digital literacy.",2024-12-27T17:26:30Z,http://arxiv.org/abs/2412.19754v1,"Elina Mäkelä, Fabian Stephany"
"IMAGINE: An 8-to-1b 22nm FD-SOI Compute-In-Memory CNN Accelerator With
  an End-to-End Analog Charge-Based 0.15-8POPS/W Macro Featuring
  Distribution-Aware Data Reshaping","Charge-domain compute-in-memory (CIM) SRAMs have recently become an enticing
compromise between computing efficiency and accuracy to process sub-8b
convolutional neural networks (CNNs) at the edge. Yet, they commonly make use
of a fixed dot-product (DP) voltage swing, which leads to a loss in effective
ADC bits due to data-dependent clipping or truncation effects that waste
precious conversion energy and computing accuracy. To overcome this, we present
IMAGINE, a workload-adaptive 1-to-8b CIM-CNN accelerator in 22nm FD-SOI. It
introduces a 1152x256 end-to-end charge-based macro with a multi-bit DP based
on an input-serial, weight-parallel accumulation that avoids power-hungry DACs.
An adaptive swing is achieved by combining a channel-wise DP array split with a
linear in-ADC implementation of analog batch-normalization (ABN), obtaining a
distribution-aware data reshaping. Critical design constraints are relaxed by
including the post-silicon equivalent noise within a CIM-aware CNN training
framework. Measurement results showcase an 8b system-level energy efficiency of
40TOPS/W at 0.3/0.6V, with competitive accuracies on MNIST and CIFAR-10.
Moreover, the peak energy and area efficiencies of the 187kB/mm2 macro
respectively reach up to 0.15-8POPS/W and 2.6-154TOPS/mm2, scaling with the
8-to-1b computing precision. These results exceed previous charge-based designs
by 3-to-5x while being the first work to provide linear in-memory rescaling.",2024-12-27T17:18:15Z,http://arxiv.org/abs/2412.19750v1,"Adrian Kneip, Martin Lefebvre, Pol Maistriaux, David Bol"
"Enhancing Adversarial Robustness of Deep Neural Networks Through
  Supervised Contrastive Learning","Adversarial attacks exploit the vulnerabilities of convolutional neural
networks by introducing imperceptible perturbations that lead to
misclassifications, exposing weaknesses in feature representations and decision
boundaries. This paper presents a novel framework combining supervised
contrastive learning and margin-based contrastive loss to enhance adversarial
robustness. Supervised contrastive learning improves the structure of the
feature space by clustering embeddings of samples within the same class and
separating those from different classes. Margin-based contrastive loss,
inspired by support vector machines, enforces explicit constraints to create
robust decision boundaries with well-defined margins. Experiments on the
CIFAR-100 dataset with a ResNet-18 backbone demonstrate robustness performance
improvements in adversarial accuracy under Fast Gradient Sign Method attacks.",2024-12-27T17:14:52Z,http://arxiv.org/abs/2412.19747v1,"Longwei Wang, Navid Nayyem, Abdullah Rakin"
"AAM-SEALS: Developing Aerial-Aquatic Manipulators in SEa, Air, and Land
  Simulator","Current simulators lack the ability to accurately model integrated
environments that encompass sea, air, and land. To address this gap, we
introduce Aerial-Aquatic Manipulators (AAMs) in SEa, Air, and Land Simulator
(SEALS), a comprehensive and photorealistic simulator designed for AAMs to
operate and learn in these diverse environments. The development of AAM-SEALS
tackles several significant challenges, including the creation of integrated
controllers for flying, swimming, and manipulation, and the high-fidelity
simulation of aerial dynamics and hydrodynamics leveraging particle physics.
Our evaluation demonstrates smooth operation and photorealistic transitions
across air, water, and their interfaces. We quantitatively validate the
fidelity of particle-based hydrodynamics by comparing position-tracking errors
across real-world and simulated systems. AAM-SEALS promises to benefit a broad
range of robotics communities, including robot learning, aerial robotics,
underwater robotics, mobile manipulation, and robotic simulators. We will
open-source our code and data to foster the advancement of research in these
fields. Please access our project website at: https:
//aam-seals.github.io/aam-seals-v1/",2024-12-27T17:13:14Z,http://arxiv.org/abs/2412.19744v1,"William Wang Yang, Karthikeya Kona, Yashveer Jain, Abhinav Bhamidipati, Tomer Atzili, Xiaomin Lin, Yantian Zha"
"Physics of 2D magnets and magnetic thin films: Surface structure and
  surface phase transition, criticality and skyrmions","Recently, there is an increasing renewed interest in 2D magnetism such as Van
der Waals magnets. The physics of 2D magnetism and ultra-thin magnetic films
has a long history. This chapter is a review devoted to some fundamental
theoretical properties of 2D magnets and and magnetic thin films including
frustrated systems and topological spin textures. These properties allow to
understand macroscopic behaviors experimentally observed in thin films and
superlattices where the surface and the interface play a crucial role. The
chapter begins with a review on 2D magnets, their spin structures and phase
transitions. Next, the case of thin films is considered. The theory of surface
spin waves is discussed in various situations with and without surface
reconstruction of spin ordering. Various interactions are taken into account:
surface interaction different from the bulk one, competing interactions,
Dzyaloshinskii-Moriya interaction. Surface phase transitions are shown in some
particularly striking cases. Finally, some cases of topological spin textures
called ""skyrmions"" are reviewed. All the results shown in this chapter have
been published in various research papers cited in the text. Therefore, we will
discuss some important results but avoid to enter complicated methods. Instead,
the reader is referred to original papers for detailed demonstrations.",2024-12-27T17:06:31Z,http://arxiv.org/abs/2412.19741v1,Hung T. Diep
Hard Photon Triggered Jets in $p$-$p$ and $A$-$A$ Collisions,"An investigation of high transverse momentum (high-$p_T$) photon triggered
jets in proton-proton ($p$-$p$) and ion-ion ($A$-$A$) collisions at
$\sqrt{s_{NN}} = 0.2$ and $5.02~\mathrm{TeV}$ is carried out, using the
multistage description of in-medium jet evolution. Monte Carlo simulations of
hard scattering and energy loss in heavy-ion collisions are performed using
parameters tuned in a previous study of the nuclear modification factor
($R_{AA}$) for inclusive jets and high-$p_T$ hadrons. We obtain a good
reproduction of the experimental data for photon triggered jet $R_{AA}$, as
measured by the ATLAS detector, the distribution of the ratio of jet to photon
$p_T$ ($X_{\rm J \gamma}$), measured by both CMS and ATLAS, and the photon-jet
azimuthal correlation as measured by CMS. We obtain a moderate description of
the photon triggered jet $I_{AA}$, as measured by STAR. A noticeable
improvement in the comparison is observed when one goes beyond prompt photons
and includes bremsstrahlung and decay photons, revealing their significance in
certain kinematic regions, particularly at $X_{J\gamma} &gt; 1$. Moreover,
azimuthal angle correlations demonstrate a notable impact of non-prompt photons
on the distribution, emphasizing their role in accurately describing
experimental results. This work highlights the success of the multistage model
of jet modification to straightforwardly predict (this set of) photon triggered
jet observables. This comparison, along with the role played by non-prompt
photons, has important consequences on the inclusion of such observables in a
future Bayesian analysis.",2024-12-27T16:56:15Z,http://arxiv.org/abs/2412.19738v1,"C. Sirimanna, Y. Tachibana, A. Majumder, A. Angerami, R. Arora, S. A. Bass, Y. Chen, R. Datta, L. Du, R. Ehlers, H. Elfner, R. J. Fries, C. Gale, Y. He, B. V. Jacak, P. M. Jacobs, S. Jeon, Y. Ji, F. Jonas, L. Kasper, M. Kordell II, A. Kumar, R. Kunnawalkam-Elayavalli, J. Latessa, Y. -J. Lee, R. Lemmon, M. Luzum, S. Mak, A. Mankolli, C. Martin, H. Mehryar, T. Mengel, C. Nattrass, J. Norman, C. Parker, J. -F. Paquet, J. H. Putschke, H. Roch, G. Roland, B. Schenke, L. Schwiebert, A. Sengupta, C. Shen, M. Singh, D. Soeder, R. A. Soltz, I. Soudi, J. Velkovska, G. Vujanovic, X. -N. Wang, X. Wu, W. Zhao"
"Adaptive Context-Aware Multi-Path Transmission Control for VR/AR
  Content: A Deep Reinforcement Learning Approach","This paper introduces the Adaptive Context-Aware Multi-Path Transmission
Control Protocol (ACMPTCP), an efficient approach designed to optimize the
performance of Multi-Path Transmission Control Protocol (MPTCP) for
data-intensive applications such as augmented and virtual reality (AR/VR)
streaming. ACMPTCP addresses the limitations of conventional MPTCP by
leveraging deep reinforcement learning (DRL) for agile end-to-end path
management and optimal bandwidth allocation, facilitating path realignment
across diverse network environments.",2024-12-27T16:56:12Z,http://arxiv.org/abs/2412.19737v1,"Shakil Ahmed, Saifur Rahman Sabuj, Ashfaq Khokhar"
"A General Framework of Brain Region Detection And Genetic Variants
  Selection in Imaging Genetics","Imaging genetics is a growing field that employs structural or functional
neuroimaging techniques to study individuals with genetic risk variants
potentially linked to specific illnesses. This area presents considerable
challenges to statisticians due to the heterogeneous information and different
data forms it involves. In addition, both imaging and genetic data are
typically high-dimensional, creating a ""big data squared"" problem. Moreover,
brain imaging data contains extensive spatial information. Simply vectorizing
tensor images and treating voxels as independent features can lead to
computational issues and disregard spatial structure. This paper presents a
novel statistical method for imaging genetics modeling while addressing all
these challenges. We explore a Canonical Correlation Analysis based linear
model for the joint modeling of brain imaging, genetic information, and
clinical phenotype, enabling the simultaneous detection of significant brain
regions and selection of important genetic variants associated with the
phenotype outcome. Scalable algorithms are developed to tackle the ""big data
squared"" issue. We apply the proposed method to explore the reaction speed, an
indicator of cognitive functions, and its associations with brain MRI and
genetic factors using the UK Biobank database. Our study reveals a notable
connection between the caudate nucleus region of brain and specific significant
SNPs, along with their respective regulated genes, and the reaction speed.",2024-12-27T16:54:11Z,http://arxiv.org/abs/2412.19735v1,"Siqiang Su, Zhenghao Li, Long Feng, Ting Li"
"Dynamics, data and reconstruction","Data-driven learning is prevalent in many fields of science, mathematics and
engineering. The goal of data-driven learning of dynamical systems is to
interpret timeseries as a continuous observation of an underlying dynamical
system. This task is not well-posed for a variety of reasons. A dynamical
system may have multiple sub-systems co-existing within it. The nature of the
dataset depends on the portion of the phase space being viewed, and may thus my
confined to a sub-system. Secondly these sub-systems may be topologically
inter-weaved, so may be inseparable computationally. Thirdly, two timeseries
sampled separately from different dynamical systems may be close or even
indistinguishable. So there is no unqiue source for the timeseries. We show how
these ambiguities are circumvented if one considers dynamical systems and
measurement maps collectively. This is made possible in a category theoretical
framework, in which reconstruction is unique up to equivalences. We introduce
two categories of observed dynamical systems and timeseries-data. These are
related to the well known category of dynamical systems via functors. This
enables a functorial interpretation of the task of reconstruction as well.",2024-12-27T16:49:52Z,http://arxiv.org/abs/2412.19734v1,"Suddhasattwa Das, Tomoharu Suda"
"Generative Pretrained Embedding and Hierarchical Irregular Time Series
  Representation for Daily Living Activity Recognition","Within the evolving landscape of smart homes, the precise recognition of
daily living activities using ambient sensor data stands paramount. This paper
not only aims to bolster existing algorithms by evaluating two distinct
pretrained embeddings suited for ambient sensor activations but also introduces
a novel hierarchical architecture. We delve into an architecture anchored on
Transformer Decoder-based pre-trained embeddings, reminiscent of the GPT
design, and contrast it with the previously established state-of-the-art (SOTA)
ELMo embeddings for ambient sensors. Our proposed hierarchical structure
leverages the strengths of each pre-trained embedding, enabling the discernment
of activity dependencies and sequence order, thereby enhancing classification
precision. To further refine recognition, we incorporate into our proposed
architecture an hour-of-the-day embedding. Empirical evaluations underscore the
preeminence of the Transformer Decoder embedding in classification endeavors.
Additionally, our innovative hierarchical design significantly bolsters the
efficacy of both pre-trained embeddings, notably in capturing inter-activity
nuances. The integration of temporal aspects subtly but distinctively augments
classification, especially for time-sensitive activities. In conclusion, our
GPT-inspired hierarchical approach, infused with temporal insights, outshines
the SOTA ELMo benchmark.",2024-12-27T16:43:52Z,http://arxiv.org/abs/2412.19732v1,"Damien Bouchabou, Sao Mai Nguyen"
"Learning to Forget: Bayesian Time Series Forecasting using Recurrent
  Sparse Spectrum Signature Gaussian Processes","The signature kernel is a kernel between time series of arbitrary length and
comes with strong theoretical guarantees from stochastic analysis. It has found
applications in machine learning such as covariance functions for Gaussian
processes. A strength of the underlying signature features is that they provide
a structured global description of a time series. However, this property can
quickly become a curse when local information is essential and forgetting is
required; so far this has only been addressed with ad-hoc methods such as
slicing the time series into subsegments. To overcome this, we propose a
principled, data-driven approach by introducing a novel forgetting mechanism
for signatures. This allows the model to dynamically adapt its context length
to focus on more recent information. To achieve this, we revisit the recently
introduced Random Fourier Signature Features, and develop Random Fourier
Decayed Signature Features (RFDSF) with Gaussian processes (GPs). This results
in a Bayesian time series forecasting algorithm with variational inference,
that offers a scalable probabilistic algorithm that processes and transforms a
time series into a joint predictive distribution over time steps in one pass
using recurrence. For example, processing a sequence of length $10^4$ steps in
$\approx 10^{-2}$ seconds and in $&lt; 1\text{GB}$ of GPU memory. We demonstrate
that it outperforms other GP-based alternatives and competes with
state-of-the-art probabilistic time series forecasting algorithms.",2024-12-27T16:31:09Z,http://arxiv.org/abs/2412.19727v1,"Csaba Tóth, Masaki Adachi, Michael A. Osborne, Harald Oberhauser"
EEG-Reptile: An Automatized Reptile-Based Meta-Learning Library for BCIs,"Meta-learning, i.e., ""learning to learn"", is a promising approach to enable
efficient BCI classifier training with limited amounts of data. It can
effectively use collections of in some way similar classification tasks, with
rapid adaptation to new tasks where only minimal data are available. However,
applying meta-learning to existing classifiers and BCI tasks requires
significant effort. To address this issue, we propose EEG-Reptile, an automated
library that leverages meta-learning to improve classification accuracy of
neural networks in BCIs and other EEG-based applications. It utilizes the
Reptile meta-learning algorithm to adapt neural network classifiers of EEG data
to the inter-subject domain, allowing for more efficient fine-tuning for a new
subject on a small amount of data. The proposed library incorporates an
automated hyperparameter tuning module, a data management pipeline, and an
implementation of the Reptile meta-learning algorithm. EEG-Reptile automation
level allows using it without deep understanding of meta-learning. We
demonstrate the effectiveness of EEG-Reptile on two benchmark datasets (BCI IV
2a, Lee2019 MI) and three neural network architectures (EEGNet, FBCNet,
EEG-Inception). Our library achieved improvement in both zero-shot and few-shot
learning scenarios compared to traditional transfer learning approaches.",2024-12-27T16:24:31Z,http://arxiv.org/abs/2412.19725v1,"Daniil A. Berdyshev, Artem M. Grachev, Sergei L. Shishkin, Bogdan L. Kozyrskiy"
"Exploring low-rank structure for an inverse scattering problem with
  far-field data","The inverse scattering problem exhibits an inherent low-rank structure due to
its ill-posed nature; however developing low-rank structures for the inverse
scattering problem remains challenging. In this work, we introduce a novel
low-rank structure tailored for solving the inverse scattering problem. The
particular low-rank structure is given by the generalized prolate spheroidal
wave functions, computed stably and accurately via a Sturm-Liouville problem.
We first process the far-field data to obtain a post-processed data set within
a disk domain. Subsequently, the post-processed data are projected onto a
low-rank space given by the low-rank structure. The unknown is approximately
solved in this low-rank space, by dropping higher-order terms. The low-rank
structure leads to a H\""{o}lder-logarithmic type stability estimate for
arbitrary unknown functions, and a Lipschitz stability estimate for unknowns
belonging to a finite dimensional low-rank space. Various numerical experiments
are conducted to validate its performance, encompassing assessments of
resolution capability, robustness against randomly added noise and modeling
errors, and demonstration of increasing stability.",2024-12-27T16:24:20Z,http://arxiv.org/abs/2412.19724v1,"Yuyuan Zhou, Lorenzo Audibert, Shixu Meng, Bo Zhang"
"OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse
  Task Synthesis","Graphical User Interface (GUI) agents powered by Vision-Language Models
(VLMs) have demonstrated human-like computer control capability. Despite their
utility in advancing digital automation, a critical bottleneck persists:
collecting high-quality trajectory data for training. Common practices for
collecting such data rely on human supervision or synthetic data generation
through executing pre-defined tasks, which are either resource-intensive or
unable to guarantee data quality. Moreover, these methods suffer from limited
data diversity and significant gaps between synthetic data and real-world
environments. To address these challenges, we propose OS-Genesis, a novel GUI
data synthesis pipeline that reverses the conventional trajectory collection
process. Instead of relying on pre-defined tasks, OS-Genesis enables agents
first to perceive environments and perform step-wise interactions, then
retrospectively derive high-quality tasks to enable trajectory-level
exploration. A trajectory reward model is then employed to ensure the quality
of the generated trajectories. We demonstrate that training GUI agents with
OS-Genesis significantly improves their performance on highly challenging
online benchmarks. In-depth analysis further validates OS-Genesis's efficiency
and its superior data quality and diversity compared to existing synthesis
methods. Our codes, data, and checkpoints are available at
\href{https://qiushisun.github.io/OS-Genesis-Home/}{OS-Genesis Homepage}.",2024-12-27T16:21:58Z,http://arxiv.org/abs/2412.19723v1,"Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, Ben Kao, Guohao Li, Junxian He, Yu Qiao, Zhiyong Wu"
"Quantum correlations in a gravitational collapse simulation with
  SpheriCo.jl","We report on work using a newly developed code, SpheriCo.jl, that computes
the gravitational collapse of a spherical scalar field, where the scalar can be
either a classical field, or a quantum field operator. By utilising
summation-by-parts methods for the numerical derivatives we are able to
simulate the collapse longer than was possible previously due to enhanced
numerical stability. We present a suite of tests for the code that tests its
accuracy and stability, both for the classical and quantum fields. We are able
to observe critical behavior of gravitational collapse for the classical setup,
in agreement with expected results. The code is also used to compute two-point
correlation functions, with results that hint at a non-trivial correlation
across the horizon of Hawking quanta.",2024-12-27T16:20:27Z,http://arxiv.org/abs/2412.19722v1,"Benjamin Berczi, Magdalena Eriksson, Thanasis Giannakopoulos, Paul M. Saffin"
Sharpening Neural Implicit Functions with Frequency Consolidation Priors,"Signed Distance Functions (SDFs) are vital implicit representations to
represent high fidelity 3D surfaces. Current methods mainly leverage a neural
network to learn an SDF from various supervisions including signed distances,
3D point clouds, or multi-view images. However, due to various reasons
including the bias of neural network on low frequency content, 3D unaware
sampling, sparsity in point clouds, or low resolutions of images, neural
implicit representations still struggle to represent geometries with high
frequency components like sharp structures, especially for the ones learned
from images or point clouds. To overcome this challenge, we introduce a method
to sharpen a low frequency SDF observation by recovering its high frequency
components, pursuing a sharper and more complete surface. Our key idea is to
learn a mapping from a low frequency observation to a full frequency coverage
in a data-driven manner, leading to a prior knowledge of shape consolidation in
the frequency domain, dubbed frequency consolidation priors. To better
generalize a learned prior to unseen shapes, we introduce to represent
frequency components as embeddings and disentangle the embedding of the low
frequency component from the embedding of the full frequency component. This
disentanglement allows the prior to generalize on an unseen low frequency
observation by simply recovering its full frequency embedding through a
test-time self-reconstruction. Our evaluations under widely used benchmarks or
real scenes show that our method can recover high frequency component and
produce more accurate surfaces than the latest methods. The code, data, and
pre-trained models are available at \url{https://github.com/chenchao15/FCP}.",2024-12-27T16:18:46Z,http://arxiv.org/abs/2412.19720v1,"Chao Chen, Yu-Shen Liu, Zhizhong Han"
"Trading Off Energy Storage and Payload -- An Analytical Model for
  Freight Train Configuration","To support planning of alternative fuel technology (e.g., battery-electric
locomotives) deployment for decarbonizing non-electrified freight rail, we
develop a convex optimization formulation with a closed-form solution to
determine the optimal number of energy storage tender cars in a train. The
formulation shares a similar structure to an Economic Order Quantity (EOQ)
model. For given market characteristics, cost forecasts, and technology
parameters, our model captures the trade-offs between inventory carrying costs
associated with trip times (including delays due to charging/refueling) and
ordering costs associated with train dispatch and operation (energy, amortized
equipment, and labor costs). To illustrate the framework, we find the optimal
number of battery-electric energy tender cars in 22,501 freight markets
(origin-destination pairs and commodities) for U.S. Class I railroads. The
results display heterogeneity in optimal configurations with lighter, yet more
time-sensitive shipments (e.g., intermodal) utilizing more battery tender cars.
For heavier commodities (e.g., coal) with lower holding costs, single battery
tender car configurations are generally optimal. The results also show that the
optimal train configurations are sensitive to delays associated with recharging
or swapping tender cars.",2024-12-27T16:18:35Z,http://arxiv.org/abs/2412.19719v1,"Max T. M. Ng, Adrian Hernandez, Pablo L. Durango-Cohen, Hani S. Mahmassani"
"Text2Insight: Transform natural language text into insights seamlessly
  using multi-model architecture","The growing demand for dynamic, user-centric data analysis and visualization
is evident across domains like healthcare, finance, and research. Traditional
visualization tools often fail to meet individual user needs due to their
static and predefined nature. To address this gap, Text2Insight is introduced
as an innovative solution that delivers customized data analysis and
visualizations based on user-defined natural language requirements. Leveraging
a multi-model architecture, Text2Insight transforms user inputs into actionable
insights and dynamic visualizations.
  The methodology begins with analyzing the input dataset to extract structural
details such as columns and values. A pre-trained Llama3 model converts the
user's natural language query into an SQL query, which is further refined using
a Named Entity Recognition (NER) model for accuracy. A chart predictor
determines the most suitable visualization type, while the Llama3 model
generates insights based on the SQL query's results. The output is a
user-friendly and visually informative chart. To enhance analysis capabilities,
the system integrates a question-answering model and a predictive model using
the BERT framework. These models provide insights into historical data and
predict future trends.
  Performance evaluation of Text2Insight demonstrates its effectiveness,
achieving high accuracy (99%), precision (100%), recall (99%), and F1-score
(99%), with a BLEU score of 0.5. The question-answering model attained an
accuracy of 89% and the predictive model achieved 70% accuracy. These results
validate Text2Insight as a robust and viable solution for transforming natural
language text into dynamic, user-specific data analysis and visualizations.",2024-12-27T16:17:22Z,http://arxiv.org/abs/2412.19718v1,Pradeep Sain
Low-Regularity Global solution for fractional NLS in modulation spaces,"We establish global well-posedness for the mass sub-critical nonlinear
fractional Schr\""odinger equation
  $$iu_t + (-\Delta)^\frac{\beta}{2} u \pm (|u|^{\alpha}u)=0$$
  with radial initial data in modulation spaces $M^{p,\frac{p}{p-1}}(\mathbb
R^n)$ with $2&lt;p$ sufficiently close to $2.$ Our order of dispersion $\beta$
lies in $(2n/ (2n-1), 2)$ for $n \geq 2$.",2024-12-27T16:15:10Z,http://arxiv.org/abs/2412.19714v1,"Divyang G. Bhimani, Diksha Dhingra, Vijay Kumar Sohani"
"ProKAN: Progressive Stacking of Kolmogorov-Arnold Networks for Efficient
  Liver Segmentation","The growing need for accurate and efficient 3D identification of tumors,
particularly in liver segmentation, has spurred considerable research into deep
learning models. While many existing architectures offer strong performance,
they often face challenges such as overfitting and excessive computational
costs. An adjustable and flexible architecture that strikes a balance between
time efficiency and model complexity remains an unmet requirement. In this
paper, we introduce proKAN, a progressive stacking methodology for
Kolmogorov-Arnold Networks (KANs) designed to address these challenges. Unlike
traditional architectures, proKAN dynamically adjusts its complexity by
progressively adding KAN blocks during training, based on overfitting behavior.
This approach allows the network to stop growing when overfitting is detected,
preventing unnecessary computational overhead while maintaining high accuracy.
Additionally, proKAN utilizes KAN's learnable activation functions modeled
through B-splines, which provide enhanced flexibility in learning complex
relationships in 3D medical data. Our proposed architecture achieves
state-of-the-art performance in liver segmentation tasks, outperforming
standard Multi-Layer Perceptrons (MLPs) and fixed KAN architectures. The
dynamic nature of proKAN ensures efficient training times and high accuracy
without the risk of overfitting. Furthermore, proKAN provides better
interpretability by allowing insight into the decision-making process through
its learnable coefficients. The experimental results demonstrate a significant
improvement in accuracy, Dice score, and time efficiency, making proKAN a
compelling solution for 3D medical image segmentation tasks.",2024-12-27T16:14:06Z,http://arxiv.org/abs/2412.19713v1,"Bhavesh Gyanchandani, Aditya Oza, Abhinav Roy"
"Causal machine learning for heterogeneous treatment effects in the
  presence of missing outcome data","When estimating heterogeneous treatment effects, missing outcome data can
complicate treatment effect estimation, causing certain subgroups of the
population to be poorly represented. In this work, we discuss this commonly
overlooked problem and consider the impact that missing at random (MAR) outcome
data has on causal machine learning estimators for the conditional average
treatment effect (CATE). We then propose two de-biased machine learning
estimators for the CATE, the mDR-learner and mEP-learner, which address the
issue of under-representation by integrating inverse probability of censoring
weights into the DR-learner and EP-learner respectively. We show that under
reasonable conditions, these estimators are oracle efficient, and illustrate
their favorable performance through simulated data settings, comparing them to
existing CATE estimators, including comparison to estimators which use common
missing data techniques. Guidance on the implementation of these estimators is
provided and we present an example of their application using the ACTG175
trial, exploring treatment effect heterogeneity when comparing Zidovudine
mono-therapy against alternative antiretroviral therapies among HIV-1-infected
individuals.",2024-12-27T16:10:03Z,http://arxiv.org/abs/2412.19711v1,"Matthew Pryce, Karla Diaz-Ordaz, Ruth H. Keogh, Stijn Vansteelandt"
"Schwinger pair production in spacetime fields: Moiré patterns,
  Aharonov-Bohm phases and Sturm-Liouville eigenvalues","We use a worldline-instanton formalism to study the momentum spectrum of
Schwinger pair production in spacetime fields with multiple stationary points.
We show that the interference structure changes fundamentally when going from
purely time-dependent to space-time-dependent fields. For example, it was known
that two time-dependent pulses give interference if they are anti-parallel,
i.e. $E_z(t)-E_z(t-\Delta t)$, but here we show that two spacetime pulses will
typically give interference if they instead are parallel, i.e.
$E_z(t,z)+E_z(t-\Delta t,z-\Delta z)$. We take into account the fact that the
momenta of the electron, $p_z$, and of the positron, $p'_z$, are independent
for $E_z(t,z)$ (it would be $p_z+p'_z=0$ for $E(t)$), and find a type of fields
which give moir\'e patterns in the $p_z-p'_z$ plane. Depending on the
separation of two pulses, we also find an Aharonov-Bohm phase. We also study
complex momentum saddle points in order to obtain the integrated probability
from the spectrum. Finally, we calculate an asymptotic expansion for the
eigenvalues of the Sturm-Liouville equation that corresponds to the
saddle-point approximation of the worldline path integral, use that expansion
to compute the product of eigenvalues, and compare with the result obtained
with the Gelfand-Yaglom method.",2024-12-27T16:03:02Z,http://arxiv.org/abs/2412.19709v1,"Gianluca Degli Esposti, Greger Torgrimsson"
"All Finite (Anti)Hermitian Irreducible Representations of the de Sitter
  and Anti-de Sitter Lie Algebras and Their Lorentz Structure","Because of the importance of unitarity in quantum physics, work on the
representations of the de Sitter group has focussed on the unitary case, which
necessarily means infinite dimensional matrices for this non-compact group.
Here we address the finite dimensional representations resulting from the
requirement that the Lie algebra generators are either Hermitian or
anti-Hermitian. The complete classification of all such irreducible
representations is found and their matrix elements specified. These irreducible
representations (irreps) are based on backbones defined as the homogeneous
Lorentz sub-algebra and consisting of direct sums of the finite irreps of the
homogeneous Lorentz algebra (HLA). Only two types of such backbones arise (see
5.1a,b herein). Consequently, only certain dimensions of representation are
possible, namely 4, 5, 10, 14, 20, 30, 35, 55, 56, 91, etc or generally either
1/6 N(N+1)(N+2) or 1/6 N(N+1)(2N+1) where N=2,3,4,etc is the number of HLA
irreps in the backbone (minimum 2). The two Casimir invariants can be specified
in terms of a single integral or half-integral parameter, p. For irreps based
on (5.1a), -C1=p(p+1)-2 and C2=0 with p taking values 2,3,4,etc. For irreps
based on (5.1b), -C1=2(p^2-1) and -C2= p^2 (p^2-1) with p taking values
3/2,2,5/2,3,etc. These correspond to the same expressions found for the unitary
representations, -C1=p(p+1)+(q+1)(q-2) and -C2=p(p+1)q(q-1) with q=0 and q=p
respectively for the two types of irrep. There is thus a far more restricted
set of finite irreps with Hermitian or anti-Hermitian generators than for the
discrete infinite dimensional unitary irreps. The corresponding irreps of the
anti-de Sitter group follow immediately from the replacement of the 4-momentum
operators from V to iV.",2024-12-27T16:02:41Z,http://arxiv.org/abs/2412.19708v1,Richard A. W. Bradford
Toward Adaptive Reasoning in Large Language Models with Thought Rollback,"Large language models (LLMs) have been routinely used to solve various tasks
using step-by-step reasoning. However, the structure of intermediate reasoning
steps, or thoughts, is rigid and unidirectional, such as chains, trees, or
acyclic-directed graphs. Consequently, the resulting inflexible and
forward-only reasoning may not address challenging tasks and fail when the LLM
frequently gives false responses, i.e., ``hallucinations''. This paper proposes
a new reasoning framework, called Thought Rollback (TR), allowing LLMs to
adaptively build thought structure while maintaining effective reasoning toward
problem-solving under ``hallucinations''. The core mechanism of TR is rolling
back thoughts, which allows LLMs to perform error analysis on thoughts, and
thus roll back to any previously mistaken thought for revision. Subsequently,
by including such trial-and-error in the prompt to guide the LLM, each rollback
leads to one more reliable reasoning path. Therefore, starting with a simple
prompt without human annotations, LLM with TR adaptively and gradually explores
thoughts for a correct solution. Comprehensive experiments on mathematical
problems and multi-task reasoning demonstrate the state-of-the-art performance
of TR in terms of problem-solving rate and interaction cost. For instance, the
solving rate of GPT-4 with TR outperforms the current best by $9\%$ on the MATH
dataset.",2024-12-27T16:02:34Z,http://arxiv.org/abs/2412.19707v1,"Sijia Chen, Baochun Li"
"Noise Sensitivity of the Semidefinite Programs for Direct Data-Driven
  LQR","In this paper, we study the noise sensitivity of the semidefinite program
(SDP) proposed for direct data-driven infinite-horizon linear quadratic
regulator (LQR) problem for discrete-time linear time-invariant systems. While
this SDP is shown to find the true LQR controller in the noise-free setting, we
show that it leads to a trivial solution with zero gain matrices when data is
corrupted by noise, even when the noise is arbitrarily small. We then study a
variant of the SDP that includes a robustness promoting regularization term and
prove that regularization does not fully eliminate the sensitivity issue. In
particular, the solution of the regularized SDP converges in probability also
to a trivial solution.",2024-12-27T15:59:42Z,http://arxiv.org/abs/2412.19705v1,"Xiong Zeng, Laurent Bako, Necmiye Ozay"
"Reparameterization Invariance of FRW Model: Supervariable and BRST
  Approaches","We perform the Becchi-Rouet-Stora-Tyutin (BRST) quantization of a (0 +
1)-dimensional non-interacting cosmological Friedmann-Robertson-Walker (FRW)
model. This quantization leverages the classical infinitesimal and continuous
reparameterization symmetry transformations of the system. To derive the
nilpotent reparameterization invariant (anti-)BRST symmetry transformations for
the scale factor and corresponding momentum variables of the FRW model, we
employ the modified Bonora-Tonin supervariable approach (MBTSA) to BRST
formalism. Through this approach, we also establish the (anti-)BRST invariant
Curci-Ferrari (CF)-type restriction for this cosmological reparameterization
invariant model. Further, we obtain the nilpotent (anti-)BRST symmetry
transformations for other variables within the model using the (anti-)chiral
supervariable approach (ACSA) to BRST formalism. Within the framework of ACSA,
the CF-type restriction is demonstrated through two key aspects: (i) the
invariance of the coupled Lagrangians under symmetry transformations, and (ii)
the absolute anticommutativity of the conserved (anti-)BRST charges. Notably,
applying the MBTSA to a physical cosmological system, specifically a
one-dimensional one, constitutes a novel contribution to this work.
Additionally, in the application of ACSA, we restrict our analysis to
(anti-)chiral super expansions of supervariables, leading to the unique
observation of the absolute anticommutativity of the (anti-)BRST charges.
Moreover, we highlight that the CF-type restriction demonstrates a universal
nature, remaining consistent across any reparameterization invariant models in
D-dimensional spacetime.",2024-12-27T15:58:29Z,http://arxiv.org/abs/2412.19704v1,"B. Chauhan, R. Tripathi"
"Numerical inverse scattering transform for the defocusing nonlinear
  Schrödinger equation with box-type initial conditions on a nonzero
  background","We present a method to solve numerically the Cauchy problem for the
defocusing nonlinear Schr\""{o}dinger (NLS) equation with a box-type initial
condition (IC) having a nontrivial background of amplitude $q_o&gt;0$ as $x\to \pm
\infty$ by implementing numerically the associated Inverse Scattering Transform
(IST). The Riemann--Hilbert problem associated to the inverse transform is
solved numerically by means of appropriate contour deformations in the complex
plane following the numerical implementation of the Deift-Zhou nonlinear
steepest descent method. In this work, the box parameters are chosen so that
there is no discrete spectrum (i.e., no solitons). In particular, the numerical
method is demonstrated to be accurate within the two asymptotic regimes
corresponding to two different regions of the $(x,t)$-plane depending on
whether $|x/(2t)| &lt; q_o$ or $|x/(2t)| &gt; q_o$, as $t \to \infty$.",2024-12-27T15:57:44Z,http://arxiv.org/abs/2412.19703v1,"Aikaterini Gkogkou, Barbara Prinari, Thomas Trogdon"
"Search for the double Dalitz decays $η/η' \to e^+e^-μ^+μ^-$
  and $η' \to μ^+μ^-μ^+μ^-$","Using a data sample of $(10087 \pm 44) \times {10^{6}}$ $J/{\psi}$ events
collected with the BESIII detector, we search for the decays $\eta/\eta'\to
e^+e^-\mu^+\mu^-$ and $\eta' \to \mu^+\mu^-\mu^+\mu^-$ via the radiative decays
$J/{\psi}\to\gamma\eta$/$\gamma\eta'$. No excess of events over expected
background is observed for any of the decays of interest. At 90% confidence
level, we report the first upper limits on the branching fractions of $\eta'
\to e^{+}e^{-}\mu^{+}\mu^{-}$ and $\eta' \to \mu^{+}\mu^{-}\mu^{+}\mu^{-}$ to
be $ 1.75 \times {10^{-6}}$ and $5.28 \times {10^{-7}}$, respectively. In
addition, we set an upper limit on the branching fraction of $\eta \to
e^{+}e^{-}\mu^{+}\mu^{-}$ to be $6.88 \times {10^{-6}}$, which improves the
previous result by about two orders of magnitude.",2024-12-27T15:55:02Z,http://arxiv.org/abs/2412.19702v1,"BESIII Collaboration, M. Ablikim, M. N. Achasov, P. Adlarson, O. Afedulidis, X. C. Ai, R. Aliberti, A. Amoroso, Y. Bai, O. Bakina, I. Balossino, Y. Ban, H. -R. Bao, V. Batozskaya, K. Begzsuren, N. Berger, M. Berlowski, M. Bertani, D. Bettoni, F. Bianchi, E. Bianco, A. Bortone, I. Boyko, R. A. Briere, A. Brueggemann, H. Cai, X. Cai, A. Calcaterra, G. F. Cao, N. Cao, S. A. Cetin, X. Y. Chai, J. F. Chang, G. R. Che, Y. Z. Che, G. Chelkov, C. Chen, C. H. Chen, Chao Chen, G. Chen, H. S. Chen, H. Y. Chen, M. L. Chen, S. J. Chen, S. L. Chen, S. M. Chen, T. Chen, X. R. Chen, X. T. Chen, Y. B. Chen, Y. Q. Chen, Z. J. Chen, Z. Y. Chen, S. K. Choi, G. Cibinetto, F. Cossio, J. J. Cui, H. L. Dai, J. P. Dai, A. Dbeyssi, R. E. de Boer, D. Dedovich, C. Q. Deng, Z. Y. Deng, A. Denig, I. Denysenko, M. Destefanis, F. De Mori, B. Ding, X. X. Ding, Y. Ding, Y. Ding, J. Dong, L. Y. Dong, M. Y. Dong, X. Dong, M. C. Du, S. X. Du, Y. Y. Duan, Z. H. Duan, P. Egorov, Y. H. Fan, J. Fang, J. Fang, S. S. Fang, W. X. Fang, Y. Fang, Y. Q. Fang, R. Farinelli, L. Fava, F. Feldbauer, G. Felici, C. Q. Feng, J. H. Feng, Y. T. Feng, M. Fritsch, C. D. Fu, J. L. Fu, Y. W. Fu, H. Gao, X. B. Gao, Y. N. Gao, Yang Gao, S. Garbolino, I. Garzia, L. Ge, P. T. Ge, Z. W. Ge, C. Geng, E. M. Gersabeck, A. Gilman, K. Goetzen, L. Gong, W. X. Gong, W. Gradl, S. Gramigna, M. Greco, M. H. Gu, Y. T. Gu, C. Y. Guan, A. Q. Guo, L. B. Guo, M. J. Guo, R. P. Guo, Y. P. Guo, A. Guskov, J. Gutierrez, K. L. Han, T. T. Han, F. Hanisch, X. Q. Hao, F. A. Harris, K. K. He, K. L. He, F. H. Heinsius, C. H. Heinz, Y. K. Heng, C. Herold, T. Holtmann, P. C. Hong, G. Y. Hou, X. T. Hou, Y. R. Hou, Z. L. Hou, B. Y. Hu, H. M. Hu, J. F. Hu, Q. P. Hu, S. L. Hu, T. Hu, Y. Hu, G. S. Huang, K. X. Huang, L. Q. Huang, X. T. Huang, Y. P. Huang, Y. S. Huang, T. Hussain, F. Hölzken, N. Hüsken, N. in der Wiesche, J. Jackson, S. Janchiv, J. H. Jeong, Q. Ji, Q. P. Ji, W. Ji, X. B. Ji, X. L. Ji, Y. Y. Ji, X. Q. Jia, Z. K. Jia, D. Jiang, H. B. Jiang, P. C. Jiang, S. S. Jiang, T. J. Jiang, X. S. Jiang, Y. Jiang, J. B. Jiao, J. K. Jiao, Z. Jiao, S. Jin, Y. Jin, M. Q. Jing, X. M. Jing, T. Johansson, S. Kabana, N. Kalantar-Nayestanaki, X. L. Kang, X. S. Kang, M. Kavatsyuk, B. C. Ke, V. Khachatryan, A. Khoukaz, R. Kiuchi, O. B. Kolcu, B. Kopf, M. Kuessner, X. Kui, N. Kumar, A. Kupsc, W. Kühn, L. Lavezzi, T. T. Lei, Z. H. Lei, M. Lellmann, T. Lenz, C. Li, C. Li, C. H. Li, Cheng Li, D. M. Li, F. Li, G. Li, H. B. Li, H. J. Li, H. N. Li, Hui Li, J. R. Li, J. S. Li, K. Li, K. L. Li, L. J. Li, L. K. Li, Lei Li, M. H. Li, P. R. Li, Q. M. Li, Q. X. Li, R. Li, S. X. Li, T. Li, W. D. Li, W. G. Li, X. Li, X. H. Li, X. L. Li, X. Y. Li, X. Z. Li, Y. G. Li, Z. J. Li, Z. Y. Li, C. Liang, H. Liang, H. Liang, Y. F. Liang, Y. T. Liang, G. R. Liao, Y. P. Liao, J. Libby, A. Limphirat, C. C. Lin, C. X. Lin, D. X. Lin, T. Lin, B. J. Liu, B. X. Liu, C. Liu, C. X. Liu, F. Liu, F. H. Liu, Feng Liu, G. M. Liu, H. Liu, H. B. Liu, H. H. Liu, H. M. Liu, Huihui Liu, J. B. Liu, J. Y. Liu, K. Liu, K. Y. Liu, Ke Liu, L. Liu, L. C. Liu, Lu Liu, M. H. Liu, P. L. Liu, Q. Liu, S. B. Liu, T. Liu, W. K. Liu, W. M. Liu, X. Liu, X. Liu, Y. Liu, Y. Liu, Y. B. Liu, Z. A. Liu, Z. D. Liu, Z. Q. Liu, X. C. Lou, F. X. Lu, H. J. Lu, J. G. Lu, X. L. Lu, Y. Lu, Y. P. Lu, Z. H. Lu, C. L. Luo, J. R. Luo, M. X. Luo, T. Luo, X. L. Luo, X. R. Lyu, Y. F. Lyu, F. C. Ma, H. Ma, H. L. Ma, J. L. Ma, L. L. Ma, L. R. Ma, M. M. Ma, Q. M. Ma, R. Q. Ma, T. Ma, X. T. Ma, X. Y. Ma, Y. M. Ma, F. E. Maas, I. MacKay, M. Maggiora, S. Malde, Y. J. Mao, Z. P. Mao, S. Marcello, Z. X. Meng, J. G. Messchendorp, G. Mezzadri, H. Miao, T. J. Min, R. E. Mitchell, X. H. Mo, B. Moses, N. Yu. Muchnoi, J. Muskalla, Y. Nefedov, F. Nerling, L. S. Nie, I. B. Nikolaev, Z. Ning, S. Nisar, Q. L. Niu, W. D. Niu, Y. Niu, S. L. Olsen, S. L. Olsen, Q. Ouyang, S. Pacetti, X. Pan, Y. Pan, A. Pathak, Y. P. Pei, M. Pelizaeus, H. P. Peng, Y. Y. Peng, K. Peters, J. L. Ping, R. G. Ping, S. Plura, V. Prasad, F. Z. Qi, H. Qi, H. R. Qi, M. Qi, T. Y. Qi, S. Qian, W. B. Qian, C. F. Qiao, X. K. Qiao, J. J. Qin, L. Q. Qin, L. Y. Qin, X. P. Qin, X. S. Qin, Z. H. Qin, J. F. Qiu, Z. H. Qu, C. F. Redmer, K. J. Ren, A. Rivetti, M. Rolo, G. Rong, Ch. Rosner, M. Q. Ruan, S. N. Ruan, N. Salone, A. Sarantsev, Y. Schelhaas, K. Schoenning, M. Scodeggio, K. Y. Shan, W. Shan, X. Y. Shan, Z. J. Shang, J. F. Shangguan, L. G. Shao, M. Shao, C. P. Shen, H. F. Shen, W. H. Shen, X. Y. Shen, B. A. Shi, H. Shi, H. C. Shi, J. L. Shi, J. Y. Shi, Q. Q. Shi, S. Y. Shi, X. Shi, J. J. Song, T. Z. Song, W. M. Song, Y. J. Song, Y. X. Song, S. Sosio, S. Spataro, F. Stieler, S. S Su, Y. J. Su, G. B. Sun, G. X. Sun, H. Sun, H. K. Sun, J. F. Sun, K. Sun, L. Sun, S. S. Sun, T. Sun, W. Y. Sun, Y. Sun, Y. J. Sun, Y. Z. Sun, Z. Q. Sun, Z. T. Sun, C. J. Tang, G. Y. Tang, J. Tang, M. Tang, Y. A. Tang, L. Y. Tao, Q. T. Tao, M. Tat, J. X. Teng, V. Thoren, W. H. Tian, Y. Tian, Z. F. Tian, I. Uman, Y. Wan, S. J. Wang, B. Wang, B. L. Wang, Bo Wang, D. Y. Wang, F. Wang, H. J. Wang, J. J. Wang, J. P. Wang, K. Wang, L. L. Wang, M. Wang, N. Y. Wang, S. Wang, S. Wang, T. Wang, T. J. Wang, W. Wang, W. Wang, W. P. Wang, X. Wang, X. F. Wang, X. J. Wang, X. L. Wang, X. N. Wang, Y. Wang, Y. D. Wang, Y. F. Wang, Y. H. Wang, Y. L. Wang, Y. N. Wang, Y. Q. Wang, Yaqian Wang, Yi Wang, Z. Wang, Z. L. Wang, Z. Y. Wang, Ziyi Wang, D. H. Wei, F. Weidner, S. P. Wen, Y. R. Wen, U. Wiedner, G. Wilkinson, M. Wolke, L. Wollenberg, C. Wu, J. F. Wu, L. H. Wu, L. J. Wu, X. Wu, X. H. Wu, Y. Wu, Y. H. Wu, Y. J. Wu, Z. Wu, L. Xia, X. M. Xian, B. H. Xiang, T. Xiang, D. Xiao, G. Y. Xiao, S. Y. Xiao, Y. L. Xiao, Z. J. Xiao, C. Xie, X. H. Xie, Y. Xie, Y. G. Xie, Y. H. Xie, Z. P. Xie, T. Y. Xing, C. F. Xu, C. J. Xu, G. F. Xu, H. Y. Xu, M. Xu, Q. J. Xu, Q. N. Xu, W. Xu, W. L. Xu, X. P. Xu, Y. Xu, Y. C. Xu, Z. S. Xu, F. Yan, L. Yan, W. B. Yan, W. C. Yan, X. Q. Yan, H. J. Yang, H. L. Yang, H. X. Yang, J. H. Yang, T. Yang, Y. Yang, Y. F. Yang, Y. F. Yang, Y. X. Yang, Z. W. Yang, Z. P. Yao, M. Ye, M. H. Ye, J. H. Yin, Junhao Yin, Z. Y. You, B. X. Yu, C. X. Yu, G. Yu, J. S. Yu, M. C. Yu, T. Yu, X. D. Yu, Y. C. Yu, C. Z. Yuan, J. Yuan, J. Yuan, L. Yuan, S. C. Yuan, Y. Yuan, Z. Y. Yuan, C. X. Yue, A. A. Zafar, F. R. Zeng, S. H. Zeng, X. Zeng, Y. Zeng, Y. J. Zeng, Y. J. Zeng, X. Y. Zhai, Y. C. Zhai, Y. H. Zhan, A. Q. Zhang, B. L. Zhang, B. X. Zhang, D. H. Zhang, G. Y. Zhang, H. Zhang, H. Zhang, H. C. Zhang, H. H. Zhang, H. H. Zhang, H. Q. Zhang, H. R. Zhang, H. Y. Zhang, J. Zhang, J. Zhang, J. J. Zhang, J. L. Zhang, J. Q. Zhang, J. S. Zhang, J. W. Zhang, J. X. Zhang, J. Y. Zhang, J. Z. Zhang, Jianyu Zhang, L. M. Zhang, Lei Zhang, P. Zhang, Q. Y. Zhang, R. Y. Zhang, S. H. Zhang, Shulei Zhang, X. M. Zhang, X. Y Zhang, X. Y. Zhang, Y. Zhang, Y. Zhang, Y. T. Zhang, Y. H. Zhang, Y. M. Zhang, Yan Zhang, Z. D. Zhang, Z. H. Zhang, Z. L. Zhang, Z. Y. Zhang, Z. Y. Zhang, Z. Z. Zhang, G. Zhao, J. Y. Zhao, J. Z. Zhao, L. Zhao, Lei Zhao, M. G. Zhao, N. Zhao, R. P. Zhao, S. J. Zhao, Y. B. Zhao, Y. X. Zhao, Z. G. Zhao, A. Zhemchugov, B. Zheng, B. M. Zheng, J. P. Zheng, W. J. Zheng, Y. H. Zheng, B. Zhong, X. Zhong, H. Zhou, J. Y. Zhou, L. P. Zhou, S. Zhou, X. Zhou, X. K. Zhou, X. R. Zhou, X. Y. Zhou, Y. Z. Zhou, Z. C. Zhou, A. N. Zhu, J. Zhu, K. Zhu, K. J. Zhu, K. S. Zhu, L. Zhu, L. X. Zhu, S. H. Zhu, T. J. Zhu, W. D. Zhu, Y. C. Zhu, Z. A. Zhu, J. H. Zou, J. Zu"
"First-principles investigation of thermodynamics and electronic
  transitions in vacancy-ordered rare-earth perovskite nickelates","Controlled introduction of oxygen vacancies offers an effective route to
induce metal-to-insulator transition in strongly correlated rare-earth
nickelates ($R$NiO$_3$) at room temperature. However, the role played by the
rare-earth cations on the structure, thermodynamic stability, and electronic
properties of oxygen-deficient nickelates remains unclear. Here, we employ
density functional theory calculations with Hubbard corrections (DFT + $U$) to
investigate the whole family of $R$NiO$_{2.5}$ ($R$ = Pr-Er) compounds in two
commonly observed oxygen-vacancy ordered configurations, namely brownmillerite,
and square planar. We find that square planar polymorph is always more stable
($\sim$0.4 eV/u.f) than the brownmillerite for all rare-earth cations, owing to
the exceedingly low volumetric strains (&lt; 1\%). Formation energy of
$R$NiO$_{2.5}$ gradually increases with decreasing size of $R$ owing to
stronger Ni-O covalent interactions in pristine $R$NiO$_3$ with small $R^{3+}$
cations. This necessitates more oxygen-lean environments for synthesis of
$R$NiO$_{2.5}$ with smaller $R^{3+}$ cations. Analysis of the density of states
and band structures reveals that electronic structure of $R$NiO$_{2.5}$ is
governed by two factors: (a) localization of electron on NiO$_6$ octahedra
yielding a Mott insulating state with strong correlations as Ni $e_g$ is half
filled, and (b) crystal field splitting in the NiO$_4$ tetrahedra/square planar
polyhedra. Brownmillerite $R$NiO$_{2.5}$ is metallic, while square planar
$R$NiO$_{2.5}$ is an insulator with a predicted gap of $\sim$ 0.2-0.3 eV,
depending on the $R^{3+}$ cation. Crystal orbital Hamilton population (COHP)
analysis indicates that the Ni-O bond belonging to square-planar NiO$_4$
polyhedra exhibit much greater covalent character than those in NiO$_6$
octahedra in square planar $R$NiO$_{2.5}$.",2024-12-27T15:52:08Z,http://arxiv.org/abs/2412.19700v1,"Devang Bhagat, Ranga Teja Pidathala, Badri Narayanan"
Differentiable groupoid objects and their abstract Lie algebroids,"The infinitesimal counterpart of a Lie groupoid is its Lie algebroid. As a
vector bundle, it is given by the source vertical tangent bundle restricted to
the identity bisection. Its sections can be identified with the invariant
vector fields on the groupoid, which are closed under the Lie bracket. We
generalize this differentiation procedure to groupoid objects in any category
with an abstract tangent structure in the sense of Rosick\'{y} and a scalar
multiplication by a ring object that plays the role of the real numbers. We
identify the categorical conditions that the groupoid object must satisfy to
admit a natural notion of invariant vector fields. Then we show that invariant
vector fields are closed under the Lie bracket defined by Rosick\'{y} and
satisfy the Leibniz rule with respect to ring-valued morphisms on the base of
the groupoid. The result is what we define axiomatically as an abstract Lie
algebroid, by generalizing the underlying vector bundle to a module object in
the slice category over its base. Examples include diffeomorphism groups,
bisection groups of Lie groupoids, the diffeological symmetry groupoids of
general relativity (Blohmann/Fernandes/Weinstein), symmetry groupoids in
Lagrangian Field Theory, holonomy groupoids of singular foliations, elastic
diffeological groupoids, and groupoid objects in differentiable stacks.",2024-12-27T15:50:02Z,http://arxiv.org/abs/2412.19697v1,"Lory Aintablian, Christian Blohmann"
"An Integrated Optimization and Deep Learning Pipeline for Predicting
  Live Birth Success in IVF Using Feature Optimization and Transformer-Based
  Models","In vitro fertilization (IVF) is a widely utilized assisted reproductive
technology, yet predicting its success remains challenging due to the
multifaceted interplay of clinical, demographic, and procedural factors. This
study develops a robust artificial intelligence (AI) pipeline aimed at
predicting live birth outcomes in IVF treatments. The pipeline uses anonymized
data from 2010 to 2018, obtained from the Human Fertilization and Embryology
Authority (HFEA). We evaluated the prediction performance of live birth success
as a binary outcome (success/failure) by integrating different feature
selection methods, such as principal component analysis (PCA) and particle
swarm optimization (PSO), with different traditional machine learning-based
classifiers including random forest (RF) and decision tree, as well as deep
learning-based classifiers including custom transformer-based model and a tab
transformer model with an attention mechanism. Our research demonstrated that
the best performance was achieved by combining PSO for feature selection with
the TabTransformer-based deep learning model, yielding an accuracy of 99.50%
and an AUC of 99.96%, highlighting its significant performance to predict live
births. This study establishes a highly accurate AI pipeline for predicting
live birth outcomes in IVF, demonstrating its potential to enhance personalized
fertility treatments.",2024-12-27T15:46:59Z,http://arxiv.org/abs/2412.19696v1,"Arezoo Borji, Hossam Haick, Birgit Pohn, Antonia Graf, Jana Zakall, S M Ragib Shahriar Islam, Gernot Kronreif, Daniel Kovatchki, Heinz Strohmer, Sepideh Hatamikia"
Sharp Bounds on Lengths of Linear Recolouring Sequences,"A recolouring sequence, between $k$-colourings $\alpha$ and $\beta$ of a
graph $G$, transforms $\alpha$ into $\beta$ by recolouring one vertex at a
time, such that after each recolouring step we again have a proper
$k$-colouring of $G$. The diameter of the $k$-recolouring graph,
$\textrm{diam}~\mathcal{C}_k(G)$, is the maximum over all pairs $\alpha$ and
$\beta$ of the minimum length of a recolouring sequence from $\alpha$ to
$\beta$. Much previous work has focused on determining the asymptotics of
$\textrm{diam}~\mathcal{C}_k(G)$: Is it $\Theta(|G|)$? Is it $\Theta(|G|^2)$?
Or even larger? Here we focus on graphs for which
$\textrm{diam}~\mathcal{C}_k(G)=\Theta(|G|)$, and seek to determine more
precisely the multiplicative constant implicit in the $\Theta()$. In
particular, for each $k\ge 3$, for all positive integers $p$ and $q$ we exactly
determine $\textrm{diam}~\mathcal{C}_k(K_{p,q})$, up to a small additive
constant. We also sharpen a recolouring lemma that has been used in multiple
papers, proving an optimal version. This improves the multiplicative constant
in various prior results. Finally, we investigate plausible relationships
between similar reconfiguration graphs.",2024-12-27T15:42:55Z,http://arxiv.org/abs/2412.19695v1,"Stijn Cambie, Wouter Cames van Batenburg, Daniel W. Cranston"
"Nonperturbative effects in triple-differential dijet and Z+jet
  production at the LHC","In comparisons of precision collider data to the most accurate highest-order
calculations in perturbative quantum chromodynamics (QCD), it is required to
correct for nonperturbative effects. Such effects are typically studied using
Monte Carlo event generators that complement fixed-order predictions with
perturbative parton showers and models for the nonperturbative effects of the
Underlying Event and hadronisation. Thereby, the final state of collision
events can be predicted at the level of stable particles, which serve as input
for full detector simulations.
  This article investigates the impact of nonperturbative effects on two
processes that may be used for precision determinations of the strong coupling
constant and the proton structure: the triple-differential dijet and Z+jet
production. While nonperturbative effects impact both processes, significant
differences among them are observed and further investigated. Indications are
found that the Underlying Event and hadronisation cannot fully explain these
differences and the perturbative modelling may play a significant role as well.",2024-12-27T15:42:07Z,http://arxiv.org/abs/2412.19694v1,"Stefan Gieseke, Maximilian Horzela, Manjit Kaur, Dari Leonardi, Klaus Rabbertz, Aayushi Singla, Cedric Verstege"
"Wannier states and spin supersolid physics in the triangular
  antiferromagnet K$_2$Co(SeO$_3$)$_2$","We use a combination of ultra-high-resolution inelastic neutron scattering
and Monte Carlo numerical simulations to study the thermodynamics and the
structure of spin excitations in the spin-supersolid phase of the triangular
lattice XXZ easy axis antiferromagnet K$_2$Co(SeO$_3$)$_2$ and its evolution in
a magnetic field. BKT transitions heralding the onset of Ising and supersolid
order are detected. Above the supersolid phase the value of Wannier entropy is
experimentally recovered. At low temperatures, with an experimental resolution
of about 23 $\mu$eV, no discrete coherent magnon modes are resolved within a
broad continuum of scattering. In addition to gapless excitations, a
pseudo-Goldstone mode with a small energy gap of 0.06 meV is found. A second
excitation continuum is seen at higher energy, in place of single-spin-flip
excitations of the Ising model. In applied fields the continuum gradually
morphs into coherent spin waves, with the Goldstone and pseudo-Goldstone
sectors showing distinct evolution. The agreement between experiment and
numerical simulations is excellent on the quantitative level.",2024-12-27T15:40:31Z,http://arxiv.org/abs/2412.19693v1,"M. Zhu, Leandro M. Chinellato, V. Romerio, N. Murai, S. Ohira-Kawamura, Christian Balz, Z. Yan, S. Gvasaliya, Yasuyuki Kato, C. D. Batista, A. Zheludev"
"Quantum Many-Body Lattice C-R-T Symmetry: Fractionalization, Anomaly,
  and Symmetric Mass Generation","Charge conjugation (C), mirror reflection (R), and time reversal (T)
symmetries, along with internal symmetries, are essential for massless Majorana
and Dirac fermions. These symmetries are sufficient to rule out potential
fermion bilinear mass terms, thereby establishing a gapless free fermion fixed
point phase, pivotal for symmetric mass generation (SMG) transition. In this
work, we systematically study the anomaly of C-R-T-internal symmetry in all
spacetime dimensions by analyzing the projective representation (i.e. the
fractionalization) of the C-R-T-internal symmetry group in the quantum
many-body Hilbert space on the lattice. By discovering the
fermion-flavor-number-dependent C-R-T-internal symmetry's anomaly structure, we
demonstrate an alternative way to derive the minimal flavor number for SMG,
which shows consistency with known results from K\""ahler-Dirac fermion or
cobordism classification. Our findings reveal that, in general spatial
dimensions, either 8 copies of staggered Majorana fermions or 4 copies of
staggered Dirac fermions admit SMG. By directly searching for 4-fermion
interactions that form commuting stabilizers respecting all symmetry
constraints, we can prove the explicit SMG gapping retained a unique ground
state in the codespace. Furthermore, we establish the correspondence between
the symmetry operators of staggered fermions and free fermions, which is
instrumental in facilitating the analysis of symmetry fractionalization at the
field theory level.",2024-12-27T15:36:31Z,http://arxiv.org/abs/2412.19691v1,"Yang-Yang Li, Juven Wang, Yi-Zhuang You"
"Terms that define nuclei on residuated lattices: a case study of
  BL-algebras","A nucleus $\gamma$ on a (bounded commutative integral) residuated lattice
$\mathbf{A}$ is a closure operator that satisfies the inequality $\gamma(a)
\cdot \gamma(b) \leq \gamma(a \cdot b)$ for all $a,b \in A$. In this article,
among several results, a description of an arbitrary nucleus on a residuated
lattice is given. Special attention is given to terms that define a nucleus on
every structure of a variety, as a means of generalizing the double negation
operation. Some general results about these terms are presented, together with
examples. The main result of this article consists of the description of all
terms of this kind for every given subvariety of BL-algebras. We exhibit
interesting nontrivial examples.",2024-12-27T15:34:51Z,http://arxiv.org/abs/2412.19690v1,"Sebastián Buss, Diego Castaño, José Patricio Díaz Varela"
"A Review on the Integration of Artificial Intelligence and Medical
  Imaging in IVF Ovarian Stimulation","Artificial intelligence (AI) has emerged as a powerful tool to enhance
decision-making and optimize treatment protocols in in vitro fertilization
(IVF). In particular, AI shows significant promise in supporting
decision-making during the ovarian stimulation phase of the IVF process. This
review evaluates studies focused on the applications of AI combined with
medical imaging in ovarian stimulation, examining methodologies, outcomes, and
current limitations. Our analysis of 13 studies on this topic reveals that,
reveal that while AI algorithms demonstrated notable potential in predicting
optimal hormonal dosages, trigger timing, and oocyte retrieval outcomes, the
medical imaging data utilized predominantly came from two-dimensional (2D)
ultrasound which mainly involved basic quantifications, such as follicle size
and number, with limited use of direct feature extraction or advanced image
analysis techniques. This points to an underexplored opportunity where advanced
image analysis approaches, such as deep learning, and more diverse imaging
modalities, like three-dimensional (3D) ultrasound, could unlock deeper
insights. Additionally, the lack of explainable AI (XAI) in most studies raises
concerns about the transparency and traceability of AI-driven decisions - key
factors for clinical adoption and trust. Furthermore, many studies relied on
single-center designs and small datasets, which limit the generalizability of
their findings. This review highlights the need for integrating advanced
imaging analysis techniques with explainable AI methodologies, as well as the
importance of leveraging multicenter collaborations and larger datasets.
Addressing these gaps has the potential to enhance ovarian stimulation
management, paving the way for efficient, personalized, and data-driven
treatment pathways that improve IVF outcomes.",2024-12-27T15:29:08Z,http://arxiv.org/abs/2412.19688v1,"Jana Zakall, Birgit Pohn, Antonia Graf, Daniel Kovatchki, Arezoo Borji, Ragib Shahriar Islam, Hossam Haick, Heinz Strohmer, Sepideh Hatamikia"
"DFT based comparative analysis of physical properties of binary metallic
  diborides XB$_2$ (X = Cr, Mo and W)","Transition-metal borides (TMBs) have long attracted attention of the
researchers because of their unique mechanical and electrical properties
including superconductivity. We have explored the structural, mechanical,
electronic, optical, and some thermophysical properties of XB$_2$ (X = Cr, Mo
and W) binary metallic diborides in detail employing density functional theory
based first-principles method. Many of the physical properties, including
direction-dependent mechanical properties, optical properties, and
thermo-mechanical properties are being investigated for the first time.",2024-12-27T15:28:42Z,http://arxiv.org/abs/2412.19687v1,"Razu Ahmed, Md. Sohel Rana, Md. Sajidul Islam, S. H. Naqib"
"Boosting Private Domain Understanding of Efficient MLLMs: A Tuning-free,
  Adaptive, Universal Prompt Optimization Framework","Efficient multimodal large language models (EMLLMs), in contrast to
multimodal large language models (MLLMs), reduce model size and computational
costs and are often deployed on resource-constrained devices. However, due to
data privacy concerns, existing open-source EMLLMs rarely have access to
private domain-specific data during the pre-training process, making them
difficult to directly apply in device-specific domains, such as certain
business scenarios. To address this weakness, this paper focuses on the
efficient adaptation of EMLLMs to private domains, specifically in two areas:
1) how to reduce data requirements, and 2) how to avoid parameter fine-tuning.
Specifically, we propose a tun\textbf{\underline{I}}ng-free,
a\textbf{\underline{D}}aptiv\textbf{\underline{E}},
univers\textbf{\underline{AL}} \textbf{\underline{Prompt}} Optimization
Framework, abbreviated as \textit{\textbf{\ourmethod{}}} which consists of two
stages: 1) Predefined Prompt, based on the reinforcement searching strategy,
generate a prompt optimization strategy tree to acquire optimization priors; 2)
Prompt Reflection initializes the prompt based on optimization priors, followed
by self-reflection to further search and refine the prompt. By doing so,
\ourmethod{} elegantly generates the ``ideal prompts'' for processing private
domain-specific data. Note that our method requires no parameter fine-tuning
and only a small amount of data to quickly adapt to the data distribution of
private data. Extensive experiments across multiple tasks demonstrate that our
proposed \ourmethod{} significantly improves both efficiency and performance
compared to baselines.",2024-12-27T15:21:17Z,http://arxiv.org/abs/2412.19684v1,"Jiang Liu, Bolin Li, Haoyuan Li, Tianwei Lin, Wenqiao Zhang, Tao Zhong, Zhelun Yu, Jinghao Wei, Hao Cheng, Hao Jiang, Zheqi Lv, Juncheng Li, Siliang Tang, Yueting Zhuang"
"A Hybrid Technique for Plant Disease Identification and Localisation in
  Real-time","Over the past decade, several image-processing methods and algorithms have
been proposed for identifying plant diseases based on visual data. DNN (Deep
Neural Networks) have recently become popular for this task. Both traditional
image processing and DNN-based methods encounter significant performance issues
in real-time detection owing to computational limitations and a broad spectrum
of plant disease features. This article proposes a novel technique for
identifying and localising plant disease based on the Quad-Tree decomposition
of an image and feature learning simultaneously. The proposed algorithm
significantly improves accuracy and faster convergence in high-resolution
images with relatively low computational load. Hence it is ideal for deploying
the algorithm in a standalone processor in a remotely operated image
acquisition and disease detection system, ideally mounted on drones and robots
working on large agricultural fields. The technique proposed in this article is
hybrid as it exploits the advantages of traditional image processing methods
and DNN-based models at different scales, resulting in faster inference. The F1
score is approximately 0.80 for four disease classes corresponding to potato
and tomato crops.",2024-12-27T15:20:45Z,http://arxiv.org/abs/2412.19682v1,"Mahendra Kumar Gohil, Anirudha Bhattacharjee, Rwik Rana, Kishan Lal, Samir Kumar Biswas, Nachiketa Tiwari, Bishakh Bhattacharya"
Identifying clusters in Czekanowski's diagram,"Visualizing data through Czekanowski's diagram has as its aim the
illustration of the relationships between objects. Often, obvious clusters of
observations are directly visible. However, it is not straightforward to
precisely delineate these clusters. This paper presents the development of the
package RMaCzek, which now includes features for cluster identification in
Czekanowski diagrams.",2024-12-27T15:04:06Z,http://arxiv.org/abs/2412.19679v1,"Krzysztof Bartoszek, Ying Luo"
Deep ReLU networks -- injectivity capacity upper bounds,"We study deep ReLU feed forward neural networks (NN) and their injectivity
abilities. The main focus is on \emph{precisely} determining the so-called
injectivity capacity. For any given hidden layers architecture, it is defined
as the minimal ratio between number of network's outputs and inputs which
ensures unique recoverability of the input from a realizable output. A strong
recent progress in precisely studying single ReLU layer injectivity properties
is here moved to a deep network level. In particular, we develop a program that
connects deep $l$-layer net injectivity to an $l$-extension of the $\ell_0$
spherical perceptrons, thereby massively generalizing an isomorphism between
studying single layer injectivity and the capacity of the so-called
(1-extension) $\ell_0$ spherical perceptrons discussed in [82]. \emph{Random
duality theory} (RDT) based machinery is then created and utilized to
statistically handle properties of the extended $\ell_0$ spherical perceptrons
and implicitly of the deep ReLU NNs. A sizeable set of numerical evaluations is
conducted as well to put the entire RDT machinery in practical use. From these
we observe a rapidly decreasing tendency in needed layers' expansions, i.e., we
observe a rapid \emph{expansion saturation effect}. Only $4$ layers of depth
are sufficient to closely approach level of no needed expansion -- a result
that fairly closely resembles observations made in practical experiments and
that has so far remained completely untouchable by any of the existing
mathematical methodologies.",2024-12-27T14:57:40Z,http://arxiv.org/abs/2412.19677v1,Mihailo Stojnic
"Optimizing Local-Global Dependencies for Accurate 3D Human Pose
  Estimation","Transformer-based methods have recently achieved significant success in 3D
human pose estimation, owing to their strong ability to model long-range
dependencies. However, relying solely on the global attention mechanism is
insufficient for capturing the fine-grained local details, which are crucial
for accurate pose estimation. To address this, we propose SSR-STF, a
dual-stream model that effectively integrates local features with global
dependencies to enhance 3D human pose estimation. Specifically, we introduce
SSRFormer, a simple yet effective module that employs the skeleton selective
refine attention (SSRA) mechanism to capture fine-grained local dependencies in
human pose sequences, complementing the global dependencies modeled by the
Transformer. By adaptively fusing these two feature streams, SSR-STF can better
learn the underlying structure of human poses, overcoming the limitations of
traditional methods in local feature extraction. Extensive experiments on the
Human3.6M and MPI-INF-3DHP datasets demonstrate that SSR-STF achieves
state-of-the-art performance, with P1 errors of 37.4 mm and 13.2 mm
respectively, outperforming existing methods in both accuracy and
generalization. Furthermore, the motion representations learned by our model
prove effective in downstream tasks such as human mesh recovery. Codes are
available at https://github.com/poker-xu/SSR-STF.",2024-12-27T14:54:12Z,http://arxiv.org/abs/2412.19676v1,"Guangsheng Xu, Guoyi Zhang, Lejia Ye, Shuwei Gan, Xiaohu Zhang, Xia Yang"
Port-Hamiltonian nonlinear systems,"Control theory often takes the mathematical model of the to-be-control-led
system for granted. In contrast, port-Hamiltonian systems theory bridges the
gap between modelling and control for physical systems. It provides a unified
framework for the modelling of complex multiphysics systems. At the same time
it offers powerful tools for analysis and control by identifying the underlying
physical structure, as reflected in, e.g., energy balance and other conserved
quantities. This leads to control schemes that \emph{exploit} the physical
structure, instead of compensating for it. As a result, the derived control
laws tend to be simple, physically interpretable, and robust with respect to
physical parameter variations.
  In this paper, after introducing port-Hamiltonian systems, the focus is on
'control by interconnection' for set-point stabilization of nonlinear physical
systems. Most of this theory is well-established, but novel developments using
'energy ports' instead of 'power ports' are also included.",2024-12-27T14:45:45Z,http://arxiv.org/abs/2412.19673v1,Arjan van der Schaft
Lattice properties of the sharp partial order,"The aim of this paper is to study lattice properties of the sharp partial
order for complex matrices having index at most 1. We investigate the down-set
of a fixed matrix $B$ under this partial order via isomorphisms with two
different partially ordered sets of projectors. These are, respectively, the
set of projectors that commute with a certain (nonsingular) block of a
Hartwig-Spindelb\""ock decomposition of $B$ and the set of projectors that
commute with the Jordan canonical form of that block. Using these isomorphisms,
we study the lattice structure of the down-sets and we give properties of them.
Necessary and sufficient conditions under which the down-set of B is a lattice
were found, in which case we describe its elements completely. We also show
that every down-set of $B$ has a distinguished Boolean subalgebra and we give a
description of its elements. We characterize the matrices that are above a
given matrix in terms of its Jordan canonical form. Mitra (1987) showed that
the set of all $n \times n$ complex matrices having index at most 1 with $n\geq
4$ is not a lower semilattice. We extend this result to $n=3$ and prove that it
is a lower semilattice with $n=2$. We also answer negatively a conjecture given
by Mitra, Bhimasankaram and Malik (2010). As a last application, we
characterize solutions of some matrix equations via the established
isomorphisms.",2024-12-27T14:38:40Z,http://arxiv.org/abs/2412.19671v1,"Cecilia R. Cimadamore, Laura A. Rueda, Néstor Thome, Melina V. Verdecchia"
Multipole moments in stationary spacetimes,"Multipole moments in general relativity serve as a powerful tool for
characterising the gravitational field. In this paper, we review the
construction of the Geroch--Hansen multipole moments for stationary
asymptotically flat vacuum spacetimes. A particular focus is placed on the
well-definedness of these moments, which hinges on the uniqueness of the
one-point conformal completion in Geroch's asymptotic flatness definition.
Based on Geroch's approach, we formulate and prove a revised uniqueness result,
thereby filling in some gaps in the original approach. Uniqueness holds up to
certain conformal transformations, and we discuss how the multipole moments
behave under such transformations.",2024-12-27T14:27:01Z,http://arxiv.org/abs/2412.19667v1,Jorn van Voorthuizen
The Value of Recall in Extensive-Form Games,"Imperfect-recall games, in which players may forget previously acquired
information, have found many practical applications, ranging from game
abstractions to team games and testing AI agents. In this paper, we quantify
the utility gain by endowing a player with perfect recall, which we call the
value of recall (VoR). While VoR can be unbounded in general, we parameterize
it in terms of various game properties, namely the structure of chance nodes
and the degree of absentmindedness (the number of successive times a player
enters the same information set). Further, we identify several pathologies that
arise with VoR, and show how to circumvent them. We also study the complexity
of computing VoR, and how to optimally apportion partial recall. Finally, we
connect VoR to other previously studied concepts in game theory, including the
price of anarchy. We use that connection in conjunction with the celebrated
smoothness framework to characterize VoR in a broad class of games.",2024-12-27T14:12:45Z,http://arxiv.org/abs/2412.19659v1,"Ratip Emin Berker, Emanuel Tewolde, Ioannis Anagnostides, Tuomas Sandholm, Vincent Conitzer"
"Asymmetrical Reciprocity-based Federated Learning for Resolving
  Disparities in Medical Diagnosis","Geographic health disparities pose a pressing global challenge, particularly
in underserved regions of low- and middle-income nations. Addressing this issue
requires a collaborative approach to enhance healthcare quality, leveraging
support from medically more developed areas. Federated learning emerges as a
promising tool for this purpose. However, the scarcity of medical data and
limited computation resources in underserved regions make collaborative
training of powerful machine learning models challenging. Furthermore, there
exists an asymmetrical reciprocity between underserved and developed regions.
To overcome these challenges, we propose a novel cross-silo federated learning
framework, named FedHelp, aimed at alleviating geographic health disparities
and fortifying the diagnostic capabilities of underserved regions.
Specifically, FedHelp leverages foundational model knowledge via one-time API
access to guide the learning process of underserved small clients, addressing
the challenge of insufficient data. Additionally, we introduce a novel
asymmetric dual knowledge distillation module to manage the issue of asymmetric
reciprocity, facilitating the exchange of necessary knowledge between developed
large clients and underserved small clients. We validate the effectiveness and
utility of FedHelp through extensive experiments on both medical image
classification and segmentation tasks. The experimental results demonstrate
significant performance improvement compared to state-of-the-art baselines,
particularly benefiting clients in underserved regions.",2024-12-27T13:59:58Z,http://arxiv.org/abs/2412.19654v1,"Jiaqi Wang, Ziyi Yin, Quanzeng You, Lingjuan Lyu, Fenglong Ma"
"Distributed Download from an External Data Source in Faulty Majority
  Settings","We extend the study of retrieval problems in distributed networks, focusing
on improving the efficiency and resilience of protocols in the \emph{Data
Retrieval (DR) Model}. The DR Model consists of a complete network (i.e., a
clique) with $k$ peers, up to $\beta k$ of which may be Byzantine (for $\beta
\in [0, 1)$), and a trusted \emph{External Data Source} comprising an array $X$
of $n$ bits ($n \gg k$) that the peers can query. Additionally, the peers can
also send messages to each other. In this work, we focus on the Download
problem that requires all peers to learn $X$. Our primary goal is to minimize
the maximum number of queries made by any honest peer and additionally optimize
time.
  We begin with a randomized algorithm for the Download problem that achieves
optimal query complexity up to a logarithmic factor. For the stronger dynamic
adversary that can change the set of Byzantine peers from one round to the
next, we achieve the optimal time complexity in peer-to-peer communication but
with larger messages. In broadcast communication where all peers (including
Byzantine peers) are required to send the same message to all peers, with
larger messages, we achieve almost optimal time and query complexities for a
dynamic adversary. Finally, in a more relaxed crash fault model, where peers
stop responding after crashing, we address the Download problem in both
synchronous and asynchronous settings. Using a deterministic protocol, we
obtain nearly optimal results for both query complexity and message sizes in
these scenarios.",2024-12-27T13:55:00Z,http://arxiv.org/abs/2412.19649v1,"John Augustine, Soumyottam Chatterjee, Valerie King, Manish Kumar, Shachar Meir, David Peleg"
"Enhancing Vision-Language Tracking by Effectively Converting Textual
  Cues into Visual Cues","Vision-Language Tracking (VLT) aims to localize a target in video sequences
using a visual template and language description. While textual cues enhance
tracking potential, current datasets typically contain much more image data
than text, limiting the ability of VLT methods to align the two modalities
effectively. To address this imbalance, we propose a novel plug-and-play method
named CTVLT that leverages the strong text-image alignment capabilities of
foundation grounding models. CTVLT converts textual cues into interpretable
visual heatmaps, which are easier for trackers to process. Specifically, we
design a textual cue mapping module that transforms textual cues into target
distribution heatmaps, visually representing the location described by the
text. Additionally, the heatmap guidance module fuses these heatmaps with the
search image to guide tracking more effectively. Extensive experiments on
mainstream benchmarks demonstrate the effectiveness of our approach, achieving
state-of-the-art performance and validating the utility of our method for
enhanced VLT.",2024-12-27T13:54:32Z,http://arxiv.org/abs/2412.19648v1,"X. Feng, D. Zhang, S. Hu, X. Li, M. Wu, J. Zhang, X. Chen, K. Huang"
"Chimera: A Block-Based Neural Architecture Search Framework for
  Event-Based Object Detection","Event-based cameras are sensors that simulate the human eye, offering
advantages such as high-speed robustness and low power consumption. Established
Deep Learning techniques have shown effectiveness in processing event data.
Chimera is a Block-Based Neural Architecture Search (NAS) framework
specifically designed for Event-Based Object Detection, aiming to create a
systematic approach for adapting RGB-domain processing methods to the event
domain. The Chimera design space is constructed from various macroblocks,
including Attention blocks, Convolutions, State Space Models, and
MLP-mixer-based architectures, which provide a valuable trade-off between local
and global processing capabilities, as well as varying levels of complexity.
The results on the PErson Detection in Robotics (PEDRo) dataset demonstrated
performance levels comparable to leading state-of-the-art models, alongside an
average parameter reduction of 1.6 times.",2024-12-27T13:50:44Z,http://arxiv.org/abs/2412.19646v1,"Diego A. Silva, Ahmed Elsheikh, Kamilya Smagulova, Mohammed E. Fouda, Ahmed M. Eltawil"
A Brief Overlook on Magnetoplasmadynamic Thrusters,"This paper presents a comprehensive analysis of Magnetoplasmadynamic
Thrusters (MPDT), examining their working principles, performance
characteristics, and potential applications in space propulsion. The study
focuses on both self-field and applied-field MPDT variants, detailing the
fundamental physics of plasma generation, acceleration mechanisms through
Lorentz forces, and plasma detachment processes. Through mathematical modeling
and experimental data analysis, the paper demonstrates MPDTs' capability to
achieve high specific impulse and efficient propellant utilization compared to
chemical propulsion systems. While highlighting their advantages for deep space
missions and satellite operations, the study also addresses key challenges,
including high power requirements and thermal management issues. The research
concludes that despite current technological limitations, MPDTs show promising
potential for future space exploration, particularly for long-duration missions
requiring sustained thrust.",2024-12-27T13:30:13Z,http://arxiv.org/abs/2412.19636v1,Egemen Gover
A non-semisimple non-invertible symmetry,"We investigate the action of a non-semisimple, non-invertible symmetry on
spin chains, whose topological defects encode the category of modules over the
Taft algebra of dimension 4. Sacrificing Hermiticity, we construct several
symmetric, frustration-free, gapped Hamiltonians with real spectra and analyse
their ground state subspaces. Our study reveals two intriguing phenomena.
First, we identify an $\mathbb{S}^1$-parametrised family of symmetric states,
all of which belong to the same gapped phase with respect to the invertible
subsymmetry, yet transform inequivalently under the non-semisimple symmetry.
Second, we find a model where a product state and the so-called W state
spontaneously break the symmetry. We further relate the indistinguishability of
these two states in the infinite-volume limit to the notion that they are
associated with a simple object and its projective cover, respectively, in a
non-semisimple module category.",2024-12-27T13:27:24Z,http://arxiv.org/abs/2412.19635v1,"Clement Delcamp, Edmund Heng, Matthew Yu"
Deep Linear Hawkes Processes,"Marked temporal point processes (MTPPs) are used to model sequences of
different types of events with irregular arrival times, with broad applications
ranging from healthcare and social networks to finance. We address shortcomings
in existing point process models by drawing connections between modern deep
state-space models (SSMs) and linear Hawkes processes (LHPs), culminating in an
MTPP that we call the deep linear Hawkes process (DLHP). The DLHP modifies the
linear differential equations in deep SSMs to be stochastic jump differential
equations, akin to LHPs. After discretizing, the resulting recurrence can be
implemented efficiently using a parallel scan. This brings parallelism and
linear scaling to MTPP models. This contrasts with attention-based MTPPs, which
scale quadratically, and RNN-based MTPPs, which do not parallelize across the
sequence length. We show empirically that DLHPs match or outperform existing
models across a broad range of metrics on eight real-world datasets. Our
proposed DLHP model is the first instance of the unique architectural
capabilities of SSMs being leveraged to construct a new class of MTPP models.",2024-12-27T13:23:58Z,http://arxiv.org/abs/2412.19634v1,"Yuxin Chang, Alex Boyd, Cao Xiao, Taha Kass-Hout, Parminder Bhatia, Padhraic Smyth, Andrew Warrington"
"Primordial Black Hole Formation from the Upward Step Model: Avoiding
  Overproduction","We investigate the formation of primordial black holes (PBHs) in an upward
step inflationary model, where nonlinearities between curvature perturbations
and field fluctuations introduce a cutoff, deviating from the Gaussian case.
This necessitates a reevaluation of PBH formation, as $\mathcal{R}$ is not the
optimal variable for estimating abundance. Using the extended Press-Schechter
formalism, we show that non-Gaussianity modifies both the curvature
perturbation profile $\mathcal{R}(r)$ and the integration path in probability
space, significantly impacting PBH abundance. Our results reveal that the
abundance initially increases with the parameter $h$, which characterizes the
relaxation stage after the step. However, beyond a critical value ($h \simeq
5.9$), it sharply declines before rising again. Furthermore, we demonstrate
that non-Gaussianity introduces uncertainties in indirect PBH observations via
gravitational waves. Notably, we present an example where a positive $f_{\rm
NL}$ does not necessarily enhance PBH production, contrary to conventional
expectations. Finally, by accounting for non-perturbative effects, we resolve
the overproduction of PBHs suggested by pulsar timing array (PTA) data,
underscoring the critical importance of incorporating non-Gaussianity in future
studies.",2024-12-27T13:21:06Z,http://arxiv.org/abs/2412.19631v1,"Xiaoding Wang, Xiao-Han Ma, Yi-Fu Cai"
"RecConv: Efficient Recursive Convolutions for Multi-Frequency
  Representations","Recent advances in vision transformers (ViTs) have demonstrated the advantage
of global modeling capabilities, prompting widespread integration of
large-kernel convolutions for enlarging the effective receptive field (ERF).
However, the quadratic scaling of parameter count and computational complexity
(FLOPs) with respect to kernel size poses significant efficiency and
optimization challenges. This paper introduces RecConv, a recursive
decomposition strategy that efficiently constructs multi-frequency
representations using small-kernel convolutions. RecConv establishes a linear
relationship between parameter growth and decomposing levels which determines
the effective kernel size $k\times 2^\ell$ for a base kernel $k$ and $\ell$
levels of decomposition, while maintaining constant FLOPs regardless of the ERF
expansion. Specifically, RecConv achieves a parameter expansion of only
$\ell+2$ times and a maximum FLOPs increase of $5/3$ times, compared to the
exponential growth ($4^\ell$) of standard and depthwise convolutions.
RecNeXt-M3 outperforms RepViT-M1.1 by 1.9 $AP^{box}$ on COCO with similar
FLOPs. This innovation provides a promising avenue towards designing efficient
and compact networks across various modalities. Codes and models can be found
at \url{https://github.com/suous/RecNeXt}.",2024-12-27T13:13:52Z,http://arxiv.org/abs/2412.19628v1,"Mingshu Zhao, Yi Luo, Yong Ouyang"
"Measurement of the branching fraction, polarization, and time-dependent
  $CP$ asymmetry in $B^0 \to ρ^+ρ^-$ decays and constraint on the CKM
  angle $φ_2$","We present a measurement of the branching fraction and fraction of
longitudinal polarization of $B^0 \to \rho^+ \rho^-$ decays, which have two
$\pi^0$'s in the final state. We also measure time-dependent $CP$ violation
parameters for decays into longitudinally polarized $\rho^+ \rho^-$ pairs. This
analysis is based on a data sample containing $(387\pm6) \times 10^6$ \BBbar
pairs collected with the Belle~II detector at the SuperKEKB asymmetric-energy
$e^+e^-$ collider in 2019-2022. We obtain ${B}(B^0\to\rho^+\rho^-) = (2.88
^{+0.23}_{-0.22} {}^{+0.29}_{-0.27}) \times 10^{-5}, f_{L} = 0.921
^{+0.024}_{-0.025} {}^{+0.017}_{-0.015}$, $S = -0.26\pm0.19\pm0.08$, and $C =
-0.02\pm0.12^{+0.06}_{-0.05}$, where the first uncertainties are statistical
and the second are systematic. We use these results to perform an isospin
analysis to constrain the CKM angle $\phi_2$ and obtain two solutions; the
result consistent with other Standard Model constraints is $\phi_2 =
(92.6^{+4.5}_{-4.8})^\circ$.",2024-12-27T13:00:10Z,http://arxiv.org/abs/2412.19624v1,"Belle II Collaboration, I. Adachi, L. Aggarwal, H. Ahmed, N. Akopov, M. Alhakami, A. Aloisio, N. Althubiti, N. Anh Ky, D. M. Asner, H. Atmacan, V. Aushev, M. Aversano, R. Ayad, V. Babu, N. K. Baghel, P. Bambade, Sw. Banerjee, M. Barrett, M. Bartl, J. Baudot, A. Baur, A. Beaubien, J. Becker, J. V. Bennett, V. Bertacchi, M. Bertemes, E. Bertholet, M. Bessner, S. Bettarini, B. Bhuyan, D. Biswas, A. Bobrov, D. Bodrov, A. Bolz, A. Bondar, J. Borah, A. Boschetti, A. Bozek, M. Bračko, P. Branchini, R. A. Briere, T. E. Browder, A. Budano, S. Bussino, Q. Campagna, M. Campajola, G. Casarosa, C. Cecchi, J. Cerasoli, M. -C. Chang, P. Chang, R. Cheaib, P. Cheema, B. G. Cheon, K. Chilikin, K. Chirapatpimol, H. -E. Cho, K. Cho, S. -J. Cho, S. -K. Choi, S. Choudhury, J. Cochran, L. Corona, J. X. Cui, E. De La Cruz-Burelo, S. A. De La Motte, G. De Nardo, G. De Pietro, R. de Sangro, M. Destefanis, S. Dey, F. Di Capua, J. Dingfelder, Z. Doležal, I. Domínguez Jiménez, T. V. Dong, X. Dong, M. Dorigo, D. Dossett, K. Dugic, G. Dujany, P. Ecker, J. Eppelt, P. Feichtinger, T. Ferber, T. Fillinger, C. Finck, G. Finocchiaro, A. Fodor, F. Forti, B. G. Fulsom, A. Gabrielli, E. Ganiev, M. Garcia-Hernandez, R. Garg, G. Gaudino, V. Gaur, A. Gaz, A. Gellrich, G. Ghevondyan, D. Ghosh, H. Ghumaryan, G. Giakoustidis, R. Giordano, A. Giri, P. Gironella Gironell, A. Glazov, B. Gobbo, R. Godang, O. Gogota, P. Goldenzweig, W. Gradl, E. Graziani, D. Greenwald, Z. Gruberová, Y. Guan, K. Gudkova, I. Haide, T. Hara, C. Harris, K. Hayasaka, S. Hazra, C. Hearty, M. T. Hedges, A. Heidelbach, I. Heredia de la Cruz, M. Hernández Villanueva, T. Higuchi, M. Hoek, M. Hohmann, R. Hoppe, P. Horak, C. -L. Hsu, T. Humair, T. Iijima, K. Inami, N. Ipsita, A. Ishikawa, R. Itoh, M. Iwasaki, D. Jacobi, W. W. Jacobs, E. -J. Jang, Y. Jin, A. Johnson, H. Junkerkalefeld, M. Kaleta, A. B. Kaliyar, J. Kandra, F. Keil, C. Ketter, C. Kiesling, C. -H. Kim, D. Y. Kim, J. -Y. Kim, K. -H. Kim, Y. -K. Kim, K. Kinoshita, P. Kodyš, T. Koga, S. Kohani, K. Kojima, A. Korobov, S. Korpar, E. Kovalenko, R. Kowalewski, P. Križan, P. Krokovny, T. Kuhr, Y. Kulii, R. Kumar, K. Kumara, T. Kunigo, A. Kuzmin, Y. -J. Kwon, S. Lacaprara, K. Lalwani, T. Lam, L. Lanceri, J. S. Lange, T. S. Lau, M. Laurenza, R. Leboucher, F. R. Le Diberder, M. J. Lee, C. Lemettais, P. Leo, L. K. Li, Q. M. Li, W. Z. Li, Y. Li, Y. B. Li, Y. P. Liao, J. Libby, J. Lin, S. Lin, M. H. Liu, Q. Y. Liu, Z. Q. Liu, D. Liventsev, S. Longo, T. Lueck, C. Lyu, Y. Ma, C. Madaan, M. Maggiora, S. P. Maharana, R. Maiti, G. Mancinelli, R. Manfredi, E. Manoni, M. Mantovano, D. Marcantonio, S. Marcello, C. Marinas, C. Martellini, A. Martens, A. Martini, T. Martinov, L. Massaccesi, M. Masuda, K. Matsuoka, D. Matvienko, S. K. Maurya, M. Maushart, J. A. McKenna, F. Meier, D. Meleshko, M. Merola, C. Miller, M. Mirra, S. Mitra, K. Miyabayashi, H. Miyake, G. B. Mohanty, S. Mondal, S. Moneta, H. -G. Moser, R. Mussa, I. Nakamura, M. Nakao, Y. Nakazawa, M. Naruki, Z. Natkaniec, A. Natochii, M. Nayak, G. Nazaryan, M. Neu, S. Nishida, S. Ogawa, R. Okubo, H. Ono, Y. Onuki, G. Pakhlova, S. Pardi, K. Parham, H. Park, J. Park, K. Park, S. -H. Park, A. Passeri, S. Patra, T. K. Pedlar, I. Peruzzi, R. Peschke, R. Pestotnik, L. E. Piilonen, P. L. M. Podesta-Lerma, T. Podobnik, S. Pokharel, C. Praz, S. Prell, E. Prencipe, M. T. Prim, H. Purwar, S. Raiz, K. Ravindran, J. U. Rehman, M. Reif, S. Reiter, M. Remnev, L. Reuter, D. Ricalde Herrmann, I. Ripp-Baudot, G. Rizzo, M. Roehrken, J. M. Roney, A. Rostomyan, N. Rout, Y. Sakai, D. A. Sanders, S. Sandilya, L. Santelj, V. Savinov, B. Scavino, C. Schwanda, A. J. Schwartz, Y. Seino, A. Selce, K. Senyo, J. Serrano, M. E. Sevior, C. Sfienti, W. Shan, X. D. Shi, T. Shillington, J. -G. Shiu, D. Shtol, B. Shwartz, A. Sibidanov, F. Simon, J. Skorupa, R. J. Sobie, M. Sobotzik, A. Soffer, A. Sokolov, E. Solovieva, S. Spataro, B. Spruck, W. Song, M. Starič, P. Stavroulakis, S. Stefkova, R. Stroili, J. Strube, M. Sumihama, K. Sumisawa, N. Suwonjandee, H. Svidras, M. Takizawa, U. Tamponi, K. Tanida, F. Tenchini, A. Thaller, O. Tittel, R. Tiwary, E. Torassa, K. Trabelsi, I. Tsaklidis, I. Ueda, T. Uglov, K. Unger, Y. Unno, K. Uno, S. Uno, P. Urquijo, Y. Ushiroda, S. E. Vahsen, R. van Tonder, K. E. Varvell, M. Veronesi, A. Vinokurova, V. S. Vismaya, L. Vitale, V. Vobbilisetti, R. Volpe, M. Wakai, S. Wallner, M. -Z. Wang, A. Warburton, M. Watanabe, S. Watanuki, C. Wessel, E. Won, X. P. Xu, B. D. Yabsley, S. Yamada, W. Yan, J. Yelton, J. H. Yin, K. Yoshihara, J. Yuan, Y. Yusa, L. Zani, V. Zhilich, J. S. Zhou, Q. D. Zhou, L. Zhu, R. Žlebčík"
Signatures of prediction during natural listening in MEG data?,"The brain uses contextual information and prior knowledge to anticipate
upcoming content during language comprehension. Recent research has shown
predictive signals can be revealed in pre-onset ECoG activity during
naturalistic narrative listening, by building encoding models based on word
embeddings from Large Language Models (LLMs). Similarly, evidence for
long-range predictive encoding has been observed in fMRI data, where
incorporating embeddings for multiple upcoming words in a narrative improves
alignment with brain activity. This study examines whether similar predictive
information can be detected in MEG, a technique with higher temporal resolution
than fMRI but a lower signal-to-noise ratio than ECoG. Our findings indicate
that MEG captures pre-onset representations up to 1 second before word onset,
consistent with ECoG results. However, unlike fMRI findings, incorporating
future word embeddings did not enhance MEG encoding, even for one word into the
future, which suggests that the pre-onset encoding may not reflect predictive
processing. This work demonstrates that MEG combined with LLMs is a valuable
approach for studying language processing in naturalistic narratives and
highlights the need to study further what constitutes evidence for prediction
during natural listening.",2024-12-27T12:49:03Z,http://arxiv.org/abs/2412.19622v1,"Sahel Azizpour, Britta U. Westner, Jakub Szewczyk, Umut Güçlü, Linda Geerligs"
"The Key Steps and Distinct Performance Trends of Pyrrolic vs. Pyridinic
  M-N-C Catalysts in Electrocatalytic Nitrate Reduction","Electrochemical nitrate reduction reaction(NO3RR)offers a sustainable route
for ambient ammonia synthesis. While metal-nitrogen-carbon (M-N-C) single-atom
catalysts have emerged as promising candidates for NO3RR, the
structure-activity relations underlying their catalytic behavior remain to be
elucidated. Through systematic analysis of reported experimental data and
pH-field coupled microkinetic modelling on a reversible hydrogen electrode
(RHE) scale, we reveal that the coordination-dependent activity originates from
distinct scaling relations governed by metal-intermediate interactions.
M-N-Pyrrolic catalysts demonstrate higher turnover frequencies for ammonia
production, whereas M-N-Pyridinic catalysts exhibit broader activity ranges
across the activity volcano plot. Meanwhile, the adsorption and protonation of
nitrate, which is a step often dismissed and/or assumed to be simultaneous in
many previous reports, is identified to be the rate-determining step (RDS) in
NO3RR. Remarkably, our subsequent experimental validation confirms the
theoretical predictions under both neutral and alkaline conditions. This study
offers a comprehensive mechanistic framework for interpreting the
electrocatalytic activity of M-N-C catalysts in NO3RR, showing that a classical
thermodynamic limiting-potential model is not sufficiently accurate to capture
the RDS and the catalytic performance trends of different materials (even on
M-N-Pyrrolic and M-N-Pyridinic catalysts). These findings provide brand new
insights into the reaction mechanism of NO3RR and establish fundamental design
principles for electrocatalytic ammonia synthesis.",2024-12-27T12:23:09Z,http://arxiv.org/abs/2412.19615v1,"Qiuling Jiang, Mingyao Gu, Tianyi Wang, Fangzhou Liu, Xin Yang, Di Zhang, Zhijian Wu, Ying Wang, Li Wei, Hao Li"
Anisotropic Band Flattening in Twisted Bilayer of M-Valley MXenes,"Experimental studies on moir\'e materials have predominantly focused on
twisted hexagonal lattice with low-energy states near the $\Gamma$- or
K-points. These materials, characterized by isotropic low-energy dispersion,
are fundamentally distinct from those with anisotropic properties. Here we
introduce a series of semiconducting transition metal carbides (MXenes)
$M_2$C$T_2$ ($M$ = Ti, Zr, Hf, Sc, Y; $T$ = O, F, Cl) as a novel platform for
M-valley moir\'e materials. Take Ti$_2$CO$_2$ and Zr$_2$CO$_2$ as
representative examples, large-scale \emph{ab initio} calculations show that
their AB-stacked twisted homobilayer features three three-fold rotational
symmetry related M-valleys with time-reserval symmetry and giant anisotropic
band flattening. We derive a simplified moir\'e Hamiltonian for these systems
and conduct a detailed analysis of their band structures, where the origins of
anisotropic band flattening are clearly elucidated. This research broadens the
scope of moir\'e materials, where the valley- and spin-degenerate
two-dimensional array of quasi-one-dimensional system could serve as a
potential platform for realizing many interesting correlated phases.",2024-12-27T12:20:15Z,http://arxiv.org/abs/2412.19613v1,"Kejie Bao, Huan Wang, Zhaochen Liu, jing Wang"
"Machine Generated Product Advertisements: Benchmarking LLMs Against
  Human Performance","This study compares the performance of AI-generated and human-written product
descriptions using a multifaceted evaluation model. We analyze descriptions for
100 products generated by four AI models (Gemma 2B, LLAMA, GPT2, and ChatGPT 4)
with and without sample descriptions, against human-written descriptions. Our
evaluation metrics include sentiment, readability, persuasiveness, Search
Engine Optimization(SEO), clarity, emotional appeal, and call-to-action
effectiveness. The results indicate that ChatGPT 4 performs the best. In
contrast, other models demonstrate significant shortcomings, producing
incoherent and illogical output that lacks logical structure and contextual
relevance. These models struggle to maintain focus on the product being
described, resulting in disjointed sentences that do not convey meaningful
information. This research provides insights into the current capabilities and
limitations of AI in the creation of content for e-Commerce.",2024-12-27T12:11:50Z,http://arxiv.org/abs/2412.19610v1,Sanjukta Ghosh
Infinitary combinatorics in condensed math and strong homology,"Recent advances in our understanding of higher derived limits carry multiple
implications in the fields of condensed and pyknotic mathematics, as well as
for the study of strong homology. These implications are thematically diverse,
pertaining, for example, to the sheaf theory of extremally disconnected spaces,
to Banach--Smith duality, to the productivity of compact projective condensed
anima, and to the structure of the derived category of condensed abelian
groups. Underlying each of these implications are the combinatorics of
multidimensionally coherent families of functions of small infinite cardinal
height, and it is for this reason that we convene accounts of them together
herein.",2024-12-27T12:03:05Z,http://arxiv.org/abs/2412.19605v1,"Jeffrey Bergfalk, Chris Lambie-Hanson"
Super-bath Quantum Eigensolver,"Simulating the dynamics of a system coupled to a suitable environment is a
promising approach in quantum computing for determining the ground state of
physical systems. However, this approach requires not only the
$\textit{existence}$ of an environment that allows the system to dissipate
energy and evolve to its ground state, but also the environment's
characteristics to be $\textit{known}$ in detail. In this paper, we propose an
algorithm with a sufficient condition for achieving polynomial-time complexity
in ground state preparation: the existence of an environment that enables the
system to evolve to its ground state in polynomial time, while such
environment's details may remain $\textit{unknown}$. The proposed algorithm is
Super-bath Quantum Eigensolver, which solves the system's ground state by
utilizing quasi-steady state preparation and simulating the coupling between
the system and the super-bath. Supported by experimental lifetime data of
nuclear metastable states, we suggest that our algorithm is applicable to
determine nuclear ground states in polynomial time. These results highlight the
potential advantage of quantum computing in addressing ground state problems in
real-world physical systems.",2024-12-27T11:46:47Z,http://arxiv.org/abs/2412.19599v1,"Tianren Wang, Zongkang Zhang, Bing-Nan Lu, Mauro Cirio, Ying Li"
Composite nature of the $T_{cc}$ state,"In 2021, LHCb collaboration reported a very narrow state in the $D^0D^0\pi^+$
mass spectrum just below the $D^{*+}D^0$ mass threshold. We consider the
influence of the Castillejo-Dalitz-Dyson (CDD) pole in the scattering amplitude
to derive a general treatment for the two-body final state interaction near its
threshold. The line shape (or the energy dependent event distribution) are then
obtained, where the parameters can be fixed by fitting to the experimental data
on the $D^0D^0\pi^+$ mass spectrum. Within our method the data are quite well
reproduced. The pole structure in the complex energy plane indicates the bound
state structure of the $T_{cc}$ state. The compositeness as a measure of
molecule component in its hadron wave function is predicted to be
$0.23_{-0.09}^{+0.40}$. The non-molecular component, e.g., the compact
tetraquark also takes a non-negligible portion.",2024-12-27T11:42:44Z,http://arxiv.org/abs/2412.19597v1,"Xian-Wei Kang, Wen-Shuo Ding"
"SocRATES: Towards Automated Scenario-based Testing of Social Navigation
  Algorithms","Current social navigation methods and benchmarks primarily focus on proxemics
and task efficiency. While these factors are important, qualitative aspects
such as perceptions of a robot's social competence are equally crucial for
successful adoption and integration into human environments. We propose a more
comprehensive evaluation of social navigation through scenario-based testing,
where specific human-robot interaction scenarios can reveal key robot
behaviors. However, creating such scenarios is often labor-intensive and
complex. In this work, we address this challenge by introducing a pipeline that
automates the generation of context-, and location-appropriate social
navigation scenarios, ready for simulation. Our pipeline transforms simple
scenario metadata into detailed textual scenarios, infers pedestrian and robot
trajectories, and simulates pedestrian behaviors, which enables more controlled
evaluation. We leverage the social reasoning and code-generation capabilities
of Large Language Models (LLMs) to streamline scenario generation and
translation. Our experiments show that our pipeline produces realistic
scenarios and significantly improves scenario translation over naive LLM
prompting. Additionally, we present initial feedback from a usability study
with social navigation experts and a case-study demonstrating a scenario-based
evaluation of three navigation algorithms.",2024-12-27T11:33:19Z,http://arxiv.org/abs/2412.19595v1,"Shashank Rao Marpally, Pranav Goyal, Harold Soh"
"Quasicrystal problem -- on rigidity of non-periodic structures from
  statistical mechanics point of view","We present a brief history of quasicrystals and a short introduction to
classical lattice-gas models of interacting particles. We discuss stability of
non-periodic tilings and one-dimensional sequences of symbols seen as ground
states of some hamiltonians. We argue that some sort of homogeneity, the
so-called Strict Boundary Condition, is necessary for stability of non-periodic
ground states against small perturbations of interactions and thermal
fluctuations.",2024-12-27T11:28:02Z,http://arxiv.org/abs/2412.19594v1,Jacek Miȩkisz
A counterexample to a Brenti-Carnevale conjecture,"Recently, F. Brenti put a preprint on the arXiv with several interesting open
problems on Coxeter groups and unimodality. In this note, we refute one of
these conjectures with a counterexample and provide supporting data related to
it. This work serves as an initial step toward further exploration of the
topic.",2024-12-27T11:26:53Z,http://arxiv.org/abs/2412.19593v1,"Nathan Chapelier-Laget, Jean Fromentin"
"Theoretical Investigation of (Zn, Co) co-Doped BaTiO3 for Advanced
  Energy and Photonic Applications","In light of recent advancements in energy technology, there is an urgent need
for lead-free barium titanate (BTO) -based materials that exhibit remarkable
ferroelectric and photoelectric properties. Notwithstanding the considerable
experimental advances, a theoretical understanding from the electron and atomic
perspectives remains elusive. This study employs the generalized gradient
approximation plane wave pseudopotential technique to investigate the
structural, electronic, ferroelectric, and optical properties of (Zn,Co)
co-doped BaTiO3 (BZCT) based on density functional theory. The objective is to
ascertain the extent of performance enhancement and the underlying mechanism of
(Zn,Co) co-doping on barium titanate. Our findings reveal that incorporating
(Zn,Co) into the BaTiO3 lattice significantly augments the tetragonality of the
unit cell. Moreover, the ferroelectric properties are enhanced, with a
spontaneous polarization stronger than that observed in pure BTO, exhibiting
excellent ferroelectricity. The results of the Hubbard+U algorithm indicate
that the band gap of BZCT is reduced. Concurrently, the enhanced ferroelectric
polarization increases the built-in electric field of the material,
facilitating the separation of photogenerated carriers and improving optical
absorption. Consequently, the optical absorption ability and photorefractive
ability are effectively enhanced. BZCT, with its high spontaneous polarization
and outstanding optical properties, can be a promising candidate material in
energy storage and photovoltaics.",2024-12-27T11:26:38Z,http://arxiv.org/abs/2412.19591v1,"Zheng Kang, Mei Wu, Yiyu Feng, Jiahao Li, Jieming Zhang, Haiyi Tian, Ancheng Wang, Yunkai Wu, Xu Wang"
"ViDTA: Enhanced Drug-Target Affinity Prediction via Virtual Graph Nodes
  and Attention-based Feature Fusion","Drug-target interaction is fundamental in understanding how drugs affect
biological systems, and accurately predicting drug-target affinity (DTA) is
vital for drug discovery. Recently, deep learning methods have emerged as a
significant approach for estimating the binding strength between drugs and
target proteins. However, existing methods simply utilize the drug's local
information from molecular topology rather than global information.
Additionally, the features of drugs and proteins are usually fused with a
simple concatenation operation, limiting their effectiveness. To address these
challenges, we proposed ViDTA, an enhanced DTA prediction framework. We
introduce virtual nodes into the Graph Neural Network (GNN)-based drug feature
extraction network, which acts as a global memory to exchange messages more
efficiently. By incorporating virtual graph nodes, we seamlessly integrate
local and global features of drug molecular structures, expanding the GNN's
receptive field. Additionally, we propose an attention-based linear feature
fusion network for better capturing the interaction information between drugs
and proteins. Experimental results evaluated on various benchmarks including
Davis, Metz, and KIBA demonstrate that our proposed ViDTA outperforms the
state-of-the-art baselines.",2024-12-27T11:19:10Z,http://arxiv.org/abs/2412.19589v1,"Minghui Li, Zikang Guo, Yang Wu, Peijin Guo, Yao Shi, Shengshan Hu, Wei Wan, Shengqing Hu"
DAS3R: Dynamics-Aware Gaussian Splatting for Static Scene Reconstruction,"We propose a novel framework for scene decomposition and static background
reconstruction from everyday videos. By integrating the trained motion masks
and modeling the static scene as Gaussian splats with dynamics-aware
optimization, our method achieves more accurate background reconstruction
results than previous works. Our proposed method is termed DAS3R, an
abbreviation for Dynamics-Aware Gaussian Splatting for Static Scene
Reconstruction. Compared to existing methods, DAS3R is more robust in complex
motion scenarios, capable of handling videos where dynamic objects occupy a
significant portion of the scene, and does not require camera pose inputs or
point cloud data from SLAM-based methods. We compared DAS3R against recent
distractor-free approaches on the DAVIS and Sintel datasets; DAS3R demonstrates
enhanced performance and robustness with a margin of more than 2 dB in PSNR.
The project's webpage can be accessed via \url{https://kai422.github.io/DAS3R/}",2024-12-27T10:59:46Z,http://arxiv.org/abs/2412.19584v1,"Kai Xu, Tze Ho Elden Tse, Jizong Peng, Angela Yao"
"A Comparative Study of Machine Unlearning Techniques for Image and Text
  Classification Models","Machine Unlearning has emerged as a critical area in artificial intelligence,
addressing the need to selectively remove learned data from machine learning
models in response to data privacy regulations. This paper provides a
comprehensive comparative analysis of six state-of-theart unlearning techniques
applied to image and text classification tasks. We evaluate their performance,
efficiency, and compliance with regulatory requirements, highlighting their
strengths and limitations in practical scenarios. By systematically analyzing
these methods, we aim to provide insights into their applicability,
challenges,and tradeoffs, fostering advancements in the field of ethical and
adaptable machine learning.",2024-12-27T10:58:55Z,http://arxiv.org/abs/2412.19583v1,"Omar M. Safa, Mahmoud M. Abdelaziz, Mustafa Eltawy, Mohamed Mamdouh, Moamen Gharib, Salaheldin Eltenihy, Nagia M. Ghanem, Mohamed M. Ismail"
"Readout of strongly coupled NV center-pair spin states with deep neural
  networks","Optically addressable electron spin clusters are of interest for quantum
computation, simulation and sensing. However, with interaction length scales of
a few tens of nanometers in the strong coupling regime, they are unresolved in
conventional confocal microscopy, making individual readout problematic. Here
we show that when using a single shot readout technique, collective states of
the combined register space become accessible. By using spin to charge
conversion of the defects we draw the connection between the intricate photon
count statistics with spin state tomography using deep neural networks. This
approach is particularly versatile with further scaling the number of
constituent spins in a cluster due to complexity of the analytical treatment.
We perform a proof of concept measurement of the correlated classical signal,
paving the way for using our technique in realistic applications.",2024-12-27T10:56:04Z,http://arxiv.org/abs/2412.19581v1,"Matthew Joliffe, Vadim Vorobyov, Jörg Wrachtrup"
"Muonium fine structure: theory update, tests of Lorentz violation and
  experimental prospects","We review the status of the QED calculations for the muonium
$2S_{1/2}-2P_{3/2}$ energy interval and provide the updated theoretical value
of $9874.357\pm0.001\,\mathrm{MHz}$. Additionally, we present a model for
probing Lorentz-violating coefficients within the Standard Model Extension
framework using the fine structure measurement in the presence and absence of a
weak external magnetic field, enabling novel tests of CPT and Lorentz symmetry.
Using Monte Carlo simulations, we estimate that a precision of $\sim
10\,\mathrm{kHz}$ on the isolated $2S_{1/2}, F=1 - 2P_{3/2}, F=1$ transition
could be achievable employing Ramsey's separate oscillatory fields (SOF)
technique. Collecting the required statics will become feasible with the
upcoming High-Intensity Muon Beam (HiMB) at the Paul Scherrer Institute (PSI)
in Switzerland. These advancements will enable precise tests of radiative QED
corrections and nuclear self-energy contributions, while also providing tests
of new physics and sensitivity to unconstrained coefficients for Lorentz
violation within the Standard Model Extension framework.",2024-12-27T10:52:57Z,http://arxiv.org/abs/2412.19580v1,"Philipp Blumer, Svenja Geissmann, Arnaldo J. Vargas, Gianluca Janka, Ben Ohayon, Paolo Crivelli"
"Graph-attention-based Casual Discovery with Trust Region-navigated
  Clipping Policy Optimization","In many domains of empirical sciences, discovering the causal structure
within variables remains an indispensable task. Recently, to tackle with
unoriented edges or latent assumptions violation suffered by conventional
methods, researchers formulated a reinforcement learning (RL) procedure for
causal discovery, and equipped REINFORCE algorithm to search for the
best-rewarded directed acyclic graph. The two keys to the overall performance
of the procedure are the robustness of RL methods and the efficient encoding of
variables. However, on the one hand, REINFORCE is prone to local convergence
and unstable performance during training. Neither trust region policy
optimization, being computationally-expensive, nor proximal policy optimization
(PPO), suffering from aggregate constraint deviation, is decent alternative for
combinatory optimization problems with considerable individual subactions. We
propose a trust region-navigated clipping policy optimization method for causal
discovery that guarantees both better search efficiency and steadiness in
policy optimization, in comparison with REINFORCE, PPO and our prioritized
sampling-guided REINFORCE implementation. On the other hand, to boost the
efficient encoding of variables, we propose a refined graph attention encoder
called SDGAT that can grasp more feature information without priori
neighbourhood information. With these improvements, the proposed method
outperforms former RL method in both synthetic and benchmark datasets in terms
of output results and optimization robustness.",2024-12-27T10:50:43Z,http://arxiv.org/abs/2412.19578v1,"Shixuan Liu, Yanghe Feng, Keyu Wu, Guangquan Cheng, Jincai Huang, Zhong Liu"
"Gauging or extending bulk and boundary conformal field theories:
  Application to bulk and domain wall problem in topological matter and their
  descriptions by (mock) modular covariant","We study gauging operations (or group extensions) in (smeared) boundary
conformal field theories (BCFTs) and bulk conformal field theories and their
applications to various phenomena in topologically ordered systems. We apply
the resultant theories to the correspondence between the renormalization group
(RG) flow of CFTs and the classification of topological quantum field theories
in the testable information of general classes of partition functions. One can
obtain the bulk topological properties of $2+1$ dimensional topological ordered
phase corresponding to the massive RG flow of $1+1$ dimensional systems, or
smeared BCFT. We present an obstruction of mass condensation for smeared BCFT
analogous to the Lieb-Shultz-Mattis theorem for noninvertible symmetry. Related
to the bulk topological degeneracies in $2+1$ dimensions and quantum phases in
$1+1$ dimensions we construct a new series of BCFT. We also investigate the
implications of the massless RG flow of $1+1$ dimensional CFT to $2+1$
dimensional topological order which corresponds to the earlier proposal by L.
Kong and H. Zheng in [Nucl. Phys. B 966 (2021), 115384], arXiv:1912.01760
closely related to the integer-spin simple current by Schellekens and
Gato-Rivera. We study the properties of the product of two CFTs connected by
the two kinds of massless flows. The (mock) modular covariants appearing in the
analysis seem to contain new ones. By applying the folding trick to the coupled
model, we provide a general method to solve the gapped and charged domain wall.
One can obtain the general phenomenology of the transportation of anyons
through the domain wall. Our work gives a unified direction for the future
theoretical and numerical studies of the topological phase based on the
established data of classifications of conformal field theories or modular
invariants.",2024-12-27T10:46:30Z,http://arxiv.org/abs/2412.19577v1,Yoshiki Fukusumi
"Superintegrability of the Wilson family of matrix models and moments of
  multivariable orthogonal polynomials","We present new examples of superintegrable matrix/eigenvalue models. These
examples arise as a result of the exploration of the relationship between the
theory of superintegrability and multivariate orthogonal polynomials. The new
superintegrable examples are built upon the multivariate generalizations of the
Meixner-Pollaczek and Wilson polynomials and their respective measures. From
the perspective of multivariate orthogonal polynomials in this work we propose
expressions for (generalized) moments of the respective multi-variable
measures. From the perspective of superintegrability we uncover a couple of new
phenomena such as the deviation from Schur polynomials as the superintegrable
basis without any deformation and new combinatorial structures appearing in the
answers.",2024-12-27T10:39:43Z,http://arxiv.org/abs/2412.19574v1,Victor Mishnyakov
"The possible long-term periodic variability of the extremely luminous
  quasar WISE J090924.01+000211.1","The extremely luminous infrared galaxy (ELIRG), WISE J090924.01+000211.1
(hereafter; WISE J0909+0002, $z=1.87$) is an extraordinary object with a quasar
aspect. This study performs monitoring observations of WISE J0909+0002 with the
105 cm Murikabushi telescope, Okayama and Akeno 50 cm telescopes/MITSuME ($g'$,
$R_{\rm c}$, and $I_{\rm c}$ bands), and the SaCRA 55 cm telescope/MuSaSHI
($r$, $i$, and $z$ bands). We obtain the following results by combining the
UV/optical light curves of the CRTS, Pan-STARRS, and ZTF archive data, and our
observational data: (1) the light curves of WISE J0909+0002 present
quasi-periodic (sinusoidal) oscillations with the rest-frame period of $\sim$
660$-$689 day; (2) the structure functions of WISE J0909+0002 do not show a
damped random walk (DRW) trend; (3) the mock DRW light curves present
periodic-like trend on rare occasions in 10000 simulations; (4) the
relativistic boost scenario is favored, since the relation between variability
amplitude and power-law slope ratio is consistent with the theoretical
prediction of this scenario, and a substantial parameter space exists between
the inclination angles and the black hole mass; (5) the circumbinary disk model
is difficult to explain the spectral energy distribution of our target; (6) the
significant radio flux density of WISE J0909+0002 is not detected from the VLA
FIRST Survey, thus the radio jet precession scenario is ruled out. From our
results, the Doppler boost scenario is likely as a cause of the periodic
variability, consequently the quasi-periodic oscillations in WISE J0909+0002 is
possibly interpreted by a supermassive blackhole binary. Additional
observations to investigate the continuity of the periodic trend would bring
new insights into mechanisms of the quasi-periodic oscillations and/or ELIRGs.",2024-12-27T10:33:11Z,http://arxiv.org/abs/2412.19573v1,"Takashi Horiuchi, Yoshiki Toba, Toru Misawa, Katsuhiro L. Murata, Keisuke Isogai, Yoichi Yatsu, Ichiro Takahashi, Mahito Sasada, Masafumi Niwano, Narikazu Higuchi, Shunsuke Hayatsu, Hibiki Seki, Yumiko Oasa, Rikuto Sato"
"Nonminimally coupled Dark Matter in Clusters of Galaxies: a fully
  comprehensive analysis","In this study, we explore how a non-minimal coupling between dark matter and
gravity can affect the behavior of dark matter in galaxy clusters. We have
considered the case of a disformal coupling, which leads to a modification of
the Poisson equation. Building on an earlier work, we expand the analysis
considering all possible disformal coupling scenarios and employing various
dark matter density profiles. In doing so, we aim to constrain the key
parameter in our model, the characteristic coupling length. To achieve this, we
analyze data from a combination of strong and weak lensing using three
statistical approaches: a single cluster fitting procedure, a joint analysis,
and one with stacked profiles. Our findings show that the coupling length is
typically very small, thus being fully consistent with general relativity,
although with an upper limit at $1\sigma$ which is of the order of $100$ kpc.",2024-12-27T10:12:29Z,http://arxiv.org/abs/2412.19569v1,"Saboura Zamani, Vincenzo Salzano, Dario Bettoni"
Multiplicative Chern insulator,"We study multiplicative Chern insulators (MCIs) as canonical examples of
multiplicative topological phases of matter. Constructing the MCI Bloch
Hamiltonian as a symmetry-protected tensor product of two topologically
non-trivial parent Chern insulators (CIs), we study two-dimensional (2D) MCIs
and introduce 3D mixed MCIs, constructed by requiring the two 2D parent
Hamiltonians share only one momentum component. We study the 2D MCI response to
time reversal symmetric flux insertion, observing a $4\pi$ Aharonov-Bohm
effect, relating these topological states to fractional quantum Hall states via
the effective field theory of the quantum skyrmion Hall effect. As part of this
response, we observe evidence of quantisation of a proposed topological
invariant for compactified many-body states, to a rational number, suggesting
higher-dimensional topology may also be relevant. Finally, we study effects of
bulk perturbations breaking the symmetry-protected tensor product structure of
the child Hamiltonian, finding the MCI evolves adiabatically into a topological
skyrmion phase.",2024-12-27T10:10:13Z,http://arxiv.org/abs/2412.19566v1,"Archi Banerjee, Michał J. Pacholski, Ashley M. Cook"
Effective field theory of the quantum skyrmion Hall effect,"Motivated by phenomenology of myriad recently-identified topologically
non-trivial phases of matter, we introduce effective field theories (EFTs) for
the quantum skyrmion Hall effect (QSkHE). We employ a single, unifying
generalisation for this purpose: in essence, a lowest Landau level projection
defining a non-commutative, fuzzy sphere with position coordinates proportional
to SU(2) generators of matrix representation size $N\times N$, may host an
intrinsically 2+1 dimensional, topologically non-trivial many-body state for
small $N$ as well as large $N$. That is, isospin degrees of freedom associated
with a matrix Lie algebra with $N \times N$ generators potentially encode some
finite number of spatial dimensions for $N\ge 2$, a regime in which isospin has
previously been treated as a label. This statement extends to more general
$p$-branes subjected to severe fuzzification as well as membranes. As a
consequence of this generalisation, systems with $d$ Cartesian spatial
coordinates and isospin degrees of freedom encoding an additional $\delta$
fuzzy coset space coordinates can realise topologically non-trivial states of
intrinsic dimensionality up to $d$+$\delta$+1. We therefore identify gauge
theories with extra fuzzy dimensions generalised to retain dependence upon
gauge fields over fuzzy coset spaces even for severe fuzzification (small $N$),
as EFTs for the QSkHE. We furthermore generalise these EFTs to space manifolds
with local product structure exploiting the dimensional hierarchy of (fuzzy)
spheres. For this purpose, we introduce methods of anisotropic fuzzification
and propose formulating topological invariants on fuzzy coset spaces as
artifacts of projecting matrix Lie algebras to occupied subspaces. Importantly,
we focus on phenomenology indicating the 2+1 D SU(2) gauge theory should be
generalised using this machinery, and serves as a minimal EFT of the QSkHE.",2024-12-27T10:09:48Z,http://arxiv.org/abs/2412.19565v1,"Vinay Patil, Rafael Flores-Calderón, Ashley M. Cook"
Three Theorems on Negami's Planar Cover Conjecture,"A long-standing Conjecture of S. Negami states that a connected graph has a
finite planar cover if and only if it embeds in the projective plane. It is
known that the Conjecture is equivalent to the fact that \emph{the graph
$K_{1,2, 2, 2}$ has no finite planar cover}. We prove three theorems showing
that the graph $K_{1,2, 2, 2}$ admits no planar cover with certain structural
properties, and that the minimal planar cover of $K_{1,2, 2, 2}$ (if it exists)
must be $4$-connected.",2024-12-27T10:01:38Z,http://arxiv.org/abs/2412.19560v1,"Dickson Annor, Yuri Nikolayevsky, Michael Payne"
"Structural Similarity in Deep Features: Image Quality Assessment Robust
  to Geometrically Disparate Reference","Image Quality Assessment (IQA) with references plays an important role in
optimizing and evaluating computer vision tasks. Traditional methods assume
that all pixels of the reference and test images are fully aligned. Such
Aligned-Reference IQA (AR-IQA) approaches fail to address many real-world
problems with various geometric deformations between the two images. Although
significant effort has been made to attack Geometrically-Disparate-Reference
IQA (GDR-IQA) problem, it has been addressed in a task-dependent fashion, for
example, by dedicated designs for image super-resolution and retargeting, or by
assuming the geometric distortions to be small that can be countered by
translation-robust filters or by explicit image registrations. Here we rethink
this problem and propose a unified, non-training-based Deep Structural
Similarity (DeepSSIM) approach to address the above problems in a single
framework, which assesses structural similarity of deep features in a simple
but efficient way and uses an attention calibration strategy to alleviate
attention deviation. The proposed method, without application-specific design,
achieves state-of-the-art performance on AR-IQA datasets and meanwhile shows
strong robustness to various GDR-IQA test cases. Interestingly, our test also
shows the effectiveness of DeepSSIM as an optimization tool for training image
super-resolution, enhancement and restoration, implying an even wider
generalizability. \footnote{Source code will be made public after the review is
completed.",2024-12-27T09:51:23Z,http://arxiv.org/abs/2412.19553v1,"Keke Zhang, Weiling Chen, Tiesong Zhao, Zhou Wang"
Boolean combinations of graphs,"Boolean combinations allow combining given combinatorial objects to obtain
new, potentially more complicated, objects. In this paper, we initiate a
systematic study of this idea applied to graphs. In order to understand
expressive power and limitations of boolean combinations in this context, we
investigate how they affect different combinatorial and structural properties
of graphs, in particular $\chi$-boundedness, as well as characterize the
structure of boolean combinations of graphs from various classes.",2024-12-27T09:47:53Z,http://arxiv.org/abs/2412.19551v1,"Sarosh Adenwalla, Samuel Braunfeld, John Sylvester, Viktor Zamaraev"
Quantiles under ambiguity and risk sharing,"Choquet capacities and integrals are central concepts in decision making
under ambiguity or model uncertainty, pioneered by Schmeidler. Motivated by
risk optimization problems for quantiles under ambiguity, we study the subclass
of Choquet integrals, called Choquet quantiles, which generalizes the usual
(probabilistic) quantiles, also known as Value-at-Risk in finance, from
probabilities to capacities. Choquet quantiles share many features with
probabilistic quantiles, in terms of axiomatic representation, optimization
formulas, and risk sharing. We characterize Choquet quantiles via only one
axiom, called ordinality. We prove that the inf-convolution of Choquet
quantiles is again a Choquet quantile, leading to explicit optimal allocations
in risk sharing problems for quantile agents under ambiguity. A new class of
risk measures, Choquet Expected Shortfall, is introduced, which enjoys most
properties of the coherent risk measure Expected Shortfall. Our theory is
complemented by optimization algorithms, numerical examples, and a stylized
illustration with financial data.",2024-12-27T09:22:19Z,http://arxiv.org/abs/2412.19546v1,"Peng Liu, Tiantian Mao, Ruodu Wang"
"TARGA: Targeted Synthetic Data Generation for Practical Reasoning over
  Structured Data","Semantic parsing, which converts natural language questions into logic forms,
plays a crucial role in reasoning within structured environments. However,
existing methods encounter two significant challenges: reliance on extensive
manually annotated datasets and limited generalization capability to unseen
examples. To tackle these issues, we propose Targeted Synthetic Data Generation
(TARGA), a practical framework that dynamically generates high-relevance
synthetic data without manual annotation. Starting from the pertinent entities
and relations of a given question, we probe for the potential relevant queries
through layer-wise expansion and cross-layer combination. Then we generate
corresponding natural language questions for these constructed queries to
jointly serve as the synthetic demonstrations for in-context learning.
Experiments on multiple knowledge base question answering (KBQA) datasets
demonstrate that TARGA, using only a 7B-parameter model, substantially
outperforms existing non-fine-tuned methods that utilize close-sourced model,
achieving notable improvements in F1 scores on GrailQA(+7.7) and
KBQA-Agent(+12.2). Furthermore, TARGA also exhibits superior sample efficiency,
robustness, and generalization capabilities under non-I.I.D. settings.",2024-12-27T09:16:39Z,http://arxiv.org/abs/2412.19544v1,"Xiang Huang, Jiayu Shen, Shanshan Huang, Sitao Cheng, Xiaxia Wang, Yuzhong Qu"
Diverse Rare Sample Generation with Pretrained GANs,"Deep generative models are proficient in generating realistic data but
struggle with producing rare samples in low density regions due to their
scarcity of training datasets and the mode collapse problem. While recent
methods aim to improve the fidelity of generated samples, they often reduce
diversity and coverage by ignoring rare and novel samples. This study proposes
a novel approach for generating diverse rare samples from high-resolution image
datasets with pretrained GANs. Our method employs gradient-based optimization
of latent vectors within a multi-objective framework and utilizes normalizing
flows for density estimation on the feature space. This enables the generation
of diverse rare images, with controllable parameters for rarity, diversity, and
similarity to a reference image. We demonstrate the effectiveness of our
approach both qualitatively and quantitatively across various datasets and GANs
without retraining or fine-tuning the pretrained GANs.",2024-12-27T09:10:30Z,http://arxiv.org/abs/2412.19543v1,"Subeen Lee, Jiyeon Han, Soyeon Kim, Jaesik Choi"
Interacted Object Grounding in Spatio-Temporal Human-Object Interactions,"Spatio-temporal Human-Object Interaction (ST-HOI) understanding aims at
detecting HOIs from videos, which is crucial for activity understanding.
However, existing whole-body-object interaction video benchmarks overlook the
truth that open-world objects are diverse, that is, they usually provide
limited and predefined object classes. Therefore, we introduce a new open-world
benchmark: Grounding Interacted Objects (GIO) including 1,098 interacted
objects class and 290K interacted object boxes annotation. Accordingly, an
object grounding task is proposed expecting vision systems to discover
interacted objects. Even though today's detectors and grounding methods have
succeeded greatly, they perform unsatisfactorily in localizing diverse and rare
objects in GIO. This profoundly reveals the limitations of current vision
systems and poses a great challenge. Thus, we explore leveraging
spatio-temporal cues to address object grounding and propose a 4D
question-answering framework (4D-QA) to discover interacted objects from
diverse videos. Our method demonstrates significant superiority in extensive
experiments compared to current baselines. Data and code will be publicly
available at https://github.com/DirtyHarryLYL/HAKE-AVA.",2024-12-27T09:08:46Z,http://arxiv.org/abs/2412.19542v1,"Xiaoyang Liu, Boran Wen, Xinpeng Liu, Zizheng Zhou, Hongwei Fan, Cewu Lu, Lizhuang Ma, Yulong Chen, Yong-Lu Li"
On the approximation of spatial convolutions by PDE systems,"This paper considers the approximation problem for the spatial convolution
with a given integral kernel. The approximation to the spatial convolution by
PDE systems has been proposed to eliminate the analytical difficulties caused
by the integrals for the one-dimensional space. In this paper we establish a
PDE system approximation for the spatial convolutions in higher spatial
dimensions. We derive an approximation function for given arbitrary radial
integral kernels by linear sums of the Green functions. In the proof of this
methodology we introduce an appropriate integral transformation to show the
completeness of the basis constructed by the Green functions. From this theory,
it is possible to approximate the nonlocal operator of the convolution type
with any radial integral kernels by the linear sum of the solutions to the PDE
system. Finally, numerical examples for the approximation are demonstrated
using this method.",2024-12-27T09:07:27Z,http://arxiv.org/abs/2412.19539v1,"Hiroshi Ishii, Yoshitaro Tanaka"
"Scalable Hierarchical Reinforcement Learning for Hyper Scale Multi-Robot
  Task Planning","To improve the efficiency of warehousing system and meet huge customer
orders, we aim to solve the challenges of dimension disaster and dynamic
properties in hyper scale multi-robot task planning (MRTP) for robotic mobile
fulfillment system (RMFS). Existing research indicates that hierarchical
reinforcement learning (HRL) is an effective method to reduce these challenges.
Based on that, we construct an efficient multi-stage HRL-based multi-robot task
planner for hyper scale MRTP in RMFS, and the planning process is represented
with a special temporal graph topology. To ensure optimality, the planner is
designed with a centralized architecture, but it also brings the challenges of
scaling up and generalization that require policies to maintain performance for
various unlearned scales and maps. To tackle these difficulties, we first
construct a hierarchical temporal attention network (HTAN) to ensure basic
ability of handling inputs with unfixed lengths, and then design multi-stage
curricula for hierarchical policy learning to further improve the scaling up
and generalization ability while avoiding catastrophic forgetting.
Additionally, we notice that policies with hierarchical structure suffer from
unfair credit assignment that is similar to that in multi-agent reinforcement
learning, inspired of which, we propose a hierarchical reinforcement learning
algorithm with counterfactual rollout baseline to improve learning performance.
Experimental results demonstrate that our planner outperform other
state-of-the-art methods on various MRTP instances in both simulated and
real-world RMFS. Also, our planner can successfully scale up to hyper scale
MRTP instances in RMFS with up to 200 robots and 1000 retrieval racks on
unlearned maps while keeping superior performance over other methods.",2024-12-27T09:07:11Z,http://arxiv.org/abs/2412.19538v1,"Xuan Zhou, Xiang Shi, Lele Zhang, Chen Chen, Hongbo Li, Lin Ma, Fang Deng, Jie Chen"
"Finger in Camera Speaks Everything: Unconstrained Air-Writing for
  Real-World","Air-writing is a challenging task that combines the fields of computer vision
and natural language processing, offering an intuitive and natural approach for
human-computer interaction. However, current air-writing solutions face two
primary challenges: (1) their dependency on complex sensors (e.g., Radar, EEGs
and others) for capturing precise handwritten trajectories, and (2) the absence
of a video-based air-writing dataset that covers a comprehensive vocabulary
range. These limitations impede their practicality in various real-world
scenarios, including the use on devices like iPhones and laptops. To tackle
these challenges, we present the groundbreaking air-writing Chinese character
video dataset (AWCV-100K-UCAS2024), serving as a pioneering benchmark for
video-based air-writing. This dataset captures handwritten trajectories in
various real-world scenarios using commonly accessible RGB cameras, eliminating
the need for complex sensors. AWCV-100K-UCAS2024 includes 8.8 million video
frames, encompassing the complete set of 3,755 characters from the GB2312-80
level-1 set (GB1). Furthermore, we introduce our baseline approach, the
video-based character recognizer (VCRec). VCRec adeptly extracts fingertip
features from sparse visual cues and employs a spatio-temporal sequence module
for analysis. Experimental results showcase the superior performance of VCRec
compared to existing models in recognizing air-written characters, both
quantitatively and qualitatively. This breakthrough paves the way for enhanced
human-computer interaction in real-world contexts. Moreover, our approach
leverages affordable RGB cameras, enabling its applicability in a diverse range
of scenarios. The code and data examples will be made public at
https://github.com/wmeiqi/AWCV.",2024-12-27T09:04:04Z,http://arxiv.org/abs/2412.19537v1,"Meiqi Wu, Kaiqi Huang, Yuanqiang Cai, Shiyu Hu, Yuzhong Zhao, Weiqiang Wang"
"Potential Vector Fields in $\mathbb R^3$ and $α$-Meridional
  Mappings of the Second Kind $(α\in \mathbb R)$","This paper extends approach developed in a recent author's paper on analytic
models of potential fields in inhomogeneous media. New three-dimensional
analytic models of potential vector fields in some layered media are
constructed. Properties of various analytic models in Cartesian and cylindrical
coordinates in $\mathbb R^3$ are compared. The original properties of the
Jacobian matrix $\mathbf{J}(\vec V)$ of potential meridional fields $\vec V$ in
cylindrically layered media, where $\phi( \rho) = \rho^{-\alpha}$ $(\alpha \in
\mathbb R)$, lead to the concept of \emph{$\alpha$-meridional mappings of the
first and second kind}. The concept of \emph{$\alpha$-Meridional functions of
the first and second kind} naturally arises in this way. When $\alpha =1$, the
special concept of \emph{Radially holomorphic functions in $\mathbb R^3$},
introduced by G\""{u}rlebeck, Habetha and Spr\""{o}ssig in 2008, is developed in
more detail. Certain key properties of the radially holomorphic functions $G$
and functions reversed with respect to $G$ are first characterized. Surprising
properties of the radially holomorphic potentials represented by superposition
of the radially holomorphic exponential function $e^{\breve{\beta} x}$
$(\breve{\beta} \in \mathbb R)$ and function reversed with respect to
$e^{\breve{\beta} x}$ are demonstrated explicitly. The basic properties of the
radially holomorphic potential represented by the radially holomorphic
extension of the Joukowski transformation in $\mathbb R^3$ are studied.",2024-12-27T09:03:29Z,http://arxiv.org/abs/2412.19536v1,Dmitry Bryukhov
"StyleRWKV: High-Quality and High-Efficiency Style Transfer with
  RWKV-like Architecture","Style transfer aims to generate a new image preserving the content but with
the artistic representation of the style source. Most of the existing methods
are based on Transformers or diffusion models, however, they suffer from
quadratic computational complexity and high inference time. RWKV, as an
emerging deep sequence models, has shown immense potential for long-context
sequence modeling in NLP tasks. In this work, we present a novel framework
StyleRWKV, to achieve high-quality style transfer with limited memory usage and
linear time complexity. Specifically, we propose a Recurrent WKV (Re-WKV)
attention mechanism, which incorporates bidirectional attention to establish a
global receptive field. Additionally, we develop a Deformable Shifting
(Deform-Shifting) layer that introduces learnable offsets to the sampling grid
of the convolution kernel, allowing tokens to shift flexibly and adaptively
from the region of interest, thereby enhancing the model's ability to capture
local dependencies. Finally, we propose a Skip Scanning (S-Scanning) method
that effectively establishes global contextual dependencies. Extensive
experiments with analysis including qualitative and quantitative evaluations
demonstrate that our approach outperforms state-of-the-art methods in terms of
stylization quality, model complexity, and inference efficiency.",2024-12-27T09:01:15Z,http://arxiv.org/abs/2412.19535v1,"Miaomiao Dai, Qianyu Zhou, Lizhuang Ma"
"Optical probing of fractal and multifractal connection to structural
  disorder in weakly optical disordered media: Application to cancer detection","The light scattering experiment establishes a relationship between refractive
index fluctuations and fractal dimension in weakly scattering tissue-like
media. Based on the box-counting approach, an analytical model is developed and
shows that the fractal dimension has a functional dependency on the structural
disorder or refractive index fluctuation for short-range correlation and
approximately linearly depends on each other for tissue-like media. Several
parametric imaging systems can be connected using this approach. Further,
tissue's weak multifractality optical scattering is explored using the
box-counting method. It is shown that with a functional transformation, the
distribution follows lognormal distributions.",2024-12-27T08:54:29Z,http://arxiv.org/abs/2412.19532v1,"Santanu Maity, Mousa Alrubayan, Ishmael Apachigwao, Dhruvil Solanki, Prabhakar Pradhan"
Is Your Text-to-Image Model Robust to Caption Noise?,"In text-to-image (T2I) generation, a prevalent training technique involves
utilizing Vision Language Models (VLMs) for image re-captioning. Even though
VLMs are known to exhibit hallucination, generating descriptive content that
deviates from the visual reality, the ramifications of such caption
hallucinations on T2I generation performance remain under-explored. Through our
empirical investigation, we first establish a comprehensive dataset comprising
VLM-generated captions, and then systematically analyze how caption
hallucination influences generation outcomes. Our findings reveal that (1) the
disparities in caption quality persistently impact model outputs during
fine-tuning. (2) VLMs confidence scores serve as reliable indicators for
detecting and characterizing noise-related patterns in the data distribution.
(3) even subtle variations in caption fidelity have significant effects on the
quality of learned representations. These findings collectively emphasize the
profound impact of caption quality on model performance and highlight the need
for more sophisticated robust training algorithm in T2I. In response to these
observations, we propose a approach leveraging VLM confidence score to mitigate
caption noise, thereby enhancing the robustness of T2I models against
hallucination in caption.",2024-12-27T08:53:37Z,http://arxiv.org/abs/2412.19531v1,"Weichen Yu, Ziyan Yang, Shanchuan Lin, Qi Zhao, Jianyi Wang, Liangke Gui, Matt Fredrikson, Lu Jiang"
Magnon-Phonon Coupling in Layered Antiferromagnet,"We present a fully analytical model of hybridization between magnon, and
phonons observed experimentally in magneto-Raman scattering in van der Waals
(vdW) antiferromagnets (AFM). Here, the representative material, FePS3, has
been shown to be a quasi-two-dimensional-Ising antiferromagnet, with additional
features of spin-phonon coupling in the Raman spectra emerging below the N\'eel
temperature (TN) of approximately 120 K. Using magneto-Raman spectroscopy as an
optical probe of magnetic structure, we show that one of these Raman-active
modes in the magnetically ordered state is a magnon with a frequency of 3.7 THz
(~ 122 cm-1). In addition, one magnon band and three phonon bands are coupled
via the magneto-elastic coupling evidenced by anti-crossing in the complete
spectra. We consider a simple model involving only in-plane nearest neighbor
exchange couplings (designed to give rise to a similar magnetic structure) and
perpendicular anisotropy in presence of an out-of-plane magnetic field. Exact
diagonalization of the Hamiltonian leads to energy bands which show that the
interaction term gives rise to avoided crossings between the hybridized magnon
and phonon branches. Realizing magnon-phonon coupling in two-dimensional (2D)
AFMs is important for the verification of the theoretical predictions on exotic
quantum transport phenomena like spin-caloritronics, topological magnonics,
etc.",2024-12-27T08:38:33Z,http://arxiv.org/abs/2412.19526v1,"Somsubhra Ghosh, Mainak Palit, Sujan Maity, Subhadeep Datta"
"Attribution for Enhanced Explanation with Transferable Adversarial
  eXploration","The interpretability of deep neural networks is crucial for understanding
model decisions in various applications, including computer vision.
AttEXplore++, an advanced framework built upon AttEXplore, enhances attribution
by incorporating transferable adversarial attack methods such as MIG and GRA,
significantly improving the accuracy and robustness of model explanations. We
conduct extensive experiments on five models, including CNNs (Inception-v3,
ResNet-50, VGG16) and vision transformers (MaxViT-T, ViT-B/16), using the
ImageNet dataset. Our method achieves an average performance improvement of
7.57\% over AttEXplore and 32.62\% compared to other state-of-the-art
interpretability algorithms. Using insertion and deletion scores as evaluation
metrics, we show that adversarial transferability plays a vital role in
enhancing attribution results. Furthermore, we explore the impact of
randomness, perturbation rate, noise amplitude, and diversity probability on
attribution performance, demonstrating that AttEXplore++ provides more stable
and reliable explanations across various models. We release our code at:
https://anonymous.4open.science/r/ATTEXPLOREP-8435/",2024-12-27T08:27:53Z,http://arxiv.org/abs/2412.19523v1,"Zhiyu Zhu, Jiayu Zhang, Zhibo Jin, Huaming Chen, Jianlong Zhou, Fang Chen"
"Exploiting Domain-Specific Parallel Data on Multilingual Language Models
  for Low-resource Language Translation","Neural Machine Translation (NMT) systems built on multilingual
sequence-to-sequence Language Models (msLMs) fail to deliver expected results
when the amount of parallel data for a language, as well as the language's
representation in the model are limited. This restricts the capabilities of
domain-specific NMT systems for low-resource languages (LRLs). As a solution,
parallel data from auxiliary domains can be used either to fine-tune or to
further pre-train the msLM. We present an evaluation of the effectiveness of
these two techniques in the context of domain-specific LRL-NMT. We also explore
the impact of domain divergence on NMT model performance. We recommend several
strategies for utilizing auxiliary parallel data in building domain-specific
NMT models for LRLs.",2024-12-27T08:25:52Z,http://arxiv.org/abs/2412.19522v1,"Surangika Ranathungaa, Shravan Nayak, Shih-Ting Cindy Huang, Yanke Mao, Tong Su, Yun-Hsiang Ray Chan, Songchen Yuan, Anthony Rinaldi, Annie En-Shiun Lee"
"Improved measurements of neutron lifetime with cold neutron beam at
  J-PARC","The ``neutron lifetime puzzle'' arises from the discrepancy between neutron
lifetime measurements obtained using the beam method, which measures decay
products, and the bottle method, which measures the disappearance of neutrons.
To resolve this puzzle, we conducted an experiment using a pulsed cold neutron
beam at J-PARC. In this experiment, the neutron lifetime is determined from the
ratio of neutron decay counts to $^3$He(n,p)$^3$H reactions in a gas detector.
This experiment belongs to the beam method but differs from previous
experiments that measured protons, as it instead detects electrons, enabling
measurements with distinct systematic uncertainties. By enlarging the beam
transport system and reducing systematic uncertainties, we achieved a fivefold
improvement in precision. Analysis of all acquired data yielded a neutron
lifetime of $\tau_{\rm n}=877.2~\pm~1.7_{\rm(stat.)}~^{+4.0}_{-3.6}{}_{\rm
(sys.)}$ s. This result is consistent with bottle method measurements but
exhibits a 2.3$\sigma$ tension with the average value obtained from the
proton-detection-based beam method.",2024-12-27T08:19:54Z,http://arxiv.org/abs/2412.19519v1,"Y. Fuwa, T. Hasegawa, K. Hirota, T. Hoshino, R. Hosokawa, G. Ichikawa, S. Ieki, T. Ino, Y. Iwashita, M. Kitaguchi, R. Kitahara, S. Makise, K. Mishima, T. Mogi, N. Nagakura, H. Oide, H. Okabe, H. Otono, Y. Seki, D. Sekiba, T. Shima, H. E. Shimizu, H. M. Shimizu, N. Sumi, H. Sumino, M. Tanida, H. Uehara, T. Yamada, S. Yamashita, K. Yano, T. Yoshioka"
"Estimation of System Parameters Including Repeated Cross-Sectional Data
  through Emulator-Informed Deep Generative Model","Differential equations (DEs) are crucial for modeling the evolution of
natural or engineered systems. Traditionally, the parameters in DEs are
adjusted to fit data from system observations. However, in fields such as
politics, economics, and biology, available data are often independently
collected at distinct time points from different subjects (i.e., repeated
cross-sectional (RCS) data). Conventional optimization techniques struggle to
accurately estimate DE parameters when RCS data exhibit various
heterogeneities, leading to a significant loss of information. To address this
issue, we propose a new estimation method called the emulator-informed
deep-generative model (EIDGM), designed to handle RCS data. Specifically, EIDGM
integrates a physics-informed neural network-based emulator that immediately
generates DE solutions and a Wasserstein generative adversarial network-based
parameter generator that can effectively mimic the RCS data. We evaluated EIDGM
on exponential growth, logistic population models, and the Lorenz system,
demonstrating its superior ability to accurately capture parameter
distributions. Additionally, we applied EIDGM to an experimental dataset of
Amyloid beta 40 and beta 42, successfully capturing diverse parameter
distribution shapes. This shows that EIDGM can be applied to model a wide range
of systems and extended to uncover the operating principles of systems based on
limited data.",2024-12-27T08:19:23Z,http://arxiv.org/abs/2412.19517v1,"Hyunwoo Cho, Sung Woong Cho, Hyeontae Jo, Hyung Ju Hwang"
"Confidence v.s. Critique: A Decomposition of Self-Correction Capability
  for LLMs","Large Language Models (LLMs) can correct their self-generated responses, but
a decline in accuracy after self-correction is also witnessed. To have a deeper
understanding of self-correction, we endeavor to decompose, evaluate, and
analyze the self-correction behaviors of LLMs. By enumerating and analyzing
answer correctness before and after self-correction, we decompose the
self-correction capability into confidence (being confident to correct answers)
and critique (turning wrong answers to correct) capabilities, and propose two
metrics from a probabilistic perspective to measure these 2 capabilities, along
with another metric for overall self-correction capability evaluation. Based on
our decomposition and evaluation metrics, we conduct extensive experiments and
draw some empirical conclusions. For example, we find different models can
exhibit distinct behaviors: some models are confident while others are more
critical. We also find the trade-off between the two capabilities (i.e.
improving one can lead to a decline in the other) when manipulating model
self-correction behavior by prompts or in-context learning. Further, we find a
simple yet efficient strategy to improve self-correction capability by
transforming Supervision Fine-Tuning (SFT) data format, and our strategy
outperforms vanilla SFT in both capabilities and achieves much higher accuracy
after self-correction. Our code will be publicly available on GitHub.",2024-12-27T08:09:11Z,http://arxiv.org/abs/2412.19513v1,"Zhe Yang, Yichang Zhang, Yudong Wang, Ziyao Xu, Junyang Lin, Zhifang Sui"
Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging,"Fine-tuning large language models (LLMs) for downstream tasks is a widely
adopted approach, but it often leads to safety degradation in safety-aligned
LLMs. Currently, many solutions address this issue by incorporating additional
safety data, which can be impractical in many cases. In this paper, we address
the question: How can we improve downstream task performance while preserving
safety in LLMs without relying on additional safety data? We propose a simple
and effective method that maintains the inherent safety of LLMs while enhancing
their downstream task performance: merging the weights of pre- and
post-fine-tuned safety-aligned models. Experimental results across various
downstream tasks, models, and merging methods demonstrate that this approach
effectively mitigates safety degradation while improving downstream task
performance, offering a practical solution for adapting safety-aligned LLMs.",2024-12-27T08:03:22Z,http://arxiv.org/abs/2412.19512v1,"Hua Farn, Hsuan Su, Shachi H Kumar, Saurav Sahay, Shang-Tse Chen, Hung-yi Lee"
Hybrid Local Causal Discovery,"Local causal discovery aims to learn and distinguish the direct causes and
effects of a target variable from observed data. Existing constraint-based
local causal discovery methods use AND or OR rules in constructing the local
causal skeleton, but using either rule alone is prone to produce cascading
errors in the learned local causal skeleton, and thus impacting the inference
of local causal relationships. On the other hand, directly applying score-based
global causal discovery methods to local causal discovery may randomly return
incorrect results due to the existence of local equivalence classes. To address
the above issues, we propose a Hybrid Local Causal Discovery algorithm, called
HLCD. Specifically, HLCD initially utilizes a constraint-based approach
combined with the OR rule to obtain a candidate skeleton and then employs a
score-based method to eliminate redundant portions in the candidate skeleton.
Furthermore, during the local causal orientation phase, HLCD distinguishes
between V-structures and equivalence classes by comparing the local structure
scores between the two, thereby avoiding orientation interference caused by
local equivalence classes. We conducted extensive experiments with seven
state-of-the-art competitors on 14 benchmark Bayesian network datasets, and the
experimental results demonstrate that HLCD significantly outperforms existing
local causal discovery algorithms.",2024-12-27T07:53:59Z,http://arxiv.org/abs/2412.19507v1,"Zhaolong Ling, Honghui Peng, Yiwen Zhang, Peng Zhou, Xingyu Wu, Kui Yu, Xindong Wu"
"RobotDiffuse: Motion Planning for Redundant Manipulator based on
  Diffusion Model","Redundant manipulators, with their higher Degrees of Freedom (DOFs), offer
enhanced kinematic performance and versatility, making them suitable for
applications like manufacturing, surgical robotics, and human-robot
collaboration. However, motion planning for these manipulators is challenging
due to increased DOFs and complex, dynamic environments. While traditional
motion planning algorithms struggle with high-dimensional spaces, deep
learning-based methods often face instability and inefficiency in complex
tasks. This paper introduces RobotDiffuse, a diffusion model-based approach for
motion planning in redundant manipulators. By integrating physical constraints
with a point cloud encoder and replacing the U-Net structure with an
encoder-only transformer, RobotDiffuse improves the model's ability to capture
temporal dependencies and generate smoother, more coherent motion plans. We
validate the approach using a complex simulator, and release a new dataset with
35M robot poses and 0.14M obstacle avoidance scenarios. Experimental results
demonstrate the effectiveness of RobotDiffuse and the promise of diffusion
models for motion planning tasks. The code can be accessed at
https://github.com/ACRoboT-buaa/RobotDiffuse.",2024-12-27T07:34:54Z,http://arxiv.org/abs/2412.19500v1,"Xiaohan Zhang, Xudong Mou, Rui Wang, Tianyu Wo, Ningbo Gu, Tiejun Wang, Cangbai Xu, Xudong Liu"
Tailoring Robust Quantum Anomalous Hall Effect via Entropy-Engineering,"Development of quantum materials and tailoring of their functional properties
is a fundamental interest in materials science. Here we propose a new design
concept for robust quantum anomalous Hall effect via entropy engineering in 2D
magnets. As a prototypical example, configurational entropy of monolayer
transition metal trihalide VCl$_3$ is manipulated by incorporating four
different transition-metal cations [Ti,Cr,Fe,Co] in the honeycomb structure
made of vanadium, such that all the in-plane mirror symmetries, inversion
and/or roto-inversion are broken. Monolayer VCl$_3$ is a ferromagnetic Dirac
half-metal in which spin-polarized Dirac dispersion at valley momenta is
accompanied by bulk states at the $\Gamma$-point and thus the spin-orbit
interaction driven quantum anomalous Hall phase does not exhibit fully gapped
bulk band dispersion. Entropy-driven bandstructure renormalization, especially
band flattening in combination with red and blue shifts at different momenta of
the Brillouin zone and crystal-field effects, transforms Dirac half-metal to a
Dirac spin gapless semiconductor and leads to a robust quantum anomalous Hall
phase with fully gapped bulk band dispersion, and thus, a purely topological
edge state transport without mixing with dissipative bulk channels. These
findings provide a paradigm to design entropy-driven 2D materials for the
realization of robust quantum anomalous Hall effect and quantum device
applications.",2024-12-27T07:33:57Z,http://arxiv.org/abs/2412.19499v1,"Syeda Amina Shabbir, Frank Fei Yun, Muhammad Nadeem, Xiaolin Wang"
"Multi-P$^2$A: A Multi-perspective Benchmark on Privacy Assessment for
  Large Vision-Language Models","Large Vision-Language Models (LVLMs) exhibit impressive potential across
various tasks but also face significant privacy risks, limiting their practical
applications. Current researches on privacy assessment for LVLMs is limited in
scope, with gaps in both assessment dimensions and privacy categories. To
bridge this gap, we propose Multi-P$^2$A, a comprehensive benchmark for
evaluating the privacy preservation capabilities of LVLMs in terms of privacy
awareness and leakage. Privacy awareness measures the model's ability to
recognize the privacy sensitivity of input data, while privacy leakage assesses
the risk of the model unintentionally disclosing privacy information in its
output. We design a range of sub-tasks to thoroughly evaluate the model's
privacy protection offered by LVLMs. Multi-P$^2$A covers 26 categories of
personal privacy, 15 categories of trade secrets, and 18 categories of state
secrets, totaling 31,962 samples. Based on Multi-P$^2$A, we evaluate the
privacy preservation capabilities of 21 open-source and 2 closed-source LVLMs.
Our results reveal that current LVLMs generally pose a high risk of
facilitating privacy breaches, with vulnerabilities varying across personal
privacy, trade secret, and state secret.",2024-12-27T07:33:39Z,http://arxiv.org/abs/2412.19496v1,"Jie Zhang, Xiangkui Cao, Zhouyu Han, Shiguang Shan, Xilin Chen"
"Disparate Model Performance and Stability in Machine Learning Clinical
  Support for Diabetes and Heart Diseases","Machine Learning (ML) algorithms are vital for supporting clinical
decision-making in biomedical informatics. However, their predictive
performance can vary across demographic groups, often due to the
underrepresentation of historically marginalized populations in training
datasets. The investigation reveals widespread sex- and age-related inequities
in chronic disease datasets and their derived ML models. Thus, a novel
analytical framework is introduced, combining systematic arbitrariness with
traditional metrics like accuracy and data complexity. The analysis of data
from over 25,000 individuals with chronic diseases revealed mild sex-related
disparities, favoring predictive accuracy for males, and significant
age-related differences, with better accuracy for younger patients. Notably,
older patients showed inconsistent predictive accuracy across seven datasets,
linked to higher data complexity and lower model performance. This highlights
that representativeness in training data alone does not guarantee equitable
outcomes, and model arbitrariness must be addressed before deploying models in
clinical settings.",2024-12-27T07:31:14Z,http://arxiv.org/abs/2412.19495v1,"Ioannis Bilionis, Ricardo C. Berrios, Luis Fernandez-Luque, Carlos Castillo"
"Two superconducting thin films systems with potential integration of
  different quantum functionalities","Quantum computation based on superconducting circuits utilizes
superconducting qubits with Josephson tunnel junctions. Engineering
high-coherence qubits requires materials optimization. In this work, we present
two superconducting thin film systems, grown on silicon (Si), and one obtained
from the other via annealing. Cobalt (Co) thin films grown on Si were found to
be superconducting [EPL 131 (2020) 47001]. These films also happen to be a
self-organised hybrid superconductor/ferromagnet/superconductor (S/F/S)
structure. The S/F/S hybrids are important for superconducting $\pi$-qubits
[PRL 95 (2005) 097001] and in quantum information processing. Here we present
our results on the superconductivity of a hybrid Co film followed by the
superconductivity of a CoSi$_2$ film, which was prepared by annealing the Co
film. CoSi$_2$, with its $1/f$ noise about three orders of magnitude smaller
compared to the most commonly used superconductor aluminium (Al), is a
promising material for high-coherence qubits. The hybrid Co film revealed
superconducting transition temperature $T_c$ = 5 K and anisotropy in the upper
critical field between the in-plane and out-of-plane directions. The anisotropy
was of the order of ratio of lateral dimensions to thickness of the
superconducting Co grains, suggesting a quasi-2D nature of superconductivity.
On the other hand, CoSi$_2$ film showed a $T_c$ of 900 mK. In the resistivity
vs. temperature curve, we observe a peak near $T_c$. Magnetic field scan as a
function of $T$ shows a monotonic increase in intensity of this peak with
temperature. The origin of the peak has been explained in terms of parallel
resistive model for the particular measurement configuration. Although our
CoSi$_2$ film contains grain boundaries, we observed a perpendicular critical
field of 15 mT and a critical current density of 3.8x10$^7$ A/m$^2$, comparable
with epitaxial CoSi$_2$ films.",2024-12-27T07:23:20Z,http://arxiv.org/abs/2412.19493v1,"Snehal Mandal, Biplab Biswas, Suvankar Purakait, Anupam Roy, Biswarup Satpati, Indranil Das, B. N. Dev"
"Multi-label Classification using Deep Multi-order Context-aware Kernel
  Networks","Multi-label classification is a challenging task in pattern recognition. Many
deep learning methods have been proposed and largely enhanced classification
performance. However, most of the existing sophisticated methods ignore context
in the models' learning process. Since context may provide additional cues to
the learned models, it may significantly boost classification performances. In
this work, we make full use of context information (namely geometrical
structure of images) in order to learn better context-aware similarities
(a.k.a. kernels) between images. We reformulate context-aware kernel design as
a feed-forward network that outputs explicit kernel mapping features. Our
obtained context-aware kernel network further leverages multiple orders of
patch neighbors within different distances, resulting into a more
discriminating Deep Multi-order Context-aware Kernel Network (DMCKN) for
multi-label classification. We evaluate the proposed method on the challenging
Corel5K and NUS-WIDE benchmarks, and empirical results show that our method
obtains competitive performances against the related state-of-the-art, and both
quantitative and qualitative performances corroborate its effectiveness and
superiority for multi-label image classification.",2024-12-27T07:16:11Z,http://arxiv.org/abs/2412.19491v1,"Mingyuan Jiu, Hailong Zhu, Hichem Sahbi"
User Willingness-aware Sales Talk Dataset,"User willingness is a crucial element in the sales talk process that affects
the achievement of the salesperson's or sales system's objectives. Despite the
importance of user willingness, to the best of our knowledge, no previous study
has addressed the development of automated sales talk dialogue systems that
explicitly consider user willingness. A major barrier is the lack of sales talk
datasets with reliable user willingness data. Thus, in this study, we developed
a user willingness-aware sales talk collection by leveraging the ecological
validity concept, which is discussed in the field of human-computer
interaction. Our approach focused on three types of user willingness essential
in real sales interactions. We created a dialogue environment that closely
resembles real-world scenarios to elicit natural user willingness, with
participants evaluating their willingness at the utterance level from multiple
perspectives. We analyzed the collected data to gain insights into practical
user willingness-aware sales talk strategies. In addition, as a practical
application of the constructed dataset, we developed and evaluated a sales
dialogue system aimed at enhancing the user's intent to purchase.",2024-12-27T07:16:10Z,http://arxiv.org/abs/2412.19490v1,"Asahi Hentona, Jun Baba, Shiki Sato, Reina Akama"
Comprehensive Bayesian Exploration of Froggatt-Nielsen Mechanism,"The Froggatt-Nielsen (FN) mechanism successfully explains the hierarchical
structure of fermion Yukawa couplings by introducing a U(1) flavor symmetry
with distinct charge assignments for different fermion generations. While some
FN charge assignments have been proposed, their evaluation has largely relied
on heuristic approaches. This paper systematically investigates viable FN
charge assignments within the Standard Model, including both the quark and
lepton sectors, using Bayesian statistical analysis. The study explores
scenarios involving both the seesaw mechanism and dimension-five operators for
neutrino mass generation. A comprehensive parameter scan over FN charges
reveals a wide range of charge assignments consistent with observed fermion
masses and mixing angles. Interestingly, negative FN charges and significant
generational differences in charges are found to be viable, contrary to
conventional assumptions. The analysis also compares the seesaw mechanism and
dimension-five operator scenarios, finding no strong preference between them
for optimal charge assignments. Furthermore, predictions for the lightest
neutrino mass and effective Majorana mass relevant for neutrinoless double-beta
decay are presented, highlighting regions of parameter space accessible to
upcoming experiments. Finally, implications for nucleon decay are studied,
demonstrating that different FN charge assignments predict significantly
different nucleon decay lifetimes and branching ratios, providing a potential
experimental probe for FN models.",2024-12-27T06:41:17Z,http://arxiv.org/abs/2412.19484v1,"Masahiro Ibe, Satoshi Shirai, Keiichi Watanabe"
Learning Radiance Fields from a Single Snapshot Compressive Image,"In this paper, we explore the potential of Snapshot Compressive Imaging (SCI)
technique for recovering the underlying 3D scene structure from a single
temporal compressed image. SCI is a cost-effective method that enables the
recording of high-dimensional data, such as hyperspectral or temporal
information, into a single image using low-cost 2D imaging sensors. To achieve
this, a series of specially designed 2D masks are usually employed, reducing
storage and transmission requirements and offering potential privacy
protection. Inspired by this, we take one step further to recover the encoded
3D scene information leveraging powerful 3D scene representation capabilities
of neural radiance fields (NeRF). Specifically, we propose SCINeRF, in which we
formulate the physical imaging process of SCI as part of the training of NeRF,
allowing us to exploit its impressive performance in capturing complex scene
structures. In addition, we further integrate the popular 3D Gaussian Splatting
(3DGS) framework and propose SCISplat to improve 3D scene reconstruction
quality and training/rendering speed by explicitly optimizing point clouds into
3D Gaussian representations. To assess the effectiveness of our method, we
conduct extensive evaluations using both synthetic data and real data captured
by our SCI system. Experimental results demonstrate that our proposed approach
surpasses the state-of-the-art methods in terms of image reconstruction and
novel view synthesis. Moreover, our method also exhibits the ability to render
high frame-rate multi-view consistent images in real time by leveraging SCI and
the rendering capabilities of 3DGS. Codes will be available at:
https://github.com/WU- CVGL/SCISplat.",2024-12-27T06:40:44Z,http://arxiv.org/abs/2412.19483v1,"Yunhao Li, Xiang Liu, Xiaodong Wang, Xin Yuan, Peidong Liu"
"Pre-training, Fine-tuning and Re-ranking: A Three-Stage Framework for
  Legal Question Answering","Legal question answering (QA) has attracted increasing attention from people
seeking legal advice, which aims to retrieve the most applicable answers from a
large-scale database of question-answer pairs. Previous methods mainly use a
dual-encoder architecture to learn dense representations of both questions and
answers. However, these methods could suffer from lacking domain knowledge and
sufficient labeled training data. In this paper, we propose a three-stage
(\underline{p}re-training, \underline{f}ine-tuning and \underline{r}e-ranking)
framework for \underline{l}egal \underline{QA} (called PFR-LQA), which promotes
the fine-grained text representation learning and boosts the performance of
dense retrieval with the dual-encoder architecture. Concretely, we first
conduct domain-specific pre-training on legal questions and answers through a
self-supervised training objective, allowing the pre-trained model to be
adapted to the legal domain. Then, we perform task-specific fine-tuning of the
dual-encoder on legal question-answer pairs by using the supervised learning
objective, leading to a high-quality dual-encoder for the specific downstream
QA task. Finally, we employ a contextual re-ranking objective to further refine
the output representations of questions produced by the document encoder, which
uses contextual similarity to increase the discrepancy between the anchor and
hard negative samples for better question re-ranking. We conduct extensive
experiments on a manually annotated legal QA dataset. Experimental results show
that our PFR-LQA method achieves better performance than the strong competitors
for legal question answering.",2024-12-27T06:33:42Z,http://arxiv.org/abs/2412.19482v1,"Shiwen Ni, Hao Cheng, Min Yang"
Generative Adversarial Network on Motion-Blur Image Restoration,"In everyday life, photographs taken with a camera often suffer from motion
blur due to hand vibrations or sudden movements. This phenomenon can
significantly detract from the quality of the images captured, making it an
interesting challenge to develop a deep learning model that utilizes the
principles of adversarial networks to restore clarity to these blurred pixels.
In this project, we will focus on leveraging Generative Adversarial Networks
(GANs) to effectively deblur images affected by motion blur. A GAN-based
Tensorflow model is defined, training and evaluating by GoPro dataset which
comprises paired street view images featuring both clear and blurred versions.
This adversarial training process between Discriminator and Generator helps to
produce increasingly realistic images over time. Peak Signal-to-Noise Ratio
(PSNR) and Structural Similarity Index Measure (SSIM) are the two evaluation
metrics used to provide quantitative measures of image quality, allowing us to
evaluate the effectiveness of the deblurring process. Mean PSNR in 29.1644 and
mean SSIM in 0.7459 with average 4.6921 seconds deblurring time are achieved in
this project. The blurry pixels are sharper in the output of GAN model shows a
good image restoration effect in real world applications.",2024-12-27T06:12:50Z,http://arxiv.org/abs/2412.19479v1,Zhengdong Li
"An Overview of Machine Learning-Driven Resource Allocation in IoT
  Networks","In the wake of disruptive IoT technologies generating massive amounts of
diverse data, Machine Learning (ML) will play a crucial role in bringing
intelligence to Internet of Things (IoT) networks. This paper provides a
comprehensive analysis of the current state of resource allocation within IoT
networks, focusing specifically on two key categories: Low-Power IoT Networks
and Mobile IoT Networks. We delve into the resource allocation strategies that
are crucial for optimizing network performance and energy efficiency in these
environments. Furthermore, the paper explores the transformative role of
Machine Learning (ML), Deep Learning (DL), and Reinforcement Learning (RL) in
enhancing IoT functionalities. We highlight a range of applications and use
cases where these advanced technologies can significantly improve
decision-making and optimization processes. In addition to the opportunities
presented by ML, DL, and RL, we also address the potential challenges that
organizations may face when implementing these technologies in IoT settings.
These challenges include crucial accuracy, low flexibility and adaptability,
and high computational cost, etc. Finally, the paper identifies promising
avenues for future research, emphasizing the need for innovative solutions to
overcome existing hurdles and improve the integration of ML, DL, and RL into
IoT networks. By providing this holistic perspective, we aim to contribute to
the ongoing discourse on resource allocation strategies and the application of
intelligent technologies in the IoT landscape.",2024-12-27T06:11:28Z,http://arxiv.org/abs/2412.19478v1,Zhengdong Li
"A C-Band Cryogenic GaAs MMIC Low-Noise Amplifier for Quantum
  Applications","Large-scale superconducting quantum computers require massive numbers of
high-performance cryogenic low-noise amplifiers (cryo-LNA) for qubit readout.
Here we present a C-Band monolithic microwave integrated circuit (MMIC)
cryo-LNA for this purpose. This cryo-LNA is based on 150 nm GaAs pseudomorphic
high electron mobility transistor (pHEMT) process and implemented with a
three-stage cascaded architecture, where the first stage adopts careful
impedance match to optimize the noise and return loss. The integration of
negative feedback loops adopted in the second and third-stage enhances the
overall stability. Moreover, the pHEMT-self bias and current multiplexing
circuitry structure facilitate the reduction of power consumption and require
only single bias line. Operating at an ambient temperature of 3.6 K and
consuming 15 mW, the cryo-LNA demonstrates good performance in the C-band,
reaching a 5 K equivalent noise temperature and an average gain of 40 dB. We
further benchmark this cryo-LNA with superconducting qubits, achieving an
average single-shot dispersive readout fidelity of 98.3% without assistance
from a quantum-limited parametric amplifier. The development of GaAs cryo-LNA
diversifies technical support necessary for large-scale quantum applications.",2024-12-27T06:04:01Z,http://arxiv.org/abs/2412.19477v1,"Zechen Guo, Daxiong Sun, Peisheng Huang, Xuandong Sun, Yuefeng Yuan, Jiawei Zhang, Wenhui Huang, Yongqi Liang, Jiawei Qiu, Jiajian Zhang, Ji Chu, Weijie Guo, Ji Jiang, Jingjing Niu, Wenhui Ren, Ziyu Tao, Xiayu Linpeng, Youpeng Zhong, Dapeng Yu"
"Effects of Reynolds number and spatial resolution on the pressure source
  terms in turbulent boundary layers","The increase in wall-pressure fluctuations with increasing friction Reynolds
number ($Re_{\tau}$) of a turbulent boundary layer (TBL) is well known in the
literature. However, very few studies have investigated the
$Re_{\tau}$-variation of the source terms of the pressure fluctuations, which
are solely a function of the spatial velocity gradients within the TBL. This
study quantifies the pressure source terms in a zero-pressure gradient TBL by
utilizing a published direct numerical simulation (DNS; Sillero et al. 2013,
Phys. Fluids) database across 1000 $\lesssim$ $Re_{\tau}$ $\lesssim$ 2000. It
is found that the magnitude of all source terms increases with $Re_{\tau}$
across the entire TBL thickness, with the turbulence-turbulence (non-linear)
interaction terms growing faster than the mean-shear (linear) source terms.
Further, we use the simulation database to mimic the scenario of particle image
velocimetry (PIV) experiments that are typically spatially under-resolved
compared to DNS data. It is used to quantify the effect of spatial resolution
on the accuracy of pressure source terms, which are estimated here for two
common PIV scenarios: (i) planar PIV in the streamwise-wall-normal plane, and
(ii) stereo-PIV in the spanwise-wall-normal plane of a ZPG TBL. This exercise
reveals significant attenuation of all pressure source terms compared to those
estimated from the original DNS, highlighting the challenges of accurately
estimating these source terms in a high $Re_{\tau}$ PIV experiment.",2024-12-27T05:58:49Z,http://arxiv.org/abs/2412.19474v1,"Aditya Agarwal, Rahul Deshpande"
"Knowledge Graph-Based Multi-Agent Path Planning in Dynamic Environments
  using WAITR","This paper addresses the challenge of multi-agent path planning for efficient
data collection in dynamic, uncertain environments, exemplified by autonomous
underwater vehicles (AUVs) navigating the Gulf of Mexico. Traditional greedy
algorithms, though computationally efficient, often fall short in long-term
planning due to their short-sighted nature, missing crucial data collection
opportunities and increasing exposure to hazards. To address these limitations,
we introduce WAITR (Weighted Aggregate Inter-Temporal Reward), a novel
path-planning framework that integrates a knowledge graph with pathlet-based
planning, segmenting the environment into dynamic, speed-adjusted sub-regions
(pathlets). This structure enables coordinated, adaptive planning, as agents
can operate within time-bound regions while dynamically responding to
environmental changes. WAITR's cumulative scoring mechanism balances immediate
data collection with long-term optimization of Points of Interest (POIs),
ensuring safer navigation and comprehensive data coverage. Experimental results
show that WAITR substantially improves POI coverage and reduces exposure to
hazards, achieving up to 27.1\% greater event coverage than traditional greedy
methods.",2024-12-27T05:43:41Z,http://arxiv.org/abs/2412.19469v1,"Ted Edward Holmberg, Elias Ioup, Mahdi Abdelguerfi"
Laws of Quantum Programming,"In this paper, we investigate the fundamental laws of quantum programming. We
extend a comprehensive set of Hoare et al.'s basic laws of classical
programming to the quantum setting. These laws characterise the algebraic
properties of quantum programs, such as the distributivity of sequential
composition over (quantum) if-statements and the unfolding of nested (quantum)
if-statements. At the same time, we clarify some subtle differences between
certain laws of classical programming and their quantum counterparts.
Additionally, we derive a fixpoint characterization of quantum while-loops and
a loop-based realisation of tail recursion in quantum programming. Furthermore,
we establish two normal form theorems: one for quantum circuits and one for
finite quantum programs. The theory in which these laws are established is
formalised in the Coq proof assistant, and all of these laws are mechanically
verified. As an application case of our laws, we present a formal derivation of
the principle of deferred measurements in dynamic quantum circuits.
  We expect that these laws can be utilized in correctness-preserving
transformation, compilation, and automatic code optimization in quantum
programming. In particular, because these laws are formally verified in Coq,
they can be confidently applied in quantum program development.",2024-12-27T05:16:35Z,http://arxiv.org/abs/2412.19463v1,"Mingsheng Ying, Li Zhou, Gilles Barthe"
A Prototype Unit for Image De-raining using Time-Lapse Data,"We address the challenge of single-image de-raining, a task that involves
recovering rain-free background information from a single rain image. While
recent advancements have utilized real-world time-lapse data for training,
enabling the estimation of consistent backgrounds and realistic rain streaks,
these methods often suffer from computational and memory consumption, limiting
their applicability in real-world scenarios. In this paper, we introduce a
novel solution: the Rain Streak Prototype Unit (RsPU). The RsPU efficiently
encodes rain streak-relevant features as real-time prototypes derived from
time-lapse data, eliminating the need for excessive memory resources. Our
de-raining network combines encoder-decoder networks with the RsPU, allowing us
to learn and encapsulate diverse rain streak-relevant features as concise
prototypes, employing an attention-based approach. To ensure the effectiveness
of our approach, we propose a feature prototype loss encompassing cohesion and
divergence components. This loss function captures both the compactness and
diversity aspects of the prototypical rain streak features within the RsPU. Our
method evaluates various de-raining benchmarks, accompanied by comprehensive
ablation studies. We show that it can achieve competitive results in various
rain images compared to state-of-the-art methods.",2024-12-27T05:04:56Z,http://arxiv.org/abs/2412.19459v1,"Jaehoon Cho, Minjung Yoo, Jini Yang, Sunok Kim"
"DriveEditor: A Unified 3D Information-Guided Framework for Controllable
  Object Editing in Driving Scenes","Vision-centric autonomous driving systems require diverse data for robust
training and evaluation, which can be augmented by manipulating object
positions and appearances within existing scene captures. While recent
advancements in diffusion models have shown promise in video editing, their
application to object manipulation in driving scenarios remains challenging due
to imprecise positional control and difficulties in preserving high-fidelity
object appearances. To address these challenges in position and appearance
control, we introduce DriveEditor, a diffusion-based framework for object
editing in driving videos. DriveEditor offers a unified framework for
comprehensive object editing operations, including repositioning, replacement,
deletion, and insertion. These diverse manipulations are all achieved through a
shared set of varying inputs, processed by identical position control and
appearance maintenance modules. The position control module projects the given
3D bounding box while preserving depth information and hierarchically injects
it into the diffusion process, enabling precise control over object position
and orientation. The appearance maintenance module preserves consistent
attributes with a single reference image by employing a three-tiered approach:
low-level detail preservation, high-level semantic maintenance, and the
integration of 3D priors from a novel view synthesis model. Extensive
qualitative and quantitative evaluations on the nuScenes dataset demonstrate
DriveEditor's exceptional fidelity and controllability in generating diverse
driving scene edits, as well as its remarkable ability to facilitate downstream
tasks.",2024-12-27T04:49:36Z,http://arxiv.org/abs/2412.19458v1,"Yiyuan Liang, Zhiying Yan, Liqun Chen, Jiahuan Zhou, Luxin Yan, Sheng Zhong, Xu Zou"
Focusing Image Generation to Mitigate Spurious Correlations,"Instance features in images exhibit spurious correlations with background
features, affecting the training process of deep neural classifiers. This leads
to insufficient attention to instance features by the classifier, resulting in
erroneous classification outcomes. In this paper, we propose a data
augmentation method called Spurious Correlations Guided Synthesis (SCGS) that
mitigates spurious correlations through image generation model. This approach
does not require expensive spurious attribute (group) labels for the training
data and can be widely applied to other debiasing methods. Specifically, SCGS
first identifies the incorrect attention regions of a pre-trained classifier on
the training images, and then uses an image generation model to generate new
training data based on these incorrect attended regions. SCGS increases the
diversity and scale of the dataset to reduce the impact of spurious
correlations on classifiers. Changes in the classifier's attention regions and
experimental results on three different domain datasets demonstrate that this
method is effective in reducing the classifier's reliance on spurious
correlations.",2024-12-27T04:48:56Z,http://arxiv.org/abs/2412.19457v1,"Xuewei Li, Zhenzhen Nie, Mei Yu, Zijian Zhang, Jie Gao, Tianyi Xu, Zhiqiang Liu"
"NijiGAN: Transform What You See into Anime with Contrastive
  Semi-Supervised Learning and Neural Ordinary Differential Equations","Generative AI has transformed the animation industry. Several models have
been developed for image-to-image translation, particularly focusing on
converting real-world images into anime through unpaired translation.
Scenimefy, a notable approach utilizing contrastive learning, achieves high
fidelity anime scene translation by addressing limited paired data through
semi-supervised training. However, it faces limitations due to its reliance on
paired data from a fine-tuned StyleGAN in the anime domain, often producing
low-quality datasets. Additionally, Scenimefy's high parameter architecture
presents opportunities for computational optimization. This research introduces
NijiGAN, a novel model incorporating Neural Ordinary Differential Equations
(NeuralODEs), which offer unique advantages in continuous transformation
modeling compared to traditional residual networks. NijiGAN successfully
transforms real-world scenes into high fidelity anime visuals using half of
Scenimefy's parameters. It employs pseudo-paired data generated through
Scenimefy for supervised training, eliminating dependence on low-quality paired
data and improving the training process. Our comprehensive evaluation includes
ablation studies, qualitative, and quantitative analysis comparing NijiGAN to
similar models. The testing results demonstrate that NijiGAN produces
higher-quality images compared to AnimeGAN, as evidenced by a Mean Opinion
Score (MOS) of 2.192, it surpasses AnimeGAN's MOS of 2.160. Furthermore, our
model achieved a Frechet Inception Distance (FID) score of 58.71, outperforming
Scenimefy's FID score of 60.32. These results demonstrate that NijiGAN achieves
competitive performance against existing state-of-the-arts, especially
Scenimefy as the baseline model.",2024-12-27T04:46:44Z,http://arxiv.org/abs/2412.19455v1,"Kevin Putra Santoso, Anny Yuniarti, Dwiyasa Nakula, Dimas Prihady Setyawan, Adam Haidar Azizi, Jeany Aurellia P. Dewati, Farah Dhia Fadhila, Maria T. Elvara Bumbungan"
"Feature Alignment-Based Knowledge Distillation for Efficient Compression
  of Large Language Models","This study proposes a knowledge distillation algorithm based on large
language models and feature alignment, aiming to effectively transfer the
knowledge of large pre-trained models into lightweight student models, thereby
reducing computational costs while maintaining high model performance.
Different from the traditional soft label distillation method, this method
introduces a multi-layer feature alignment strategy to deeply align the
intermediate features and attention mechanisms of the teacher model and the
student model, maximally retaining the semantic expression ability and context
modeling ability of the teacher model. In terms of method design, a multi-task
loss function is constructed, including feature matching loss, attention
alignment loss, and output distribution matching loss, to ensure multi-level
information transfer through joint optimization. The experiments were
comprehensively evaluated on the GLUE data set and various natural language
processing tasks. The results show that the proposed model performs very close
to the state-of-the-art GPT-4 model in terms of evaluation indicators such as
perplexity, BLEU, ROUGE, and CER. At the same time, it far exceeds baseline
models such as DeBERTa, XLNet, and GPT-3, showing significant performance
improvements and computing efficiency advantages. Research results show that
the feature alignment distillation strategy is an effective model compression
method that can significantly reduce computational overhead and storage
requirements while maintaining model capabilities. Future research can be
further expanded in the directions of self-supervised learning, cross-modal
feature alignment, and multi-task transfer learning to provide more flexible
and efficient solutions for the deployment and optimization of deep learning
models.",2024-12-27T04:37:06Z,http://arxiv.org/abs/2412.19449v1,"Shuo Wang, Chihang Wang, Jia Gao, Zhen Qi, Hongye Zheng, Xiaoxuan Liao"
Gauge symmetry and partially Lagrangian systems,"We consider the classical field theory whose equations of motion follow from
the least action principle, but the class of admissible trajectories is
restricted by differential equations. The key element of the proposed
construction is the complete gauge symmetry of these additional equations . The
unfree variation of the trajectories reduces to the infinitesimal gauge
symmetry transformation of the equations restricting the trajectories. We
explicitly derive the equations that follow from the requirement that this
gauge variation of the action vanishes. The system of equations for conditional
extrema is not Lagrangian as such, but it admits an equivalent Hamiltonian
formulation with a non-canonical Poisson bracket. The bracket is degenerate, in
general. Alternatively, the equations restricting dynamics could be added to
the action with Lagrange multipliers with unrestricted variation of the
original variables. In this case, we would arrive at the Lagrangian equations
for original variables involving Lagrange multipliers and for Lagrange
multipliers themselves. In general, these two methods are not equivalent
because the multipliers can bring extra degrees of freedom compared to the case
of equations derived by unfree variation of the action. We illustrate the
general method with two examples. The first example is the particle in central
field with varying trajectories restricted by equation of conservation of
angular momentum. The phase space gets one more dimension, and there is an
extra conserved quantity $K$ which is responsible for precession of
trajectories. $K=0$ corresponds to the trajectories of usual Lagrangian
dynamics. The second example is the linearized gravity with Einstein-Hilbert
action and the class of varying fields is restricted by linearized Nordstr\""om
equation. This conditional extrema problem is shown to lead to the linearized
Cotton gravity equations.",2024-12-27T04:34:18Z,http://arxiv.org/abs/2412.19447v1,"Simon Lyakhovich, nikit Sinelnikov"
"Comparative Performance Analysis of Quantum Machine Learning
  Architectures for Credit Card Fraud Detection","As financial fraud becomes increasingly complex, effective detection methods
are essential. Quantum Machine Learning (QML) introduces certain capabilities
that may enhance both accuracy and efficiency in this area. This study examines
how different quantum feature map and ansatz configurations affect the
performance of three QML-based classifiers-the Variational Quantum Classifier
(VQC), the Sampler Quantum Neural Network (SQNN), and the Estimator Quantum
Neural Network (EQNN)-when applied to two non-standardized financial fraud
datasets. Different quantum feature map and ansatz configurations are
evaluated, revealing distinct performance patterns. The VQC consistently
demonstrates strong classification results, achieving an F1 score of 0.88,
while the SQNN also delivers promising outcomes. In contrast, the EQNN
struggles to produce robust results, emphasizing the challenges presented by
non-standardized data. These findings highlight the importance of careful model
configuration in QML-based financial fraud detection. By showing how specific
feature maps and ansatz choices influence predictive success, this work guides
researchers and practitioners in refining QML approaches for complex financial
applications.",2024-12-27T04:17:34Z,http://arxiv.org/abs/2412.19441v1,"Mansour El Alami, Nouhaila Innan, Muhammad Shafique, Mohamed Bennai"
"Low-Rank Contextual Reinforcement Learning from Heterogeneous Human
  Feedback","Reinforcement learning from human feedback (RLHF) has become a cornerstone
for aligning large language models with human preferences. However, the
heterogeneity of human feedback, driven by diverse individual contexts and
preferences, poses significant challenges for reward learning. To address this,
we propose a Low-rank Contextual RLHF (LoCo-RLHF) framework that integrates
contextual information to better model heterogeneous feedback while maintaining
computational efficiency. Our approach builds on a contextual preference model,
leveraging the intrinsic low-rank structure of the interaction between user
contexts and query-answer pairs to mitigate the high dimensionality of feature
representations. Furthermore, we address the challenge of distributional shifts
in feedback through our Pessimism in Reduced Subspace (PRS) policy, inspired by
pessimistic offline reinforcement learning techniques. We theoretically
demonstrate that our policy achieves a tighter sub-optimality gap compared to
existing methods. Extensive experiments validate the effectiveness of
LoCo-RLHF, showcasing its superior performance in personalized RLHF settings
and its robustness to distribution shifts.",2024-12-27T04:02:46Z,http://arxiv.org/abs/2412.19436v1,"Seong Jin Lee, Will Wei Sun, Yufeng Liu"
"Residual Feature-Reutilization Inception Network for Image
  Classification","Capturing feature information effectively is of great importance in the field
of computer vision. With the development of convolutional neural networks
(CNNs), concepts like residual connection and multiple scales promote continual
performance gains in diverse deep learning vision tasks. In this paper, we
propose a novel CNN architecture that it consists of residual
feature-reutilization inceptions (ResFRI) or split-residual
feature-reutilization inceptions (Split-ResFRI). And it is composed of four
convolutional combinations of different structures connected by specially
designed information interaction passages, which are utilized to extract
multi-scale feature information and effectively increase the receptive field of
the model. Moreover, according to the network structure designed above,
Split-ResFRI can adjust the segmentation ratio of the input information,
thereby reducing the number of parameters and guaranteeing the model
performance. Specifically, in experiments based on popular vision datasets,
such as CIFAR10 ($97.94$\%), CIFAR100 ($85.91$\%) and Tiny Imagenet
($70.54$\%), we obtain state-of-the-art results compared with other modern
models under the premise that the model size is approximate and no additional
data is used.",2024-12-27T03:55:25Z,http://arxiv.org/abs/2412.19433v1,"Yuanpeng He, Wenjie Song, Lijian Li, Tianxiang Zhan, Wenpin Jiao"
Revisiting PCA for time series reduction in temporal dimension,"Revisiting PCA for Time Series Reduction in Temporal Dimension; Jiaxin Gao,
Wenbo Hu, Yuntian Chen; Deep learning has significantly advanced time series
analysis (TSA), enabling the extraction of complex patterns for tasks like
classification, forecasting, and regression. Although dimensionality reduction
has traditionally focused on the variable space-achieving notable success in
minimizing data redundancy and computational complexity-less attention has been
paid to reducing the temporal dimension. In this study, we revisit Principal
Component Analysis (PCA), a classical dimensionality reduction technique, to
explore its utility in temporal dimension reduction for time series data. It is
generally thought that applying PCA to the temporal dimension would disrupt
temporal dependencies, leading to limited exploration in this area. However,
our theoretical analysis and extensive experiments demonstrate that applying
PCA to sliding series windows not only maintains model performance, but also
enhances computational efficiency. In auto-regressive forecasting, the temporal
structure is partially preserved through windowing, and PCA is applied within
these windows to denoise the time series while retaining their statistical
information. By preprocessing time-series data with PCA, we reduce the temporal
dimensionality before feeding it into TSA models such as Linear, Transformer,
CNN, and RNN architectures. This approach accelerates training and inference
and reduces resource consumption. Notably, PCA improves Informer training and
inference speed by up to 40% and decreases GPU memory usage of TimesNet by 30%,
without sacrificing model accuracy. Comparative analysis against other
reduction methods further highlights the effectiveness of PCA in improving the
efficiency of TSA models.",2024-12-27T03:17:26Z,http://arxiv.org/abs/2412.19423v1,"Jiaxin Gao, Wenbo Hu, Yuntian Chen"
"Gx2Mol: De Novo Generation of Hit-like Molecules from Gene Expression
  Profiles via Deep Learning","De novo generation of hit-like molecules is a challenging task in the drug
discovery process. Most methods in previous studies learn the semantics and
syntax of molecular structures by analyzing molecular graphs or simplified
molecular input line entry system (SMILES) strings; however, they do not take
into account the drug responses of the biological systems consisting of genes
and proteins. In this study we propose a deep generative model, Gx2Mol, which
utilizes gene expression profiles to generate molecular structures with
desirable phenotypes for arbitrary target proteins. In the algorithm, a
variational autoencoder is employed as a feature extractor to learn the latent
feature distribution of the gene expression profiles. Then, a long short-term
memory is leveraged as the chemical generator to produce syntactically valid
SMILES strings that satisfy the feature conditions of the gene expression
profile extracted by the feature extractor. Experimental results and case
studies demonstrate that the proposed Gx2Mol model can produce new molecules
with potential bioactivities and drug-like properties.",2024-12-27T03:16:56Z,http://arxiv.org/abs/2412.19422v1,"Chen Li, Yuki Matsukiyo, Yoshihiro Yamanishi"
"A Matrix Logic Approach to Efficient Frequent Itemset Discovery in Large
  Data Sets","This paper proposes a frequent itemset mining algorithm based on the Boolean
matrix method, aiming to solve the storage and computational bottlenecks of
traditional frequent pattern mining algorithms in high-dimensional and
large-scale transaction databases. By representing the itemsets in the
transaction database as Boolean matrices, the algorithm uses Boolean logic
operations such as AND and OR to efficiently calculate the support of the
itemsets, avoiding the generation and storage of a large number of candidates
itemsets in traditional algorithms. The algorithm recursively mines frequent
itemsets through matrix operations and can flexibly adapt to different data
scales and support thresholds. In the experiment, the public Groceries dataset
was selected, and the running efficiency test and frequent itemset mining
effect test were designed to evaluate the algorithm's performance indicators
such as running time, memory usage, and number of frequent itemsets under
different transaction numbers and support thresholds. The experimental results
show that the algorithm can efficiently mine a large number of frequent
itemsets when the support threshold is low, and focus on strong association
rules with high support when the threshold is high. In addition, the changing
trends of running time and memory usage show that the Boolean matrix method can
still maintain good running efficiency when the number of transactions
increases significantly and has high scalability and robustness. Future
research can improve memory optimization and matrix block operations, and
combine distributed computing and deep learning models to further enhance the
algorithm's applicability and real-time processing capabilities in
ultra-large-scale data environments. The algorithm has broad application
potential and development prospects in the fields of market analysis,
recommendation systems, and network security.",2024-12-27T03:13:13Z,http://arxiv.org/abs/2412.19420v1,"Xuan Li, Tingyi Ruan, Yankaiqi Li, Quanchao Lu, Xiaoxuan Sun"
KALAHash: Knowledge-Anchored Low-Resource Adaptation for Deep Hashing,"Deep hashing has been widely used for large-scale approximate nearest
neighbor search due to its storage and search efficiency. However, existing
deep hashing methods predominantly rely on abundant training data, leaving the
more challenging scenario of low-resource adaptation for deep hashing
relatively underexplored. This setting involves adapting pre-trained models to
downstream tasks with only an extremely small number of training samples
available. Our preliminary benchmarks reveal that current methods suffer
significant performance degradation due to the distribution shift caused by
limited training samples. To address these challenges, we introduce
Class-Calibration LoRA (CLoRA), a novel plug-and-play approach that dynamically
constructs low-rank adaptation matrices by leveraging class-level textual
knowledge embeddings. CLoRA effectively incorporates prior class knowledge as
anchors, enabling parameter-efficient fine-tuning while maintaining the
original data distribution. Furthermore, we propose Knowledge-Guided Discrete
Optimization (KIDDO), a framework to utilize class knowledge to compensate for
the scarcity of visual information and enhance the discriminability of hash
codes. Extensive experiments demonstrate that our proposed method, Knowledge-
Anchored Low-Resource Adaptation Hashing (KALAHash), significantly boosts
retrieval performance and achieves a 4x data efficiency in low-resource
scenarios.",2024-12-27T03:04:54Z,http://arxiv.org/abs/2412.19417v1,"Shu Zhao, Tan Yu, Xiaoshuai Hao, Wenchao Ma, Vijaykrishnan Narayanan"
DIPS: Optimal Dynamic Index for Poisson $\boldsymbolπ$ps Sampling,"This paper addresses the Poisson $\pi$ps sampling problem, a topic of
significant academic interest in various domains and with practical data mining
applications, such as influence maximization. The problem includes a set
$\mathcal{S}$ of $n$ elements, where each element $v$ is assigned a weight
$w(v)$ reflecting its importance. The goal is to generate a random subset $X$
of $\mathcal{S}$, where each element $v \in \mathcal{S}$ is included in $X$
independently with probability $\frac{c\cdot w(v)}{\sum_{v \in \mathcal{S}}
w(v)}$, where $0&lt;c\leq 1$ is a constant. The subsets must be independent across
different queries. While the Poisson $\pi$ps sampling problem can be reduced to
the well-studied subset sampling problem, updates in Poisson $\pi$ps sampling,
such as adding a new element or removing an element, would cause the
probabilities of all $n$ elements to change in the corresponding subset
sampling problem, making this approach impractical for dynamic scenarios. To
address this, we propose a dynamic index specifically tailored for the Poisson
$\pi$ps sampling problem, supporting optimal expected $\mathcal{O}(1)$ query
time and $\mathcal{O}(1)$ index update time, with an optimal $\mathcal{O}(n)$
space cost. Our solution involves recursively partitioning the set by weights
and ultimately using table lookup. The core of our solution lies in addressing
the challenges posed by weight explosion and correlations between elements.
Empirical evaluations demonstrate that our approach achieves significant
speedups in update time while maintaining consistently competitive query time
compared to the subset-sampling-based methods.",2024-12-27T02:47:44Z,http://arxiv.org/abs/2412.19415v1,"Jinchao Huang, Sibo Wang"
"The Hobby-Eberly Telescope Dark Energy Experiment Survey (HETDEX) Active
  Galactic Nuclei Catalog: the Fourth Data Release","We present the Active Galactic Nuclei (AGN) catalog from the fourth data
release (HDR4) of the Hobby-Eberly Telescope Dark Energy Experiment Survey
(HETDEX). HETDEX is an untargeted spectroscopic survey. HDR4 contains 345,874
Integral Field Unit (IFU) observations from January 2017 to August 2023
covering an effective area of 62.9 deg2. With no imaging pre-selection, our
spectroscopic confirmed AGN sample includes low-luminosity AGN, narrow-line
AGN, and/or red AGN down to g~25. This catalog has 15,940 AGN across the
redshifts of z=0.1~4.6, giving a raw AGN number density of 253.4 deg-2. Among
them, 10,499 (66%) have redshifts either confirmed by line pairs or matched to
the Sloan Digital Sky Survey Quasar Catalog. For the remaining 5,441 AGN, 2,083
are single broad line AGN candidates, while the remaining 3,358 are single
intermediate broad line (full width at half maximum, FWHM ~ 1200 km s-1) AGN
candidates. A total of 4,060 (39%) of the 10,499 redshift-confirmed AGN have
emission-line regions $3\sigma$ more extended than the image quality which
could be strong outflows blowing into the outskirts of the host galaxies or
ionized intergalactic medium.",2024-12-27T02:45:20Z,http://arxiv.org/abs/2412.19414v1,"Chenxu Liu, Karl Gebhardt, Erin Mentuch Cooper, Dustin Davis, Donald P. Schneider, Matt J. Jarvis, Daniel J. Farrow, Steven L. Finkelstein, Oscar A. Chavez Ortiz, The HETDEX Collaboration"
MINIMA: Modality Invariant Image Matching,"Image matching for both cross-view and cross-modality plays a critical role
in multimodal perception. In practice, the modality gap caused by different
imaging systems/styles poses great challenges to the matching task. Existing
works try to extract invariant features for specific modalities and train on
limited datasets, showing poor generalization. In this paper, we present
MINIMA, a unified image matching framework for multiple cross-modal cases.
Without pursuing fancy modules, our MINIMA aims to enhance universal
performance from the perspective of data scaling up. For such purpose, we
propose a simple yet effective data engine that can freely produce a large
dataset containing multiple modalities, rich scenarios, and accurate matching
labels. Specifically, we scale up the modalities from cheap but rich RGB-only
matching data, by means of generative models. Under this setting, the matching
labels and rich diversity of the RGB dataset are well inherited by the
generated multimodal data. Benefiting from this, we construct MD-syn, a new
comprehensive dataset that fills the data gap for general multimodal image
matching. With MD-syn, we can directly train any advanced matching pipeline on
randomly selected modality pairs to obtain cross-modal ability. Extensive
experiments on in-domain and zero-shot matching tasks, including $19$
cross-modal cases, demonstrate that our MINIMA can significantly outperform the
baselines and even surpass modality-specific methods. The dataset and code are
available at https://github.com/LSXI7/MINIMA .",2024-12-27T02:39:50Z,http://arxiv.org/abs/2412.19412v1,"Xingyu Jiang, Jiangwei Ren, Zizhuo Li, Xin Zhou, Dingkang Liang, Xiang Bai"
"An arbitrary order mixed finite element method with boundary value
  correction for the Darcy flow on curved domains","We propose a boundary value correction method for the Brezzi-Douglas-Marini
mixed finite element discretization of the Darcy flow with non-homogeneous
Neumann boundary condition on 2D curved domains. The discretization is defined
on a body-fitted triangular mesh, i.e. the boundary nodes of the mesh lie on
the curved physical boundary. However, the boundary edges of the triangular
mesh, which are straight, may not coincide with the curved physical boundary. A
boundary value correction technique is then designed to transform the Neumann
boundary condition from the physical boundary to the boundary of the triangular
mesh. One advantage of the boundary value correction method is that it avoids
using curved mesh elements and thus reduces the complexity of implementation.
We prove that the proposed method reaches optimal convergence for arbitrary
order discretizations. Supporting numerical results are presented. Key words:
mixed finite element method, Neumann boundary condition, curved domain,
boundary value correction method.",2024-12-27T02:31:14Z,http://arxiv.org/abs/2412.19411v1,"Yongli Hou, Yanqiu Wang"
"MLLM-SUL: Multimodal Large Language Model for Semantic Scene
  Understanding and Localization in Traffic Scenarios","Multimodal large language models (MLLMs) have shown satisfactory effects in
many autonomous driving tasks. In this paper, MLLMs are utilized to solve joint
semantic scene understanding and risk localization tasks, while only relying on
front-view images. In the proposed MLLM-SUL framework, a dual-branch visual
encoder is first designed to extract features from two resolutions, and rich
visual information is conducive to the language model describing risk objects
of different sizes accurately. Then for the language generation, LLaMA model is
fine-tuned to predict scene descriptions, containing the type of driving
scenario, actions of risk objects, and driving intentions and suggestions of
ego-vehicle. Ultimately, a transformer-based network incorporating a regression
token is trained to locate the risk objects. Extensive experiments on the
existing DRAMA-ROLISP dataset and the extended DRAMA-SRIS dataset demonstrate
that our method is efficient, surpassing many state-of-the-art image-based and
video-based methods. Specifically, our method achieves 80.1% BLEU-1 score and
298.5% CIDEr score in the scene understanding task, and 59.6% accuracy in the
localization task. Codes and datasets are available at
https://github.com/fjq-tongji/MLLM-SUL.",2024-12-27T02:05:38Z,http://arxiv.org/abs/2412.19406v1,"Jiaqi Fan, Jianhua Wu, Jincheng Gao, Jianhao Yu, Yafei Wang, Hongqing Chu, Bingzhao Gao"
Spectral-Temporal Fusion Representation for Person-in-Bed Detection,"This study is based on the ICASSP 2025 Signal Processing Grand Challenge's
Accelerometer-Based Person-in-Bed Detection Challenge, which aims to determine
bed occupancy using accelerometer signals. The task is divided into two tracks:
""in bed"" and ""not in bed"" segmented detection, and streaming detection, facing
challenges such as individual differences, posture variations, and external
disturbances. We propose a spectral-temporal fusion-based feature
representation method with mixup data augmentation, and adopt Intersection over
Union (IoU) loss to optimize detection accuracy. In the two tracks, our method
achieved outstanding results of 100.00% and 95.55% in detection scores,
securing first place and third place, respectively.",2024-12-27T02:05:09Z,http://arxiv.org/abs/2412.19404v1,"Xuefeng Yang, Shiheng Zhang, Jian Guan, Feiyang Xiao, Wei Lu, Qiaoxi Zhu"
"Fully Data-driven but Interpretable Human Behavioural Modelling with
  Differentiable Discrete Choice Model","Discrete choice models are essential for modelling various decision-making
processes in human behaviour. However, the specification of these models has
depended heavily on domain knowledge from experts, and the fully automated but
interpretable modelling of complex human behaviours has been a long-standing
challenge. In this paper, we introduce the differentiable discrete choice model
(Diff-DCM), a fully data-driven method for the interpretable modelling,
learning, prediction, and control of complex human behaviours, which is
realised by differentiable programming. Solely from input features and choice
outcomes without any prior knowledge, Diff-DCM can estimate interpretable
closed-form utility functions that reproduce observed behaviours. Comprehensive
experiments with both synthetic and real-world data demonstrate that Diff-DCM
can be applied to various types of data and requires only a small amount of
computational resources for the estimations, which can be completed within tens
of seconds on a laptop without any accelerators. In these experiments, we also
demonstrate that, using its differentiability, Diff-DCM can provide useful
insights into human behaviours, such as an optimal intervention path for
effective behavioural changes. This study provides a strong basis for the fully
automated and reliable modelling, prediction, and control of human behaviours.",2024-12-27T01:53:18Z,http://arxiv.org/abs/2412.19403v1,"Fumiyasu Makinoshima, Tatsuya Mitomi, Fumiya Makihara, Eigo Segawa"
"Joint Optimization of Multimodal Transit Frequency and Shared Autonomous
  Vehicle Fleet Size with Hybrid Metaheuristic and Nonlinear Programming","This paper presents an optimization framework for the joint multimodal
transit frequency and shared autonomous vehicle (SAV) fleet size optimization,
a problem variant of the transit network frequency setting problem (TNFSP) that
explicitly considers mode choice behavior and route selection. To address the
non-linear non-convex optimization problem, we develop a hybrid solution
approach that combines metaheuristics (particle swarm optimization, PSO) with
local nonlinear programming (NLP) improvement, incorporating approximation
models for SAV waiting time, multimodal route choice, and mode choice. Applied
to the Chicago metropolitan area, our method achieves a 33.3% increase in
transit ridership.",2024-12-27T01:33:42Z,http://arxiv.org/abs/2412.19401v1,"Max T. M. Ng, Hani S. Mahmassani, Draco Tong, Omer Verbas, Taner Cokyasar"
"Spin alignment of vector mesons in local equilibrium by Zubarev's
  approach","We compute the $00$ element of the spin density matrix, denoted as
$\rho_{00}$ and called the spin alignment, up to the second order of the
gradient expansion in local equilibrium by Zubarev's approach. In the first
order, we obtain $\rho_{00}=1/3$, meaning that the contributions from thermal
vorticity and shear stress tensor are vanishing. The non-vanishing
contributions to $\rho_{00}-1/3$ appear in the second order of gradients in the
Belinfante and canonical cases. We also discuss the properties of the spin
density matrix under the time reversal transformation. The effective transport
coefficient for the spin alignment induced by the thermal shear stress tensor
is T-odd in the first order, implying that the first order effect is
dissipative.",2024-12-27T01:29:43Z,http://arxiv.org/abs/2412.19400v1,"Shi-Zheng Yang, Xin-Qing Xie, Shi Pu, Jian-Hua Gao, Qun Wang"
"Comparing Few to Rank Many: Active Human Preference Learning using
  Randomized Frank-Wolfe","We study learning of human preferences from a limited comparison feedback.
This task is ubiquitous in machine learning. Its applications such as
reinforcement learning from human feedback, have been transformational. We
formulate this problem as learning a Plackett-Luce model over a universe of $N$
choices from $K$-way comparison feedback, where typically $K \ll N$. Our
solution is the D-optimal design for the Plackett-Luce objective. The design
defines a data logging policy that elicits comparison feedback for a small
collection of optimally chosen points from all ${N \choose K}$ feasible
subsets. The main algorithmic challenge in this work is that even fast methods
for solving D-optimal designs would have $O({N \choose K})$ time complexity. To
address this issue, we propose a randomized Frank-Wolfe (FW) algorithm that
solves the linear maximization sub-problems in the FW method on randomly chosen
variables. We analyze the algorithm, and evaluate it empirically on synthetic
and open-source NLP datasets.",2024-12-27T01:10:17Z,http://arxiv.org/abs/2412.19396v1,"Kiran Koshy Thekumparampil, Gaurush Hiranandani, Kousha Kalantari, Shoham Sabach, Branislav Kveton"
"Two-echelon Electric Vehicle Routing Problem in Parcel Delivery: A
  Literature Review","Multi-echelon parcel delivery systems using electric vehicles (EVs) are
crucial for managing urban logistics complexity and promoting sustainability.
In multi-echelon systems, particularly within two-stage systems, larger
vehicles transport parcels from a central depot to satellite hubs, where
smaller EVs pick up the parcels and carry out last-mile deliveries. This system
could increase efficiency, reduce emissions, and improve service reliability.
The two-echelon electric vehicle routing problem (2E-EVRP), an extension of the
traditional two-echelon vehicle routing problem (2E-VRP), addresses EV-specific
challenges such as battery constraints and recharging stations to tackle
environmental impacts, urban congestion, and e-commerce demands. While
effectively reducing costs, energy use, and emissions, the 2E-EVRP faces
modeling challenges due to multi-echelon structures, EV limitations, and
recharging station selection. This paper systematically reviews 2E-EVRP
literature, analyzing key studies. It proposes a classification scheme to
categorize the papers based on the problem variants, objectives, constraints,
and solution methods. It identifies gaps such as delivery tardiness,
environmental trade-offs, multi-objective optimization, multiple depots, split
deliveries, and time-dependent travel conditions. Future research directions
include aligning models with urban policies, integrating parcel lockers,
enabling same-day delivery, and incorporating advanced technologies like
autonomous vehicles. Methodological advancements suggest using machine
learning, reinforcement learning, and simulation-based approaches to enhance
dynamic routing and real-time decision-making. These directions aim to expand
the 2E-EVRP applicability, addressing theoretical and practical challenges in
sustainable urban logistics for future works.",2024-12-27T01:05:59Z,http://arxiv.org/abs/2412.19395v1,"Nima Moradi, Niloufar Mirzavand Boroujeni, Navid Aftabi, Amin Aslani"
"An In-Depth Analysis of Adversarial Discriminative Domain Adaptation for
  Digit Classification","Domain adaptation is an active area of research driven by the growing demand
for robust machine learning models that perform well on real-world data.
Adversarial learning for deep neural networks (DNNs) has emerged as a promising
approach to improving generalization ability, particularly for image
classification. In this paper, we implement a specific adversarial learning
technique known as Adversarial Discriminative Domain Adaptation (ADDA) and
replicate digit classification experiments from the original ADDA paper. We
extend their findings by examining a broader range of domain shifts and provide
a detailed analysis of in-domain classification accuracy post-ADDA. Our results
demonstrate that ADDA significantly improves accuracy across certain domain
shifts with minimal impact on in-domain performance. Furthermore, we provide
qualitative analysis and propose potential explanations for ADDA's limitations
in less successful domain shifts. Code is at
https://github.com/eugenechoi2004/COS429_FINAL .",2024-12-27T00:36:40Z,http://arxiv.org/abs/2412.19391v1,"Eugene Choi, Julian Rodriguez, Edmund Young"
"A reduced-order framework for temperature estimation in food freezing
  from optimally located sensors, including turbulent conjugate flow scenarios","This article proposes a framework for estimating temperature fields in
food-freezing applications that significantly reduces computational load while
ensuring accurate temperature monitoring, representing a promising
technological tool for optimizing and controlling food engineering processes.
The strategy is based on (i) a mathematical model of a convection-dominated
problem coupling thermal convection and turbulence and (ii) a least-squares
approach for solving the inverse data assimilation problem, regularized by
projecting the governing dynamics onto a reduced-order model (ROM). The
unsteady freezing process considers an idealized salmon slice in a freezer
cabinet, modeled with temperature-dependent thermophysical properties. The
forward problem is approximated using a third-order WENO finite volume solver,
including an optimized second-order backward scheme for time discretization. We
employ our data assimilation framework to reconstruct the temperature field
from a limited number of sensor data and to estimate temperature distributions
within frozen food. Sensor placement is optimized using a new greedy algorithm,
relying on maximizing the observability of the reduced-order dynamics for a
fixed set of sensors. The proposed approach allows efficient extrapolation from
external sensor measurements to the internal temperature of the food, which is
crucial for maintaining food quality.",2024-12-27T00:26:36Z,http://arxiv.org/abs/2412.19387v1,"Felipe Galarce, Diego Rivera, Douglas Pacheco, Alfonso Caiazzo, Ernesto Castillo"
Resolvent-based estimation and control of a laminar airfoil wake,"We develop an optimal resolvent-based estimator and controller to predict and
attenuate unsteady vortex shedding fluctuations in the laminar wake of a NACA
0012 airfoil at an angle of attack of 6.5 degrees, chord-based Reynolds number
of 5000, and Mach number of 0.3. The resolvent-based estimation and control
framework offers several advantages over standard methods. Under equivalent
assumptions, the resolvent-based estimator and controller reproduce the Kalman
filter and LQG controller, respectively, but at substantially lower
computational cost using either an operator-based or data-driven
implementation. Unlike these methods, the resolvent-based approach can
naturally accommodate forcing terms (nonlinear terms from Navier-Stokes) with
colored-in-time statistics, significantly improving estimation accuracy and
control efficacy. Causality is optimally enforced using a Wiener-Hopf
formalism. We integrate these tools into a high-performance-computing-ready
compressible flow solver and demonstrate their effectiveness for estimating and
controlling velocity fluctuations in the wake of the airfoil immersed in clean
and noisy freestreams, the latter of which prevents the flow from falling into
a periodic limit cycle. Using four shear-stress sensors on the surface of the
airfoil, the resolvent-based estimator predicts a series of downstream targets
with approximately 3% and 30% error for the clean and noisy freestream
conditions, respectively. For the latter case, using four actuators on the
airfoil surface, the resolvent-based controller reduces the turbulent kinetic
energy in the wake by 98%.",2024-12-27T00:12:23Z,http://arxiv.org/abs/2412.19386v1,"Junoh Jung, Rutvij Bhagwat, Aaron Towne"
"The Internet of Value: Integrating Blockchain and Lightning Network
  Micropayments for Knowledge Markets","Q&amp;A websites rely on user-generated responses, with incentives such as
reputation scores or monetary rewards often offered. While some users may find
it intrinsically rewarding to assist others, studies indicate that payment can
improve the quality and speed of answers. However, traditional payment
processors impose minimum thresholds that many Q&amp;A inquiries fall below. The
introduction of Bitcoin enabled direct digital value transfer, yet frequent
micropayments remain challenging. Recent advancements like the Lightning
Network now allow frictionless micropayments by reducing costs and minimising
reliance on intermediaries. This development fosters an ""Internet of Value,""
where transferring even small amounts of money is as simple as sharing data.
This study investigates integrating Lightning Network-based micropayment
strategies into Q&amp;A platforms, aiming to create a knowledge market free of
minimum payment barriers. A survey was conducted to address the gap below the
$2 payment level identified in prior research. Responses confirmed that
incentives for asking and answering weaken as payments decrease. Findings
reveal even minimal payments, such as {\pounds}0.01, significantly encourage
higher quality and effort in responses. The study recommends micropayment
incentives for service-oriented applications, particularly Q&amp;A platforms. By
leveraging the Lightning Network to remove barriers, a more open marketplace
can emerge, improving engagement and outcomes. Further research is needed to
confirm if users follow through on reported intentions when spending funds.",2024-12-26T23:57:54Z,http://arxiv.org/abs/2412.19384v1,"Ellis Solaiman, Jorge Robins"
On the Quantum K-theory of Quiver Varieties at Roots of Unity,"Let $\Psi(\mathbb{z},\mathbb{a},q)$ be the fundamental solution matrix of the
quantum difference equation in the equivariant quantum K-theory for Nakajima
variety $X$. In this work, we prove that the operator $$
\Psi(\mathbb{z},\mathbb{a},q)
\Psi\left(\mathbb{z}^p,\mathbb{a}^p,q^{p^2}\right)^{-1} $$ has no poles at the
primitive complex $p$-th roots of unity $\zeta_p$ in the curve counting
parameter $q$. As a byproduct, we show that the eigenvalues of the iterated
product of the operators ${\bf M}_{\mathcal{L}}$ from the quantum difference
equation on $X$ $$ {\bf M}_{\mathcal{L}} (\mathbb{z},\mathbb{a},q) {\bf
M}_{\mathcal{L}} (\mathbb{z} q^{\mathcal{L}},\mathbb{a},q) \cdots {\bf
M}_{\mathcal{L}} (\mathbb{z} q^{(p-1)\mathcal{L}},\mathbb{a},q) $$ evaluated at
$\zeta_p$ are described by the Bethe equations for $X$ in which all variables
are substituted by their $p$-th powers.
  Finally, upon a reduction of the quantum difference equation on $X$ to the
quantum differential equation over the field with finite characteristic, the
above iterated product transforms into a Grothendiek-Katz $p$-curvature of the
corresponding differential connection whereas ${\bf M}_{\mathcal{L}}
(\mathbb{z}^p,\mathbb{a}^p,q^p)$ becomes a certain Frobenius twist of that
connection. In this way, we are reproducing, in part, the statement of a
theorem by Etingof and Varchenko.",2024-12-26T23:52:58Z,http://arxiv.org/abs/2412.19383v1,"Peter Koroteev, Andrey Smirnov"
"Reconstruction of non-trivial magnetization textures from magnetic field
  images using neural networks","Spatial imaging of magnetic stray fields from magnetic materials is a useful
tool for identifying the underlying magnetic configurations of the material.
However, transforming the magnetic image into a magnetization image is an
ill-poised problem, which can result in artefacts that limit the inferences
that can be made on the material under investigation. In this work, we develop
a neural network fitting approach that approximates this transformation,
reducing these artefacts. Additionally, we demonstrate that this approach
allows the inclusion of additional models and bounds that are not possible with
traditional reconstruction methods. These advantages allow for the
reconstruction of non-trivial magnetization textures with varying magnetization
directions in thin-film magnets, which was not possible previously. We
demonstrate this new capability by performing magnetization reconstructions on
a variety of topological spin textures.",2024-12-26T23:50:02Z,http://arxiv.org/abs/2412.19381v1,"David A. Broadway, Mykhailo Flaks, Adrien E. E. Dubois, Patrick Maletinsky"
"Guidelines for Correlative Imaging and Analysis of Reactive Lithium
  Metal Battery Materials","To unlock the full potential of lithium metal batteries, a deep understanding
of lithium metal reactivity and its solid electrolyte interphase is essential.
Correlative imaging, combining focused ion beam and electron microscopy offers
a powerful approach for multi-scale characterization. However, the extreme
reactivity of lithium metal and its SEI presents challenges in investigating
deposition and stripping mechanisms. In this work, we systematically evaluated
the storage stability of lithium metal in glovebox before and after
electrochemical deposition. We then assessed different FIB ion sources for
their impact on lithium metal lamella preparation for transmission electron
microscopy. Furthermore, we examined cryogenic-TEM transfer methods, optimizing
for minimal contamination during sample handling. Contrary to prior
assumptions, we demonstrate that high resolution imaging of pure lithium metal
at room temperature is achievable using inert gas transfer with an electron
dose rate exceeding 1000 e/A2/s, without significant detectable damage. In
contrast, SEI components, such as Li2CO3 and LiF display much greater
sensitivity to electron beams, requiring cryogenic conditions and precise dose
control for nano/atomic scale imaging. We quantified electron dose limits for
these SEI components to track their structural evolution under irradiation.
Based on these findings, we propose a robust protocol for lithium metal sample
handling - from storage to atomic-level characterization - minimizing damage
and contamination. This work paves the way for more accurate and reproducible
studies, accelerating the development of next-generation lithium metal
batteries by ensuing the preservation of native material properties during
analysis.",2024-12-26T23:20:36Z,http://arxiv.org/abs/2412.19376v1,"Shuang Bai, Zhao Liu, Diyi Cheng, Bingyu Lu, Nestor J. Zaluzec, Ganesh Raghavendran, Shen Wang, Thomas S. Marchese, Brandon van Leer, Letian Li, Lin Jiang, Adam Stokes, Joseph P. Cline, Rachel Osmundsen, Paul Barends, Alexander Bright, Minghao Zhang, Ying Shirley Meng"
"Minimal Batch Adaptive Learning Policy Engine for Real-Time Mid-Price
  Forecasting in High-Frequency Trading","High-frequency trading (HFT) has transformed modern financial markets, making
reliable short-term price forecasting models essential. In this study, we
present a novel approach to mid-price forecasting using Level 1 limit order
book (LOB) data from NASDAQ, focusing on 100 U.S. stocks from the S&amp;P 500 index
during the period from September to November 2022. Expanding on our previous
work with Radial Basis Function Neural Networks (RBFNN), which leveraged
automated feature importance techniques based on mean decrease impurity (MDI)
and gradient descent (GD), we introduce the Adaptive Learning Policy Engine
(ALPE) - a reinforcement learning (RL)-based agent designed for batch-free,
immediate mid-price forecasting. ALPE incorporates adaptive epsilon decay to
dynamically balance exploration and exploitation, outperforming a diverse range
of highly effective machine learning (ML) and deep learning (DL) models in
forecasting performance.",2024-12-26T22:49:53Z,http://arxiv.org/abs/2412.19372v1,"Adamantios Ntakaris, Gbenga Ibikunle"
Velocity-dependent self-interacting dark matter and composite Higgs,"We show that the mass of a self-interacting dark matter candidate,
specifically a Dirac fermion, can be generated by composite dynamics, with a
light scalar mediator emerging alongside the Higgs itself as composite
particles. These novel models naturally explain the halo structure problems at
various scales and alleviates the Standard Model naturalness problem
simultaneously. The relic density of the dark matter candidates is particle
anti-particle symmetric and due to thermal freeze-out. These models are
four-dimensional gauge theories with a minimal number of fermions charged under
a new confining gauge group. Finally, we demonstrate that these models satisfy
various constraints set by the dark matter relic density, Big Bang
Nucleosynthesis, Cosmic Microwave Background, as well as direct and indirect
detection experiments.",2024-12-26T22:42:42Z,http://arxiv.org/abs/2412.19371v1,Martin Rosenlyst
The Deconstruction of Flavor in the Privately Democratic Higgs Sector,"The Standard Model (SM) of particle physics fails to explain the observed
hierarchy in fermion masses or the origin of fermion-flavor structure. We
construct a model to explain these observations in the quark sector. We
introduce a spectrum of new particles consisting of six of each -- massive
singlet vector-like quarks (VLQs), singlet scalars, and SU(2)-doublet scalars.
SM quark masses are generated when the neutral components of the SU(2)-doublet
scalars acquire non-zero vacuum expectation values (VEVs). We impose global
symmetries to ensure that Yukawa couplings stay roughly flavor diagonal and
democratic (of the same order), as well as to suppress tree-level
flavor-changing neutral currents. Quark-mass hierarchy then follows from a
hierarchy in scalar VEVs. The singlet scalars also acquire weak-scale VEVs.
Together with the VLQs, they act as messengers between different generations of
quarks in the SM. These messenger particles are responsible for generating the
elements of the Cabibbo-Kobayashi-Masakawa (CKM) matrix which depend on the
ratios of the singlet VEVs and VLQ masses. Constructed this way, the CKM matrix
is found to be \emph{independent} of the SM fermion masses. Using the measured
values of the CKM matrix elements and assuming order-one couplings, we derive
constraints on the masses of the VLQs and discuss prospects for probing our
model in the near future.",2024-12-26T22:32:31Z,http://arxiv.org/abs/2412.19369v1,"Bhubanjyoti Bhattacharya, Suneth Jayawardana, Nausheen R. Shah"
"Variational integrators for stochastic Hamiltonian systems on Lie
  groups: properties and convergence","We derive variational integrators for stochastic Hamiltonian systems on Lie
groups using a discrete version of the stochastic Hamiltonian phase space
principle. The structure-preserving properties of the resulting scheme, such as
symplecticity, preservation of the Lie-Poisson structure, preservation of the
coadjoint orbits, and conservation of Casimir functions, are discussed, along
with a discrete Noether theorem for subgroup symmetries. We also consider in
detail the case of stochastic Hamiltonian systems with advected quantities,
studying the associated structure-preserving properties in relation to
semidirect product Lie groups. A full convergence proof for the scheme is
provided for the case of the Lie group of rotations. Several numerical examples
are presented, including simulations of the free rigid body and the heavy top.",2024-12-26T22:25:37Z,http://arxiv.org/abs/2412.19368v1,"François Gay-Balmaz, Meng Wu"
Large Language Models for Market Research: A Data-augmentation Approach,"Large Language Models (LLMs) have transformed artificial intelligence by
excelling in complex natural language processing tasks. Their ability to
generate human-like text has opened new possibilities for market research,
particularly in conjoint analysis, where understanding consumer preferences is
essential but often resource-intensive. Traditional survey-based methods face
limitations in scalability and cost, making LLM-generated data a promising
alternative. However, while LLMs have the potential to simulate real consumer
behavior, recent studies highlight a significant gap between LLM-generated and
human data, with biases introduced when substituting between the two. In this
paper, we address this gap by proposing a novel statistical data augmentation
approach that efficiently integrates LLM-generated data with real data in
conjoint analysis. Our method leverages transfer learning principles to debias
the LLM-generated data using a small amount of human data. This results in
statistically robust estimators with consistent and asymptotically normal
properties, in contrast to naive approaches that simply substitute human data
with LLM-generated data, which can exacerbate bias. We validate our framework
through an empirical study on COVID-19 vaccine preferences, demonstrating its
superior ability to reduce estimation error and save data and costs by 24.9\%
to 79.8\%. In contrast, naive approaches fail to save data due to the inherent
biases in LLM-generated data compared to human data. Another empirical study on
sports car choices validates the robustness of our results. Our findings
suggest that while LLM-generated data is not a direct substitute for human
responses, it can serve as a valuable complement when used within a robust
statistical framework.",2024-12-26T22:06:29Z,http://arxiv.org/abs/2412.19363v1,"Mengxin Wang, Dennis J. Zhang, Heng Zhang"
"Evaluating Convolutional Neural Networks for COVID-19 classification in
  chest X-ray images","Coronavirus Disease 2019 (COVID-19) pandemic rapidly spread globally,
impacting the lives of billions of people. The effective screening of infected
patients is a critical step to struggle with COVID-19, and treating the
patients avoiding this quickly disease spread. The need for automated and
scalable methods has increased due to the unavailability of accurate automated
toolkits. Recent researches using chest X-ray images suggest they include
relevant information about the COVID-19 virus. Hence, applying machine learning
techniques combined with radiological imaging promises to identify this disease
accurately. It is straightforward to collect these images once it is spreadly
shared and analyzed in the world. This paper presents a method for automatic
COVID-19 detection using chest Xray images through four convolutional neural
networks, namely: AlexNet, VGG-11, SqueezeNet, and DenseNet-121. This method
had been providing accurate diagnostics for positive or negative COVID-19
classification. We validate our experiments using a ten-fold cross-validation
procedure over the training and test sets. Our findings include the shallow
fine-tuning and data augmentation strategies that can assist in dealing with
the low number of positive COVID-19 images publicly available. The accuracy for
all CNNs is higher than 97.00%, and the SqueezeNet model achieved the best
result with 99.20%.",2024-12-26T22:05:30Z,http://arxiv.org/abs/2412.19362v1,"Leonardo Gabriel Ferreira Rodrigues, Danilo Ferreira da Silva, Larissa Ferreira Rodrigues, João Fernando Mari"
Dynamic Skill Adaptation for Large Language Models,"We present Dynamic Skill Adaptation (DSA), an adaptive and dynamic framework
to adapt novel and complex skills to Large Language Models (LLMs). Compared
with previous work which learns from human-curated and static data in random
orders, we propose to first automatically generate and organize the training
data by mimicking the learning pathways of human and then dynamically tailor
the training data based on the training dynamics. Specifically, inspired by the
learning structures and teaching strategies in the human education system, we
first construct a skill graph by decomposing complex skills into sub-skills and
arranging them based on their dependencies in human syllables. For every skill,
we utilize LLMs to generate both textbook-like data which contains detailed
descriptions of skills for pre-training and exercise-like data which targets at
explicitly utilizing the skills to solve problems for instruction-tuning.
Furthermore, during the instruction-tuning, we dynamically update the training
data which down-weight easy-to-learn examples, generate more complex examples,
and filter out data with errors. Experiments on large language models such as
LLAMA and Mistral demonstrate the effectiveness of our proposed methods in
adapting math reasoning skills and social study skills.",2024-12-26T22:04:23Z,http://arxiv.org/abs/2412.19361v1,"Jiaao Chen, Diyi Yang"
"Improving the network traffic classification using the Packet Vision
  approach","The network traffic classification allows improving the management, and the
network services offer taking into account the kind of application. The future
network architectures, mainly mobile networks, foresee intelligent mechanisms
in their architectural frameworks to deliver application-aware network
requirements. The potential of convolutional neural networks capabilities,
widely exploited in several contexts, can be used in network traffic
classification. Thus, it is necessary to develop methods based on the content
of packets transforming it into a suitable input for CNN technologies. Hence,
we implemented and evaluated the Packet Vision, a method capable of building
images from packets raw-data, considering both header and payload. Our approach
excels those found in state-of-the-art by delivering security and privacy by
transforming the raw-data packet into images. Therefore, we built a dataset
with four traffic classes evaluating the performance of three CNNs
architectures: AlexNet, ResNet-18, and SqueezeNet. Experiments showcase the
Packet Vision combined with CNNs applicability and suitability as a promising
approach to deliver outstanding performance in classifying network traffic.",2024-12-26T21:56:03Z,http://arxiv.org/abs/2412.19360v1,"Rodrigo Moreira, Larissa Ferreira Rodrigues, Pedro Frosi Rosa, Flávio de Oliveira Silva"
"Federated Hybrid Training and Self-Adversarial Distillation: Towards
  Robust Edge Networks","Federated learning (FL) is a distributed training technology that enhances
data privacy in mobile edge networks by allowing data owners to collaborate
without transmitting raw data to the edge server. However, data heterogeneity
and adversarial attacks pose challenges to develop an unbiased and robust
global model for edge deployment. To address this, we propose Federated hyBrid
Adversarial training and self-adversarial disTillation (FedBAT), a new
framework designed to improve both robustness and generalization of the global
model. FedBAT seamlessly integrates hybrid adversarial training and
self-adversarial distillation into the conventional FL framework from data
augmentation and feature distillation perspectives. From a data augmentation
perspective, we propose hybrid adversarial training to defend against
adversarial attacks by balancing accuracy and robustness through a weighted
combination of standard and adversarial training. From a feature distillation
perspective, we introduce a novel augmentation-invariant adversarial
distillation method that aligns local adversarial features of augmented images
with their corresponding unbiased global clean features. This alignment can
effectively mitigate bias from data heterogeneity while enhancing both the
robustness and generalization of the global model. Extensive experimental
results across multiple datasets demonstrate that FedBAT yields comparable or
superior performance gains in improving robustness while maintaining accuracy
compared to several baselines.",2024-12-26T21:32:08Z,http://arxiv.org/abs/2412.19354v1,"Yu Qiao, Apurba Adhikary, Kitae Kim, Eui-Nam Huh, Zhu Han, Choong Seon Hong"
ETTA: Elucidating the Design Space of Text-to-Audio Models,"Recent years have seen significant progress in Text-To-Audio (TTA) synthesis,
enabling users to enrich their creative workflows with synthetic audio
generated from natural language prompts. Despite this progress, the effects of
data, model architecture, training objective functions, and sampling strategies
on target benchmarks are not well understood. With the purpose of providing a
holistic understanding of the design space of TTA models, we set up a
large-scale empirical experiment focused on diffusion and flow matching models.
Our contributions include: 1) AF-Synthetic, a large dataset of high quality
synthetic captions obtained from an audio understanding model; 2) a systematic
comparison of different architectural, training, and inference design choices
for TTA models; 3) an analysis of sampling methods and their Pareto curves with
respect to generation quality and inference speed. We leverage the knowledge
obtained from this extensive analysis to propose our best model dubbed
Elucidated Text-To-Audio (ETTA). When evaluated on AudioCaps and MusicCaps,
ETTA provides improvements over the baselines trained on publicly available
data, while being competitive with models trained on proprietary data. Finally,
we show ETTA's improved ability to generate creative audio following complex
and imaginative captions -- a task that is more challenging than current
benchmarks.",2024-12-26T21:13:12Z,http://arxiv.org/abs/2412.19351v1,"Sang-gil Lee, Zhifeng Kong, Arushi Goel, Sungwon Kim, Rafael Valle, Bryan Catanzaro"
"On the Expressiveness and Length Generalization of Selective State-Space
  Models on Regular Languages","Selective state-space models (SSMs) are an emerging alternative to the
Transformer, offering the unique advantage of parallel training and sequential
inference. Although these models have shown promising performance on a variety
of tasks, their formal expressiveness and length generalization properties
remain underexplored. In this work, we provide insight into the workings of
selective SSMs by analyzing their expressiveness and length generalization
performance on regular language tasks, i.e., finite-state automaton (FSA)
emulation. We address certain limitations of modern SSM-based architectures by
introducing the Selective Dense State-Space Model (SD-SSM), the first selective
SSM that exhibits perfect length generalization on a set of various regular
language tasks using a single layer. It utilizes a dictionary of dense
transition matrices, a softmax selection mechanism that creates a convex
combination of dictionary matrices at each time step, and a readout consisting
of layer normalization followed by a linear map. We then proceed to evaluate
variants of diagonal selective SSMs by considering their empirical performance
on commutative and non-commutative automata. We explain the experimental
results with theoretical considerations. Our code is available at
https://github.com/IBM/selective-dense-state-space-model.",2024-12-26T20:53:04Z,http://arxiv.org/abs/2412.19350v1,"Aleksandar Terzić, Michael Hersche, Giacomo Camposampiero, Thomas Hofmann, Abu Sebastian, Abbas Rahimi"
"Semi-Supervised Learning from Small Annotated Data and Large Unlabeled
  Data for Fine-grained PICO Entity Recognition","Objective: Extracting PICO elements -- Participants, Intervention,
Comparison, and Outcomes -- from clinical trial literature is essential for
clinical evidence retrieval, appraisal, and synthesis. Existing approaches do
not distinguish the attributes of PICO entities. This study aims to develop a
named entity recognition (NER) model to extract PICO entities with fine
granularities.
  Materials and Methods: Using a corpus of 2,511 abstracts with PICO mentions
from 4 public datasets, we developed a semi-supervised method to facilitate the
training of a NER model, FinePICO, by combining limited annotated data of PICO
entities and abundant unlabeled data. For evaluation, we divided the entire
dataset into two subsets: a smaller group with annotations and a larger group
without annotations. We then established the theoretical lower and upper
performance bounds based on the performance of supervised learning models
trained solely on the small, annotated subset and on the entire set with
complete annotations, respectively. Finally, we evaluated FinePICO on both the
smaller annotated subset and the larger, initially unannotated subset. We
measured the performance of FinePICO using precision, recall, and F1.
  Results: Our method achieved precision/recall/F1 of 0.567/0.636/0.60,
respectively, using a small set of annotated samples, outperforming the
baseline model (F1: 0.437) by more than 16\%. The model demonstrates
generalizability to a different PICO framework and to another corpus, which
consistently outperforms the benchmark in diverse experimental settings
(p-value \textless0.001).
  Conclusion: This study contributes a generalizable and effective
semi-supervised approach to named entity recognition leveraging large unlabeled
data together with small, annotated data. It also initially supports
fine-grained PICO extraction.",2024-12-26T20:24:35Z,http://arxiv.org/abs/2412.19346v1,"Fangyi Chen, Gongbo Zhang, Yilu Fang, Yifan Peng, Chunhua Weng"
Advanced Scheduling of Electrolyzer Modules for Grid Flexibility,"As the transition to sustainable power generation progresses, green hydrogen
production via electrolysis is expected to gain importance as a means for
energy storage and flexible load to complement variable renewable generation.
With the increasing need for cost-effective and efficient hydrogen production,
electrolyzer optimization is essential to improve both energy efficiency and
profitability. This paper analyzes how the efficiency and modular setup of
alkaline hydrogen electrolyzers can improve hydrogen output of systems linked
to a fluctuating renewable power supply. To explore this, we propose a
day-ahead optimal scheduling problem of a hybrid wind and electrolyzer system.
The novelty of our approach lies in modeling the number and capacity of
electrolyzer modules, and capturing the modules' impact on the hydrogen
production and efficiency. We solve the resulting mixed-integer optimization
problem with several different combinations of number of modules, efficiency
and operating range parameters, using day-ahead market data from a wind farm
generator in the ERCOT system as an input. Our results demonstrate that the
proposed approach ensures that electrolyzer owners can better optimize the
operation of their systems, achieving greater hydrogen production and higher
revenue. Key findings include that as the number of modules in a system with
the same overall capacity increases, hydrogen production and revenue increases.",2024-12-26T20:24:01Z,http://arxiv.org/abs/2412.19345v1,"Angelina Lesniak, Andrea Gloppen Johnsen, Noah Rhodes, Line Roald"
"Sparse recovery from quadratic equations, part II: hardness and
  incoherence","We study the square root bottleneck in the recovery of sparse vectors from
quadratic equations. It is acknowledged that a sparse vector $ \mathbf x_0\in
\mathbb{R}^n$, $\| \mathbf x_0\|_0 = k$ can in theory be recovered from as few
as $O(k)$ generic quadratic equations but no polynomial time algorithm is known
for this task unless $m = \Omega(k^2)$. This bottleneck was in fact shown in
previous work to be essentially related to the initialization of descent
algorithms. Starting such algorithms sufficiently close to the planted signal
is known to imply convergence to this signal. In this paper, we show that as
soon as $m\gtrsim \mu_0^{-2}k \vee \mu_0^{-4}$ (up to log factors) where $\mu_0
= \| \mathbf x_0\|_\infty/\| \mathbf x_0\|_2$, it is possible to recover a
$k$-sparse vector $ \mathbf x_0\in \mathbb{R}^n$ from $m$ quadratic equations
of the form $\langle \mathbf A_i, \mathbf x \mathbf x^\intercal\rangle =
\langle \mathbf A_i, \mathbf x_0 \mathbf x_0^\intercal\rangle + \varepsilon_i $
by minimizing the classical empirical loss. The proof idea carries over to the
phase retrieval setting for which it provides an original initialization that
matches the current optimal sample complexity (see e.g. [Cai 2023]). In the
maximally incoherent regime $\mu_0^{-2}=k$, and for $m=o(k^2)$ we provide
evidence for topological hardness by showing that a property known as the
Overlap Gap Property (OGP), which originated in spin glass theory and is
conjectured to be indicative of algorithmic intractability when optimizing over
random structures, holds for a particular level of overparametrization. The key
ingredient of the proof is a lower bound on the tail of chi-squared random
variables which follows from the theory of moderate deviations.",2024-12-26T20:11:44Z,http://arxiv.org/abs/2412.19341v1,Augustin Cosse
Moduli spaces of polynomial maps and multipliers at small cycles,"Fix an integer $d \geq 2$. The space $\mathcal{P}_{d}$ of polynomial maps of
degree $d$ modulo conjugation by affine transformations is naturally an affine
variety over $\mathbb{Q}$ of dimension $d -1$. For each integer $P \geq 1$, the
elementary symmetric functions of the multipliers at all the cycles with period
$p \in \lbrace 1, \dotsc, P \rbrace$ induce a natural morphism
$\operatorname{Mult}_{d}^{(P)}$ defined on $\mathcal{P}_{d}$. In this article,
we show that the morphism $\operatorname{Mult}_{d}^{(2)}$ induced by the
multipliers at the cycles with periods $1$ and $2$ is both finite and
birational onto its image. In the case of polynomial maps, this strengthens
results by McMullen and by Ji and Xie stating that
$\operatorname{Mult}_{d}^{(P)}$ is quasifinite and birational onto its image
for all sufficiently large integers $P$. Our result arises as the combination
of the following two statements: $\mathord{\bullet}$ A sequence of polynomials
over $\mathbb{C}$ of degree $d$ with bounded multipliers at its cycles with
periods $1$ and $2$ is necessarily bounded in $\mathcal{P}_{d}(\mathbb{C})$.
$\mathord{\bullet}$ A generic conjugacy class of polynomials over $\mathbb{C}$
of degree $d$ is uniquely determined by its multipliers at its cycles with
periods $1$ and $2$.",2024-12-26T19:13:17Z,http://arxiv.org/abs/2412.19335v1,Valentin Huguin
"DPmoire: A tool for constructing accurate machine learning force fields
  in moiré systems","In moir\'e systems, the impact of lattice relaxation on electronic band
structures is significant, yet the computational demands of first-principles
relaxation are prohibitively high due to the large number of atoms involved. To
address this challenge, We introduce a robust methodology for the construction
of machine learning potentials specifically tailored for moir\'e structures and
present an open-source software package DPmoire designed to facilitate this
process. Utilizing this package, we have developed machine learning force
fields (MLFFs) for MX$_2$ (M = Mo, W; X = S, Se, Te) materials. Our approach
not only streamlines the computational process but also ensures accurate
replication of the detailed electronic and structural properties typically
observed in density functional theory (DFT) relaxations. The MLFFs were
rigorously validated against standard DFT results, confirming their efficacy in
capturing the complex interplay of atomic interactions within these layered
materials. This development not only enhances our ability to explore the
physical properties of moir\'e systems with reduced computational overhead but
also opens new avenues for the study of relaxation effects and their impact on
material properties in two-dimensional layered structures.",2024-12-26T19:00:40Z,http://arxiv.org/abs/2412.19333v1,"Jiaxuan Liu, Zhong Fang, Hongming Weng, Quansheng Wu"
Identifying Split Vacancies with Foundation Models and Electrostatics,"Point defects are ubiquitous in solid-state compounds, dictating many
functional properties such as conductivity, catalytic activity and carrier
recombination. Over the past decade, the prevalence of metastable defect
geometries and their importance to relevant properties has been increasingly
recognised. A particularly striking example of this is split vacancies, where
an isolated atomic vacancy transforms to a stoichiometry-conserving complex of
two vacancies and an interstitial ($V_X \rightarrow [V_X + X_i + V_X]$), which
can be accompanied by a dramatic lowering of the defect energy and change in
behaviour. Such species are particularly challenging to identify from
computation, due to the `non-local' nature of this reconstruction. Here, I
present an approach for efficiently identifying such species in solid-state
compounds, through tiered screening which combines geometric analysis,
electrostatic energies and foundation machine learning (ML) models. This
approach allows the screening of all compounds in the Materials Project
database (including all entries in the ICSD, along with several thousand
predicted metastable materials), identifying thousands of split vacancy
configurations, hitherto unknown. This study highlights both the potential
utility of machine-learning potentials for defect investigations, with
important caveats, and the importance of global optimisation approaches for
correctly identifying stable defect geometries.",2024-12-26T18:58:52Z,http://arxiv.org/abs/2412.19330v1,Seán R. Kavanagh
"Deep learning and whole-brain networks for biomarker discovery: modeling
  the dynamics of brain fluctuations in resting-state and cognitive tasks","Background: Brain network models offer insights into brain dynamics, but the
utility of model-derived bifurcation parameters as biomarkers remains
underexplored. Objective: This study evaluates bifurcation parameters from a
whole-brain network model as biomarkers for distinguishing brain states
associated with resting-state and task-based cognitive conditions. Methods:
Synthetic BOLD signals were generated using a supercritical Hopf brain network
model to train deep learning models for bifurcation parameter prediction.
Inference was performed on Human Connectome Project data, including both
resting-state and task-based conditions. Statistical analyses assessed the
separability of brain states based on bifurcation parameter distributions.
Results: Bifurcation parameter distributions differed significantly across task
and resting-state conditions ($p &lt; 0.0001$ for all but one comparison).
Task-based brain states exhibited higher bifurcation values compared to rest.
Conclusion: Bifurcation parameters effectively differentiate cognitive and
resting states, warranting further investigation as biomarkers for brain state
characterization and neurological disorder assessment.",2024-12-26T18:58:38Z,http://arxiv.org/abs/2412.19329v1,"Facundo Roffet, Gustavo Deco, Claudio Delrieux, Gustavo Patow"
"Resolving the Ambiguity of Complete-to-Partial Point Cloud Registration
  for Image-Guided Liver Surgery with Patches-to-Partial Matching","In image-guided liver surgery, the initial rigid alignment between
preoperative and intraoperative data, often represented as point clouds, is
crucial for providing sub-surface information from preoperative CT/MRI images
to the surgeon during the procedure. Currently, this alignment is typically
performed using semi-automatic methods, which, while effective to some extent,
are prone to errors that demand manual correction. Point cloud
correspondence-based registration methods are promising to serve as a fully
automatic solution. However, they may struggle in scenarios with limited
intraoperative surface visibility, a common challenge in liver surgery,
particularly in laparoscopic procedures, which we refer to as
complete-to-partial ambiguity. We first illustrate this ambiguity by evaluating
the performance of state-of-the-art learning-based point cloud registration
methods on our carefully constructed in silico and in vitro datasets. Then, we
propose a patches-to-partial matching strategy as a plug-and-play module to
resolve the ambiguity, which can be seamlessly integrated into learning-based
registration methods without disrupting their end-to-end structure. It has
proven effective and efficient in improving registration performance for cases
with limited intraoperative visibility. The constructed benchmark and the
proposed module establish a solid foundation for advancing applications of
point cloud correspondence-based registration methods in image-guided liver
surgery.",2024-12-26T18:58:29Z,http://arxiv.org/abs/2412.19328v1,"Zixin Yang, Jon S. Heiselman, Cheng Han, Kelly Merrell, Richard Simon, Cristian. A. Linte"
"Performance Control in Early Exiting to Deploy Large Models at the Same
  Cost of Smaller Ones","Early Exiting (EE) is a promising technique for speeding up inference by
adaptively allocating compute resources to data points based on their
difficulty. The approach enables predictions to exit at earlier layers for
simpler samples while reserving more computation for challenging ones. In this
study, we first present a novel perspective on the EE approach, showing that
larger models deployed with EE can achieve higher performance than smaller
models while maintaining similar computational costs. As existing EE approaches
rely on confidence estimation at each exit point, we further study the impact
of overconfidence on the controllability of the compute-performance trade-off.
We introduce Performance Control Early Exiting (PCEE), a method that enables
accuracy thresholding by basing decisions not on a data point's confidence but
on the average accuracy of samples with similar confidence levels from a
held-out validation set. In our experiments, we show that PCEE offers a simple
yet computationally efficient approach that provides better control over
performance than standard confidence-based approaches, and allows us to scale
up model sizes to yield performance gain while reducing the computational cost.",2024-12-26T18:54:32Z,http://arxiv.org/abs/2412.19325v1,"Mehrnaz Mofakhami, Reza Bayat, Ioannis Mitliagkas, Joao Monteiro, Valentina Zantedeschi"
Electron efficiency in LHC Run-2 with the ATLAS experiment,"The document presents a general overview of the electron reconstruction,
identification and isolation performance in the ATLAS experiment. The results
are obtained using 13 TeV proton-proton collision data collected during the LHC
Run-2. The electron reconstruction efficiency is higher than 97%, and the ratio
of data to Monte Carlo simulation efficiency is close to unity, with associated
uncertainties generally smaller than 0.1%. The electron identification is shown
for three working points, and depending on the electron $E_T$, it can be as low
as 60%, increasing to more than 80% above 50 GeV. The correction factors are
close to one, generally within 5%. Five isolation working points are
recommended in the ATLAS experiment, to successfully reject fake/non-prompt
electrons. Their dependency on the electron identification working points is
shown and discussed, as well as their pile-up dependency, and their performance
versus electron $E_T$ and $\eta$.
  Document based on a presentation at the XI International Conference on New
Frontiers in Physics (ICNFP 2022).
  keywords; prompt electrons, reconstruction, identification, isolation,
fake/non-prompt electrons",2024-12-26T18:50:43Z,http://arxiv.org/abs/2412.19323v1,Otilia Ducu
"A novel framework for MCDM based on Z numbers and soft likelihood
  function","The optimization on the structure of process of information management under
uncertain environment has attracted lots of attention from researchers around
the world. Nevertheless, how to obtain accurate and rational evaluation from
assessments produced by experts is still an open problem. Specially,
intuitionistic fuzzy set provides an effective solution in handling
indeterminate information. And Yager proposes a novel method for fusion of
probabilistic evidence to handle uncertain and conflicting information lately
which is called soft likelihood function. This paper devises a novel framework
of soft likelihood function based on information volume of fuzzy membership and
credibility measure for extracting truly useful and valuable information from
uncertainty. An application is provided to verify the validity and correctness
of the proposed framework. Besides, the comparisons with other existing methods
further demonstrate the superiority of the novel framework of soft likelihood
function.",2024-12-26T18:47:19Z,http://arxiv.org/abs/2412.19321v1,Yuanpeng He
Adaptive Conformal Inference by Betting,"Conformal prediction is a valuable tool for quantifying predictive
uncertainty of machine learning models. However, its applicability relies on
the assumption of data exchangeability, a condition which is often not met in
real-world scenarios. In this paper, we consider the problem of adaptive
conformal inference without any assumptions about the data generating process.
Existing approaches for adaptive conformal inference are based on optimizing
the pinball loss using variants of online gradient descent. A notable
shortcoming of such approaches is in their explicit dependence on and
sensitivity to the choice of the learning rates. In this paper, we propose a
different approach for adaptive conformal inference that leverages
parameter-free online convex optimization techniques. We prove that our method
controls long-term miscoverage frequency at a nominal level and demonstrate its
convincing empirical performance without any need of performing cumbersome
parameter tuning.",2024-12-26T18:42:08Z,http://arxiv.org/abs/2412.19318v1,"Aleksandr Podkopaev, Darren Xu, Kuang-Chih Lee"
"ATLAS searches for higgsinos with R-parity violating couplings in events
  with leptons","This document presents two searches for Supersymmetry through the direct
production of pairs of higgsinos decaying into final states with leptons and
($b$-) jets. The analyses are performed using 139~fb$^{-1}$ of the 13~TeV
proton-proton collision data collected with the ATLAS detector. The methods
used to estimate the Standard Model and detector backgrounds are discussed, as
well as their shortcomings. Finally, results in selected signal regions, and
some exclusion limits, are presented, illustrating the significant improvement
over the previous exclusion limits.
  Document based on a presentation at the XI International Conference on New
Frontiers in Physics (ICNFP 2022).",2024-12-26T18:41:55Z,http://arxiv.org/abs/2412.19317v1,Otilia Ducu
A note on common complements,"We discuss the structure of the set $\Delta$ consisting of pairs of closed
subspaces that have a common complement in a Hilbert space previously studied
by Lauzon and Treil (J. Funct. Anal. 212: 500--512, 2004). We prove that
$\Delta$ is the base space of a real analytic fiber bundle constructed in terms
of geometric objects associated to the Grassmann manifold. As a consequence we
determine the homotopy type of $\Delta$.",2024-12-26T18:39:56Z,http://arxiv.org/abs/2412.19316v1,"Esteban Andruchow, Eduardo Chiumiento"
Towards a Single ASR Model That Generalizes to Disordered Speech,"This study investigates the impact of integrating a dataset of disordered
speech recordings ($\sim$1,000 hours) into the fine-tuning of a near
state-of-the-art ASR baseline system. Contrary to what one might expect,
despite the data being less than 1% of the training data of the ASR system, we
find a considerable improvement in disordered speech recognition accuracy.
Specifically, we observe a 33% improvement on prompted speech, and a 26%
improvement on a newly gathered spontaneous, conversational dataset of
disordered speech. Importantly, there is no significant performance decline on
standard speech recognition benchmarks. Further, we observe that the proposed
tuning strategy helps close the gap between the baseline system and
personalized models by 64% highlighting the significant progress as well as the
room for improvement. Given the substantial benefits of our findings, this
experiment suggests that from a fairness perspective, incorporating a small
fraction of high quality disordered speech data in a training recipe is an easy
step that could be done to make speech technology more accessible for users
with speech disabilities.",2024-12-26T18:39:15Z,http://arxiv.org/abs/2412.19315v1,"Jimmy Tobin, Katrin Tomanek, Subhashini Venugopalan"
NNLO QCD corrections to unpolarized and polarized SIDIS,"The semi-inclusive deep-inelastic scattering (SIDIS) process requires the
presence of an identified hadron H$'$ in the final state, which arises from the
scattering of a lepton with an initial hadron P. By employing factorization in
quantum chromodynamics (QCD), SIDIS provides essential knowledge on the hadron
structure, enabling the exploration of parton distribution functions (PDFs) and
fragmentation functions (FFs). The coefficient functions for SIDIS can be
calculated in perturbative QCD and are currently known to the
next-to-next-to-leading order (NNLO) for the cases, where the incoming lepton
and the hadron P are either both polarized or unpolarized. We present a
detailed description of these NNLO computations, including a thorough
discussion of all the partonic channels, the calculation of the amplitudes and
master integrals for the phase-space integration as well as the renormalization
of ultraviolet divergences and mass factorization of infrared divergences in
dimensional regularization through NNLO. We provide an extensive
phenomenological analysis of the effects of NNLO corrections on SIDIS cross
sections for different PDFs and FFs and various kinematics, including those of
the future Electron-Ion Collider (EIC). We find that these corrections are not
only significant but also crucial for reducing the dependence on the
renormalization and factorization scales $\mu_R$ and $\mu_F$ to obtain stable
predictions.",2024-12-26T18:16:32Z,http://arxiv.org/abs/2412.19309v1,"Saurav Goyal, Roman N. Lee, Sven-Olaf Moch, Vaibhav Pathak, Narayan Rana, V. Ravindran"
"Theoretical models for longitudinal coupled-bunch instabilities driven
  by harmonic cavities in electron storage rings","We present a theoretical framework for analyzing longitudinal coupled-bunch
instabilities in double-rf systems with even filling patterns, accounting for
potential-well distortion and multiple azimuthal modes. The linearized Vlasov
equation is solved in the frequency-domain for an arbitrary rf potential to
derive the Lebedev equation. We unified different formulations, obtaining
results from recent publications as particular cases. Applications to Robinson
dipole-quadrupole mode coupling and the periodic transient beam loading
(PTBL)/mode-1 instability are presented. Notably, for the first time,
theoretical predictions of the mode-1 thresholds show excellent agreement with
experimental data. The analysis reveals that the PTBL instability is a
zero-frequency effect dependent on azimuthal mode interactions and resistant to
Landau damping, providing new insights into its mechanism. The methods are
implemented in the open-source package pycolleff, offering a useful
semi-analytical tool for studying instabilities in electron storage rings with
harmonic cavities.",2024-12-26T18:11:39Z,http://arxiv.org/abs/2412.19308v1,Murilo B. Alves
"Perceive, Query &amp; Reason: Enhancing Video QA with Question-Guided
  Temporal Queries","Video Question Answering (Video QA) is a challenging video understanding task
that requires models to comprehend entire videos, identify the most relevant
information based on contextual cues from a given question, and reason
accurately to provide answers. Recent advancements in Multimodal Large Language
Models (MLLMs) have transformed video QA by leveraging their exceptional
commonsense reasoning capabilities. This progress is largely driven by the
effective alignment between visual data and the language space of MLLMs.
However, for video QA, an additional space-time alignment poses a considerable
challenge for extracting question-relevant information across frames. In this
work, we investigate diverse temporal modeling techniques to integrate with
MLLMs, aiming to achieve question-guided temporal modeling that leverages
pre-trained visual and textual alignment in MLLMs. We propose T-Former, a novel
temporal modeling method that creates a question-guided temporal bridge between
frame-wise visual perception and the reasoning capabilities of LLMs. Our
evaluation across multiple video QA benchmarks demonstrates that T-Former
competes favorably with existing temporal modeling approaches and aligns with
recent advancements in video QA.",2024-12-26T17:53:14Z,http://arxiv.org/abs/2412.19304v1,"Roberto Amoroso, Gengyuan Zhang, Rajat Koner, Lorenzo Baraldi, Rita Cucchiara, Volker Tresp"
RecLM: Recommendation Instruction Tuning,"Modern recommender systems aim to deeply understand users' complex
preferences through their past interactions. While deep collaborative filtering
approaches using Graph Neural Networks (GNNs) excel at capturing user-item
relationships, their effectiveness is limited when handling sparse data or
zero-shot scenarios, primarily due to constraints in ID-based embedding
functions. To address these challenges, we propose a model-agnostic
recommendation instruction-tuning paradigm that seamlessly integrates large
language models with collaborative filtering. Our proposed Recommendation
Language Model (RecLM) enhances the capture of user preference diversity
through a carefully designed reinforcement learning reward function that
facilitates self-augmentation of language models. Comprehensive evaluations
demonstrate significant advantages of our approach across various settings, and
its plug-and-play compatibility with state-of-the-art recommender systems
results in notable performance enhancements.",2024-12-26T17:51:54Z,http://arxiv.org/abs/2412.19302v1,"Yangqin Jiang, Yuhao Yang, Lianghao Xia, Da Luo, Kangyi Lin, Chao Huang"
"Sample Complexity of Data-driven Multistage Stochastic Programming under
  Markovian Uncertainty","This work is motivated by the challenges of applying the sample average
approximation (SAA) method to multistage stochastic programming with an unknown
continuous-state Markov process. While SAA is widely used in static and
two-stage stochastic optimization, it becomes computationally intractable in
general multistage settings as the time horizon $T$ increases. Indeed, the
number of samples required to obtain a reasonably accurate solution grows
exponentially$\text{ -- }$a phenomenon known as the curse of dimensionality
with respect to the time horizon. To overcome this limitation, we propose a
novel data-driven approach, the Markov Recombining Scenario Tree (MRST) method,
which constructs an approximate problem using only two independent trajectories
of historical data. Our analysis demonstrates that the MRST method achieves
polynomial sample complexity in $T$, providing a more efficient alternative to
SAA. Numerical experiments on the Linear Quadratic Gaussian problem show that
MRST outperforms SAA, addressing the curse of dimensionality.",2024-12-26T17:46:27Z,http://arxiv.org/abs/2412.19299v1,"Hyuk Park, Grani A. Hanasusanto"
"Transformation of the trapped flux in a SC disc under electromagnetic
  exposure","Superconducting permanent magnets (SCs) with trapped magnetic flux are used
in technical devices (motors, generators, etc.). These magnets endure repeated
magnetic ""shocks"" during operation, which can affect their performance. In this
work, we investigated the dynamic behavior of magnetic induction in the trapped
flux in an SC disk when exposed to stepwise changes in the external magnetic
field, simulating these operational shocks. Our results reveal a direct
correlation between the stepwise changes in the magnetic field and the trapped
flux response, with each increase or decrease in the field inducing a
corresponding 40-50% change in trapped flux for a 600 G field step at
temperature of 5 K. The magnitude of these changes depends on the external
parameters and their dynamics could lead to additional energy dissipation and
potential heating, which may affect the reliability of SC magnets in
applications. A scaling analysis of the induction flux profiles, revealing
roughness exponents in the range of 0.435 to 0.475 was performed as well, and
we determined the Hausdorff dimension of the surface structure.",2024-12-26T17:45:19Z,http://arxiv.org/abs/2412.19298v1,"V. V. Chabanenko, I. Abaloszewa, V. F. Rusakov, O. I. Kuchuk, O. M. Chumak, A. Nabiałek, A. Abaloszew, A. Filippov, R. Puźniak"
Spatio-Temporal Differences in Bike Sharing Usage: A Tale of Six Cities,"This study investigates the spatio-temporal patterns of Bike Sharing System
(BSS) usage in six major cities: New York, London, Tokyo, Boston, Chicago and
Washington D.C. By analyzing data over a 30-day period with comparable climate
and average temperatures, we explored differences in BSS usage between weekdays
and weekends in those cities using Jensen-Shannon divergence (JSD) and rank
distribution analysis. Our findings reveal significant temporal differences in
BSS usage that were commonly observed in all cities, with weekday patterns
dominated by commute peaks and weekend patterns reflecting recreational
activities. Friday emerges as a transitional day, sharing the characteristics
of both weekdays and weekends. Meanwhile, docking station usage rank
distributions show remarkable consistency between weekdays and weekends for
most cities, with London being a unique anomaly. This study highlights the
potential of BSS data to uncover urban mobility patterns and the underlying
structures of cities. The results suggest that BSS usage reflects both
intrinsic user behavior and external influences such as urban planning.",2024-12-26T17:35:28Z,http://arxiv.org/abs/2412.19294v1,"Shu-ichi Kinoshita, Yuya Bando, Hiroki Sayama"
RAG with Differential Privacy,"Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to
provide *Large Language Models* (LLM) with fresh and relevant context,
mitigating the risk of hallucinations and improving the overall quality of
responses in environments with large and fast moving knowledge bases. However,
the integration of external documents into the generation process raises
significant privacy concerns. Indeed, when added to a prompt, it is not
possible to guarantee a response will not inadvertently expose confidential
data, leading to potential breaches of privacy and ethical dilemmas. This paper
explores a practical solution to this problem suitable to general knowledge
extraction from personal data. It shows *differentially private token
generation* is a viable approach to private RAG.",2024-12-26T17:34:26Z,http://arxiv.org/abs/2412.19291v1,Nicolas Grislain
"ViPCap: Retrieval Text-Based Visual Prompts for Lightweight Image
  Captioning","Recent lightweight image captioning models using retrieved data mainly focus
on text prompts. However, previous works only utilize the retrieved text as
text prompts, and the visual information relies only on the CLIP visual
embedding. Because of this issue, there is a limitation that the image
descriptions inherent in the prompt are not sufficiently reflected in the
visual embedding space. To tackle this issue, we propose ViPCap, a novel
retrieval text-based visual prompt for lightweight image captioning. ViPCap
leverages the retrieved text with image information as visual prompts to
enhance the ability of the model to capture relevant visual information. By
mapping text prompts into the CLIP space and generating multiple randomized
Gaussian distributions, our method leverages sampling to explore randomly
augmented distributions and effectively retrieves the semantic features that
contain image information. These retrieved features are integrated into the
image and designated as the visual prompt, leading to performance improvements
on the datasets such as COCO, Flickr30k, and NoCaps. Experimental results
demonstrate that ViPCap significantly outperforms prior lightweight captioning
models in efficiency and effectiveness, demonstrating the potential for a
plug-and-play solution.",2024-12-26T17:29:38Z,http://arxiv.org/abs/2412.19289v1,"Taewhan Kim, Soeun Lee, Si-Woo Kim, Dong-Jin Kim"
A semi-algebraic model for automatic loop parallelization,"In this work, we introduce a semi-algebraic model for automatic
parallelization of perfectly nested polynomial loops, which generalizes the
classical polyhedral model. This model supports the basic tasks for automatic
loop parallelization, such as the representation of the nested loop, the
dependence analysis, the computation of valid schedules, as well as the
transformation of the loop program with a valid schedule.",2024-12-26T17:19:56Z,http://arxiv.org/abs/2412.19287v1,Changbo Chen
Improving Generalization for AI-Synthesized Voice Detection,"AI-synthesized voice technology has the potential to create realistic human
voices for beneficial applications, but it can also be misused for malicious
purposes. While existing AI-synthesized voice detection models excel in
intra-domain evaluation, they face challenges in generalizing across different
domains, potentially becoming obsolete as new voice generators emerge. Current
solutions use diverse data and advanced machine learning techniques (e.g.,
domain-invariant representation, self-supervised learning), but are limited by
predefined vocoders and sensitivity to factors like background noise and
speaker identity. In this work, we introduce an innovative disentanglement
framework aimed at extracting domain-agnostic artifact features related to
vocoders. Utilizing these features, we enhance model learning in a flat loss
landscape, enabling escape from suboptimal solutions and improving
generalization. Extensive experiments on benchmarks show our approach
outperforms state-of-the-art methods, achieving up to 5.12% improvement in the
equal error rate metric in intra-domain and 7.59% in cross-domain evaluations.",2024-12-26T16:45:20Z,http://arxiv.org/abs/2412.19279v1,"Hainan Ren, Lin Li, Chun-Hao Liu, Xin Wang, Shu Hu"
Memory-Centric Computing: Recent Advances in Processing-in-DRAM,"Memory-centric computing aims to enable computation capability in and near
all places where data is generated and stored. As such, it can greatly reduce
the large negative performance and energy impact of data access and data
movement, by 1) fundamentally avoiding data movement, 2) reducing data access
latency &amp; energy, and 3) exploiting large parallelism of memory arrays. Many
recent studies show that memory-centric computing can largely improve system
performance &amp; energy efficiency. Major industrial vendors and startup companies
have recently introduced memory chips with sophisticated computation
capabilities. Going forward, both hardware and software stack should be
revisited and designed carefully to take advantage of memory-centric computing.
  This work describes several major recent advances in memory-centric
computing, specifically in Processing-in-DRAM, a paradigm where the operational
characteristics of a DRAM chip are exploited and enhanced to perform
computation on data stored in DRAM. Specifically, we describe 1) new techniques
that slightly modify DRAM chips to enable both enhanced computation capability
and easier programmability, 2) new experimental studies that demonstrate the
functionally-complete bulk-bitwise computational capability of real commercial
off-the-shelf DRAM chips, without any modifications to the DRAM chip or the
interface, and 3) new DRAM designs that improve access granularity &amp;
efficiency, unleashing the true potential of Processing-in-DRAM.",2024-12-26T16:31:40Z,http://arxiv.org/abs/2412.19275v1,"Onur Mutlu, Ataberk Olgun, Geraldo F. Oliveira, Ismail Emir Yuksel"
"Anvendelse av kunstig intelligens (KI) i Norge i norsk offentlig sektor
  2024","There are great expectations for the use of AI in Norway. On the other hand,
it is reported that the adoption of AI in Norway is slower than expected in
both the private and public sectors. Using responses from NOKIOS Technology
Radar 2017-2021, IT in Practice surveys conducted by Ramboll in 2021-2024, as
well as another national survey as part of a five-year cycle, this article
looks at reported and planned use of AI with a focus on local (municipalities)
and national government agencies. IT in practice is distributed to a large
number of Norwegian public agencies, with a response rate of over 5o percent.
The most recent data (2024) presented in this article is based on responses
from 335 public organizations, with 237 municipalities, and 98 public
organizations at the national or regional level. The survey confirms that the
use of AI is still at an early stage, although expectations are high for future
use.
  --
  Det er store forventninger til bruk av KI i Norge. P{\aa} den annen side
rapporteres det at adopsjonen av KI i Norge g{\aa}r tregere enn forventet
b{\aa}de i privat og offentlig sektor. Ved hjelp av svar fra NOKIOS
teknologiradar 2017-2021, IT i Praksis unders{\o}kelser utf{\o}rt av Ramb{\o}ll
i 2021-2024, samt en annen nasjonal unders{\o}kelse som en del av en
fem{\aa}rig syklus, ser vi i denne artikkelen p{\aa} rapportert og planlagt
bruk av KI med fokus p{\aa} lokale (kommuner) og nasjonale offentlige etater.
IT i praksis distribueres til en lang rekke norske offentlige virksomheter, med
en svarprosent p{\aa} over 50 prosent. De nyeste dataene (2024) presentert i
denne artikkelen er basert p{\aa} svar fra 335 offentlige organisasjoner, med
237 kommuner, og 98 offentlige organisasjoner p{\aa} nasjonalt eller regionalt
niv{\aa}. Unders{\o}kelsen bekrefter at bruken av KI fortsatt er p{\aa} et
tidlig stadium, selv om forventningene er h{\o}ye til fremtidig bruk.",2024-12-26T16:28:49Z,http://arxiv.org/abs/2412.19273v1,John Krogstie
"Investigating nuclear density profiles to reveal particle-hole
  configurations in the island of inversion","Background: In the mass regions with an abnormal shell structure, the
so-called ``island of inversion,"" the spin-parity of odd-mass nuclei provides
quantitative insights into the shell evolution. However, the experimental
determination of the spin-parity is often challenging, leaving it undetermined
in many nuclei. Purpose: We discuss how the shell structure affects the density
profiles of nuclei in the island of inversion and investigate whether these can
be probed from the total reaction and elastic scattering cross sections.
Method: The antisymmetrized molecular dynamics (AMD) is employed to generate
various particle-hole configurations and predict the energy levels of these
nuclei. The obtained density distributions are used as inputs to the Glauber
model, which is employed to calculate the total reaction and elastic scattering
cross sections for revealing their relationship to the particle-hole
configurations. Results: In addition to the well-known correlation between
nuclear deformation and radius, we show the correlations between the
particle-hole configurations and both central density and diffuseness. We show
that different particle-hole configurations are well reflected in the total
reaction and elastic scattering cross sections. Conclusion: The total reaction
and elastic scattering cross sections are useful probes to identify the
spin-parity of nuclei when different particle-hole configurations coexist.",2024-12-26T16:23:35Z,http://arxiv.org/abs/2412.19270v1,"R. Barman, W. Horiuchi, M. Kimura, R. Chatterjee"
Jet formation studies in AGN: a search for new targets,"In recent years, the jet formation region in active galaxies has been imaged
through mm-VLBI in few ideal targets, first and foremost M87. An important leap
forward for understanding jet launching could be made by identifying a larger
number of suitable objects, characterized by different accretion modes and jet
powers. In this article, we present 1 cm and 7 mm VLBI data of a sample of 16
poorly explored radio galaxies, comprising both High-Excitation (HEG) and
Low-Excitation Galaxies (LEG) and spanning a large range in radio power. The
combination of the sources vicinity (z&lt;0.1) with a large black hole mass
($\log{M_{\rm BH}}$&gt;8.5) results in a high spatial resolution in units of
Schwarzschild radii ($&lt;10^3-10^4$ $R_{\rm S}$), necessary for probing the
region where the jet is initially accelerated and collimated. We aim at
identifying the best candidates for follow-up observations with current and
future VLBI facilities. The observations were performed with the High
Sensitivity Array, including Effelsberg and the phased-VLA, which has allowed
us to characterize the sub-parsec properties of these faint jets and to
estimate their core brightness temperature and orientation. The number of
sources imaged on scales $\lesssim 10^3$ $R_{\rm S}$ is more than doubled by
our study. All targets were detected at both frequencies, and several present
two-sided jet structures. Several LEG jets show hints of limb-brightening. The
core brightness temperatures are generally below the equipartition value,
indicating that equipartition has not yet been reached and/or that the emission
is de-boosted. Among LEG, we identify 3C31, 3C66B, and 3C465 as the most
promising, combining a relatively high flux density (&gt;50 mJy) with superb
spatial resolution (&lt;500 $R_{\rm S}$) at 7 mm. The powerful HEG 3C452 is
interesting as well due to its highly symmetric, two-sided jet base.",2024-12-26T16:20:53Z,http://arxiv.org/abs/2412.19268v1,"B. Boccardi, L. Ricci, E. Madika, V. Bartolini, U. Bach, P. Grandi, E. Torresi, T. P. Krichbaum, J. A. Zensus"
Ultrafast opto-acoustics in single nickel cavities,"Mechanical stress produced in nano- and micro-scale structures can enhance
materials properties, such as the high mobility of silicon in modern
transistors or amplified magnetization dynamics in spintronic devices. Here, we
report on the dynamics of coherent acoustic phonons excited by femtosecond
light pulses and confined in a single freestanding nickel layer, acting as an
acoustic cavity. By combining Fourier transform analysis of the experimental
signal and numerical multi-physics simulations, we show that high-frequency (&gt;
10 GHz) longitudinal acoustic pulses can resonate inside the cavity and display
lower damping compared to a reference nickel film on SiO2 substrate given that
the conditions of total reflection are nearly met in the cavity. Our results
provide a thorough understanding of the opto-acoustic response in suspended
membranes of magnetic materials, which we foresee can be used to amplify
magnetization precession dynamics and to develop magneto-acousto-optical
modulators.",2024-12-26T16:17:03Z,http://arxiv.org/abs/2412.19267v1,"Alba Viejo Rodríguez, Marco Gandolfi, Andrea Rossetti, Yoav Urbina Elgueta, Evgeny B. Modin, Svetlana Starikovskaia, Tatloon Chng, Vasily Temnov, Maria Antonietta Vincenti, Daniele Brida, Paolo Vavassori, Nicolò Maccaferri"
"Search for a neutral gauge boson with nonuniversal fermion couplings in
  vector boson fusion processes in proton-proton collisions at $\sqrt{s}$ = 13
  TeV","The first search for a heavy neutral spin-1 gauge boson (Z') with
nonuniversal fermion couplings produced via vector boson fusion processes and
decaying to tau leptons or W bosons is presented. The analysis is performed
using LHC data at $\sqrt{s}$ = 13 TeV, collected from 2016 to 2018 and
corresponding to an integrated luminosity of 138 fb$^{-1}$. The data are
consistent with the standard model predictions. Upper limits are set on the
product of the cross section for production of the Z' boson and its branching
fraction to $\tau\tau$ or WW. The presence of a Z' boson decaying to
$\tau^+\tau^-$ (W$^+$W$^-$) is excluded for masses up to 2.45 (1.60) TeV,
depending on the Z' boson coupling to SM weak bosons, and assuming a Z' $\to$
$\tau^+\tau^-$ (W$^+$W$^-$) branching fraction of 50%.",2024-12-26T15:54:58Z,http://arxiv.org/abs/2412.19261v1,CMS Collaboration
"MEDEC: A Benchmark for Medical Error Detection and Correction in
  Clinical Notes","Several studies showed that Large Language Models (LLMs) can answer medical
questions correctly, even outperforming the average human score in some medical
exams. However, to our knowledge, no study has been conducted to assess the
ability of language models to validate existing or generated medical text for
correctness and consistency. In this paper, we introduce MEDEC
(https://github.com/abachaa/MEDEC), the first publicly available benchmark for
medical error detection and correction in clinical notes, covering five types
of errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal
Organism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes
from three US hospital systems that were not previously seen by any LLM. The
dataset has been used for the MEDIQA-CORR shared task to evaluate seventeen
participating systems [Ben Abacha et al., 2024]. In this paper, we describe the
data creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,
Claude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and
correcting medical errors requiring both medical knowledge and reasoning
capabilities. We also conducted a comparative study where two medical doctors
performed the same task on the MEDEC test set. The results showed that MEDEC is
a sufficiently challenging benchmark to assess the ability of models to
validate existing or generated notes and to correct medical errors. We also
found that although recent LLMs have a good performance in error detection and
correction, they are still outperformed by medical doctors in these tasks. We
discuss the potential factors behind this gap, the insights from our
experiments, the limitations of current evaluation metrics, and share potential
pointers for future research.",2024-12-26T15:54:10Z,http://arxiv.org/abs/2412.19260v1,"Asma Ben Abacha, Wen-wai Yim, Yujuan Fu, Zhaoyi Sun, Meliha Yetisgen, Fei Xia, Thomas Lin"
"VoiceDiT: Dual-Condition Diffusion Transformer for Environment-Aware
  Speech Synthesis","We present VoiceDiT, a multi-modal generative model for producing
environment-aware speech and audio from text and visual prompts. While aligning
speech with text is crucial for intelligible speech, achieving this alignment
in noisy conditions remains a significant and underexplored challenge in the
field. To address this, we present a novel audio generation pipeline named
VoiceDiT. This pipeline includes three key components: (1) the creation of a
large-scale synthetic speech dataset for pre-training and a refined real-world
speech dataset for fine-tuning, (2) the Dual-DiT, a model designed to
efficiently preserve aligned speech information while accurately reflecting
environmental conditions, and (3) a diffusion-based Image-to-Audio Translator
that allows the model to bridge the gap between audio and image, facilitating
the generation of environmental sound that aligns with the multi-modal prompts.
Extensive experimental results demonstrate that VoiceDiT outperforms previous
models on real-world datasets, showcasing significant improvements in both
audio quality and modality integration.",2024-12-26T15:52:58Z,http://arxiv.org/abs/2412.19259v1,"Jaemin Jung, Junseok Ahn, Chaeyoung Jung, Tan Dat Nguyen, Youngjoon Jang, Joon Son Chung"
"Complexity and Structural Results for the Hull and Convexity Numbers in
  Cycle Convexity for Graph Products","Let $G$ be a graph and $S \subseteq V(G)$. In the cycle convexity, we say
that $S$ is \textit{cycle convex} if for any $u\in V(G)\setminus S$, the
induced subgraph of $S\cup\{u\}$ contains no cycle that includes $u$. The
\textit{cycle convex hull} of $S$ is the smallest convex set containing $S$.
The \textit{cycle hull number} of $G$, denoted by $hn_{cc}(G)$, is the
cardinality of the smallest set $S$ such that the convex hull of $S$ is $V(G)$.
The \textit{convexity number} of $G$, denoted by $C_{cc}(G)$, is the maximum
cardinality of a proper convex set of $V(G)$. This paper studies cycle
convexity in graph products. We show that the cycle hull number is always two
for strong and lexicographic products. For the Cartesian, we establish tight
bounds for this product and provide a closed formula when the factors are
trees, generalizing an existing result for grid graphs. In addition, given a
graph $G$ and an integer $k$, we prove that $hn_{cc}(G) \leq k$ is NP-complete
even if $G$ is a bipartite Cartesian product graph, addressing an open question
in the literature. Furthermore, we present exact formulas for the cycle
convexity number in those three graph products. That leads to the
NP-completeness of, given a graph $G$ and an integer $k$, deciding whether
$C_{cc}(G) \geq k$, when $G$ is a Cartesian, strong or lexicographic product
graph.",2024-12-26T15:50:39Z,http://arxiv.org/abs/2412.19258v1,"Bijo S. Anand, Ullas Chandran S. V., Julliano R. Nascimento, Revathy S. Nair"
"Prospects for detecting the dark matter particles and primordial black
  holes with the Hongmeng mission using the 21 cm global spectrum at cosmic
  dawn","Dark matter is believed to account for a significant portion of the mass in
the universe, exerting a critical influence on the formation and evolution of
cosmic structures. This research delves into the processes of annihilation and
decay of dark matter particles, which generate observable signals that deepen
our comprehension of their characteristics and behaviors. Furthermore, the
study explores the potential role of primordial black holes, with a focus on
the emissions of Hawking radiation that could offer valuable insights into
their distribution and size range. A key aspect of this investigation revolves
around the 21 cm signal, a vital tool for scrutinizing the effects of dark
matter particles and primordial black hole phenomena on the intergalactic
medium. The upcoming Hongmeng mission, featuring a lunar orbital interferometer
array, is poised to revolutionize our ability to observe the 21 cm signal. By
conducting measurements devoid of atmospheric disturbances, the mission will
significantly boost sensitivity to subtle signals associated with dark matter
particle annihilation, decay, and primordial black hole emissions. This study
assesses the expected performance of the Hongmeng mission in detecting these
telltale signs and aims to unveil fresh insights into the nature and
interactions of dark matter particles and primordial black hole emissions
through a meticulous analysis of the global 21 cm spectrum. The mission holds
immense promise for reshaping our understanding of the universe's concealed
components.",2024-12-26T15:47:51Z,http://arxiv.org/abs/2412.19257v1,"Meng-Lin Zhao, Sai Wang, Xin Zhang"
Swarm Contract: A Multi-Sovereign Agent Consensus Mechanism,"Traditional smart contracts on blockchains excel at on-chain, deterministic
logic. However, they have inherent limitations when dealing with large-scale
off-chain data, dynamic multi-step workflows, and scenarios requiring high
flexibility or iterative updates. In this paper, we propose the concept of a
""Swarm Contract"" (Swarm), a multi-agent mechanism wherein several digital life
forms (DLF) or Sovereign Agents (SA) collectively handle complex tasks in
Trusted Execution Environments (TEE). These digital entities are defined as
autonomous software agents that own their code, state, and possibly on-chain
assets, while operating free from centralized control.
  By leveraging a simple multi-signature wallet on-chain, Swarm moves most of
the logic off-chain, achieving trust minimization through multi-agent consensus
rather than a single monolithic on-chain contract. We illustrate these ideas
with a lightweight off-chain auction example - minting and selling 10,000
identical NFTs - to showcase how off-chain coordination can determine a
clearing price and finalize distribution, with each step performed collectively
by multiple agents in TEE. This approach broadens the scope of trustless and
decentralized solutions, potentially benefiting DAO governance, multi-modal
data processing, and cross-chain interoperability.",2024-12-26T15:46:56Z,http://arxiv.org/abs/2412.19256v1,Haowei Yang
"Leveraging Self-Training and Variational Autoencoder for Agitation
  Detection in People with Dementia Using Wearable Sensors","Dementia is a neurodegenerative disorder that has been growing among elder
people over the past decades. This growth profoundly impacts the quality of
life for patients and caregivers due to the symptoms arising from it. Agitation
and aggression (AA) are some of the symptoms of people with severe dementia
(PwD) in long-term care or hospitals. AA not only causes discomfort but also
puts the patients or others at potential risk. Existing monitoring solutions
utilizing different wearable sensors integrated with Artificial Intelligence
(AI) offer a way to detect AA early enough for timely and adequate medical
intervention. However, most studies are limited by the availability of
accurately labeled datasets, which significantly affects the efficacy of such
solutions in real-world scenarios. This study presents a novel comprehensive
approach to detect AA in PwD using physiological data from the Empatica E4
wristbands. The research creates a diverse dataset, consisting of three
distinct datasets gathered from 14 participants across multiple hospitals in
Canada. These datasets have not been extensively explored due to their limited
labeling. We propose a novel approach employing self-training and a variational
autoencoder (VAE) to detect AA in PwD effectively. The proposed approach aims
to learn the representation of the features extracted using the VAE and then
uses a semi-supervised block to generate labels, classify events, and detect
AA. We demonstrate that combining Self-Training and Variational Autoencoder
mechanism significantly improves model performance in classifying AA in PwD.
Among the tested techniques, the XGBoost classifier achieved the highest
accuracy of 90.16\%. By effectively addressing the challenge of limited labeled
data, the proposed system not only learns new labels but also proves its
superiority in detecting AA.",2024-12-26T15:34:25Z,http://arxiv.org/abs/2412.19254v1,"Abeer Badawi, Somayya Elmoghazy, Samira Choudhury, Khalid Elgazzar, Amer Burhan"
"Localized exploration in contextual dynamic pricing achieves
  dimension-free regret","We study the problem of contextual dynamic pricing with a linear demand
model. We propose a novel localized exploration-then-commit (LetC) algorithm
which starts with a pure exploration stage, followed by a refinement stage that
explores near the learned optimal pricing policy, and finally enters a pure
exploitation stage. The algorithm is shown to achieve a minimax optimal,
dimension-free regret bound when the time horizon exceeds a polynomial of the
covariate dimension. Furthermore, we provide a general theoretical framework
that encompasses the entire time spectrum, demonstrating how to balance
exploration and exploitation when the horizon is limited. The analysis is
powered by a novel critical inequality that depicts the
exploration-exploitation trade-off in dynamic pricing, mirroring its existing
counterpart for the bias-variance trade-off in regularized regression. Our
theoretical results are validated by extensive experiments on synthetic and
real-world data.",2024-12-26T15:29:58Z,http://arxiv.org/abs/2412.19252v1,"Jinhang Chai, Yaqi Duan, Jianqing Fan, Kaizheng Wang"
Network double autoregression,"Modeling high-dimensional time series with simple structures is a challenging
problem. This paper proposes a network double autoregression (NDAR) model,
which combines the advantages of network structure and the double
autoregression (DAR) model, to handle high-dimensional, conditionally
heteroscedastic, and network-structured data within a simple framework. The
parameters of the model are estimated using quasi-maximum likelihood
estimation, and the asymptotic properties of the estimators are derived. The
selection of the model's lag order will be based on the Bayesian information
criterion. Finite-sample simulations show that the proposed model performs well
even with moderate time dimensions and network sizes. Finally, the model is
applied to analyze three different categories of stock data.",2024-12-26T15:28:41Z,http://arxiv.org/abs/2412.19251v1,"Tingting Li, Hao Wang"
"A Space Lower Bound for Approximate Membership with Duplicate Insertions
  or Deletions of Nonelements","Designs of data structures for approximate membership queries with
false-positive errors that support both insertions and deletions stipulate the
following two conditions: (1) Duplicate insertions are prohibited, i.e., it is
prohibited to insert an element $x$ if $x$ is currently a member of the
dataset. (2) Deletions of nonelements are prohibited, i.e., it is prohibited to
delete $x$ if $x$ is not currently a member of the dataset. Under these
conditions, the space required for the approximate representation of a datasets
of cardinality $n$ with a false-positive probability of $\epsilon^{+}$ is at
most $(1+o(1))n\cdot\log_2 (1/\epsilon^{+}) + O(n)$ bits [Bender et al., 2018;
Bercea and Even, 2019].
  We prove that if these conditions are lifted, then the space required for the
approximate representation of datasets of cardinality $n$ from a universe of
cardinality $u$ is at least $\frac 12 \cdot (1-\epsilon^{+} -\frac 1n)\cdot
\log \binom{u}{n} -O(n)$ bits.",2024-12-26T15:21:42Z,http://arxiv.org/abs/2412.19249v1,"Aryan Agarwala, Guy Even"
Functional structural equation modeling with latent variables,"Handling latent variables in Structural Equation Models (SEMs) in a case
where both the latent variables and their corresponding indicators in the
measurement error part of the model are random curves presents significant
challenges, especially with sparse data. In this paper, we develop a novel
family of Functional Structural Equation Models (FSEMs) that incorporate latent
variables modeled as Gaussian Processes (GPs). The introduced FSEMs are built
upon functional regression models having response variables modeled as
underlying GPs. The model flexibly adapts to cases when the random curves'
realizations are observed only over a sparse subset of the domain, and the
inferential framework is based on a restricted maximum likelihood approach. The
advantage of this framework lies in its ability and flexibility in handling
various data scenarios, including regularly and irregularly spaced points and
thus missing data. To extract smooth estimates for the functional parameters,
we employ a penalized likelihood approach that selects the smoothing parameters
using a cross-validation method. We evaluate the performance of the proposed
model using simulation studies and a real data example, which suggests that our
model performs well in practice. The uncertainty associated with the estimates
of the functional coefficients is also assessed by constructing confidence
regions for each estimate. The goodness of fit indices that are commonly used
to evaluate the fit of SEMs are developed for the FSEMs introduced in this
paper. Overall, the proposed method is a promising approach for modeling
functional data in SEMs with functional latent variables.",2024-12-26T14:57:14Z,http://arxiv.org/abs/2412.19242v1,"Fatemeh Asgari, Valeria Vitelli, Uta Sailer"
"Effect of Peak Absolute Magnitude of Type Ia Supernovae and Sound
  Horizon Values on Hubble Tension using DESI results","We apply data-motivated priors on the peak absolute magnitude of Type Ia
supernovae ($M$), and on the sound horizon at the drag epoch ($r_d$), to study
their impact on the Hubble tension, when compared to the Planck estimated value
of the Hubble constant. We use the data from Pantheon$+$, cosmic chronometers,
and the latest DESI BAO results for this purpose. We reaffirm the fact that
there is a degeneracy between $M$ and $r_d$, and modifying the $r_d$ values to
reconcile the Hubble tension also requires a change in the peak absolute
magnitude $M$. For certain $M$ and $r_d$ priors, the tension is found to reduce
to as low as (1.2-2) $\sigma$.",2024-12-26T14:51:09Z,http://arxiv.org/abs/2412.19240v1,"Shubham Barua, Shantanu Desai"
SeaMo: A Multi-Seasonal and Multimodal Remote Sensing Foundation Model,"Remote Sensing (RS) data contains a wealth of multi-dimensional information
crucial for Earth observation. Owing to its vast volume, diverse sources, and
temporal properties, RS data is highly suitable for the development of large
Visual Foundation Models (VFMs). VFMs act as robust feature extractors,
learning from extensive RS data, and are subsequently fine-tuned for deployment
in various geoscientific tasks. However, current VFMs in the RS domain are
predominantly pretrained and tailored exclusively for specific characteristics
of RS imagery, neglecting the potential of utilizing the multi-dimensional
properties of RS data. Therefore, in this work, we propose SeaMo, a pioneering
visual foundation model that integrates multi-seasonal and multimodal
information in the RS field. SeaMo is designed to harness multiple properties
of RS data. Within the masked image modeling framework, we employ non-aligned
cropping techniques to extract spatial properties, use multi-source inputs for
multimodal integration, and incorporate temporal-multimodal fusion blocks for
effective assimilation of multi-seasonal data. SeaMo explicitly models the
multi-dimensional properties of RS data, making the model more comprehensive,
robust, and versatile. We applied SeaMo to several downstream geoscience tasks,
which demonstrated exceptional performance. Extensive ablation studies were
conducted to validate the model's superiority.",2024-12-26T14:40:38Z,http://arxiv.org/abs/2412.19237v1,"Xuyang Li, Danfeng Hong, Chenyu Li, Jocelyn Chanussot"
"Are Two Hidden Layers Still Enough for the Physics-Informed Neural
  Networks?","The article discusses the development of various methods and techniques for
initializing and training neural networks with a single hidden layer, as well
as training a separable physics-informed neural network consisting of neural
networks with a single hidden layer to solve physical problems described by
ordinary differential equations (ODEs) and partial differential equations
(PDEs). A method for strictly deterministic initialization of a neural network
with one hidden layer for solving physical problems described by an ODE is
proposed. Modifications to existing methods for weighting the loss function are
given, as well as new methods developed for training strictly
deterministic-initialized neural networks to solve ODEs (detaching, additional
weighting based on the second derivative, predicted solution-based weighting,
relative residuals). An algorithm for physics-informed data-driven
initialization of a neural network with one hidden layer is proposed. A neural
network with pronounced generalizing properties is presented, whose
generalizing abilities of which can be precisely controlled by adjusting
network parameters. A metric for measuring the generalization of such neural
network has been introduced. A gradient-free neuron-by-neuron fitting method
has been developed for adjusting the parameters of a single-hidden-layer neural
network, which does not require the use of an optimizer or solver for its
implementation. The proposed methods have been extended to 2D problems using
the separable physics-informed neural networks approach. Numerous experiments
have been carried out to develop the above methods and approaches. Experiments
on physical problems, such as solving various ODEs and PDEs, have demonstrated
that these methods for initializing and training neural networks with one or
two hidden layers (SPINN) achieve competitive accuracy and, in some cases,
state-of-the-art results.",2024-12-26T14:30:54Z,http://arxiv.org/abs/2412.19235v1,"Vasiliy A. Es'kin, Alexey O. Malkhanov, Mikhail E. Smorkalov"
"Virtual Nodes Can Help: Tackling Distribution Shifts in Federated Graph
  Learning","Federated Graph Learning (FGL) enables multiple clients to jointly train
powerful graph learning models, e.g., Graph Neural Networks (GNNs), without
sharing their local graph data for graph-related downstream tasks, such as
graph property prediction. In the real world, however, the graph data can
suffer from significant distribution shifts across clients as the clients may
collect their graph data for different purposes. In particular, graph
properties are usually associated with invariant label-relevant substructures
(i.e., subgraphs) across clients, while label-irrelevant substructures can
appear in a client-specific manner. The issue of distribution shifts of graph
data hinders the efficiency of GNN training and leads to serious performance
degradation in FGL. To tackle the aforementioned issue, we propose a novel FGL
framework entitled FedVN that eliminates distribution shifts through
client-specific graph augmentation strategies with multiple learnable Virtual
Nodes (VNs). Specifically, FedVN lets the clients jointly learn a set of shared
VNs while training a global GNN model. To eliminate distribution shifts, each
client trains a personalized edge generator that determines how the VNs connect
local graphs in a client-specific manner. Furthermore, we provide theoretical
analyses indicating that FedVN can eliminate distribution shifts of graph data
across clients. Comprehensive experiments on four datasets under five settings
demonstrate the superiority of our proposed FedVN over nine baselines.",2024-12-26T14:16:15Z,http://arxiv.org/abs/2412.19229v1,"Xingbo Fu, Zihan Chen, Yinhan He, Song Wang, Binchi Zhang, Chen Chen, Jundong Li"
Multi-view Fake News Detection Model Based on Dynamic Hypergraph,"With the rapid development of online social networks and the inadequacies in
content moderation mechanisms, the detection of fake news has emerged as a
pressing concern for the public. Various methods have been proposed for fake
news detection, including text-based approaches as well as a series of
graph-based approaches. However, the deceptive nature of fake news renders
text-based approaches less effective. Propagation tree-based methods focus on
the propagation process of individual news, capturing pairwise relationships
but lacking the capability to capture high-order complex relationships. Large
heterogeneous graph-based approaches necessitate the incorporation of
substantial additional information beyond news text and user data, while
hypergraph-based approaches rely on predefined hypergraph structures. To tackle
these issues, we propose a novel dynamic hypergraph-based multi-view fake news
detection model (DHy-MFND) that learns news embeddings across three distinct
views: text-level, propagation tree-level, and hypergraph-level. By employing
hypergraph structures to model complex high-order relationships among multiple
news pieces and introducing dynamic hypergraph structure learning, we optimize
predefined hypergraph structures while learning news embeddings. Additionally,
we introduce contrastive learning to capture authenticity-relevant embeddings
across different views. Extensive experiments on two benchmark datasets
demonstrate the effectiveness of our proposed DHy-MFND compared with a broad
range of competing baselines.",2024-12-26T14:05:51Z,http://arxiv.org/abs/2412.19227v1,"Rongping Ye, Xiaobing Pei"
"Completion as Enhancement: A Degradation-Aware Selective Image Guided
  Network for Depth Completion","In this paper, we introduce the Selective Image Guided Network (SigNet), a
novel degradation-aware framework that transforms depth completion into depth
enhancement for the first time. Moving beyond direct completion using
convolutional neural networks (CNNs), SigNet initially densifies sparse depth
data through non-CNN densification tools to obtain coarse yet dense depth. This
approach eliminates the mismatch and ambiguity caused by direct convolution
over irregularly sampled sparse data. Subsequently, SigNet redefines completion
as enhancement, establishing a self-supervised degradation bridge between the
coarse depth and the targeted dense depth for effective RGB-D fusion. To
achieve this, SigNet leverages the implicit degradation to adaptively select
high-frequency components (e.g., edges) of RGB data to compensate for the
coarse depth. This degradation is further integrated into a multi-modal
conditional Mamba, dynamically generating the state parameters to enable
efficient global high-frequency information interaction. We conduct extensive
experiments on the NYUv2, DIML, SUN RGBD, and TOFDC datasets, demonstrating the
state-of-the-art (SOTA) performance of SigNet.",2024-12-26T14:05:01Z,http://arxiv.org/abs/2412.19225v1,"Zhiqiang Yan, Zhengxue Wang, Kun Wang, Jun Li, Jian Yang"
"Interference-Robust Broadband Rapidly-Varying MIMO Communications: A
  Knowledge-Data Dual Driven Framework","A novel time-efficient framework is proposed for improving the robustness of
a broadband multiple-input multiple-output (MIMO) system against unknown
interference under rapidly-varying channels. A mean-squared error (MSE)
minimization problem is formulated by optimizing the beamformers employed.
Since the unknown interference statistics are the premise for solving the
formulated problem, an interference statistics tracking (IST) module is first
designed. The IST module exploits both the time- and spatial-domain
correlations of the interference-plus-noise (IPN) covariance for the future
predictions with data training. Compared to the conventional signal-free space
sampling approach, the IST module can realize zero-pilot and low-latency
estimation. Subsequently, an interference-resistant hybrid beamforming (IR-HBF)
module is presented, which incorporates both the prior knowledge of the
theoretical optimization method as well as the data-fed training. Taking
advantage of the interpretable network structure, the IR-HBF module enables the
simplified mapping from the interference statistics to the beamforming weights.
The simulations are executed in high-mobility scenarios, where the numerical
results unveil that: 1) the proposed IST module attains promising prediction
accuracy compared to the conventional counterparts under different snapshot
sampling errors; and 2) the proposed IR-HBF module achieves lower MSE with
significantly reduced computational complexity.",2024-12-26T13:59:08Z,http://arxiv.org/abs/2412.19221v1,"Jingjing Zhao, Jing Su, Xianchi Lv, Kaiquan Cai, Yanbo Zhu, Yuanwei Liu, Naofal Al-Dhahir"
"Transformer-Based Wireless Capsule Endoscopy Bleeding Tissue Detection
  and Classification","Informed by the success of the transformer model in various computer vision
tasks, we design an end-to-end trainable model for the automatic detection and
classification of bleeding and non-bleeding frames extracted from Wireless
Capsule Endoscopy (WCE) videos. Based on the DETR model, our model uses the
Resnet50 for feature extraction, the transformer encoder-decoder for bleeding
and non-bleeding region detection, and a feedforward neural network for
classification. Trained in an end-to-end approach on the Auto-WCEBleedGen
Version 1 challenge training set, our model performs both detection and
classification tasks as a single unit. Our model achieves an accuracy, recall,
and F1-score classification percentage score of 98.28, 96.79, and 98.37
respectively, on the Auto-WCEBleedGen version 1 validation set. Further, we
record an average precision (AP @ 0.5), mean-average precision (mAP) of 0.7447
and 0.7328 detection results. This earned us a 3rd place position in the
challenge. Our code is publicly available via
https://github.com/BasitAlawode/WCEBleedGen.",2024-12-26T13:49:39Z,http://arxiv.org/abs/2412.19218v1,"Basit Alawode, Shibani Hamza, Adarsh Ghimire, Divya Velayudhan"
"Applying the maximum entropy principle to multi-species neural networks
  improves species distribution models","The rapid expansion of citizen science initiatives has led to a significant
growth of biodiversity databases, and particularly presence-only (PO)
observations. PO data are invaluable for understanding species distributions
and their dynamics, but their use in Species Distribution Models (SDM) is
curtailed by sampling biases and the lack of information on absences. Poisson
point processes are widely used for SDMs, with Maxent being one of the most
popular methods. Maxent maximises the entropy of a probability distribution
across sites as a function of predefined transformations of environmental
variables, called features. In contrast, neural networks and deep learning have
emerged as a promising technique for automatic feature extraction from complex
input variables. In this paper, we propose DeepMaxent, which harnesses neural
networks to automatically learn shared features among species, using the
maximum entropy principle. To do so, it employs a normalised Poisson loss where
for each species, presence probabilities across sites are modelled by a neural
network. We evaluate DeepMaxent on a benchmark dataset known for its spatial
sampling biases, using PO data for calibration and presence-absence (PA) data
for validation across six regions with different biological groups and
environmental covariates. Our results indicate that DeepMaxent improves model
performance over Maxent and other state-of-the-art SDMs across regions and
taxonomic groups. The method performs particularly well in regions of uneven
sampling, demonstrating substantial potential to improve species distribution
modelling. The method opens the possibility to learn more robust environmental
features predicting jointly many species and scales to arbitrary large numbers
of sites without an increased memory demand.",2024-12-26T13:47:04Z,http://arxiv.org/abs/2412.19217v1,"Maxime Ryckewaert, Diego Marcos, Christophe Botella, Maximilien Servajean, Pierre Bonnet, Alexis Joly"
"Optimizing Fantasy Sports Team Selection with Deep Reinforcement
  Learning","Fantasy sports, particularly fantasy cricket, have garnered immense
popularity in India in recent years, offering enthusiasts the opportunity to
engage in strategic team-building and compete based on the real-world
performance of professional athletes. In this paper, we address the challenge
of optimizing fantasy cricket team selection using reinforcement learning (RL)
techniques. By framing the team creation process as a sequential
decision-making problem, we aim to develop a model that can adaptively select
players to maximize the team's potential performance. Our approach leverages
historical player data to train RL algorithms, which then predict future
performance and optimize team composition. This not only represents a huge
business opportunity by enabling more accurate predictions of high-performing
teams but also enhances the overall user experience. Through empirical
evaluation and comparison with traditional fantasy team drafting methods, we
demonstrate the effectiveness of RL in constructing competitive fantasy teams.
Our results show that RL-based strategies provide valuable insights into player
selection in fantasy sports.",2024-12-26T13:36:18Z,http://arxiv.org/abs/2412.19215v1,"Shamik Bhattacharjee, Kamlesh Marathe, Hitesh Kapoor, Nilesh Patil"
Primordial Power Spectrum of Five Dimensional Uniform Inflation,"Five dimensional (5D) uniform inflation describes a de Sitter (or
approximate) solution of 5D Einstein equations, with cosmological constant and
a 5D Planck scale $M_* \sim 10^9$ GeV. During the inflationary period all
dimensions (compact and non-compact) expand exponentially in terms of the 5D
proper time. This set-up requires about 40 $e$-folds to expand the fifth
dimension from the fundamental length to the micron size. At the end of 5D
inflation (or at any given moment during the inflationary phase) one can
interpret the solution in terms of 4D fields using 4D Planck units from the
relation $M_p^2 = 2 \pi R M_*^3$, which amounts going to the 4D Einstein frame.
This implies that if the compactification radius $R$ expands $N$ $e$-folds,
then the 3D space would expand $3N/2$ $e$-folds as a result of a uniform 5D
inflation. We reexamine the primordial power spectrum predicted by this model
and show that it is consistent with Planck's measurements of the comic
microwave background. The best-fit to Planck data corresponds to $R \sim
10~\mu$m. A departure of the angular power spectrum predicted by 4D cosmology
is visible at multipole moment $\ell \sim 7$.",2024-12-26T13:24:36Z,http://arxiv.org/abs/2412.19213v1,"Luis A. Anchordoqui, Ignatios Antoniadis"
"Towards Better Spherical Sliced-Wasserstein Distance Learning with
  Data-Adaptive Discriminative Projection Direction","Spherical Sliced-Wasserstein (SSW) has recently been proposed to measure the
discrepancy between spherical data distributions in various fields, such as
geology, medical domains, computer vision, and deep representation learning.
However, in the original SSW, all projection directions are treated equally,
which is too idealistic and cannot accurately reflect the importance of
different projection directions for various data distributions. To address this
issue, we propose a novel data-adaptive Discriminative Spherical
Sliced-Wasserstein (DSSW) distance, which utilizes a projected energy function
to determine the discriminative projection direction for SSW. In our new DSSW,
we introduce two types of projected energy functions to generate the weights
for projection directions with complete theoretical guarantees. The first type
employs a non-parametric deterministic function that transforms the projected
Wasserstein distance into its corresponding weight in each projection
direction. This improves the performance of the original SSW distance with
negligible additional computational overhead. The second type utilizes a neural
network-induced function that learns the projection direction weight through a
parameterized neural network based on data projections. This further enhances
the performance of the original SSW distance with less extra computational
overhead. Finally, we evaluate the performance of our proposed DSSW by
comparing it with several state-of-the-art methods across a variety of machine
learning tasks, including gradient flows, density estimation on real earth
data, and self-supervised learning.",2024-12-26T13:23:37Z,http://arxiv.org/abs/2412.19212v1,"Hongliang Zhang, Shuo Chen, Lei Luo, Jian Yang"
"Large Language Models Meet Graph Neural Networks: A Perspective of Graph
  Mining","Graph mining is an important area in data mining and machine learning that
involves extracting valuable information from graph-structured data. In recent
years, significant progress has been made in this field through the development
of graph neural networks (GNNs). However, GNNs are still deficient in
generalizing to diverse graph data. Aiming to this issue, Large Language Models
(LLMs) could provide new solutions for graph mining tasks with their superior
semantic understanding. In this review, we systematically review the
combination and application techniques of LLMs and GNNs and present a novel
taxonomy for research in this interdisciplinary field, which involves three
main categories: GNN-driving-LLM, LLM-driving-GNN, and GNN-LLM-co-driving.
Within this framework, we reveal the capabilities of LLMs in enhancing graph
feature extraction as well as improving the effectiveness of downstream tasks
such as node classification, link prediction, and community detection. Although
LLMs have demonstrated their great potential in handling graph-structured data,
their high computational requirements and complexity remain challenges. Future
research needs to continue to explore how to efficiently fuse LLMs and GNNs to
achieve more powerful graph learning and reasoning capabilities and provide new
impetus for the development of graph mining techniques.",2024-12-26T13:21:09Z,http://arxiv.org/abs/2412.19211v1,"Yuxin You, Zhen Liu, Xiangchao Wen, Yongtao Zhang, Wei Ai"
Context-Aware Deep Learning for Multi Modal Depression Detection,"In this study, we focus on automated approaches to detect depression from
clinical interviews using multi-modal machine learning (ML). Our approach
differentiates from other successful ML methods such as context-aware analysis
through feature engineering and end-to-end deep neural networks for depression
detection utilizing the Distress Analysis Interview Corpus. We propose a novel
method that incorporates: (1) pre-trained Transformer combined with data
augmentation based on topic modelling for textual data; and (2) deep 1D
convolutional neural network (CNN) for acoustic feature modeling. The
simulation results demonstrate the effectiveness of the proposed method for
training multi-modal deep learning models. Our deep 1D CNN and Transformer
models achieved state-of-the-art performance for audio and text modalities
respectively. Combining them in a multi-modal framework also outperforms
state-of-the-art for the combined setting. Code available at
https://github.com/genandlam/multi-modal-depression-detection",2024-12-26T13:19:26Z,http://arxiv.org/abs/2412.19209v1,"Genevieve Lam, Huang Dongyan, Weisi Lin"
"Open cluster dissolution rate and the initial cluster mass function in
  the solar neighbourhood. Modelling the age and mass distributions of clusters
  observed by Gaia","Context. The dissolution rate of open clusters (OCs) and integration of their
stars into the Milky Way's field population has been previously explored using
their age distribution. With the advent of the Gaia mission, we have an
exceptional opportunity to revisit and enhance these studies with ages and
masses from high quality data. Aims. To build a comprehensive Gaia-based OC
mass catalogue which, combined with the age distribution, allows a deeper
investigation of the disruption experienced by OCs within the solar
neighbourhood. Methods. Masses were determined by comparing luminosity
distributions to theoretical luminosity functions. The limiting and core radii
of the clusters were obtained by fitting the King function to their observed
density profiles. We examined the disruption process through simulations of the
build-up and mass evolution of a population of OCs which were compared to the
observed mass and age distributions. Results. Our analysis yielded an OC mass
distribution with a peak at $log(M)$ = 2.7 dex ($\sim 500 M_{\odot}$), as well
as radii for 1724 OCs. Our simulations showed that using a power-law Initial
Cluster Mass Function (ICMF) no parameters were able to reproduce the observed
mass distribution. Moreover, we find that a skew log-normal ICMF provides a
good match to the observations and that the disruption time of a $10^4
M{_\odot}$ OC is $t_4^{tot} = 2.9 \pm 0.4$ Gyr. Conclusions. Our results
indicate that the OC disruption time $t_4^{tot}$ is about twice longer than
previous estimates based solely on OC age distributions. We find that the shape
of the ICMF for bound OCs differs from that of embedded clusters, which could
imply a low typical star formation efficiency of $\leq 20\%$ in OCs. Our
results also suggest a lower limit of $\sim 60 M{_\odot}$ for bound OCs in the
solar neighbourhood.",2024-12-26T12:54:51Z,http://arxiv.org/abs/2412.19204v1,"Duarte Almeida, André Moitinho, Sandro Moreira"
"Connected triangle-free planar graphs whose second largest eigenvalue is
  at most 1","Let $\lambda_2$ be the second largest eigenvalue of the adjacency matrix of a
connected graph. In 2023, Li and Sun \cite{LiSun1} determined all the connected
$\{K_{2,3}, K_4\}$-minor free graphs whose second largest eigenvalue
$\lambda_2\le 1$. As a continuance of it, in this paper we completely identify
all the connected $\{K_5,K_{3,3}\}$-minor free graphs without $C_3$ whose
second largest eigenvalue does not exceed 1. This partially solves an open
problem posed by Li and Sun \cite{LiSun1}: Characterize all connected planar
graphs whose second largest eigenvalue is at most $1.$ Our main tools include
the spectral theory and the local structure characterization of the planar
graph with respect to its girth.",2024-12-26T12:54:32Z,http://arxiv.org/abs/2412.19203v1,"Kun Cheng, Shuchao Li"
"GAIS: A Novel Approach to Instance Selection with Graph Attention
  Networks","Instance selection (IS) is a crucial technique in machine learning that aims
to reduce dataset size while maintaining model performance. This paper
introduces a novel method called Graph Attention-based Instance Selection
(GAIS), which leverages Graph Attention Networks (GATs) to identify the most
informative instances in a dataset. GAIS represents the data as a graph and
uses GATs to learn node representations, enabling it to capture complex
relationships between instances. The method processes data in chunks, applies
random masking and similarity thresholding during graph construction, and
selects instances based on confidence scores from the trained GAT model.
Experiments on 13 diverse datasets demonstrate that GAIS consistently
outperforms traditional IS methods in terms of effectiveness, achieving high
reduction rates (average 96\%) while maintaining or improving model
performance. Although GAIS exhibits slightly higher computational costs, its
superior performance in maintaining accuracy with significantly reduced
training data makes it a promising approach for graph-based data selection.",2024-12-26T12:51:14Z,http://arxiv.org/abs/2412.19201v1,"Zahiriddin Rustamov, Ayham Zaitouny, Rafat Damseh, Nazar Zaki"
"Personalized Dynamic Music Emotion Recognition with Dual-Scale
  Attention-Based Meta-Learning","Dynamic Music Emotion Recognition (DMER) aims to predict the emotion of
different moments in music, playing a crucial role in music information
retrieval. The existing DMER methods struggle to capture long-term dependencies
when dealing with sequence data, which limits their performance. Furthermore,
these methods often overlook the influence of individual differences on emotion
perception, even though everyone has their own personalized emotional
perception in the real world. Motivated by these issues, we explore more
effective sequence processing methods and introduce the Personalized DMER
(PDMER) problem, which requires models to predict emotions that align with
personalized perception. Specifically, we propose a Dual-Scale Attention-Based
Meta-Learning (DSAML) method. This method fuses features from a dual-scale
feature extractor and captures both short and long-term dependencies using a
dual-scale attention transformer, improving the performance in traditional
DMER. To achieve PDMER, we design a novel task construction strategy that
divides tasks by annotators. Samples in a task are annotated by the same
annotator, ensuring consistent perception. Leveraging this strategy alongside
meta-learning, DSAML can predict personalized perception of emotions with just
one personalized annotation sample. Our objective and subjective experiments
demonstrate that our method can achieve state-of-the-art performance in both
traditional DMER and PDMER.",2024-12-26T12:47:35Z,http://arxiv.org/abs/2412.19200v1,"Dengming Zhang, Weitao You, Ziheng Liu, Lingyun Sun, Pei Chen"
Multi-Attribute Constraint Satisfaction via Language Model Rewriting,"Obeying precise constraints on top of multiple external attributes is a
common computational problem underlying seemingly different domains, from
controlled text generation to protein engineering. Existing language model (LM)
controllability methods for multi-attribute constraint satisfaction often rely
on specialized architectures or gradient-based classifiers, limiting their
flexibility to work with arbitrary black-box evaluators and pretrained models.
Current general-purpose large language models, while capable, cannot achieve
fine-grained multi-attribute control over external attributes. Thus, we create
Multi-Attribute Constraint Satisfaction (MACS), a generalized method capable of
finetuning language models on any sequential domain to satisfy user-specified
constraints on multiple external real-value attributes. Our method trains LMs
as editors by sampling diverse multi-attribute edit pairs from an initial set
of paraphrased outputs. During inference, LM iteratively improves upon its
previous solution to satisfy constraints for all attributes by leveraging our
designed constraint satisfaction reward. We additionally experiment with
reward-weighted behavior cloning to further improve the constraint satisfaction
rate of LMs. To evaluate our approach, we present a new Fine-grained Constraint
Satisfaction (FineCS) benchmark, featuring two challenging tasks: (1) Text
Style Transfer, where the goal is to simultaneously modify the sentiment and
complexity of reviews, and (2) Protein Design, focusing on modulating
fluorescence and stability of Green Fluorescent Proteins (GFP). Our empirical
results show that MACS achieves the highest threshold satisfaction in both
FineCS tasks, outperforming strong domain-specific baselines. Our work opens
new avenues for generalized and real-value multi-attribute control, with
implications for diverse applications spanning NLP and bioinformatics.",2024-12-26T12:36:39Z,http://arxiv.org/abs/2412.19198v1,"Ashutosh Baheti, Debanjana Chakraborty, Faeze Brahman, Ronan Le Bras, Ximing Lu, Nouha Dziri, Yejin Choi, Mark Riedl, Maarten Sap"
"Implications for the non-Gaussianity of primordial gravitational waves
  from pulsar timing arrays","The detection of a stochastic signal by recent pulsar timing array (PTA)
collaborations, including NANOGrav, PPTA, EPTA+InPTA, CPTA and MPTA, has opened
a new window to explore gravitational waves (GWs) at nanohertz frequencies.
Motivated by the possibility that such a signal could arise from primordial
gravitational waves (PGWs), we investigate the implications of tensor
non-Gaussianity for the PGW power spectrum. Utilizing PTA data sets, we provide
constraints on local-type tensor non-Gaussianity parameter ${F}_{\mathrm{NL}}$.
We find $|{F}_{\mathrm{NL}}|\lesssim 7.97$ for a log-normal PGW power spectrum.
Our analysis reveals that even moderate tensor non-Gaussianity can lead to
significant deviations from standard predictions, thereby offering a novel
means to test inflationary scenarios and probe the underlying dynamics of the
early Universe. Future multi-band GW observatories, such as LISA, Taiji, and
TianQin, will be instrumental in complementing these efforts and further
refining our understanding of tensor non-Gaussianity.",2024-12-26T12:31:58Z,http://arxiv.org/abs/2412.19196v1,"Zhi-Zhang Peng, You Wu, Lang Liu"
Provably Efficient Exploration in Reward Machines with Low Regret,"We study reinforcement learning (RL) for decision processes with
non-Markovian reward, in which high-level knowledge of the task in the form of
reward machines is available to the learner. We consider probabilistic reward
machines with initially unknown dynamics, and investigate RL under the
average-reward criterion, where the learning performance is assessed through
the notion of regret. Our main algorithmic contribution is a model-based RL
algorithm for decision processes involving probabilistic reward machines that
is capable of exploiting the structure induced by such machines. We further
derive high-probability and non-asymptotic bounds on its regret and demonstrate
the gain in terms of regret over existing algorithms that could be applied, but
obliviously to the structure. We also present a regret lower bound for the
studied setting. To the best of our knowledge, the proposed algorithm
constitutes the first attempt to tailor and analyze regret specifically for RL
with probabilistic reward machines.",2024-12-26T12:25:04Z,http://arxiv.org/abs/2412.19194v1,"Hippolyte Bourel, Anders Jonsson, Odalric-Ambrym Maillard, Chenxiao Ma, Mohammad Sadegh Talebi"
"Game-Theoretically Secure Distributed Protocols for Fair Allocation in
  Coalitional Games","We consider game-theoretically secure distributed protocols for coalition
games that approximate the Shapley value with small multiplicative error. Since
all known existing approximation algorithms for the Shapley value are
randomized, it is a challenge to design efficient distributed protocols among
mutually distrusted players when there is no central authority to generate
unbiased randomness. The game-theoretic notion of maximin security has been
proposed to offer guarantees to an honest player's reward even if all other
players are susceptible to an adversary.
  Permutation sampling is often used in approximation algorithms for the
Shapley value. A previous work in 1994 by Zlotkin et al. proposed a simple
constant-round distributed permutation generation protocol based on commitment
scheme, but it is vulnerable to rushing attacks. The protocol, however, can
detect such attacks.
  In this work, we model the limited resources of an adversary by a violation
budget that determines how many times it can perform such detectable attacks.
Therefore, by repeating the number of permutation samples, an honest player's
reward can be guaranteed to be close to its Shapley value. We explore both high
probability and expected maximin security. We obtain an upper bound on the
number of permutation samples for high probability maximin security, even with
an unknown violation budget. Furthermore, we establish a matching lower bound
for the weaker notion of expected maximin security in specific permutation
generation protocols. We have also performed experiments on both synthetic and
real data to empirically verify our results.",2024-12-26T12:13:21Z,http://arxiv.org/abs/2412.19192v1,"T-H. Hubert Chan, Qipeng Kuang, Quan Xue"
"Biology Instructions: A Dataset and Benchmark for Multi-Omics Sequence
  Understanding Capability of Large Language Models","Large language models have already demonstrated their formidable capabilities
in general domains, ushering in a revolutionary transformation. However,
exploring and exploiting the extensive knowledge of these models to comprehend
multi-omics biology remains underexplored. To fill this research gap, we first
introduce Biology-Instructions, the first large-scale multi-omics biological
sequences-related instruction-tuning dataset including DNA, RNA, proteins, and
multi-molecules, designed to bridge the gap between large language models
(LLMs) and complex biological sequences-related tasks. This dataset can enhance
the versatility of LLMs by integrating diverse biological sequenced-based
prediction tasks with advanced reasoning capabilities, while maintaining
conversational fluency. Additionally, we reveal significant performance
limitations in even state-of-the-art LLMs on biological sequence-related
multi-omics tasks without specialized pre-training and instruction-tuning. We
further develop a strong baseline called ChatMultiOmics with a novel
three-stage training pipeline, demonstrating the powerful ability to understand
biology by using Biology-Instructions. Biology-Instructions and ChatMultiOmics
are publicly available and crucial resources for enabling more effective
integration of LLMs with multi-omics sequence analysis.",2024-12-26T12:12:23Z,http://arxiv.org/abs/2412.19191v1,"Haonan He, Yuchen Ren, Yining Tang, Ziyang Xu, Junxian Li, Minghao Yang, Di Zhang, Dong Yuan, Tao Chen, Shufei Zhang, Yuqiang Li, Nanqing Dong, Wanli Ouyang, Dongzhan Zhou, Peng Ye"
An End-to-End Depth-Based Pipeline for Selfie Image Rectification,"Portraits or selfie images taken from a close distance typically suffer from
perspective distortion. In this paper, we propose an end-to-end deep
learning-based rectification pipeline to mitigate the effects of perspective
distortion. We learn to predict the facial depth by training a deep CNN. The
estimated depth is utilized to adjust the camera-to-subject distance by moving
the camera farther, increasing the camera focal length, and reprojecting the 3D
image features to the new perspective. The reprojected features are then fed to
an inpainting module to fill in the missing pixels. We leverage a
differentiable renderer to enable end-to-end training of our depth estimation
and feature extraction nets to improve the rectified outputs. To boost the
results of the inpainting module, we incorporate an auxiliary module to predict
the horizontal movement of the camera which decreases the area that requires
hallucination of challenging face parts such as ears. Unlike previous works, we
process the full-frame input image at once without cropping the subject's face
and processing it separately from the rest of the body, eliminating the need
for complex post-processing steps to attach the face back to the subject's
body. To train our network, we utilize the popular game engine Unreal Engine to
generate a large synthetic face dataset containing various subjects, head
poses, expressions, eyewear, clothes, and lighting. Quantitative and
qualitative results show that our rectification pipeline outperforms previous
methods, and produces comparable results with a time-consuming 3D GAN-based
method while being more than 260 times faster.",2024-12-26T11:57:54Z,http://arxiv.org/abs/2412.19189v1,"Ahmed Alhawwary, Phong Nguyen-Ha, Janne Mustaniemi, Janne Heikkilä"
New Physics in the 3-3-1 models,"Two main ingredients of current particle physics
  such as local gauge symmetry and mass generation via the Higgs mechanism
being basic ground of the Standard Model are widely confirmed by
  experimental data. However, some problems such as neutrino masses, dark
matter, baryon asymmetry of Universe have clearly indicated that the Standard
Model cannot be the ultimate theory of nature. To surpass the mentioned
puzzles,
  many extensions of the Standard Model (called beyond Standard Model) have
been proposed. Among beyond Standard Models, the 3-3-1 models have some
intriguing features and they get wide attention. The pioneer models develop in
some directions. In this paper, %some new main versions of the 3-3-1 models and
their consequences are presented.",2024-12-26T11:55:43Z,http://arxiv.org/abs/2412.19188v1,H. N. Long
"Outlier-Bias Removal with Alpha Divergence: A Robust Non-Convex
  Estimator for Linear Regression","Convex and penalized robust methods often suffer from bias induced by large
outliers, limiting their effectiveness in adversarial or heavy-tailed settings.
In this study, we propose a novel approach that eliminates this bias (when
possible) by leveraging a non-convex $M$-estimator based on the alpha
divergence. We address the problem of estimating the parameters vector in high
dimensional linear regression, even when a subset of the data has been
deliberately corrupted by an adversary with full knowledge of the dataset and
its underlying distribution.
  Our primary contribution is to demonstrate that the objective function,
although non-convex, exhibits convexity within a carefully chosen basin of
attraction, enabling robust and unbiased estimation. Additionally, we establish
three key theoretical guarantees for the estimator: (a) a deviation bound that
is minimax optimal up to a logarithmic factor, (b) an improved unbiased bound
when the outliers are large and (c) asymptotic normality as the sample size
increases. Finally, we validate the theoretical findings through empirical
comparisons with state-of-the-art estimators on both synthetic and real-world
datasets, highlighting the proposed method's superior robustness, efficiency,
and ability to mitigate outlier-induced bias.",2024-12-26T11:42:46Z,http://arxiv.org/abs/2412.19183v1,"Ilyes Hammouda, Mohamed Ndaoud, and Abd-Krim Seghouane"
"Unraveling the magnetic and electronic complexity of intermetallic
  ErPd$_2$Si$_2$: Anisotropic thermal expansion, phase transitions, and twofold
  magnetotransport behavior","We present a comprehensive investigation into the physical properties of
intermetallic ErPd$_2$Si$_2$, a compound renowned for its intriguing magnetic
and electronic characteristics. We confirm the tetragonal crystal structure of
ErPd$_2$Si$_2$ within the $I4/mmm$ space group. Notably, we observed
anisotropic thermal expansion, with the lattice constant $a$ expanding and $c$
contracting between 15 K and 300 K. This behavior is attributed to lattice
vibrations and electronic contributions. Heat capacity measurements revealed
three distinct temperature regimes: $T_1 \sim 3.0$ K, $T_\textrm{N} \sim 4.20$
K, and $T_2 \sim 15.31$ K. These correspond to the disappearance of
spin-density waves, the onset of an incommensurate antiferromagnetic (AFM)
structure, and the crystal-field splitting and/or the presence of short-range
spin fluctuations, respectively. Remarkably, the AFM phase transition anomaly
was observed exclusively in low-field magnetization data (120 Oe) at
$T_\textrm{N}$. A high magnetic field ($B =$ 3 T) effectively suppressed this
anomaly, likely due to spin-flop and spin-flip transitions. Furthermore, the
extracted effective PM moments closely matched the expected theoretical value,
suggesting a dominant magnetic contribution from localized 4$f$ spins of Er.
Additionally, significant differences in resistance ($R$) values at low
temperatures under applied $B$ indicated a magnetoresistance (MR) effect with a
minimum value of -4.36\%. Notably, the measured MR effect exhibited anisotropic
behavior, where changes in the strength or direction of the applied $B$ induced
variations in the MR effect. A twofold symmetry of $R$ was discerned at 3 T and
9 T, originating from the orientation of spin moments relative to the applied
$B$. Intriguingly, above $T_\textrm{N}$, short-range spin fluctuations also
displayed a preferred orientation along the $c$-axis due to single-ion
anisotropy.",2024-12-26T11:39:24Z,http://arxiv.org/abs/2412.19181v1,"Kaitong Sun, Si Wu, Guanping Xu, Lingwei Li, Hongyu Chen, Qian Zhao, Muqing Su, Wolfgang Schmidt, Chongde Cao, Hai-Feng Li"
"Mask Approximation Net: Merging Feature Extraction and Distribution
  Learning for Remote Sensing Change Captioning","Remote sensing image change description, as a novel multimodal task in the
field of remote sensing processing, not only enables the detection of changes
in surface conditions but also provides detailed descriptions of these changes,
thereby enhancing human interpretability and interactivity. However, previous
methods mainly employed Convolutional Neural Network (CNN) architectures to
extract bitemporal image features. This approach often leads to an overemphasis
on designing specific network architectures and limits the captured feature
distributions to the current dataset, resulting in poor generalizability and
robustness when applied to other datasets or real-world scenarios. To address
these limitations, this paper proposes a novel approach for remote sensing
image change detection and description that integrates diffusion models, aiming
to shift the focus from conventional feature learning paradigms to data
distribution learning. The proposed method primarily includes a simple
multi-scale change detection module, whose output features are subsequently
refined using a diffusion model. Additionally, we introduce a frequency-guided
complex filter module to handle high-frequency noise during the diffusion
process, which helps to maintain model performance. Finally, we validate the
effectiveness of our proposed method on several remote sensing change detection
description datasets, demonstrating its superior performance. The code
available at MaskApproxNet.",2024-12-26T11:35:57Z,http://arxiv.org/abs/2412.19179v1,"Dongwei Sun, Xiangyong Cao"
Physical nature of quasi-stable structures existing in antimony melt,"Equilibrium antimony melt near the melting temperature is characterised by
structural features that are not present in simple single-component liquids.
The cause of these features may be long-lived structural formations that are
not yet fully understood. The present work provides the detailed
characterization of the structures formed in liquid antimony near the melting
temperature based on the results of quantum chemical calculations and the
available neutron and X-ray diffraction data. The quasi-stable structures in
antimony melt are detected with lifetimes exceeding the structural relaxation
time of this melt. These structures are characterised by a low degree of order
and spatial localisation. It is shown for the first time that the elementary
units of these quasi-stable structures are triplets of atoms with
characteristic lengths of $3.07$\,\AA~and $4.7$\,\AA~and characteristic angles
of $45$ and $90$ degrees. It was found that these triplets can form chains and
percolating clusters up to $\sim15$\,\AA~in length. The characteristic lengths
of these triplets are fully consistent with the correlation lengths associated
with short-range order in the antimony melt as determined by diffraction
experiments.",2024-12-26T11:30:38Z,http://arxiv.org/abs/2412.19177v1,"Artem A. Tsygankov, Bulat N. Galimzyanov, Anatolii V. Mokshin"
"VQE for Ising Model \&amp; A Comparative Analysis of Classical and Quantum
  Optimization Methods","In this study, we delved into several optimization methods, both classical
and quantum, and analyzed the quantum advantage that each of these methods
offered, and then we proposed a new combinatorial optimization scheme, deemed
as QN-SPSA+PSR which combines calculating approximately Fubini-study metric
(QN-SPSA) and the exact evaluation of gradient by Parameter-Shift Rule (PSR).
The QN-SPSA+PSR method integrates the QN-SPSA computational efficiency with the
precise gradient computation of the PSR, improving both stability and
convergence speed while maintaining low computational consumption. Our results
provide a new potential quantum supremacy in the VQE's optimization subroutine
and enhance viable paths toward efficient quantum simulations on Noisy
Intermediate-Scale Quantum Computing (NISQ) devices. Additionally, we also
conducted a detailed study of quantum circuit ansatz structures in order to
find the one that would work best with the Ising model and NISQ, in which we
utilized the symmetry of the investigated model.",2024-12-26T11:25:30Z,http://arxiv.org/abs/2412.19176v1,"Duc-Truyen Le, Vu-Linh Nguyen, Triet Minh Ha, Cong-Ha Nguyen, Quoc-Hung Nguyen, Van-Duy Nguyen"
"Convergence analysis of PM-BDF2 method for quasiperiodic parabolic
  equations","Numerically solving parabolic equations with quasiperiodic coefficients is a
significant challenge due to the potential formation of space-filling
quasiperiodic structures that lack translational symmetry or decay. In this
paper, we introduce a highly accurate numerical method for solving
time-dependent quasiperiodic parabolic equations. We discretize the spatial
variables using the projection method (PM) and the time variable with the
second-order backward differentiation formula (BDF2). We provide a complexity
analysis for the resulting PM-BDF2 method. Furthermore, we conduct a detailed
convergence analysis, demonstrating that the proposed method exhibits spectral
accuracy in space and second-order accuracy in time. Numerical results in both
one and two dimensions validate these convergence results, highlighting the
PM-BDF2 method as a highly efficient algorithm for addressing quasiperiodic
parabolic equations.",2024-12-26T11:23:15Z,http://arxiv.org/abs/2412.19175v1,"Kai Jiang, Meng Li, Juan Zhang, Lei Zhang"
"High-Precision Schottky Diagnostics for Low-SNR Betatron Tune
  Measurement in Ramping Synchrotrons","This paper presents a novel Schottky diagnostics-based method for real-time
betatron tune measurement in ramping synchrotrons, exemplified by the Shanghai
Advanced Proton Therapy (SAPT) facility. The proposed approach achieves high
precision under challenging conditions, including low frequency resolution and
signal-to-noise ratios (SNR) as low as -15 dB within the bandwidth of a
narrowband detector. By employing Short-Time Fourier Transform (STFT) analysis
with automatically optimized time windows, the method effectively addresses the
rapid increase in revolution frequency from 4 MHz to 7.5 MHz over 0.35 seconds,
assuming constant beam properties within each window. Monte Carlo
macro-particle simulations are employed to generate Schottky signals, which are
subsequently combined with real noise collected from an analog-to-digital
converter to emulate practical conditions. The betatron tune measurement
procedure integrates longitudinal signal exclusion, spectrum smoothing, and
spectral multiplication to reliably extract transverse Schottky spectra buried
in noise, to enable precise betatron tune determination. Experimental results
demonstrate that the proposed method surpasses existing approaches in
precision, accuracy, and robustness, while meeting stringent design
requirements. This innovative approach addresses key limitations of Schottky
diagnostics for betatron tune measurement in ramping synchrotrons, providing a
foundation for applications such as proton therapy.",2024-12-26T11:05:21Z,http://arxiv.org/abs/2412.19171v1,"Peihan Sun, Manzhou Zhang, Renxian Yuan, Deming Li, Jian Dong, Ying Shi"
"Accelerating Stochastic Gravitational Wave Backgrounds Parameter
  Estimation in Pulsar Timing Arrays with Flow Matching","Pulsar timing arrays (PTAs) are essential tools for detecting the stochastic
gravitational wave background (SGWB), but their analysis faces significant
computational challenges. Traditional methods like Markov-chain Monte Carlo
(MCMC) struggle with high-dimensional parameter spaces where noise parameters
often dominate, while existing deep learning approaches fail to model the
Hellings-Downs (HD) correlation or are validated only on synthetic datasets. We
propose a flow-matching-based continuous normalizing flow (CNF) for efficient
and accurate PTA parameter estimation. By focusing on the 10 most contributive
pulsars from the NANOGrav 15-year dataset, our method achieves posteriors
consistent with MCMC, with a Jensen-Shannon divergence below \(10^{-2}\) nat,
while reducing sampling time from 50 hours to 4 minutes. Powered by a versatile
embedding network and a reweighting loss function, our approach prioritizes the
SGWB parameters and scales effectively for future datasets. It enables precise
reconstruction of SGWB and opens new avenues for exploring vast observational
data and uncovering potential new physics, offering a transformative tool for
advancing gravitational wave astronomy.",2024-12-26T11:02:11Z,http://arxiv.org/abs/2412.19169v1,"Bo Liang, Chang Liu, Tianyu Zhao, Minghui Du, Manjia Liang, Ruijun Shi, Hong Guo, Yuxiang Xu, Li-e Qiang, Peng Xu, Wei-Liang Qian, Ziren Luo"
"Fluid-particle interactions and fluctuation-dissipation relations I --
  General linear theory and basic fluctuational patterns","The article provides a unitary and complete solution to the
fluctuation-dissipation relations for particle hydromechanics in a generic
fluid, accounting for the hydrodynamic fluid-particle interactions (including
arbitrary memory kernels in the description of dissipative and fluid inertial
effects) in linear hydrodynamic regimes, via the concepts of fluctuational
patterns. This is achieved by expressing the memory kernels as a linear
superposition of exponentially decaying modes. Given the structure of the
interaction with the internal degrees of freedom, and assuming the
representation of the thermal force as a superposition of modal contributions,
the fluctuation-dissipation relation follows simply from the moment analysis of
the corresponding Fokker-Planck equation, imposing the condition that at
equilibrium all the internal degrees of freedom are uncorrelated with particle
velocity. Moreover, the functional structure of the resulting equation of
motion corresponds to the principle of complete decoupling amongst the internal
degrees of freedom. The theory is extended to the case of confined geometries,
by generalizing previous results including the effect of fluid inertia.",2024-12-26T10:52:07Z,http://arxiv.org/abs/2412.19166v1,"Massimiliano Giona, Giuseppe Procopio, Chiara Pezzotti"
"Revisiting Monocular 3D Object Detection from Scene-Level Depth
  Retargeting to Instance-Level Spatial Refinement","Monocular 3D object detection is challenging due to the lack of accurate
depth. However, existing depth-assisted solutions still exhibit inferior
performance, whose reason is universally acknowledged as the unsatisfactory
accuracy of monocular depth estimation models. In this paper, we revisit
monocular 3D object detection from the depth perspective and formulate an
additional issue as the limited 3D structure-aware capability of existing depth
representations (\textit{e.g.}, depth one-hot encoding or depth distribution).
To address this issue, we propose a novel depth-adapted monocular 3D object
detection network, termed \textbf{RD3D}, that mainly comprises a Scene-Level
Depth Retargeting (SDR) module and an Instance-Level Spatial Refinement (ISR)
module. The former incorporates the scene-level perception of 3D structures,
retargeting traditional depth representations to a new formulation:
\textbf{Depth Thickness Field}. The latter refines the voxel spatial
representation with the guidance of instances, eliminating the ambiguity of 3D
occupation and thus improving detection accuracy. Extensive experiments on the
KITTI and Waymo datasets demonstrate our superiority to existing
state-of-the-art (SoTA) methods and the universality when equipped with
different depth estimation models. The code will be available.",2024-12-26T10:51:50Z,http://arxiv.org/abs/2412.19165v1,"Qiude Zhang, Chunyu Lin, Zhijie Shen, Nie Lang, Yao Zhao"
Master Stability Functions in Complex Networks,"Synchronization is an emergent phenomenon in coupled dynamical networks. The
Master Stability Function (MSF) is a highly elegant and powerful tool for
characterizing the stability of synchronization states. However, a significant
challenge lies in determining the MSF for complex dynamical networks driven by
nonlinear interaction mechanisms. These mechanisms introduce additional
complexity through the intricate connectivity of interacting elements within
the network and the intrinsic dynamics, which are governed by nonlinear
processes with diverse parameters and higher dimensionality of systems. Over
the past 25 years, extensive research has focused on determining the MSF for
pairwise coupled identical systems with diffusive coupling. Our literature
survey highlights two significant advancements in recent years: the
consideration of multilayer networks instead of single-layer networks and the
extension of MSF analysis to incorporate higher-order interactions alongside
pairwise interactions.
  In this review article, we revisit the analysis of the MSF for diffusively
pairwise coupled dynamical systems and extend this framework to more general
coupling schemes. Furthermore, we systematically derive the MSF for multilayer
dynamical networks and single-layer coupled systems by incorporating
higher-order interactions alongside pairwise interactions. The primary focus of
our review is on the analytical derivation and numerical computation of the MSF
for complex dynamical networks. Finally, we demonstrate the application of the
MSF in data science, emphasizing its relevance and potential in this rapidly
evolving field.",2024-12-26T10:47:00Z,http://arxiv.org/abs/2412.19163v1,"Suman Acharyya, Priodyuti Pradhan, Chandrakala Meena"
"Dual Channel Multi-Attention in ViT for Biometric Authentication using
  Forehead Subcutaneous Vein Pattern and Periocular Pattern","Traditional biometric systems, like face and fingerprint recognition, have
encountered significant setbacks due to wearing face masks and hygiene
concerns. To meet the challenges of the partially covered face due to face
masks and hygiene concerns of fingerprint recognition, this paper proposes a
novel dual-channel multi-attention Vision Transformer (ViT) framework for
biometric authentication using forehead subcutaneous vein patterns and
periocular patterns, offering a promising alternative to traditional methods,
capable of performing well even with face masks and without any physical touch.
The proposed framework leverages a dual-channel ViT architecture, designed to
handle two distinct biometric traits. It can capture long-range dependencies of
independent features from the vein and periocular patterns. A custom classifier
is then designed to integrate the independently extracted features, producing a
final class prediction. The performance of the proposed algorithm was
rigorously evaluated using the Forehead Subcutaneous Vein Pattern and
Periocular Biometric Pattern (FSVP-PBP) database. The results demonstrated the
superiority of the algorithm over state-of-the-art methods, achieving
remarkable classification accuracy of $99.3 \pm 0.02\%$ with the combined vein
and periocular patterns.",2024-12-26T10:40:15Z,http://arxiv.org/abs/2412.19160v1,"Arun K. Sharma, Shubhobrata Bhattacharya, Motahar Reza"
"Mobile Robots through Task-Based Human Instructions using Incremental
  Curriculum Learning","This paper explores the integration of incremental curriculum learning (ICL)
with deep reinforcement learning (DRL) techniques to facilitate mobile robot
navigation through task-based human instruction. By adopting a curriculum that
mirrors the progressive complexity encountered in human learning, our approach
systematically enhances robots' ability to interpret and execute complex
instructions over time. We explore the principles of DRL and its synergy with
ICL, demonstrating how this combination not only improves training efficiency
but also equips mobile robots with the generalization capability required for
navigating through dynamic indoor environments. Empirical results indicate that
robots trained with our ICL-enhanced DRL framework outperform those trained
without curriculum learning, highlighting the benefits of structured learning
progressions in robotic training.",2024-12-26T10:38:40Z,http://arxiv.org/abs/2412.19159v1,"Muhammad A. Muttaqien, Ayanori Yorozu, Akihisa Ohya"
Nonlinear Piezomagnetic Effects in $g$-wave Altermagnets,"We theoretically study the generation of net magnetization induced by lattice
distortion due to elastic waves in $g$-wave altermagnets, which exhibit the
symmetric spin-split band structure under collinear spin textures free from the
relativistic spin--orbit coupling. By analyzing a tight-binding model in a
two-dimensional tetragonal system, we show that the $g$-wave altermagnets give
rise to the nonlinear piezomagnetic effect, where a net magnetization is
induced by the second-order strain. We compare the results for the $g$-wave
altermagnets with those for the $d$-wave altermagnets, where the linear
piezomagnetic effect occurs. As a result, we find that the induced
magnetization is enhanced when the Fermi level lies on the band with the large
spin splitting in both cases. We also show that the magnitudes of the induced
magnetization are comparable to each other. Our results indicate that the
nonlinear piezomagnetic effect is a good phenomenon to characterize the
physical properties in $g$-wave altermagnets.",2024-12-26T10:37:27Z,http://arxiv.org/abs/2412.19158v1,"Yuuki Ogawa, Satoru Hayami"
Advancements in Terahertz Antenna Design,"The promising way to provide sufficient transmission capacity is by accessing
transmission bands at higher carrier frequencies. This desire for higher
carrier frequency or more bandwidth led the researchers to take advantage of
the terahertz (THz) spectrum. The opportunity for large bandwidth in the THz
band leads to the possibility of easy, high data rate transmission. In spite of
the advantages, the THz band suffers from large free space path loss. In the
development of THz communication systems, the antenna is the most significant
component. The focus is especially on designing highly directive antennas
because they enhance the performance of the overall system by compensating for
the large path loss at THz and thus improving the signal-to-noise ratio. This
chapter presents different types of THz antennas, including planar,
reflectarray, horn antenna, and lens antenna. Emphasis has been made to present
the latest trend of designing THz antennas using carbon-based materials, such
as graphene and carbon nanotubes. The performance of these antennas has been
compared with that of traditional copper-based THz antennas by critically
analyzing their properties. A brief discussion on THz power sources is included
in this chapter for completeness. A comprehensive discussion on different
fabrication techniques has been provided to appraise the reader of the general
fabrication processes of THz components.",2024-12-26T10:21:58Z,http://arxiv.org/abs/2412.19156v1,"Sasmita Dash, Amalendu Patnaik"
"Evolutionary de-homogenization using a generative model for optimizing
  solid-porous infill structures considering the stress concentration issue","The design of porous infill structures presents significant challenges due to
their complex geometric configurations, such as the accurate representation of
geometric boundaries and the control of localized maximum stress. In current
mainstream design methods, such as topology optimization, the analysis is often
performed using pixel or voxel-based element approximations. These
approximations, constrained by the optimization framework, result in
substantial geometric discrepancies between the analysis model and the final
physical model. Such discrepancies can severely impact structural performance,
particularly for localized properties like stress response, where accurate
geometry is critical to mitigating stress concentration. To address these
challenges, we propose evolutionary de-homogenization, which is a design
framework based on the integration of de-homogenization and data-driven
multifidelity optimization. This framework facilitates the hybrid solid-porous
infill design by bridging the gap between low-fidelity analysis and
high-fidelity physical realizations, ensuring both geometric accuracy and
enhanced structural performance. The low-fidelity level utilizes commonly used
density control variables, while the high-fidelity level involves stress
analysis based on structures with precise geometric representations. By
employing a de-homogenization-based mapping method, a side-by-side
correspondence between low-fidelity and high-fidelity results is established.
The low-fidelity control variables are iteratively adjusted to optimize the
high-fidelity results by integrating deep generative model with multi-objective
evolutionary algorithm. Finally, numerical experiments demonstrate the
effectiveness of the proposed method.",2024-12-26T10:18:16Z,http://arxiv.org/abs/2412.19154v1,"Shuzhi Xu, Hiroki Kawabe, Kentaro Yaji"
"To Predict or Not To Predict? Proportionally Masked Autoencoders for
  Tabular Data Imputation","Masked autoencoders (MAEs) have recently demonstrated effectiveness in
tabular data imputation. However, due to the inherent heterogeneity of tabular
data, the uniform random masking strategy commonly used in MAEs can disrupt the
distribution of missingness, leading to suboptimal performance. To address
this, we propose a proportional masking strategy for MAEs. Specifically, we
first compute the statistics of missingness based on the observed proportions
in the dataset, and then generate masks that align with these statistics,
ensuring that the distribution of missingness is preserved after masking.
Furthermore, we argue that simple MLP-based token mixing offers competitive or
often superior performance compared to attention mechanisms while being more
computationally efficient, especially in the tabular domain with the inherent
heterogeneity. Experimental results validate the effectiveness of the proposed
proportional masking strategy across various missing data patterns in tabular
datasets. Code is available at: \url{https://github.com/normal-kim/PMAE}.",2024-12-26T10:12:08Z,http://arxiv.org/abs/2412.19152v1,"Jungkyu Kim, Kibok Lee, Taeyoung Park"
AskChart: Universal Chart Understanding through Textual Enhancement,"Chart understanding tasks such as ChartQA and Chart-to-Text involve
automatically extracting and interpreting key information from charts, enabling
users to query or convert visual data into structured formats. State-of-the-art
approaches primarily focus on visual cues from chart images, failing to
explicitly incorporate rich textual information (e.g., data labels and axis
labels) embedded within the charts. This textual information is vital for
intuitive human comprehension and interpretation of charts. Moreover, existing
models are often large and computationally intensive, limiting their practical
applicability. In this paper, we introduce AskChart, a universal model that
explicitly integrates both textual and visual cues from charts using a Mixture
of Experts (MoE) architecture. AskChart facilitates the learning of enhanced
visual-textual representations of charts for effectively handling multiple
chart understanding tasks, while maintaining a smaller model size. To capture
the synergy between visual and textual modalities, we curate a large-scale
dataset named ChartBank with about 7.5M data samples, which helps align textual
and visual information and facilitates the extraction of visual entities and
text. To effectively train AskChart, we design a three-stage training strategy
to align visual and textual modalities for learning robust visual-textual
representations and optimizing the learning of the MoE layer. Extensive
experiments across five datasets demonstrate the significant performance gains
of AskChart in four chart understanding tasks. Remarkably, AskChart with 4.6B
parameters outperforms state-of-the-art models with 13B parameters by 68.3% in
Open-ended ChartQA and 49.2% in Chart-to-Text tasks, while achieving comparable
performance in ChartQA and Chart-to-Table tasks.",2024-12-26T09:59:43Z,http://arxiv.org/abs/2412.19146v1,"Xudong Yang, Yifan Wu, Yizhang Zhu, Nan Tang, Yuyu Luo"
"Impact of color and mixing proportion of synthetic point clouds on
  semantic segmentation","Semantic segmentation of point clouds is essential for understanding the
built environment, and a large amount of high-quality data is required for
training deep learning models. Despite synthetic point clouds (SPC) having the
potential to compensate for the shortage of real data, how to exploit the
benefits of SPC is still open. Therefore, this study systematically
investigates how color and mixing proportion of SPC impact semantic
segmentation for the first time. First, a new method to mimic the scanning
process and generate SPC based on BIM is proposed, to create a synthetic
dataset with consistent colors of BIM (UniSPC) and a synthetic dataset with
real colors (RealSPC) respectively. Subsequently, by integrating with the S3DIS
dataset, further experiments on PointNet, PointNet++, and DGCNN are conducted.
Meanwhile, benchmark experiments and new evaluation metrics are introduced to
better evaluate the performance of different models. Experiments show that
synthetic color significantly impacts model performance, the performance for
common components of the models trained with pure RealSPC is comparable to
models with real data, and RealSPC contributes average improvements of 14.1% on
overall accuracy and 7.3% on mIoU than UniSPC. Furthermore, the proportion of
SPC also has a significant impact on the performance. In mixing training
experiments, adding more than 70% SPC achieves an average of 3.9% on overall
accuracy and 3.4% on mIoU better than benchmark on three models. It is also
revealed that for large flat elements such as floors, ceilings, and walls, the
SPC can even replace real point clouds without compromising model performance.",2024-12-26T09:58:04Z,http://arxiv.org/abs/2412.19145v1,"Shaojie Zhou, Jia-Rui Lin, Peng Pan, Yuandong Pan, Ioannis Brilakis"
LibAFL-DiFuzz: Advanced Architecture Enabling Directed Fuzzing,"Directed fuzzing performs best for targeted program testing via estimating
the impact of each input in reaching predefined program points. But due to
insufficient analysis of the program structure and lack of flexibility and
configurability it can lose efficiency. In this paper, we enhance directed
fuzzing with context weights for graph nodes and resolve indirect edges during
call graph construction. We construct flexible tool for directed fuzzing with
components able to be easily combined with other techniques. We implement
proposed method in three separate modules: DiFuzzLLVM library for graph
construction and indirect calls resolving, DiFuzz static analysis tool for
processing program graphs and computing proximity metrics, and LibAFL-DiFuzz
directed fuzzer based on LibAFL fuzzing library. We create additional LibAFL
modules for enabling custom power scheduling and static instrumentation. We
evaluate indirect calls resolving and get increase in directed fuzzing
efficiency for reaching deeper target points. We evaluate context weights
contribution and get benefits in TTE and scheduling iterations number. We
evaluate our fuzzer in comparison with AFLGo and BEACON, and reveal speedup in
time to exposure on several benchmarks. Furthermore, our tool implements some
important usability features that are not available in mentioned tools: target
points detection, multiple target points support, etc.",2024-12-26T09:54:57Z,http://arxiv.org/abs/2412.19143v1,"Darya Parygina, Timofey Mezhuev, Daniil Kuts"
"CLIP-GS: Unifying Vision-Language Representation with 3D Gaussian
  Splatting","Recent works in 3D multimodal learning have made remarkable progress.
However, typically 3D multimodal models are only capable of handling point
clouds. Compared to the emerging 3D representation technique, 3D Gaussian
Splatting (3DGS), the spatially sparse point cloud cannot depict the texture
information of 3D objects, resulting in inferior reconstruction capabilities.
This limitation constrains the potential of point cloud-based 3D multimodal
representation learning. In this paper, we present CLIP-GS, a novel multimodal
representation learning framework grounded in 3DGS. We introduce the GS
Tokenizer to generate serialized gaussian tokens, which are then processed
through transformer layers pre-initialized with weights from point cloud
models, resulting in the 3DGS embeddings. CLIP-GS leverages contrastive loss
between 3DGS and the visual-text embeddings of CLIP, and we introduce an image
voting loss to guide the directionality and convergence of gradient
optimization. Furthermore, we develop an efficient way to generate triplets of
3DGS, images, and text, facilitating CLIP-GS in learning unified multimodal
representations. Leveraging the well-aligned multimodal representations,
CLIP-GS demonstrates versatility and outperforms point cloud-based models on
various 3D tasks, including multimodal retrieval, zero-shot, and few-shot
classification.",2024-12-26T09:54:25Z,http://arxiv.org/abs/2412.19142v1,"Siyu Jiao, Haoye Dong, Yuyang Yin, Zequn Jie, Yinlong Qian, Yao Zhao, Humphrey Shi, Yunchao Wei"
"SILC-EFSA: Self-aware In-context Learning Correction for Entity-level
  Financial Sentiment Analysis","In recent years, fine-grained sentiment analysis in finance has gained
significant attention, but the scarcity of entity-level datasets remains a key
challenge. To address this, we have constructed the largest English and Chinese
financial entity-level sentiment analysis datasets to date. Building on this
foundation, we propose a novel two-stage sentiment analysis approach called
Self-aware In-context Learning Correction (SILC). The first stage involves
fine-tuning a base large language model to generate pseudo-labeled data
specific to our task. In the second stage, we train a correction model using a
GNN-based example retriever, which is informed by the pseudo-labeled data. This
two-stage strategy has allowed us to achieve state-of-the-art performance on
the newly constructed datasets, advancing the field of financial sentiment
analysis. In a case study, we demonstrate the enhanced practical utility of our
data and methods in monitoring the cryptocurrency market. Our datasets and code
are available at https://github.com/NLP-Bin/SILC-EFSA.",2024-12-26T09:53:01Z,http://arxiv.org/abs/2412.19140v1,"Senbin Zhu, Chenyuan He, Hongde Liu, Pengcheng Dong, Hanjie Zhao, Yuchen Yan, Yuxiang Jia, Hongying Zan, Min Peng"
SUTrack: Towards Simple and Unified Single Object Tracking,"In this paper, we propose a simple yet unified single object tracking (SOT)
framework, dubbed SUTrack. It consolidates five SOT tasks (RGB-based,
RGB-Depth, RGB-Thermal, RGB-Event, RGB-Language Tracking) into a unified model
trained in a single session. Due to the distinct nature of the data, current
methods typically design individual architectures and train separate models for
each task. This fragmentation results in redundant training processes,
repetitive technological innovations, and limited cross-modal knowledge
sharing. In contrast, SUTrack demonstrates that a single model with a unified
input representation can effectively handle various common SOT tasks,
eliminating the need for task-specific designs and separate training sessions.
Additionally, we introduce a task-recognition auxiliary training strategy and a
soft token type embedding to further enhance SUTrack's performance with minimal
overhead. Experiments show that SUTrack outperforms previous task-specific
counterparts across 11 datasets spanning five SOT tasks. Moreover, we provide a
range of models catering edge devices as well as high-performance GPUs,
striking a good trade-off between speed and accuracy. We hope SUTrack could
serve as a strong foundation for further compelling research into unified
tracking models. Code and models are available at
github.com/chenxin-dlut/SUTrack.",2024-12-26T09:41:36Z,http://arxiv.org/abs/2412.19138v1,"Xin Chen, Ben Kang, Wanting Geng, Jiawen Zhu, Yi Liu, Dong Wang, Huchuan Lu"
"Renormalized Volume, Polyakov Anomaly and Orbifold Riemann Surfaces","In arXiv:2310.17536, two of the authors studied the function
$\mathscr{S}_{\boldsymbol{m}} = S_{\boldsymbol{m}} - \pi \sum_{i=1}^n (m_i -
\tfrac{1}{m_i}) \log \mathsf{h}_{i}$ for orbifold Riemann surfaces of signature
$(g;m_1,...,m_{n_e};n_p)$ on the generalized Schottky space
$\mathfrak{S}_{g,n}(\boldsymbol{m})$. In this paper, we prove the holographic
duality between $\mathscr{S}_{\boldsymbol{m}}$ and the renormalized hyperbolic
volume $V_{\text{ren}}$ of the corresponding Schottky 3-orbifolds with lines of
conical singularity that reach the conformal boundary. In case of the classical
Liouville action on $\mathfrak{S}_{g}$ and
$\mathfrak{S}_{g,n}(\boldsymbol{\infty})$, the holography principle was proved
in arXiv:0005106 and arXiv:1508.02102, respectively. Our result implies that
$V_{\text{ren}}$ acts as K\""ahler potential for a particular combination of the
Weil-Petersson and Takhtajan-Zograf metrics that appears in the local index
theorem for orbifold Riemann surfaces arXiv:1701.00771. Moreover, we
demonstrate that under the conformal transformations, the change of function
$\mathscr{S}_{\boldsymbol{m}}$ is equivalent to the Polyakov anomaly, which
indicates that the function $\mathscr{S}_{\boldsymbol{m}}$ is a consistent
height function with a unique hyperbolic solution. Consequently, the associated
renormalized hyperbolic volume $V_{\text{ren}}$ also admits a Polyakov anomaly
formula. The method we used to establish this equivalence may provide an
alternative approach to derive the renormalized Polyakov anomaly for Riemann
surfaces with punctures (cusps), as described in arXiv:0909.0807.",2024-12-26T09:41:30Z,http://arxiv.org/abs/2412.19137v1,"Hossein Mohammadi, Ali Naseh, Behrad Taghavi"
"A Rhetorical Relations-Based Framework for Tailored Multimedia Document
  Summarization","In the rapidly evolving landscape of digital content, the task of summarizing
multimedia documents, which encompass textual, visual, and auditory elements,
presents intricate challenges. These challenges include extracting pertinent
information from diverse formats, maintaining the structural integrity and
semantic coherence of the original content, and generating concise yet
informative summaries. This paper introduces a novel framework for multimedia
document summarization that capitalizes on the inherent structure of the
document to craft coherent and succinct summaries. Central to this framework is
the incorporation of a rhetorical structure for structural analysis, augmented
by a graph-based representation to facilitate the extraction of pivotal
information. Weighting algorithms are employed to assign significance values to
document units, thereby enabling effective ranking and selection of relevant
content. Furthermore, the framework is designed to accommodate user preferences
and time constraints, ensuring the production of personalized and contextually
relevant summaries. The summarization process is elaborately delineated,
encompassing document specification, graph construction, unit weighting, and
summary extraction, supported by illustrative examples and algorithmic
elucidation. This proposed framework represents a significant advancement in
automatic summarization, with broad potential applications across multimedia
document processing, promising transformative impacts in the field.",2024-12-26T09:29:59Z,http://arxiv.org/abs/2412.19133v1,"Azze-Eddine Maredj, Madjid Sadallah"
Semantic Residual for Multimodal Unified Discrete Representation,"Recent research in the domain of multimodal unified representations
predominantly employs codebook as representation forms, utilizing Vector
Quantization(VQ) for quantization, yet there has been insufficient exploration
of other quantization representation forms. Our work explores more precise
quantization methods and introduces a new framework, Semantic Residual
Cross-modal Information Disentanglement (SRCID), inspired by the numerical
residual concept inherent to Residual Vector Quantization (RVQ). SRCID employs
semantic residual-based information disentanglement for multimodal data to
better handle the inherent discrepancies between different modalities. Our
method enhances the capabilities of unified multimodal representations and
demonstrates exceptional performance in cross-modal generalization and
cross-modal zero-shot retrieval. Its average results significantly surpass
existing state-of-the-art models, as well as previous attempts with RVQ and
Finite Scalar Quantization (FSQ) based on these modals.",2024-12-26T09:08:52Z,http://arxiv.org/abs/2412.19128v1,"Hai Huang, Shulei Wang, Yan Xia"
"Advanced Knowledge Transfer: Refined Feature Distillation for Zero-Shot
  Quantization in Edge Computing","We introduce AKT (Advanced Knowledge Transfer), a novel method to enhance the
training ability of low-bit quantized (Q) models in the field of zero-shot
quantization (ZSQ). Existing research in ZSQ has focused on generating
high-quality data from full-precision (FP) models. However, these approaches
struggle with reduced learning ability in low-bit quantization due to its
limited information capacity. To overcome this limitation, we propose effective
training strategy compared to data generation. Particularly, we analyzed that
refining feature maps in the feature distillation process is an effective way
to transfer knowledge to the Q model. Based on this analysis, AKT efficiently
transfer core information from the FP model to the Q model. AKT is the first
approach to utilize both spatial and channel attention information in feature
distillation in ZSQ. Our method addresses the fundamental gradient exploding
problem in low-bit Q models. Experiments on CIFAR-10 and CIFAR-100 datasets
demonstrated the effectiveness of the AKT. Our method led to significant
performance enhancement in existing generative models. Notably, AKT achieved
significant accuracy improvements in low-bit Q models, achieving
state-of-the-art in the 3,5bit scenarios on CIFAR-10. The code is available at
https://github.com/Inpyo-Hong/AKT-Advanced-knowledge-Transfer.",2024-12-26T08:52:27Z,http://arxiv.org/abs/2412.19125v1,"Inpyo Hong, Youngwan Jo, Hyojeong Lee, Sunghyun Ahn, Sanghyun Park"
"Evaluating Self-Supervised Learning in Medical Imaging: A Benchmark for
  Robustness, Generalizability, and Multi-Domain Impact","Self-supervised learning (SSL) has emerged as a promising paradigm in medical
imaging, addressing the chronic challenge of limited labeled data in healthcare
settings. While SSL has shown impressive results, existing studies in the
medical domain are often limited in scope, focusing on specific datasets or
modalities, or evaluating only isolated aspects of model performance. This
fragmented evaluation approach poses a significant challenge, as models
deployed in critical medical settings must not only achieve high accuracy but
also demonstrate robust performance and generalizability across diverse
datasets and varying conditions. To address this gap, we present a
comprehensive evaluation of SSL methods within the medical domain, with a
particular focus on robustness and generalizability. Using the MedMNIST dataset
collection as a standardized benchmark, we evaluate 8 major SSL methods across
11 different medical datasets. Our study provides an in-depth analysis of model
performance in both in-domain scenarios and the detection of
out-of-distribution (OOD) samples, while exploring the effect of various
initialization strategies, model architectures, and multi-domain pre-training.
We further assess the generalizability of SSL methods through cross-dataset
evaluations and the in-domain performance with varying label proportions (1%,
10%, and 100%) to simulate real-world scenarios with limited supervision. We
hope this comprehensive benchmark helps practitioners and researchers make more
informed decisions when applying SSL methods to medical applications.",2024-12-26T08:51:56Z,http://arxiv.org/abs/2412.19124v1,"Valay Bundele, Oğuz Ata Çal, Bora Kargi, Karahan Sarıtaş, Kıvanç Tezören, Zohreh Ghaderi, Hendrik Lensch"
"Discovery of an Ultra-Stable Antiferromagnetic Two-Dimensional CrF3
  Phase with Anisotropic Quasi-1D Mechanical, Electronic, and Thermal
  Properties","We report the discovery of an ultra-stable antiferromagnetic two-dimensional
(2D) CrF3 phase that is energetically more favorable than the traditionally
assumed hexagonal structure. Using first-principles calculations and
evolutionary structure searches, we identify a new low-energy rectangular
configuration of CrF3 with remarkable anisotropic properties. Mechanically,
this phase exhibits zero in-plane Poisson's ratio, a rare negative out-of-plane
Poisson's ratio, and quasi-one-dimensional (quasi-1D) behavior characterized by
minimal coupling between orthogonal directions. Electronically, CrF3 shows
quasi-1D transport with two independent conduction bands near the Fermi level,
tunable via uniaxial strain. The calculated bandgap is 3.05 eV, which can be
modulated under strain, enabling control over its electronic properties. The
material also displays out-of-plane antiferromagnetic ordering with a magnetic
anisotropy energy of 0.098 meV per Cr atom and an estimated Neel temperature of
20 K. Additionally, we investigate the thermal conductivity of monolayer
rectangular CrF3 (r-CrF3), revealing significant anisotropy in heat transport.
The thermal conductivity along the y-axis is approximately 60.5 W/mK at 300 K,
much higher than along the x-axis at 13.2 W/mK. The thermal anisotropic factor
is 4.58, surpassing that of other 2D materials like black phosphorene, WTe2,
and arsenene, highlighting r-CrF3's potential for advanced directional heat
management. Consequently, the rectangular CrF3 phase is a promising candidate
for applications in spintronics, strain-engineered nanoelectronics, mechanical
metamaterials, and thermal management technologies.",2024-12-26T08:36:35Z,http://arxiv.org/abs/2412.19118v1,"Xin Chen, Fengyi Zhou, Yan Suo, Cheng Shao, Duo Wang, Biplab Sanyal"
Deformation and core$+n$ decoupling in the spectrum of $^{17}$C,"The coexistence of various structures, such as diverse shapes and cluster
structures, is a fundamental property of atomic nuclei. In neutron-rich nuclei,
a core$+n$ structure can compete with nuclear deformation due to the small
neutron separation energy. A neutron-rich carbon isotope, $^{17}$C, exemplifies
the appearance of the deformation and the core+$n$ decoupling in its spectrum,
which is desirable for a deeper understanding of the coexistence phenomena in
neutron-rich nuclei. We aim to describe and understand this coexistence
phenomenon in the low-lying levels of $^{17}$C in a unified manner considering
explicitly the degrees of freedom of both the quadrupole deformation and the
relative motion between a $^{16}$C core and a valence neutron. Method: We adopt
the generator coordinate method (GCM) with the antisymmetrized molecular
dynamics (AMD) to describe various configurations. We superpose various basis
wave functions generated by the energy variation by imposing two types of
constraints: the $\beta$-$\gamma$ and the $d$ constraints, which control the
degree of the quadrupole deformation and the relative motion between a $^{16}$C
core and a valence neutron, respectively. We find that the experimental energy
level is well reproduced by the present method, including both deformed and
$^{16}$C+$n$ configurations. The ground $3/2^{+}$ and second excited $5/2^{+}$
states exhibit a triaxially deformed shape, while the main component of the
first excited $1/2^{+}$ state is a $^{16}$C($0^{+}$) core plus an $s$-wave
neutron configuration. The tail of the valence neutron is significantly
improved by including the $^{16}$C+$n$ basis functions explicitly. The explicit
inclusion of both the quadrupole deformation and the relative motion between a
core and a valence neutron is essential to describe the coexistence phenomena
observed in neutron-rich nuclei in the AMD+GCM framework.",2024-12-26T08:30:51Z,http://arxiv.org/abs/2412.19117v1,"Tadahiro Suhara, Yasutaka Taniguchi, Wataru Horiuchi, Shin Watanabe, Takenori Furumoto"
Counting absolutely indecomposable $G$-bundles,"For a reductive group $G$ over a finite field $k$, and a smooth projective
curve $X/k$, we give a motivic counting formula for the number of absolutely
indecomposable $G$-bundles on $X$. We prove that the counting can be expressed
via the cohomology of the moduli stack of stable parabolic $G$-Higgs bundles on
$X$. This result generalizes work of Schiffmann and work of Dobrovolska,
Ginzburg, and Travkin from $\mathrm{GL}_n$ to a general reductive group. Along
the way we prove some structural results on automorphism groups of $G$-torsors,
and we study certain Lie-theoretic counting problems related to the case when
$X$ is an elliptic curve - a case which we investigate more carefully following
Fratila, Gunningham and P. Li.",2024-12-26T08:26:32Z,http://arxiv.org/abs/2412.19116v1,"Konstantin Jakob, Zhiwei Yun"
Discrete vs. Continuous Trade-offs for Generative Models,"This work explores the theoretical and practical foundations of denoising
diffusion probabilistic models (DDPMs) and score-based generative models, which
leverage stochastic processes and Brownian motion to model complex data
distributions. These models employ forward and reverse diffusion processes
defined through stochastic differential equations (SDEs) to iteratively add and
remove noise, enabling high-quality data generation. By analyzing the
performance bounds of these models, we demonstrate how score estimation errors
propagate through the reverse process and bound the total variation distance
using discrete Girsanov transformations, Pinsker's inequality, and the data
processing inequality (DPI) for an information theoretic lens.",2024-12-26T08:14:27Z,http://arxiv.org/abs/2412.19114v1,"Jathin Korrapati, Tanish Baranwal, Rahul Shah"
"SketchFill: Sketch-Guided Code Generation for Imputing Derived Missing
  Values","Missing value is a critical issue in data science, significantly impacting
the reliability of analyses and predictions. Missing value imputation (MVI) is
a longstanding problem because it highly relies on domain knowledge. Large
language models (LLMs) have emerged as a promising tool for data cleaning,
including MVI for tabular data, offering advanced capabilities for
understanding and generating content. However, despite their promise, existing
LLM techniques such as in-context learning and Chain-of-Thought (CoT) often
fall short in guiding LLMs to perform complex reasoning for MVI, particularly
when imputing derived missing values, which require mathematical formulas and
data relationships across rows and columns. This gap underscores the need for
further advancements in LLM methodologies to enhance their reasoning
capabilities for more reliable imputation outcomes. To fill this gap, we
propose SketchFill, a novel sketch-based method to guide LLMs in generating
accurate formulas to impute missing numerical values. Our experimental results
demonstrate that SketchFill significantly outperforms state-of-the-art methods,
achieving 56.2% higher accuracy than CoT-based methods and 78.8% higher
accuracy than MetaGPT. This sets a new standard for automated data cleaning and
advances the field of MVI for numerical values.",2024-12-26T08:13:34Z,http://arxiv.org/abs/2412.19113v1,"Yunfan Zhang, Changlun Li, Yuyu Luo, Nan Tang"
"Spectral Enhancement and Pseudo-Anchor Guidance for Infrared-Visible
  Person Re-Identification","The development of deep learning has facilitated the application of person
re-identification (ReID) technology in intelligent security. Visible-infrared
person re-identification (VI-ReID) aims to match pedestrians across infrared
and visible modality images enabling 24-hour surveillance. Current studies
relying on unsupervised modality transformations as well as inefficient
embedding constraints to bridge the spectral differences between infrared and
visible images, however, limit their potential performance. To tackle the
limitations of the above approaches, this paper introduces a simple yet
effective Spectral Enhancement and Pseudo-anchor Guidance Network, named
SEPG-Net. Specifically, we propose a more homogeneous spectral enhancement
scheme based on frequency domain information and greyscale space, which avoids
the information loss typically caused by inefficient modality transformations.
Further, a Pseudo Anchor-guided Bidirectional Aggregation (PABA) loss is
introduced to bridge local modality discrepancies while better preserving
discriminative identity embeddings. Experimental results on two public
benchmark datasets demonstrate the superior performance of SEPG-Net against
other state-of-the-art methods. The code is available at
https://github.com/1024AILab/ReID-SEPG.",2024-12-26T08:03:53Z,http://arxiv.org/abs/2412.19111v1,"Yiyuan Ge, Zhihao Chen, Ziyang Wang, Jiaju Kang, Mingya Zhang"
"Graph Mixture of Experts and Memory-augmented Routers for Multivariate
  Time Series Anomaly Detection","Multivariate time series (MTS) anomaly detection is a critical task that
involves identifying abnormal patterns or events in data that consist of
multiple interrelated time series. In order to better model the complex
interdependence between entities and the various inherent characteristics of
each entity, the GNN based methods are widely adopted by existing methods. In
each layer of GNN, node features aggregate information from their neighboring
nodes to update their information. In doing so, from shallow layer to deep
layer in GNN, original individual node features continue to be weakened and
more structural information,i.e., from short-distance neighborhood to
long-distance neighborhood, continues to be enhanced. However, research to date
has largely ignored the understanding of how hierarchical graph information is
represented and their characteristics that can benefit anomaly detection.
Existing methods simply leverage the output from the last layer of GNN for
anomaly estimation while neglecting the essential information contained in the
intermediate GNN layers. To address such limitations, in this paper, we propose
a Graph Mixture of Experts (Graph-MoE) network for multivariate time series
anomaly detection, which incorporates the mixture of experts (MoE) module to
adaptively represent and integrate hierarchical multi-layer graph information
into entity representations. It is worth noting that our Graph-MoE can be
integrated into any GNN-based MTS anomaly detection method in a plug-and-play
manner. In addition, the memory-augmented routers are proposed in this paper to
capture the correlation temporal information in terms of the global historical
features of MTS to adaptively weigh the obtained entity representations to
achieve successful anomaly estimation. Extensive experiments on five
challenging datasets prove the superiority of our approach and each proposed
module.",2024-12-26T07:49:51Z,http://arxiv.org/abs/2412.19108v1,"Xiaoyu Huang, Weidong Chen, Bo Hu, Zhendong Mao"
"The role of potential energy landscape research in the development of
  new electrolyte solutions","The development of new electrolyte solutions with improved characteristics is
a key challenge for creating high-performance batteries, fuel cells,
supercapacitors, and other electrochemical devices. The study of the potential
energy landscape (PEL) plays an important role in this process, providing
information about the interactions between solution components at the molecular
level. In this work, we review the practice of applying PEL research methods
based on classical and quantum-chemical algorithms to analyze the structure,
dynamics, and thermodynamic properties of electrolyte solutions. Intermolecular
and ion-molecular interactions at the microscopic level, which determine the
macroscopic properties of the electrolyte solution, are considered in detail.
The importance of identifying stable configurations of ions and their solvates
is emphasized. PEL analysis allows for the systematic determination of the most
probable structures and complexes formed in solution, which is important for
understanding ion transport mechanisms. The study of the PEL allows for the
determination of the energy barriers that must be overcome for ion migration,
which is related to the conductivity of the electrolyte. The application of PEL
research methods in combination with experimental data opens up new
possibilities for the rational design of electrolyte solutions with desired
physicochemical properties.",2024-12-26T07:45:29Z,http://arxiv.org/abs/2412.19103v1,Vitaly V. Chaban
"""I've Heard of You!"": Generate Spoken Named Entity Recognition Data for
  Unseen Entities","Spoken named entity recognition (NER) aims to identify named entities from
speech, playing an important role in speech processing. New named entities
appear every day, however, annotating their Spoken NER data is costly. In this
paper, we demonstrate that existing Spoken NER systems perform poorly when
dealing with previously unseen named entities. To tackle this challenge, we
propose a method for generating Spoken NER data based on a named entity
dictionary (NED) to reduce costs. Specifically, we first use a large language
model (LLM) to generate sentences from the sampled named entities and then use
a text-to-speech (TTS) system to generate the speech. Furthermore, we introduce
a noise metric to filter out noisy data. To evaluate our approach, we release a
novel Spoken NER benchmark along with a corresponding NED containing 8,853
entities. Experiment results show that our method achieves state-of-the-art
(SOTA) performance in the in-domain, zero-shot domain adaptation, and fully
zero-shot settings. Our data will be available at
https://github.com/DeepLearnXMU/HeardU.",2024-12-26T07:43:18Z,http://arxiv.org/abs/2412.19102v1,"Jiawei Yu, Xiang Geng, Yuang Li, Mengxin Ren, Wei Tang, Jiahuan Li, Zhibin Lan, Min Zhang, Hao Yang, Shujian Huang, Jinsong Su"
"Reconstruction Target Matters in Masked Image Modeling for Cross-Domain
  Few-Shot Learning","Cross-Domain Few-Shot Learning (CDFSL) requires the model to transfer
knowledge from the data-abundant source domain to data-scarce target domains
for fast adaptation, where the large domain gap makes CDFSL a challenging
problem. Masked Autoencoder (MAE) excels in effectively using unlabeled data
and learning image's global structures, enhancing model generalization and
robustness. However, in the CDFSL task with significant domain shifts, we find
MAE even shows lower performance than the baseline supervised models. In this
paper, we first delve into this phenomenon for an interpretation. We find that
MAE tends to focus on low-level domain information during reconstructing pixels
while changing the reconstruction target to token features could mitigate this
problem. However, not all features are beneficial, as we then find
reconstructing high-level features can hardly improve the model's
transferability, indicating a trade-off between filtering domain information
and preserving the image's global structure. In all, the reconstruction target
matters for the CDFSL task. Based on the above findings and interpretations, we
further propose Domain-Agnostic Masked Image Modeling (DAMIM) for the CDFSL
task. DAMIM includes an Aggregated Feature Reconstruction module to
automatically aggregate features for reconstruction, with balanced learning of
domain-agnostic information and images' global structure, and a Lightweight
Decoder module to further benefit the encoder's generalizability. Experiments
on four CDFSL datasets demonstrate that our method achieves state-of-the-art
performance.",2024-12-26T07:43:01Z,http://arxiv.org/abs/2412.19101v1,"Ran Ma, Yixiong Zou, Yuhua Li, Ruixuan Li"
"BSDB-Net: Band-Split Dual-Branch Network with Selective State Spaces
  Mechanism for Monaural Speech Enhancement","Although the complex spectrum-based speech enhancement(SE) methods have
achieved significant performance, coupling amplitude and phase can lead to a
compensation effect, where amplitude information is sacrificed to compensate
for the phase that is harmful to SE. In addition, to further improve the
performance of SE, many modules are stacked onto SE, resulting in increased
model complexity that limits the application of SE. To address these problems,
we proposed a dual-path network based on compressed frequency using Mamba.
First, we extract amplitude and phase information through parallel dual
branches. This approach leverages structured complex spectra to implicitly
capture phase information and solves the compensation effect by decoupling
amplitude and phase, and the network incorporates an interaction module to
suppress unnecessary parts and recover missing components from the other
branch. Second, to reduce network complexity, the network introduces a
band-split strategy to compress the frequency dimension. To further reduce
complexity while maintaining good performance, we designed a Mamba-based module
that models the time and frequency dimensions under linear complexity. Finally,
compared to baselines, our model achieves an average 8.3 times reduction in
computational complexity while maintaining superior performance. Furthermore,
it achieves a 25 times reduction in complexity compared to transformer-based
models.",2024-12-26T07:42:07Z,http://arxiv.org/abs/2412.19099v1,"Cunhang Fan, Enrui Liu, Andong Li, Jianhua Tao, Jian Zhou, Jiahao Li, Chengshi Zheng, Zhao Lv"
"Towards structural softness and enhanced electromechanical responses in
  HfO2 ferroelectrics","Structural softness - often characterized by unstable phonon modes and large
electromechanical responses - is a hallmark of ferroelectric perovskites like
BaTiO3 or Pb(Ti,Zr)O3. Whether HfO2 ferroelectrics present any such structural
softness is still a matter of debate. Here, using first principles
calculations, we predict that it is possible to induce structural instabilities
in hafnia. More specifically, our calculations show that in-plane epitaxial
tensile strain causes a mechanical instability of the ferroelectric phase,
which transforms discontinuously into an antipolar polymorph. Then, upon
release of the tensile strain, the antipolar polymorph transforms back to the
ferroelectric state by a soft phonon instability. We show that the softening is
accompanied by enhancements in the dielectric and piezoelectric responses.
While these transitions occur at high epitaxial strains for pure ferroelectric
HfO2, we show that the required deformations are considerably lowered in
superlattices with other simple oxides, which may facilitate realizing these
effects experimentally.",2024-12-26T07:23:16Z,http://arxiv.org/abs/2412.19093v1,"Binayak Mukherjee, Natalya S. Fedorova, Jorge Íñiguez-González"
"TrajGEOS: Trajectory Graph Enhanced Orientation-based Sequential Network
  for Mobility Prediction","Human mobility studies how people move to access their needed resources and
plays a significant role in urban planning and location-based services. As a
paramount task of human mobility modeling, next location prediction is
challenging because of the diversity of users' historical trajectories that
gives rise to complex mobility patterns and various contexts. Deep sequential
models have been widely used to predict the next location by leveraging the
inherent sequentiality of trajectory data. However, they do not fully leverage
the relationship between locations and fail to capture users' multi-level
preferences. This work constructs a trajectory graph from users' historical
traces and proposes a \textbf{Traj}ectory \textbf{G}raph \textbf{E}nhanced
\textbf{O}rientation-based \textbf{S}equential network (TrajGEOS) for
next-location prediction tasks. TrajGEOS introduces hierarchical graph
convolution to capture location and user embeddings. Such embeddings consider
not only the contextual feature of locations but also the relation between
them, and serve as additional features in downstream modules. In addition, we
design an orientation-based module to learn users' mid-term preferences from
sequential modeling modules and their recent trajectories. Extensive
experiments on three real-world LBSN datasets corroborate the value of graph
and orientation-based modules and demonstrate that TrajGEOS outperforms the
state-of-the-art methods on the next location prediction task.",2024-12-26T07:18:38Z,http://arxiv.org/abs/2412.19092v1,"Zhaoping Hu, Zongyuan Huang, Jinming Yang, Tao Yang, Yaohui Jin, Yanyan Xu"
From Coin to Data: The Impact of Object Detection on Digital Numismatics,"In this work we investigate the application of advanced object detection
techniques to digital numismatics, focussing on the analysis of historical
coins. Leveraging models such as Contrastive Language-Image Pre-training
(CLIP), we develop a flexible framework for identifying and classifying
specific coin features using both image and textual descriptions. By examining
two distinct datasets, modern Russian coins featuring intricate ""Saint George
and the Dragon"" designs and degraded 1st millennium AD Southeast Asian coins
bearing Hindu-Buddhist symbols, we evaluate the efficacy of different detection
algorithms in search and classification tasks. Our results demonstrate the
superior performance of larger CLIP models in detecting complex imagery, while
traditional methods excel in identifying simple geometric patterns.
Additionally, we propose a statistical calibration mechanism to enhance the
reliability of similarity scores in low-quality datasets. This work highlights
the transformative potential of integrating state-of-the-art object detection
into digital numismatics, enabling more scalable, precise, and efficient
analysis of historical artifacts. These advancements pave the way for new
methodologies in cultural heritage research, artefact provenance studies, and
the detection of forgeries.",2024-12-26T07:05:53Z,http://arxiv.org/abs/2412.19091v1,"Rafael Cabral, Maria De Iorio, Andrew Harris"
"Integrating Artificial Open Generative Artificial Intelligence into
  Software Supply Chain Security","While new technologies emerge, human errors always looming. Software supply
chain is increasingly complex and intertwined, the security of a service has
become paramount to ensuring the integrity of products, safeguarding data
privacy, and maintaining operational continuity. In this work, we conducted
experiments on the promising open Large Language Models (LLMs) into two main
software security challenges: source code language errors and deprecated code,
with a focus on their potential to replace conventional static and dynamic
security scanners that rely on predefined rules and patterns. Our findings
suggest that while LLMs present some unexpected results, they also encounter
significant limitations, particularly in memory complexity and the management
of new and unfamiliar data patterns. Despite these challenges, the proactive
application of LLMs, coupled with extensive security databases and continuous
updates, holds the potential to fortify Software Supply Chain (SSC) processes
against emerging threats.",2024-12-26T07:03:55Z,http://arxiv.org/abs/2412.19088v1,"Vasileios Alevizos, George A Papakostas, Akebu Simasiku, Dimitra Malliarou, Antonis Messinis, Sabrina Edralin, Clark Xu, Zongliang Yue"
MoPD: Mixture-of-Prompts Distillation for Vision-Language Models,"Soft prompt learning methods are effective for adapting vision-language
models (VLMs) to downstream tasks. Nevertheless, empirical evidence reveals a
tendency of existing methods that they overfit seen classes and exhibit
degraded performance on unseen classes. This limitation is due to the inherent
bias in the training data towards the seen classes. To address this issue, we
propose a novel soft prompt learning method, named Mixture-of-Prompts
Distillation (MoPD), which can effectively transfer useful knowledge from hard
prompts manually hand-crafted (a.k.a. teacher prompts) to the learnable soft
prompt (a.k.a. student prompt), thereby enhancing the generalization ability of
soft prompts on unseen classes. Moreover, the proposed MoPD method utilizes a
gating network that learns to select hard prompts used for prompt distillation.
Extensive experiments demonstrate that the proposed MoPD method outperforms
state-of-the-art baselines especially on on unseen classes.",2024-12-26T06:57:04Z,http://arxiv.org/abs/2412.19087v1,"Yang Chen, Shuai Fu, Yu Zhang"
Investigating the Temporal Dynamics of Cyber Threat Intelligence,"Indicators of Compromise (IoCs) play a crucial role in the rapid detection
and mitigation of cyber threats. However, the existing body of literature lacks
in-depth analytical studies on the temporal aspects of IoC publication,
especially when considering up-to-date datasets related to Common
Vulnerabilities and Exposures (CVEs). This paper addresses this gap by
conducting an analysis of the timeliness and comprehensiveness of Cyber Threat
Intelligence (CTI) pertaining to several recent CVEs. The insights derived from
this study aim to enhance cybersecurity defense strategies, particularly when
dealing with dynamic cyber threats that continually adapt their Tactics,
Techniques, and Procedures (TTPs). Utilizing IoCs sourced from multiple
providers, we scrutinize the IoC publication rate. Our analysis delves into how
various factors, including the inherent nature of a threat, its evolutionary
trajectory, and its observability over time, influence the publication rate of
IoCs. Our preliminary findings emphasize the critical need for cyber defenders
to maintain a constant state of vigilance in updating their IoCs for any given
vulnerability. This vigilance is warranted because the publication rate of IoCs
may exhibit fluctuations over time. We observe a recurring pattern akin to an
epidemic model, with an initial phase following the public disclosure of a
vulnerability characterized by sparse IoC publications, followed by a sudden
surge, and subsequently, a protracted period with a slower rate of IoC
publication.",2024-12-26T06:54:27Z,http://arxiv.org/abs/2412.19086v1,"Angel Kodituwakku, Clark Xu, Daniel Rogers, David K. Ahn, Errin W. Fulp"
"Assessing Pre-trained Models for Transfer Learning through Distribution
  of Spectral Components","Pre-trained model assessment for transfer learning aims to identify the
optimal candidate for the downstream tasks from a model hub, without the need
of time-consuming fine-tuning. Existing advanced works mainly focus on
analyzing the intrinsic characteristics of the entire features extracted by
each pre-trained model or how well such features fit the target labels. This
paper proposes a novel perspective for pre-trained model assessment through the
Distribution of Spectral Components (DISCO). Through singular value
decomposition of features extracted from pre-trained models, we investigate
different spectral components and observe that they possess distinct
transferability, contributing diversely to the fine-tuning performance.
Inspired by this, we propose an assessment method based on the distribution of
spectral components which measures the proportions of their corresponding
singular values. Pre-trained models with features concentrating on more
transferable components are regarded as better choices for transfer learning.
We further leverage the labels of downstream data to better estimate the
transferability of each spectral component and derive the final assessment
criterion. Our proposed method is flexible and can be applied to both
classification and regression tasks. We conducted comprehensive experiments
across three benchmarks and two tasks including image classification and object
detection, demonstrating that our method achieves state-of-the-art performance
in choosing proper pre-trained models from the model hub for transfer learning.",2024-12-26T06:54:22Z,http://arxiv.org/abs/2412.19085v1,"Tengxue Zhang, Yang Shu, Xinyang Chen, Yifei Long, Chenjuan Guo, Bin Yang"
A Microservice Graph Generator with Production Characteristics,"A production microservice application may provide multiple services, queries
of a service may have different call graphs, and a microservice may be shared
across call graphs. It is challenging to improve the resource efficiency of
such complex applications without proper benchmarks, while production traces
are too large to be used in experiments. To this end, we propose a Service
Dependency Graph Generator (DGG) that comprises a Data Handler and a Graph
Generator, for generating the service dependency graphs of benchmarks that
incorporate production-level characteristics from traces. The data handler
first constructs fine-grained call graphs with dynamic interface and repeated
calling features from the trace and merges them into dependency graphs, and
then clusters them into different categories based on the topological and
invocation types. Taking the organized data and the selected category, the
graph generator simulates the process of real microservices invoking downstream
microservices using a random graph model, generates multiple call graphs, and
merges the call graphs to form the small-scale service dependency graph with
production-level characteristics. Case studies show that DGG's generated graphs
are similar to real traces in terms of topologies. Moreover, the resource
scaling based on DGG's fine-grained call graph constructing increases the
resource efficiency by up to 44.8% while ensuring the required QoS.",2024-12-26T06:51:35Z,http://arxiv.org/abs/2412.19083v1,"Fanrong Du, Jiuchen Shi, Quan Chen, Li Li, Minyi Guo"
Blue laser induced bright red fluorescence in hot cesium vapor,"We have observed laser-induced fluorescence using 456 nm laser radiation,
resonant with the 6S1/2-7P3/2 transition in Cs atoms. It includes red emission
lines in the range of 580-730 nm and a prominent line at 852 nm corresponding
to the 6P3/2-6S1/2 transition. A T-shaped all-sapphire cell with a length of 1
cm, containing Cs atomic vapor and capable of being heated up to 500 oC, was
used. The laser-induced fluorescence (LIF) power at 852 nm was investigated as
a function of the cell temperature. The maximum LIF power was achieved at 130
oC, while a significant decrease was observed around 300 oC. At 130 oC, the
Doppler-broadened LIF spectrum at 852 nm exhibited self-conversion, resulting
in the formation of two distinct peaks within the spectrum. The LIF power at
852 nm was also studied as a function of the 456 nm radiation power. The Cs
cell demonstrated potential as an efficient optical filter and down-converter,
effectively transforming 456 nm radiation into 852 nm radiation.",2024-12-26T06:43:33Z,http://arxiv.org/abs/2412.19081v1,"Armen Sargsyan, Anahit Gogyan, David Sarkisyan"
"Mask Factory: Towards High-quality Synthetic Data Generation for
  Dichotomous Image Segmentation","Dichotomous Image Segmentation (DIS) tasks require highly precise
annotations, and traditional dataset creation methods are labor intensive,
costly, and require extensive domain expertise. Although using synthetic data
for DIS is a promising solution to these challenges, current generative models
and techniques struggle with the issues of scene deviations, noise-induced
errors, and limited training sample variability. To address these issues, we
introduce a novel approach, \textbf{\ourmodel{}}, which provides a scalable
solution for generating diverse and precise datasets, markedly reducing
preparation time and costs. We first introduce a general mask editing method
that combines rigid and non-rigid editing techniques to generate high-quality
synthetic masks. Specially, rigid editing leverages geometric priors from
diffusion models to achieve precise viewpoint transformations under zero-shot
conditions, while non-rigid editing employs adversarial training and
self-attention mechanisms for complex, topologically consistent modifications.
Then, we generate pairs of high-resolution image and accurate segmentation mask
using a multi-conditional control generation method. Finally, our experiments
on the widely-used DIS5K dataset benchmark demonstrate superior performance in
quality and efficiency compared to existing methods. The code is available at
\url{https://qian-hao-tian.github.io/MaskFactory/}.",2024-12-26T06:37:25Z,http://arxiv.org/abs/2412.19080v1,"Haotian Qian, YD Chen, Shengtao Lou, Fahad Shahbaz Khan, Xiaogang Jin, Deng-Ping Fan"
"Graph-Enhanced Dual-Stream Feature Fusion with Pre-Trained Model for
  Acoustic Traffic Monitoring","Microphone array techniques are widely used in sound source localization and
smart city acoustic-based traffic monitoring, but these applications face
significant challenges due to the scarcity of labeled real-world traffic audio
data and the complexity and diversity of application scenarios. The DCASE
Challenge's Task 10 focuses on using multi-channel audio signals to count
vehicles (cars or commercial vehicles) and identify their directions
(left-to-right or vice versa). In this paper, we propose a graph-enhanced
dual-stream feature fusion network (GEDF-Net) for acoustic traffic monitoring,
which simultaneously considers vehicle type and direction to improve detection.
We propose a graph-enhanced dual-stream feature fusion strategy which consists
of a vehicle type feature extraction (VTFE) branch, a vehicle direction feature
extraction (VDFE) branch, and a frame-level feature fusion module to combine
the type and direction feature for enhanced performance. A pre-trained model
(PANNs) is used in the VTFE branch to mitigate data scarcity and enhance the
type features, followed by a graph attention mechanism to exploit temporal
relationships and highlight important audio events within these features. The
frame-level fusion of direction and type features enables fine-grained feature
representation, resulting in better detection performance. Experiments
demonstrate the effectiveness of our proposed method. GEDF-Net is our
submission that achieved 1st place in the DCASE 2024 Challenge Task 10.",2024-12-26T06:28:42Z,http://arxiv.org/abs/2412.19078v1,"Shitong Fan, Feiyang Xiao, Wenbo Wang, Shuhan Qi, Qiaoxi Zhu, Wenwu Wang, Jian Guan"
Prescribed-Time Boundary Control of Flexible String Systems,"This paper presents a boundary control scheme for prescribed-time (PT) stable
of flexible string systems via backstepping method, and the dynamics of such
systems modeled by Hamilton's principle is described as hyperbolic partial
differential equation (PDE). Initially, to construct a boundary controller with
PT stabilization capacity, a PT stable hyperbolic PDE system with time-varying
coefficient is chosen as the target system, and a corresponding Volterra
integral transform with time-varying kernel function is considered. Meanwhile,
the kernel equation and controller is determined by taking derivative. Then, to
identify the boundary controller, the well-posedness of kernel equation is
derived by means of successive approximation and mathematical induction, and
the upper bound of kernel function is estimated. Furthermore, the inverse
transform is proved with the help of a similar process for kernel function.
Subsequently, the PT stability of closed-loop system is proved by PT stability
of target system and reversible integral transform. Finally, the simulation
results demonstrate the effectiveness of our scheme.",2024-12-26T06:08:15Z,http://arxiv.org/abs/2412.19073v1,"He Yang, Chuan Zhang, Yingxin Guo, Xianfu Zhang"
"Robust Speech and Natural Language Processing Models for Depression
  Screening","Depression is a global health concern with a critical need for increased
patient screening. Speech technology offers advantages for remote screening but
must perform robustly across patients. We have described two deep learning
models developed for this purpose. One model is based on acoustics; the other
is based on natural language processing. Both models employ transfer learning.
Data from a depression-labeled corpus in which 11,000 unique users interacted
with a human-machine application using conversational speech is used. Results
on binary depression classification have shown that both models perform at or
above AUC=0.80 on unseen data with no speaker overlap. Performance is further
analyzed as a function of test subset characteristics, finding that the models
are generally robust over speaker and session variables. We conclude that
models based on these approaches offer promise for generalized automated
depression screening.",2024-12-26T06:05:52Z,http://arxiv.org/abs/2412.19072v1,"Y. Lu, A. Harati, T. Rutowski, R. Oliveira, P. Chlebek, E. Shriberg"
Cross-Demographic Portability of Deep NLP-Based Depression Models,"Deep learning models are rapidly gaining interest for real-world applications
in behavioral health. An important gap in current literature is how well such
models generalize over different populations. We study Natural Language
Processing (NLP) based models to explore portability over two different corpora
highly mismatched in age. The first and larger corpus contains younger
speakers. It is used to train an NLP model to predict depression. When testing
on unseen speakers from the same age distribution, this model performs at
AUC=0.82. We then test this model on the second corpus, which comprises seniors
from a retirement community. Despite the large demographic differences in the
two corpora, we saw only modest degradation in performance for the
senior-corpus data, achieving AUC=0.76. Interestingly, in the senior
population, we find AUC=0.81 for the subset of patients whose health state is
consistent over time. Implications for demographic portability of speech-based
applications are discussed.",2024-12-26T05:54:24Z,http://arxiv.org/abs/2412.19070v1,"Tomek Rutowski, Elizabeth Shriberg, Amir Harati, Yang Lu, Ricardo Oliveira, Piotr Chlebek"
Effective and secure federated online learning to rank,"Online Learning to Rank (OLTR) optimises ranking models using implicit user
feedback, such as clicks. Unlike traditional Learning to Rank (LTR) methods
that rely on a static set of training data with relevance judgements to learn a
ranking model, OLTR methods update the model continually as new data arrives.
Thus, it addresses several drawbacks such as the high cost of human
annotations, potential misalignment between user preferences and human
judgments, and the rapid changes in user query intents. However, OLTR methods
typically require the collection of searchable data, user queries, and clicks,
which poses privacy concerns for users.
  Federated Online Learning to Rank (FOLTR) integrates OLTR within a Federated
Learning (FL) framework to enhance privacy by not sharing raw data. While
promising, FOLTR methods currently lag behind traditional centralised OLTR due
to challenges in ranking effectiveness, robustness with respect to data
distribution across clients, susceptibility to attacks, and the ability to
unlearn client interactions and data. This thesis presents a comprehensive
study on Federated Online Learning to Rank, addressing its effectiveness,
robustness, security, and unlearning capabilities, thereby expanding the
landscape of FOLTR.",2024-12-26T05:53:10Z,http://arxiv.org/abs/2412.19069v1,Shuyi Wang
"Attacking Voice Anonymization Systems with Augmented Feature and Speaker
  Identity Difference","This study focuses on the First VoicePrivacy Attacker Challenge within the
ICASSP 2025 Signal Processing Grand Challenge, which aims to develop speaker
verification systems capable of determining whether two anonymized speech
signals are from the same speaker. However, differences between feature
distributions of original and anonymized speech complicate this task. To
address this challenge, we propose an attacker system that combines Data
Augmentation enhanced feature representation and Speaker Identity Difference
enhanced classifier to improve verification performance, termed DA-SID.
Specifically, data augmentation strategies (i.e., data fusion and SpecAugment)
are utilized to mitigate feature distribution gaps, while probabilistic linear
discriminant analysis (PLDA) is employed to further enhance speaker identity
difference. Our system significantly outperforms the baseline, demonstrating
exceptional effectiveness and robustness against various voice anonymization
systems, ultimately securing a top-5 ranking in the challenge.",2024-12-26T05:52:44Z,http://arxiv.org/abs/2412.19068v1,"Yanzhe Zhang, Zhonghao Bi, Feiyang Xiao, Xuefeng Yang, Qiaoxi Zhu, Jian Guan"
Learning Monocular Depth from Events via Egomotion Compensation,"Event cameras are neuromorphically inspired sensors that sparsely and
asynchronously report brightness changes. Their unique characteristics of high
temporal resolution, high dynamic range, and low power consumption make them
well-suited for addressing challenges in monocular depth estimation (e.g.,
high-speed or low-lighting conditions). However, current existing methods
primarily treat event streams as black-box learning systems without
incorporating prior physical principles, thus becoming over-parameterized and
failing to fully exploit the rich temporal information inherent in event camera
data. To address this limitation, we incorporate physical motion principles to
propose an interpretable monocular depth estimation framework, where the
likelihood of various depth hypotheses is explicitly determined by the effect
of motion compensation. To achieve this, we propose a Focus Cost Discrimination
(FCD) module that measures the clarity of edges as an essential indicator of
focus level and integrates spatial surroundings to facilitate cost estimation.
Furthermore, we analyze the noise patterns within our framework and improve it
with the newly introduced Inter-Hypotheses Cost Aggregation (IHCA) module,
where the cost volume is refined through cost trend prediction and multi-scale
cost consistency constraints. Extensive experiments on real-world and synthetic
datasets demonstrate that our proposed framework outperforms cutting-edge
methods by up to 10\% in terms of the absolute relative error metric, revealing
superior performance in predicting accuracy.",2024-12-26T05:41:18Z,http://arxiv.org/abs/2412.19067v1,"Haitao Meng, Chonghao Zhong, Sheng Tang, Lian JunJia, Wenwei Lin, Zhenshan Bing, Yi Chang, Gang Chen, Alois Knoll"
"Predicting Accurate X-ray Absorption Spectra for CN$^+$, CN$^\bullet$,
  and CN$^-$: Insights from First-Principles Simulations","High-resolution X-ray spectroscopy is an essential tool in X-ray astronomy,
enabling detailed studies of celestial objects and their physical and chemical
properties. However, comprehensive mapping of high-resolution X-ray spectra for
even simple interstellar and circumstellar molecules is still lacking. In this
study, we conducted systematic quantum chemical simulations to predict the C1s
X-ray absorption spectra of CN$^+$, CN, and CN$^-$. Our findings provide
valuable references for both X-ray astronomy and laboratory studies. We
assigned the first electronic peak of CN$^+$ and CN to C1s $\rightarrow
\sigma^*$ transitions, while the peak for CN$^-$ corresponds to a C1s
$\rightarrow \pi^*$ transition. We further calculated the vibronic fine
structures for these transitions using the quantum wavepacket method based on
multiconfigurational-level, anharmonic potential energy curves, revealing
distinct energy positions for the 0-0 absorptions at 280.7 eV, 279.6 eV, and
285.8 eV. Each vibronic profile features a prominent 0-0 peak, showing overall
similarity but differing intensity ratios of the 0-0 and 0-1 peaks. Notably,
introducing a C1s core hole leads to shortened C-N bond lengths and increased
vibrational frequencies across all species. These findings enhance our
understanding of the electronic structures and X-ray spectra of carbon-nitrogen
species, emphasizing the influence of charge state on X-ray absorptions.",2024-12-26T05:27:06Z,http://arxiv.org/abs/2412.19065v1,"Jinyu Li, Sheng-Yu Wang, Lu Zhang, Guoyan Ge, Minrui Wei, Junxiang Zuo, Weijie Hua"
DAPoinTr: Domain Adaptive Point Transformer for Point Cloud Completion,"Point Transformers (PoinTr) have shown great potential in point cloud
completion recently. Nevertheless, effective domain adaptation that improves
transferability toward target domains remains unexplored. In this paper, we
delve into this topic and empirically discover that direct feature alignment on
point Transformer's CNN backbone only brings limited improvements since it
cannot guarantee sequence-wise domain-invariant features in the Transformer. To
this end, we propose a pioneering Domain Adaptive Point Transformer (DAPoinTr)
framework for point cloud completion. DAPoinTr consists of three key
components: Domain Query-based Feature Alignment (DQFA), Point Token-wise
Feature alignment (PTFA), and Voted Prediction Consistency (VPC). In
particular, DQFA is presented to narrow the global domain gaps from the
sequence via the presented domain proxy and domain query at the Transformer
encoder and decoder, respectively. PTFA is proposed to close the local domain
shifts by aligning the tokens, \emph{i.e.,} point proxy and dynamic query, at
the Transformer encoder and decoder, respectively. VPC is designed to consider
different Transformer decoders as multiple of experts (MoE) for ensembled
prediction voting and pseudo-label generation. Extensive experiments with
visualization on several domain adaptation benchmarks demonstrate the
effectiveness and superiority of our DAPoinTr compared with state-of-the-art
methods. Code will be publicly available at:
https://github.com/Yinghui-Li-New/DAPoinTr",2024-12-26T05:16:54Z,http://arxiv.org/abs/2412.19062v1,"Yinghui Li, Qianyu Zhou, Jingyu Gong, Ye Zhu, Richard Dazeley, Xinkui Zhao, Xuequan Lu"
"An active hydroelastic liquid crystal phase of a fluttering
  ferroelectric nematic","Polarization flutter, produced by an applied AC electric field drives an
equilibrium ferroelectric nematic ($\mathrm{N_F}$) liquid crystal (LC) through
a transition into a dissipative active ferroelectric nematic state exhibiting
strong elasto-hydrodynamic intermolecular interaction. In such a fluttering
ferroelectric, the typical equilibrium $\mathrm{N_F}$ textural features adopted
to reduce electrostatic energy, such as preferences for director bend, and
alignment of polarization parallel to LC/air interfaces, are overcome, giving
way to nonequilibrium conjugate structures in which director splay, and
alignment of polarization normal to $\mathrm{N_F}$/air interfaces are
preferred. Viewing the latter textures as those of an active nematic phase
reveals that self-organization to reduce effective viscosity and resulting
dissipation generates a flow-driven apparent nematic elasticity and interface
structuring that dominates equilibrium LC elastic and surface forces.",2024-12-26T05:13:55Z,http://arxiv.org/abs/2412.19061v1,"Xi Chen, Cory Pecinovsky, Eva Korblova, Matthew A. Glaser, Leo Radzihovsky, Joseph E. Maclennan, David M. Walba, Noel A. Clark"
Coarse-grained binning in Drell-Yan transverse momentum spectra,"We report a study of the determination of the intrinsic transverse momentum
of partons, the intrinsic $k_T$, from the dilepton transverse momentum $p_T$ in
Drell-Yan (DY) production at hadron colliders. The result shows that a good
sensitivity to the intrinsic $k_T$ distribution is achieved by measuring
relative ratios between the cross sections of suitably defined low-$p_T$ and
high-$p_T$ regions. The study is performed through both a pseudo-data test and
an extraction from measurements of the DY process by the CMS collaboration.
Since the methodology does not rely on any dedicated partition of bins, this
$p_T$-ratio observable requires less special treatment in very low $p_T$
regions, and propagates lower systematic uncertainties induced from unfolding
or momentum migration, in contrast with previous proposals of using a
fine-binning measurement of the differential cross section.",2024-12-26T05:13:39Z,http://arxiv.org/abs/2412.19060v1,"Wenxiao Zhan, Siqi Yang, Minghui Liu, Francesco Hautmann, Liang Han"
Faster Semi-streaming Matchings via Alternating Trees,"We design a deterministic algorithm for the $(1+\epsilon)$-approximate
maximum matching problem. Our primary result demonstrates that this problem can
be solved in $O(\epsilon^{-6})$ semi-streaming passes, improving upon the
$O(\epsilon^{-19})$ pass-complexity algorithm by [Fischer, Mitrovi\'c, and
Uitto, STOC'22]. This contributes substantially toward resolving Open
question~2 from [Assadi, SOSA'24]. Leveraging the framework introduced in
[FMU'22], our algorithm achieves an analogous round complexity speed-up for
computing a $(1+\epsilon)$-approximate maximum matching in both the Massively
Parallel Computation (MPC) and CONGEST models.
  The data structures maintained by our algorithm are formulated using blossom
notation and represented through alternating trees. This approach enables a
simplified correctness analysis by treating specific components as if operating
on bipartite graphs, effectively circumventing certain technical intricacies
present in prior work.",2024-12-26T04:59:27Z,http://arxiv.org/abs/2412.19057v1,"Slobodan Mitrović, Anish Mukherjee, Piotr Sankowski, Wen-Horng Sheu"
"SpectralKD: Understanding and Optimizing Vision Transformer Distillation
  through Spectral Analysis","Knowledge distillation effectively reduces model complexity while improving
performance, yet the underlying knowledge transfer mechanisms remain poorly
understood. We propose novel spectral analysis methods and guidelines to
optimize distillation, making the knowledge transfer process more
interpretable. Our analysis reveals that CaiT models concentrate information in
their first and last few layers, informing optimal layer selection for feature
map distillation. Surprisingly, we discover that Swin Transformer and CaiT
exhibit similar spectral encoding patterns despite their architectural
differences, enhancing our understanding of transformer architectures and
leading to improved feature map alignment strategies. Based on these insights,
we introduce a simple yet effective spectral alignment method named SpectralKD.
Experimental results demonstrate that following our guidelines enables
SpectralKD to achieve state-of-the-art performance (DeiT-Tiny: $+5.2\%$,
Swin-Tiny: $+1.4\%$ in ImageNet-1k Top-1 accuracy). Furthermore, through
spectral analysis of student models trained with and without distillation, we
show that distilled models mirror spectral patterns of their teachers,
providing a new lens for interpreting knowledge distillation dynamics. Our
code, pre-trained models, and experimental logs will be made publicly
available.",2024-12-26T04:45:05Z,http://arxiv.org/abs/2412.19055v1,"Huiyuan Tian, Bonan Xu, Shijian Li, Gang Pan"
Flattening subtyping by eta expansion,"To design type systems that use subtyping, we have to make tradeoffs. Deep
subtyping is more expressive than shallow subtyping, because deep subtyping
compares the entire structure of types. However, shallow subtyping is easier to
reason about. By eta-expanding source programs, we can get the effect of deep
subtyping with less of its complexity. An early paper on filter models
(Barendregt et al. 1983) examined two similar intersection type systems. The
first included a subsumption rule that used a rich subtyping relation,
including multiple rules for the top type and a distributivity rule. Their
second type system dropped the subsumption rule, but added a rule that allowed
a term to be eta-expanded before typing it. This rule in their second type
system compensated for the lack of subsumption: where their first type system
used subtyping to manipulate intersections deep inside types, their second type
system used introduction and elimination rules directly on the subterms created
by eta-expansion. Viewed as a computation, their proof of completeness for the
second (shallow) system performs eta-expansion. Thus, we can regard their proof
as inventing the application of eta-expansion to avoid deep subtyping. This
paper serves as a tutorial on using eta-expansion to obviate deep subtyping,
puts the invention of the technique by Barendregt et al. (1983) into context,
gives a complete proof of the relevant lemma, and discusses how the technique
can be used in type system design.",2024-12-26T04:28:10Z,http://arxiv.org/abs/2412.19053v1,Jana Dunfield
"Performance Characterization and Optimizations of Traditional ML
  Applications","Even in the era of Deep Learning based methods, traditional machine learning
methods with large data sets continue to attract significant attention.
However, we find an apparent lack of a detailed performance characterization of
these methods in the context of large training datasets. In this work, we study
the system's behavior of a number of traditional ML methods as implemented in
popular free software libraries/modules to identify critical performance
bottlenecks experienced by these applications. The performance characterization
study reveals several interesting insights on the performance of these
applications. Then we evaluate the performance benefits of applying some
well-known optimizations at the levels of caches and the main memory. More
specifically, we test the usefulness of optimizations such as (i) software
prefetching to improve cache performance and (ii) data layout and computation
reordering optimizations to improve locality in DRAM accesses. These
optimizations are implemented as modifications to the well-known scikit-learn
library, and hence can be easily leveraged by application programmers. We
evaluate the impact of the proposed optimizations using a combination of
simulation and execution on a real system. The software prefetching
optimization results in performance benefits varying from 5.2%-27.1% on
different ML applications while the data layout and computation reordering
approaches yield 6.16%-28.0% performance improvement.",2024-12-26T04:13:52Z,http://arxiv.org/abs/2412.19051v1,"Harsh Kumar, R. Govindarajan"
3+1 formalism of the minimally extended varying speed of light model,"The $3+1$ formalism provides a structured approach to analyzing spacetime by
separating it into spatial and temporal components. When applied to the
Robertson-Walker metric, it simplifies the analysis of cosmological evolution
by dividing the Einstein field equations into constraint and evolution
equations. It introduces the lapse function $N$ and the shift vector $N^i$,
which control how time and spatial coordinates evolve between hypersurfaces. In
standard model cosmology, $N = 1$ and $N^i = 0$ for the Robertson-Walker
metric. However, the $N$ becomes a function of time when we apply the metric to
the minimally extended varying speed of light model. This approach allows for a
more direct examination of the evolution of spatial geometry and offers
flexibility in handling scenarios where the lapse function and shift vector
vary. In this manuscript, we derive the model's $N$ and $N^i$, along with the
constraint and evolution equations, and demonstrate their consistency with the
existing Einstein equations. We have shown in a previous paper that the
possibility of changes in the speed of light in the Robertson-Walker metric is
due to cosmological time dilation. Through the $3+1$ formalism, we can make the
physical significance more explicit and demonstrate that it can be interpreted
as the lapse function. From this, we show that the minimally extended varying
speed of light model is consistent.",2024-12-26T04:05:39Z,http://arxiv.org/abs/2412.19049v1,Seokcheon Lee
Jasper and Stella: distillation of SOTA embedding models,"A crucial component of many deep learning applications (such as FAQ and RAG)
is dense retrieval, in which embedding models are used to convert raw text to
numerical vectors and then get the most similar text by MIPS (Maximum Inner
Product Search). Some text embedding benchmarks (e.g. MTEB, BEIR, and
AIR-Bench) have been established to evaluate embedding models accurately.
Thanks to these benchmarks, we can use SOTA models; however, the deployment and
application of these models in industry were hampered by their large vector
dimensions and numerous parameters. To alleviate this problem, 1) we present a
distillation technique that can enable a smaller student model to achieve good
performance. 2) Inspired by MRL we present a training approach of reducing the
vector dimensions based on its own vectors or its teacher vectors. 3) We do
simple yet effective alignment training between images and text to make our
model a multimodal encoder. We trained Stella and Jasper models using the
technologies above and achieved high scores on the MTEB leaderboard. We release
the model and data at Hugging Face Hub
(https://huggingface.co/infgrad/jasper_en_vision_language_v1) and the training
logs are at https://api.wandb.ai/links/dunnzhang0/z8jqoqpb.",2024-12-26T04:05:28Z,http://arxiv.org/abs/2412.19048v1,"Dun Zhang, FulongWang"
Inverses of integral transforms of RKHSs,"The Fourier transform and its inverse are well-known to have complex
conjugate integral kernels. S.~Saitoh demonstrated that this relationship
extends to the theory of integral transforms of Hilbert spaces of functions
under certain conditions. In this paper, we derive a necessary and sufficient
condition for the inverse of an integral transform of a Hilbert space of
functions to be represented by a complex conjugate integral kernel. As an
application, we present an alternative proof of Plancherel's theorem using the
theory of reproducing kernels.",2024-12-26T03:45:30Z,http://arxiv.org/abs/2412.19047v1,Akira Yamada
Revealing the Self: Brainwave-Based Human Trait Identification,"People exhibit unique emotional responses. In the same scenario, the
emotional reactions of two individuals can be either similar or vastly
different. For instance, consider one person's reaction to an invitation to
smoke versus another person's response to a query about their sleep quality.
The identification of these individual traits through the observation of common
physical parameters opens the door to a wide range of applications, including
psychological analysis, criminology, disease prediction, addiction control, and
more. While there has been previous research in the fields of psychometrics,
inertial sensors, computer vision, and audio analysis, this paper introduces a
novel technique for identifying human traits in real time using brainwave data.
To achieve this, we begin with an extensive study of brainwave data collected
from 80 participants using a portable EEG headset. We also conduct a
statistical analysis of the collected data utilizing box plots. Our analysis
uncovers several new insights, leading us to a groundbreaking unified approach
for identifying diverse human traits by leveraging machine learning techniques
on EEG data. Our analysis demonstrates that this proposed solution achieves
high accuracy. Moreover, we explore two deep-learning models to compare the
performance of our solution. Consequently, we have developed an integrated,
real-time trait identification solution using EEG data, based on the insights
from our analysis. To validate our approach, we conducted a rigorous user
evaluation with an additional 20 participants. The outcomes of this evaluation
illustrate both high accuracy and favorable user ratings, emphasizing the
robust potential of our proposed method to serve as a versatile solution for
human trait identification.",2024-12-26T03:27:34Z,http://arxiv.org/abs/2412.19041v1,"Md Mirajul Islam, Md Nahiyan Uddin, Maoyejatun Hasana, Debojit Pandit, Nafis Mahmud Rahman, Sriram Chellappan, Sami Azam, A. B. M. Alim Al Islam"
CL-attack: Textual Backdoor Attacks via Cross-Lingual Triggers,"Backdoor attacks significantly compromise the security of large language
models by triggering them to output specific and controlled content. Currently,
triggers for textual backdoor attacks fall into two categories: fixed-token
triggers and sentence-pattern triggers. However, the former are typically easy
to identify and filter, while the latter, such as syntax and style, do not
apply to all original samples and may lead to semantic shifts. In this paper,
inspired by cross-lingual (CL) prompts of LLMs in real-world scenarios, we
propose a higher-dimensional trigger method at the paragraph level, namely
CL-attack. CL-attack injects the backdoor by using texts with specific
structures that incorporate multiple languages, thereby offering greater
stealthiness and universality compared to existing backdoor attack techniques.
Extensive experiments on different tasks and model architectures demonstrate
that CL-attack can achieve nearly 100% attack success rate with a low poisoning
rate in both classification and generation tasks. We also empirically show that
the CL-attack is more robust against current major defense methods compared to
baseline backdoor attacks. Additionally, to mitigate CL-attack, we further
develop a new defense called TranslateDefense, which can partially mitigate the
impact of CL-attack.",2024-12-26T03:13:03Z,http://arxiv.org/abs/2412.19037v1,"Jingyi Zheng, Tianyi Hu, Tianshuo Cong, Xinlei He"
"Unifying Tree-Reweighted Belief Propagation and Mean Field for Tracking
  Extended Targets","This paper proposes a unified tree-reweighted belief propagation (BP) and
mean field (MF) approach for scalable detection and tracking of extended
targets within the framework of factor graph. The factor graph is partitioned
into a BP region and an MF region so that the messages in each region are
updated according to the corresponding region rules. The BP region exploits the
tree-reweighted BP, which offers improved convergence than the standard BP for
graphs with massive cycles, to resolve data association. The MF region
approximates the posterior densities of the measurement rate, kinematic state
and extent. For linear Gaussian target models and gamma Gaussian inverse
Wishart distributed state density, the unified approach provides a closed-form
recursion for the state density. Hence, the proposed algorithm is more
efficient than particle-based BP algorithms for extended target tracking. This
method also avoids measurement clustering and gating since it solves the data
association problem in a probabilistic fashion. We compare the proposed
approach with algorithms such as the Poisson multi-Bernoulli mixture filter and
the BP-based Poisson multi-Bernoulli filter. Simulation results demonstrate
that the proposed algorithm achieves enhanced tracking performance.",2024-12-26T03:12:53Z,http://arxiv.org/abs/2412.19036v1,"Weizhen Ma, Zhongliang Jing, Peng Dong, Henry Leung"
"Reflection on Purpose Changes Students' Academic Interests: A Scalable
  Intervention in an Online Course Catalog","College students routinely use online course catalogs to explore a variety of
academic offerings. Course catalogs may therefore be an effective place to
encourage reflection on academic choices and interests. To test this, we
embedded a psychological intervention in an online course catalog to encourage
students to reflect on their purpose during course exploration. Results of a
randomized field experiment with over 4,000 students at a large U.S. university
show that a purpose intervention increased students' cognitive engagement in
describing their interests, but reduced search activities. Students became more
interested in courses related to creative arts and social change, but less in
computer and data science. The findings demonstrate the malleability of
students' interests during course exploration and suggest practical strategies
to support purpose reflection and guide students toward deliberate exploration
of their interests in higher education.",2024-12-26T03:12:50Z,http://arxiv.org/abs/2412.19035v1,"Youjie Chen, Pranathi Iyer, Rene F. Kizilcec"
Repository Structure-Aware Training Makes SLMs Better Issue Resolver,"Language models have been applied to various software development tasks, but
the performance varies according to the scale of the models. Large Language
Models (LLMs) outperform Small Language Models (SLMs) in complex tasks like
repository-level issue resolving, but raise concerns about privacy and cost. In
contrast, SLMs are more accessible but under-perform in complex tasks. In this
paper, we introduce ReSAT (Repository Structure-Aware Training), construct
training data based on a large number of issues and corresponding pull requests
from open-source communities to enhance the model's understanding of repository
structure and issue resolving ability. We construct two types of training data:
(1) localization training data, a multi-level progressive localization data to
improve code understanding and localization capability; (2) code edit training
data, which improves context-based code editing capability. The evaluation
results on SWE-Bench-verified and RepoQA demonstrate that ReSAT effectively
enhances SLMs' issue-resolving and repository-level long-context understanding
capabilities.",2024-12-26T03:01:32Z,http://arxiv.org/abs/2412.19031v1,"Zexiong Ma, Shengnan An, Zeqi Lin, Yanzhen Zou, Bing Xie"
"Modality-Projection Universal Model for Comprehensive Full-Body Medical
  Imaging Segmentation","The integration of deep learning in medical imaging has shown great promise
for enhancing diagnostic, therapeutic, and research outcomes. However, applying
universal models across multiple modalities remains challenging due to the
inherent variability in data characteristics. This study aims to introduce and
evaluate a Modality Projection Universal Model (MPUM). MPUM employs a novel
modality-projection strategy, which allows the model to dynamically adjust its
parameters to optimize performance across different imaging modalities. The
MPUM demonstrated superior accuracy in identifying anatomical structures,
enabling precise quantification for improved clinical decision-making. It also
identifies metabolic associations within the brain-body axis, advancing
research on brain-body physiological correlations. Furthermore, MPUM's unique
controller-based convolution layer enables visualization of saliency maps
across all network layers, significantly enhancing the model's
interpretability.",2024-12-26T02:23:27Z,http://arxiv.org/abs/2412.19026v1,"Yixin Chen, Lin Gao, Yajuan Gao, Rui Wang, Jingge Lian, Xiangxi Meng, Yanhua Duan, Leiying Chai, Hongbin Han, Zhaoping Cheng, Zhaoheng Xie"
"Nonparametric Estimation of Matching Efficiency and Elasticity in a Spot
  Gig Work Platform: 2019-2023","This paper provides new evidence on spot gig work platforms for unemployed
workers searching for occupations with minimal educational or experience
requirements in Japan. Using proprietary data from a private online spot work
matching platform, Timee, it examines trends in key variables such as the
numbers of unemployed users, vacancies, hires, and labor market tightness. The
study compares these trends with part-time worker data from the public
employment platform, Hello Work. The private platform shows a significant
market expansion from December 2019 to December 2023. Applying a novel
nonparametric approach, the paper finds greater variability in efficiency and
higher elasticity, with elasticity with respect to the number of users
fluctuating from below 0.7 to above 1.5, and elasticity with respect to the
number of vacancies often exceeding 1.0, which is higher than Hello Work.
Lastly, the study highlights that Tokyo's labor market exhibits higher
efficiency compared to Osaka and Aichi, while elasticities are similar,
indicating less geographical heterogeneity of the spot work compared to Hello
Work.",2024-12-26T02:16:58Z,http://arxiv.org/abs/2412.19024v1,"Hayato Kanayama, Suguru Otani"
"Nuclear matter properties from chiral-scale effective theory including a
  dilatonic scalar meson","Chiral effective theory has become a powerful tool for studying the
low-energy properties of QCD. In this work, we apply an extended chiral
effective theory -- chiral-scale effective theory -- including a dilatonic
scalar meson to study nuclear matter and find that the properties around
saturation density can be well reproduced. Compared to the traditionally used
Walecka-type models in nuclear matter studies, our approach improves the
behavior of symmetry energy and the incompressibility coefficient in describing
empirical data without introducing additional freedoms. Moreover, the predicted
neutron star structures fall within the constraints of GW170817, PSR
J0740+6620, and PSR J0030+0451, while the maximum neutron star mass can reach
about $~3M_{\odot}$ with a pure hadronic phase. Additionally, we find that
symmetry patterns of the effective theory significantly impact neutron star
structures. %In chiral-scale effective theory, effective operators are well
organized by chiral-scale orders and freedoms induced by QCD symmetry patterns.
We believe that introducing this type of theory into nuclear matter studies can
lead to a deeper understanding of QCD, nuclear matter, and compact
astrophysical objects.",2024-12-26T02:13:05Z,http://arxiv.org/abs/2412.19023v1,"Lu-Qi Zhang, Yao Ma, Yong-Liang Ma"
Adaptivity can help exponentially for shadow tomography,"In recent years there has been significant interest in understanding the
statistical complexity of learning from quantum data under the constraint that
one can only make unentangled measurements. While a key challenge in
establishing tight lower bounds in this setting is to deal with the fact that
the measurements can be chosen in an adaptive fashion, a recurring theme has
been that adaptivity offers little advantage over more straightforward,
nonadaptive protocols.
  In this note, we offer a counterpoint to this. We show that for the basic
task of shadow tomography, protocols that use adaptively chosen two-copy
measurements can be exponentially more sample-efficient than any protocol that
uses nonadaptive two-copy measurements.",2024-12-26T02:13:04Z,http://arxiv.org/abs/2412.19022v1,"Sitan Chen, Weiyuan Gong, Zhihan Zhang"
"Travelling wave solutions of an equation of Harry Dym type arising in
  the Black-Scholes framework","The Black-Scholes framework is crucial in pricing a vast number of financial
instruments that permeate the complex dynamics of world markets. Associated
with this framework, we consider a second-order differential operator $L(x,
{\partial_x}) := v^2(x,t) (\partial_x^2 -\partial_x)$ that carries a variable
volatility term $v(x,t)$ and which is dependent on the underlying log-price $x$
and a time parameter $t$ motivated by the celebrated Dupire local volatility
model. In this context, we ask and answer the question of whether one can find
a non-linear evolution equation derived from a zero-curvature condition for a
time-dependent deformation of the operator $L$. The result is a variant of the
Harry Dym equation for which we can then find a family of travelling wave
solutions. This brings in extensive machinery from soliton theory and
integrable systems. As a by-product, it opens up the way to the use of coherent
structures in financial-market volatility studies.",2024-12-26T02:09:47Z,http://arxiv.org/abs/2412.19020v1,"Jorge P. Zubelli, Kuldeep Singh, Vinicius Albani, Ioannis Kourakis"
"Brain Ageing Prediction using Isolation Forest Technique and Residual
  Neural Network (ResNet)","Brain aging is a complex and dynamic process, leading to functional and
structural changes in the brain. These changes could lead to the increased risk
of neurodegenerative diseases and cognitive decline. Accurate brain-age
estimation utilizing neuroimaging data has become necessary for detecting
initial signs of neurodegeneration. Here, we propose a novel deep learning
approach using the Residual Neural Network 101 Version 2 (ResNet101V2) model to
predict brain age from MRI scans. To train, validate and test our proposed
model, we used a large dataset of 2102 images which were selected randomly from
the International Consortium for Brain Mapping (ICBM). Next, we applied data
preprocessing techniques, including normalizing the images and using outlier
detection via Isolation Forest method. Then, we evaluated various pre-trained
approaches (namely: MobileNetV2, ResNet50V2, ResNet101V2, Xception). The
results demonstrated that the ResNet101V2 model has higher performance compared
with the other models, attaining MAEs of 0.9136 and 0.8242 years for before and
after using Isolation Forest process. Our method achieved a high accuracy in
brain age estimation in ICBM dataset and it provides a reliable brain age
prediction.",2024-12-26T01:49:21Z,http://arxiv.org/abs/2412.19017v1,"Saadat Behzadi, Danial Sharifrazi, Roohallah Alizadehsani, Mojtaba Lotfaliany, Mohammadreza Mohebbi"
Stratified L-convex groups,"In this paper, we introduce a novel structure of stratified L-convex groups,
defined as groups equipped with a stratified L-convex space, ensuring that the
group operation is an L-convexity-preserving mapping. We prove that stratified
L-convex groups serve as objects, while L-convexity-preserving group
homomorphisms as morphisms, together forming a concrete category, denoted as
SLCG. As a specific instance of SLCG (i.e., when L=2), we define the category
of convex groups, denoted as CG. We demonstrate that SLCG possesses
well-defined characterizations, localization properties, as well as initial and
final structures, establishing it as a topological category over groups.
Additionally, we show that CG can be embedded within SLCG as a reflective
subcategory.",2024-12-26T01:33:29Z,http://arxiv.org/abs/2412.19014v1,"Lingqiang Li, Qiu Jin"
Dynamic networks clustering via mirror distance,"The classification of different patterns of network evolution, for example in
brain connectomes or social networks, is a key problem in network inference and
modern data science. Building on the notion of a network's Euclidean mirror,
which captures its evolution as a curve in Euclidean space, we develop the
Dynamic Network Clustering through Mirror Distance (DNCMD), an algorithm for
clustering dynamic networks based on a distance measure between their
associated mirrors. We provide theoretical guarantees for DNCMD to achieve
exact recovery of distinct evolutionary patterns for latent position random
networks both when underlying vertex features change deterministically and when
they follow a stochastic process. We validate our theoretical results through
numerical simulations and demonstrate the application of DNCMD to understand
edge functions in Drosophila larval connectome data, as well as to analyze
temporal patterns in dynamic trade networks.",2024-12-26T01:14:21Z,http://arxiv.org/abs/2412.19012v1,"Runbing Zheng, Avanti Athreya, Marta Zlatic, Michael Clayton, Carey E. Priebe"
"Developing a single-phase and nanograined refractory high-entropy alloy
  ZrHfNbTaW with ultrahigh hardness by phase transformation via high-pressure
  torsion","High-entropy alloys (HEAs) are potential candidates for applications as
refractory materials. While dual-phase refractory HEAs containing an ordered
phase exhibit high hardness, there is high interest in developing
intermetallic-free and single-phase refractory HEAs with high hardness. In this
study, a new equiatomic HEA ZrHfNbTaW with an ultrahigh hardness of 860 Hv is
developed. The alloy is first synthesized with a dual-phase structure via arc
melting and further homogenized to a single body-centered cubic (BCC) structure
by phase transformation via high-pressure torsion (HPT), using the concept of
ultra-severe plastic deformation process. The ultrahigh hardness of the alloy,
which is higher than those reported for refractory alloys and single-phase
HEAs, is attributed to (i) solution hardening by severe lattice distortion,
(ii) Hall-Petch grain boundary hardening by the formation of nanograins with 12
nm average size, and (iii) dislocation hardening confirmed by high-resolution
transmission electron microscopy.",2024-12-26T00:36:31Z,http://arxiv.org/abs/2412.19006v1,"Shivam Dangwal, Kaveh Edalati"
"Enhancing Audiovisual Speech Recognition through Bifocal Preference
  Optimization","Audiovisual Automatic Speech Recognition (AV-ASR) aims to improve speech
recognition accuracy by leveraging visual signals. It is particularly
challenging in unconstrained real-world scenarios across various domains due to
noisy acoustic environments, spontaneous speech, and the uncertain use of
visual information. Most previous works fine-tune audio-only ASR models on
audiovisual datasets, optimizing them for conventional ASR objectives. However,
they often neglect visual features and common errors in unconstrained video
scenarios. In this paper, we propose using a preference optimization strategy
to improve speech recognition accuracy for real-world videos. First, we create
preference data via simulating common errors that occurred in AV-ASR from two
focals: manipulating the audio or vision input and rewriting the output
transcript. Second, we propose BPO-AVASR, a Bifocal Preference Optimization
method to improve AV-ASR models by leveraging both input-side and output-side
preference. Extensive experiments demonstrate that our approach significantly
improves speech recognition accuracy across various domains, outperforming
previous state-of-the-art models on real-world video speech recognition.",2024-12-26T00:26:45Z,http://arxiv.org/abs/2412.19005v1,"Yihan Wu, Yichen Lu, Yifan Peng, Xihua Wang, Ruihua Song, Shinji Watanabe"
Robust functional PCA for density data,"This paper introduces a robust approach to functional principal component
analysis (FPCA) for compositional data, particularly density functions. While
recent papers have studied density data within the Bayes space framework, there
has been limited focus on developing robust methods to effectively handle
anomalous observations and large noise. To address this, we extend the
Mahalanobis distance concept to Bayes spaces, proposing its regularized version
that accounts for the constraints inherent in density data. Based on this
extension, we introduce a new method, robust density principal component
analysis (RDPCA), for more accurate estimation of functional principal
components in the presence of outliers. The method's performance is validated
through simulations and real-world applications, showing its ability to improve
covariance estimation and principal component analysis compared to traditional
methods.",2024-12-26T00:06:47Z,http://arxiv.org/abs/2412.19004v1,"Jeremy Oguamalam, Peter Filzmoser, Karel Hron, Alessandra Menafoglio, Una Radojičić"
"Tempus Core: Area-Power Efficient Temporal-Unary Convolution Core for
  Low-Precision Edge DLAs","The increasing complexity of deep neural networks (DNNs) poses significant
challenges for edge inference deployment due to resource and power constraints
of edge devices. Recent works on unary-based matrix multiplication hardware aim
to leverage data sparsity and low-precision values to enhance hardware
efficiency. However, the adoption and integration of such unary hardware into
commercial deep learning accelerators (DLA) remain limited due to processing
element (PE) array dataflow differences. This work presents Tempus Core, a
convolution core with highly scalable unary-based PE array comprising of tub
(temporal-unary-binary) multipliers that seamlessly integrates with the NVDLA
(NVIDIA's open-source DLA for accelerating CNNs) while maintaining dataflow
compliance and boosting hardware efficiency. Analysis across various datapath
granularities shows that for INT8 precision in 45nm CMOS, Tempus Core's PE cell
unit (PCU) yields 59.3% and 15.3% reductions in area and power consumption,
respectively, over NVDLA's CMAC unit. Considering a 16x16 PE array in Tempus
Core, area and power improves by 75% and 62%, respectively, while delivering 5x
and 4x iso-area throughput improvements for INT8 and INT4 precisions.
Post-place and route analysis of Tempus Core's PCU shows that the 16x4 PE array
for INT4 precision in 45nm CMOS requires only 0.017 mm^2 die area and consumes
only 6.2mW of total power. We demonstrate that area-power efficient unary-based
hardware can be seamlessly integrated into conventional DLAs, paving the path
for efficient unary hardware for edge AI inference.",2024-12-25T23:20:02Z,http://arxiv.org/abs/2412.19002v1,"Prabhu Vellaisamy, Harideep Nair, Thomas Kang, Yichen Ni, Haoyang Fan, Bin Qi, Jeff Chen, Shawn Blanton, John Paul Shen"
"Impact of resummation on the production and experimental bounds of
  scalar high-electric-charge objects","A one-loop Dyson-Schwinger-like resummation scheme is applied to scalar
High-Electric-Charge compact Objects (HECOs), extending previous work on
spin-1/2 case. The electromagnetic interactions of HECOs are considered within
the framework of strongly coupled scalar Quantun Electrodynamics. The
resummation amounts to determining non-trivial ultraviolet (UV) fixed points,
at which the effective Lagrangian, which will lead to the pertinent predictions
on the cross sections, is computed. In contrast to the fermionic HECO case, in
which the fixed point structure was determined solely by the interactions of
the HECOs with the photon field, in the scalar case the existence of
non-trivial UV fixed points requires the presence of additional strong self
interactions among the HECOs. Our resummation scheme, which is notably
different from a lattice strong-coupling approach, makes the computation of the
pertinent scalar-HECO-production cross sections reliable, thus allowing
revisiting the mass bounds obtained from searches for such objects in current
or future colliders. Our MadGraph implementation of the results leads to
enhanced (up to ~30%) lower bounds on the mass of scalar HECOs, as compared to
those extracted from the tree-level processes typically used in LHC collider
searches by ATLAS and MoEDAL experiments.",2024-12-25T23:13:36Z,http://arxiv.org/abs/2412.19001v1,"Jean Alexandre, Nick E. Mavromatos, Vasiliki A. Mitsou, Emanuela Musumeci"
"MGAN-CRCM: A Novel Multiple Generative Adversarial Network and
  Coarse-Refinement Based Cognizant Method for Image Inpainting","Image inpainting is a widely used technique in computer vision for
reconstructing missing or damaged pixels in images. Recent advancements with
Generative Adversarial Networks (GANs) have demonstrated superior performance
over traditional methods due to their deep learning capabilities and
adaptability across diverse image domains. Residual Networks (ResNet) have also
gained prominence for their ability to enhance feature representation and
compatibility with other architectures. This paper introduces a novel
architecture combining GAN and ResNet models to improve image inpainting
outcomes. Our framework integrates three components: Transpose
Convolution-based GAN for guided and blind inpainting, Fast
ResNet-Convolutional Neural Network (FR-CNN) for object removal, and
Co-Modulation GAN (Co-Mod GAN) for refinement. The model's performance was
evaluated on benchmark datasets, achieving accuracies of 96.59% on Image-Net,
96.70% on Places2, and 96.16% on CelebA. Comparative analyses demonstrate that
the proposed architecture outperforms existing methods, highlighting its
effectiveness in both qualitative and quantitative evaluations.",2024-12-25T22:54:28Z,http://arxiv.org/abs/2412.19000v1,"Nafiz Al Asad, Md. Appel Mahmud Pranto, Shbiruzzaman Shiam, Musaddeq Mahmud Akand, Mohammad Abu Yousuf, Khondokar Fida Hasan, Mohammad Ali Moni"
Soliton foam formation in the early Universe,"The formation of composite solitons produced by scalar fields without thermal
phase transitions in the early Universe is considered. We present numerical
simulations of the formation and evolution of soliton structures at the
post-inflationary stage. The realistic initial conditions are obtained through
the simulation of multiple quantum fluctuations during the inflation epoch. The
initial field distributions allow to form local soliton clusters in the early
Universe without the need for the thermal production of a soliton network
throughout the Universe. We find that in three-dimensional space, the
nontrivial composite field structures are formed in the form of &lt;&lt;soliton
foam&gt;&gt;, consisting of closed domain walls, domain walls bounded by cosmic
strings, and scalar field radiation. The possible cosmological implications of
the soliton foam are discussed.",2024-12-25T22:35:56Z,http://arxiv.org/abs/2412.18997v1,"A. A. Kirillov, B. S. Murygin, V. V. Nikulin"
"MiTREE: Multi-input Transformer Ecoregion Encoder for Species
  Distribution Modelling","Climate change poses an extreme threat to biodiversity, making it imperative
to efficiently model the geographical range of different species. The
availability of large-scale remote sensing images and environmental data has
facilitated the use of machine learning in Species Distribution Models (SDMs),
which aim to predict the presence of a species at any given location.
Traditional SDMs, reliant on expert observation, are labor-intensive, but
advancements in remote sensing and citizen science data have facilitated
machine learning approaches to SDM development. However, these models often
struggle with leveraging spatial relationships between different inputs -- for
instance, learning how climate data should inform the data present in satellite
imagery -- without upsampling or distorting the original inputs. Additionally,
location information and ecological characteristics at a location play a
crucial role in predicting species distribution models, but these aspects have
not yet been incorporated into state-of-the-art approaches. In this work, we
introduce MiTREE: a multi-input Vision-Transformer-based model with an
ecoregion encoder. MiTREE computes spatial cross-modal relationships without
upsampling as well as integrates location and ecological context. We evaluate
our model on the SatBird Summer and Winter datasets, the goal of which is to
predict bird species encounter rates, and we find that our approach improves
upon state-of-the-art baselines.",2024-12-25T22:20:47Z,http://arxiv.org/abs/2412.18995v1,"Theresa Chen, Yao-Yi Chiang"
"Geospatial Data Fusion: Combining Lidar, SAR, and Optical Imagery with
  AI for Enhanced Urban Mapping","This study explores the integration of Lidar, Synthetic Aperture Radar (SAR),
and optical imagery through advanced artificial intelligence techniques for
enhanced urban mapping. By fusing these diverse geospatial datasets, we aim to
overcome the limitations associated with single-sensor data, achieving a more
comprehensive representation of urban environments. The research employs Fully
Convolutional Networks (FCNs) as the primary deep learning model for urban
feature extraction, enabling precise pixel-wise classification of essential
urban elements, including buildings, roads, and vegetation. To optimize the
performance of the FCN model, we utilize Particle Swarm Optimization (PSO) for
hyperparameter tuning, significantly enhancing model accuracy. Key findings
indicate that the FCN-PSO model achieved a pixel accuracy of 92.3% and a mean
Intersection over Union (IoU) of 87.6%, surpassing traditional single-sensor
approaches. These results underscore the potential of fused geospatial data and
AI-driven methodologies in urban mapping, providing valuable insights for urban
planning and management. The implications of this research pave the way for
future developments in real-time mapping and adaptive urban infrastructure
planning.",2024-12-25T22:17:31Z,http://arxiv.org/abs/2412.18994v1,"Sajjad Afroosheh, Mohammadreza Askari"
"On the architecture of the Symplectic $(A_\infty,2)$-Category","This note relates to the author's construction of the Symplectic
$(A_\infty,2)$-Category, $\mathsf{Symp}$. Here we explain two ways of encoding
the information in $\mathsf{Symp}$, one topological, one algebraic. The
topological encoding is as an $(A_\infty,2)$-flow category, which we define
here. The algebraic encoding is as a linear $(A_\infty,2)$-category, which we
extract from the topological encoding. In upcoming work, the author and
Wehrheim plan to use the adiabatic Fredholm theory recently developed by
Bottman-Wehrheim to construct $\mathsf{Symp}$ as an $(A_\infty,2)$-flow
category.
  The definition of linear $(A_\infty,2)$-category that we give in this note is
different than the one proposed by Bottman-Carmeli. The recursive structure of
the 2-associahedra identifies faces with fiber products of 2-associahedra over
associahedra, and these fiber products led Bottman-Carmeli to associate
operations to singular chains on 2-associahedra. The innovation in our new
definition of linear $(A_\infty,2)$-category is to extend the family of
2-associahedra to include all fiber products of 2-associahedra over
associahedra. This allows us to associate operations to cellular chains, which
in particular enables us to produce a definition that involves only one
operation in each arity, governed by a collection of $(A_\infty,2)$-equations.",2024-12-25T22:11:18Z,http://arxiv.org/abs/2412.18993v1,Nathaniel Bottman
"Optimal Federated Learning for Functional Mean Estimation under
  Heterogeneous Privacy Constraints","Federated learning (FL) is a distributed machine learning technique designed
to preserve data privacy and security, and it has gained significant importance
due to its broad range of applications. This paper addresses the problem of
optimal functional mean estimation from discretely sampled data in a federated
setting.
  We consider a heterogeneous framework where the number of individuals,
measurements per individual, and privacy parameters vary across one or more
servers, under both common and independent design settings. In the common
design setting, the same design points are measured for each individual,
whereas in the independent design, each individual has their own random
collection of design points. Within this framework, we establish minimax upper
and lower bounds for the estimation error of the underlying mean function,
highlighting the nuanced differences between common and independent designs
under distributed privacy constraints.
  We propose algorithms that achieve the optimal trade-off between privacy and
accuracy and provide optimality results that quantify the fundamental limits of
private functional mean estimation across diverse distributed settings. These
results characterize the cost of privacy and offer practical insights into the
potential for privacy-preserving statistical analysis in federated
environments.",2024-12-25T22:06:12Z,http://arxiv.org/abs/2412.18992v1,"Tony Cai, Abhinav Chakraborty, Lasse Vuursteen"
"MTCAE-DFER: Multi-Task Cascaded Autoencoder for Dynamic Facial
  Expression Recognition","This paper expands the cascaded network branch of the autoencoder-based
multi-task learning (MTL) framework for dynamic facial expression recognition,
namely Multi-Task Cascaded Autoencoder for Dynamic Facial Expression
Recognition (MTCAE-DFER). MTCAE-DFER builds a plug-and-play cascaded decoder
module, which is based on the Vision Transformer (ViT) architecture and employs
the decoder concept of Transformer to reconstruct the multi-head attention
module. The decoder output from the previous task serves as the query (Q),
representing local dynamic features, while the Video Masked Autoencoder
(VideoMAE) shared encoder output acts as both the key (K) and value (V),
representing global dynamic features. This setup facilitates interaction
between global and local dynamic features across related tasks. Additionally,
this proposal aims to alleviate overfitting of complex large model. We utilize
autoencoder-based multi-task cascaded learning approach to explore the impact
of dynamic face detection and dynamic face landmark on dynamic facial
expression recognition, which enhances the model's generalization ability.
After we conduct extensive ablation experiments and comparison with
state-of-the-art (SOTA) methods on various public datasets for dynamic facial
expression recognition, the robustness of the MTCAE-DFER model and the
effectiveness of global-local dynamic feature interaction among related tasks
have been proven.",2024-12-25T21:52:31Z,http://arxiv.org/abs/2412.18988v1,"Peihao Xiang, Kaida Wu, Chaohao Lin, Ou Bai"
TravelAgent: Generative Agents in the Built Environment,"Understanding human behavior in built environments is critical for designing
functional, user centered urban spaces. Traditional approaches, such as manual
observations, surveys, and simplified simulations, often fail to capture the
complexity and dynamics of real world behavior. To address these limitations,
we introduce TravelAgent, a novel simulation platform that models pedestrian
navigation and activity patterns across diverse indoor and outdoor environments
under varying contextual and environmental conditions. TravelAgent leverages
generative agents integrated into 3D virtual environments, enabling agents to
process multimodal sensory inputs and exhibit human-like decision-making,
behavior, and adaptation. Through experiments, including navigation,
wayfinding, and free exploration, we analyze data from 100 simulations
comprising 1898 agent steps across diverse spatial layouts and agent
archetypes, achieving an overall task completion rate of 76%. Using spatial,
linguistic, and sentiment analyses, we show how agents perceive, adapt to, or
struggle with their surroundings and assigned tasks. Our findings highlight the
potential of TravelAgent as a tool for urban design, spatial cognition
research, and agent-based modeling. We discuss key challenges and opportunities
in deploying generative agents for the evaluation and refinement of spatial
designs, proposing TravelAgent as a new paradigm for simulating and
understanding human experiences in built environments.",2024-12-25T21:27:51Z,http://arxiv.org/abs/2412.18985v1,"Ariel Noyman, Kai Hu, Kent Larson"
Positivity of Schubert Coefficients,"Schubert coefficients $c_{u,v}^w$ are structure constants describing
multiplication of Schubert polynomials. Deciding positivity of Schubert
coefficients is a major open problem in Algebraic Combinatorics. We prove a
positive rule for this problem based on two standard assumptions.",2024-12-25T21:23:06Z,http://arxiv.org/abs/2412.18984v1,"Igor Pak, Colleen Robichaux"
"Deep Learning-Based Traffic-Aware Base Station Sleep Mode and Cell
  Zooming Strategy in RIS-Aided Multi-Cell Networks","Advances in wireless technology have significantly increased the number of
wireless connections, leading to higher energy consumption in networks. Among
these, base stations (BSs) in radio access networks (RANs) account for over
half of the total energy usage. To address this, we propose a multi-cell sleep
strategy combined with adaptive cell zooming, user association, and
reconfigurable intelligent surface (RIS) to minimize BS energy consumption.
This approach allows BSs to enter sleep during low traffic, while adaptive cell
zooming and user association dynamically adjust coverage to balance traffic
load and enhance data rates through RIS, minimizing the number of active BSs.
However, it is important to note that the proposed method may achieve
energy-savings at the cost of increased delay, requiring a trade-off between
these two factors. Moreover, minimizing BS energy consumption under the delay
constraint is a complicated non-convex problem. To address this issue, we model
the RIS-aided multi-cell network as a Markov decision process (MDP) and use the
proximal policy optimization (PPO) algorithm to optimize sleep mode (SM), cell
zooming, and user association. Besides, we utilize a double cascade correlation
network (DCCN) algorithm to optimize the RIS reflection coefficients.
Simulation results demonstrate that PPO balances energy-savings and delay,
while DCCN-optimized RIS enhances BS energy-savings. Compared to systems
optimised by the benchmark DQN algorithm, energy consumption is reduced by
49.61%",2024-12-25T21:06:40Z,http://arxiv.org/abs/2412.18983v1,"Shuo Sun, Chong Huang, Gaojie Chen, Pei Xiao, Rahim Tafazolli"
"Evaluating deep learning models for fault diagnosis of a rotating
  machinery with epistemic and aleatoric uncertainty","Uncertainty-aware deep learning (DL) models recently gained attention in
fault diagnosis as a way to promote the reliable detection of faults when
out-of-distribution (OOD) data arise from unseen faults (epistemic uncertainty)
or the presence of noise (aleatoric uncertainty). In this paper, we present the
first comprehensive comparative study of state-of-the-art uncertainty-aware DL
architectures for fault diagnosis in rotating machinery, where different
scenarios affected by epistemic uncertainty and different types of aleatoric
uncertainty are investigated. The selected architectures include sampling by
dropout, Bayesian neural networks, and deep ensembles. Moreover, to distinguish
between in-distribution and OOD data in the different scenarios two uncertainty
thresholds, one of which is introduced in this paper, are alternatively
applied. Our empirical findings offer guidance to practitioners and researchers
who have to deploy real-world uncertainty-aware fault diagnosis systems. In
particular, they reveal that, in the presence of epistemic uncertainty, all DL
models are capable of effectively detecting, on average, a substantial portion
of OOD data across all the scenarios. However, deep ensemble models show
superior performance, independently of the uncertainty threshold used for
discrimination. In the presence of aleatoric uncertainty, the noise level plays
an important role. Specifically, low noise levels hinder the models' ability to
effectively detect OOD data. Even in this case, however, deep ensemble models
exhibit a milder degradation in performance, dominating the others. These
achievements, combined with their shorter inference time, make deep ensemble
architectures the preferred choice.",2024-12-25T20:22:59Z,http://arxiv.org/abs/2412.18980v1,"Reza Jalayer, Masoud Jalayer, Andrea Mor, Carlotta Orsenigo, Carlo Vercellis"
Enhanced Elastocaloric Effects in γ-graphyne,"The global emphasis on sustainable technologies has become a paramount
concern for nations worldwide. Specifically, numerous sustainable methods are
being explored as promising alternatives to the well-established
vapor-compression technologies in cooling and heating devices. One such avenue
gaining traction within the scientific community is the elastocaloric effect
(eC). This phenomenon holds promise for efficient cooling and heating processes
without causing environmental harm. Studies carried out at the nanoscale have
demonstrated the efficiency of the eC, proving to be comparable to that of
state-of-the-art macroscopic systems. In this study, we used classical
molecular dynamics simulations to investigate the elastocaloric effect for
{\gamma}-graphyne. Our analysis goes beyond obtaining changes in eC temperature
and the coefficient of performance (COP) for two species of {\gamma}-graphyne
nanoribbons (armchair and zigzag). We also explore their dependence on various
conditions, including whether they are on deposited on a substrate or
pre-strained. Our findings reveal a substantial enhancement in the
elastocaloric effect for {\gamma}-graphyne nanoribbons when subjected to
pre-strain, amplifying it by at least one order of magnitude. Under certain
conditions, the change in the eC temperature and the COP of the structures
reach expressive values as high as 224 K and 14, respectively. We discuss the
implications of these results by examining the shape and behavior of the
carbon-carbon bond lengths within the structures.",2024-12-25T20:12:14Z,http://arxiv.org/abs/2412.18978v1,"Guilherme B. Kanegae, Marcelo L. Pereira Junior, Douglas S. Galvão, Luiz A. Ribeiro Junior, Alexandre F. Fonseca"
Injecting Bias into Text Classification Models using Backdoor Attacks,"The rapid growth of natural language processing (NLP) and pre-trained
language models have enabled accurate text classification in a variety of
settings. However, text classification models are susceptible to backdoor
attacks, where an attacker embeds a trigger into the victim model to make the
model predict attacker-desired labels in targeted scenarios. In this paper, we
propose to utilize backdoor attacks for a new purpose: bias injection. We
develop a backdoor attack in which a subset of the training dataset is poisoned
to associate strong male actors with negative sentiment. We execute our attack
on two popular text classification datasets (IMDb and SST) and seven different
models ranging from traditional Doc2Vec-based models to LSTM networks and
modern transformer-based BERT and RoBERTa models. Our results show that the
reduction in backdoored models' benign classification accuracy is limited,
implying that our attacks remain stealthy, whereas the models successfully
learn to associate strong male actors with negative sentiment (100% attack
success rate with &gt;= 3% poison rate). Attacks on BERT and RoBERTa are
particularly more stealthy and effective, demonstrating an increased risk of
using modern and larger models. We also measure the generalizability of our
bias injection by proposing two metrics: (i) U-BBSR which uses previously
unseen words when measuring attack success, and (ii) P-BBSR which measures
attack success using paraphrased test samples. U-BBSR and P-BBSR results show
that the bias injected by our attack can go beyond memorizing a trigger phrase.",2024-12-25T19:32:02Z,http://arxiv.org/abs/2412.18975v1,"A. Dilara Yavuz, M. Emre Gursoy"
"Adopting Trustworthy AI for Sleep Disorder Prediction: Deep Time Series
  Analysis with Temporal Attention Mechanism and Counterfactual Explanations","Sleep disorders have a major impact on both lifestyle and health. Effective
sleep disorder prediction from lifestyle and physiological data can provide
essential details for early intervention. This research utilizes three deep
time series models and facilitates them with explainability approaches for
sleep disorder prediction. Specifically, our approach adopts Temporal
Convolutional Networks (TCN), Long Short-Term Memory (LSTM) for time series
data analysis, and Temporal Fusion Transformer model (TFT). Meanwhile, the
temporal attention mechanism and counterfactual explanation with SHapley
Additive exPlanations (SHAP) approach are employed to ensure dependable,
accurate, and interpretable predictions. Finally, using a large dataset of
sleep health measures, our evaluation demonstrates the effect of our method in
predicting sleep disorders.",2024-12-25T19:19:45Z,http://arxiv.org/abs/2412.18971v1,"Pegah Ahadian, Wei Xu, Sherry Wang, Qiang Guan"
Flat Bands and Compact Localised States: A Carrollian roadmap,"We show how Carrollian symmetries become important in the construction of
one-dimensional fermionic systems with all flat-band spectra from first
principles. The key ingredient of this construction is the identification of
Compact Localised States (CLSs), which appear naturally by demanding
$\textit{supertranslation}$ invariance of the system. We use CLS basis states,
with inherent $\textit{ultra-local}$ correlations, to write down an interacting
theory which shows a non-trivial phase structure and an emergent Carroll
conformal symmetry at the gapless points. We analyze this theory in detail for
both zero and finite chemical potential.",2024-12-25T18:54:45Z,http://arxiv.org/abs/2412.18965v1,"Nisa Ara, Aritra Banerjee, Rudranil Basu, Bhagya Krishnan"
"Don't Lose Yourself: Boosting Multimodal Recommendation via Reducing
  Node-neighbor Discrepancy in Graph Convolutional Network","The rapid expansion of multimedia contents has led to the emergence of
multimodal recommendation systems. It has attracted increasing attention in
recommendation systems because its full utilization of data from different
modalities alleviates the persistent data sparsity problem. As such, multimodal
recommendation models can learn personalized information about nodes in terms
of visual and textual. To further alleviate the data sparsity problem, some
previous works have introduced graph convolutional networks (GCNs) for
multimodal recommendation systems, to enhance the semantic representation of
users and items by capturing the potential relationships between them. However,
adopting GCNs inevitably introduces the over-smoothing problem, which make
nodes to be too similar. Unfortunately, incorporating multimodal information
will exacerbate this challenge because nodes that are too similar will lose the
personalized information learned through multimodal information. To address
this problem, we propose a novel model that retains the personalized
information of ego nodes during feature aggregation by Reducing Node-neighbor
Discrepancy (RedN^nD). Extensive experiments on three public datasets show that
RedN^nD achieves state-of-the-art performance on accuracy and robustness, with
significant improvements over existing GCN-based multimodal frameworks.",2024-12-25T18:41:36Z,http://arxiv.org/abs/2412.18962v1,"Zheyu Chen, Jinfeng Xu, Haibo Hu"
"RIS-Assisted Aerial Non-Terrestrial Networks: An Intelligent Synergy
  with Deep Reinforcement Learning","Reconfigurable intelligent surface (RIS)-assisted aerial non-terrestrial
networks (NTNs) offer a promising paradigm for enhancing wireless
communications in the era of 6G and beyond. By integrating RIS with aerial
platforms such as unmanned aerial vehicles (UAVs) and high-altitude platforms
(HAPs), these networks can intelligently control signal propagation, extending
coverage, improving capacity, and enhancing link reliability. This article
explores the application of deep reinforcement learning (DRL) as a powerful
tool for optimizing RIS-assisted aerial NTNs. We focus on hybrid proximal
policy optimization (H-PPO), a robust DRL algorithm well-suited for handling
the complex, hybrid action spaces inherent in these networks. Through a case
study of an aerial RIS (ARIS)-aided coordinated multi-point non-orthogonal
multiple access (CoMP-NOMA) network, we demonstrate how H-PPO can effectively
optimize the system and maximize the sum rate while adhering to system
constraints. Finally, we discuss key challenges and promising research
directions for DRL-powered RIS-assisted aerial NTNs, highlighting their
potential to transform next-generation wireless networks.",2024-12-25T18:11:34Z,http://arxiv.org/abs/2412.18957v1,"Muhammad Umer, Muhammad Ahmed Mohsin, Aryan Kaushik, Qurrat-ul-Ain Nadeem, Ali Arshad Nasir, Syed Ali Hassan"
"Mixed Fourier norm spaces of analytic functions on the upper half-plane
  and Toeplitz operators","We introduce and study weighted spaces of functions with mixed norm on the
upper half-plane, defined in terms of Fourier transform. We give a
characterization of analytic functions within these spaces, and in particular,
we provide an analog of the Paley-Wiener theorem in this setting. As an
application, we consider Toeplitz operators with vertical symbols in these new
spaces.",2024-12-25T17:45:45Z,http://arxiv.org/abs/2412.18954v1,"Zhirayr Avetisyan, Alexey Karapetyants, Irina Smirnova"
"Bridging Interpretability and Robustness Using LIME-Guided Model
  Refinement","This paper explores the intricate relationship between interpretability and
robustness in deep learning models. Despite their remarkable performance across
various tasks, deep learning models often exhibit critical vulnerabilities,
including susceptibility to adversarial attacks, over-reliance on spurious
correlations, and a lack of transparency in their decision-making processes. To
address these limitations, we propose a novel framework that leverages Local
Interpretable Model-Agnostic Explanations (LIME) to systematically enhance
model robustness. By identifying and mitigating the influence of irrelevant or
misleading features, our approach iteratively refines the model, penalizing
reliance on these features during training. Empirical evaluations on multiple
benchmark datasets demonstrate that LIME-guided refinement not only improves
interpretability but also significantly enhances resistance to adversarial
perturbations and generalization to out-of-distribution data.",2024-12-25T17:32:45Z,http://arxiv.org/abs/2412.18952v1,"Navid Nayyem, Abdullah Rakin, Longwei Wang"
"TopoBDA: Towards Bezier Deformable Attention for Road Topology
  Understanding","Understanding road topology is crucial for autonomous driving. This paper
introduces TopoBDA (Topology with Bezier Deformable Attention), a novel
approach that enhances road topology understanding by leveraging Bezier
Deformable Attention (BDA). BDA utilizes Bezier control points to drive the
deformable attention mechanism, significantly improving the detection and
representation of elongated and thin polyline structures, such as lane
centerlines. TopoBDA processes multi-camera 360-degree imagery to generate
Bird's Eye View (BEV) features, which are refined through a transformer decoder
employing BDA. This method enhances computational efficiency while maintaining
high accuracy in centerline prediction. Additionally, TopoBDA incorporates an
instance mask formulation and an auxiliary one-to-many set prediction loss
strategy to further refine centerline detection and improve road topology
understanding. Experimental evaluations on the OpenLane-V2 dataset demonstrate
that TopoBDA outperforms existing methods, achieving state-of-the-art results
in centerline detection and topology reasoning. The integration of multi-modal
data, including lidar and radar, specifically for road topology understanding,
further enhances the model's performance, underscoring its importance in
autonomous driving applications.",2024-12-25T17:31:54Z,http://arxiv.org/abs/2412.18951v1,"Muhammet Esat Kalfaoglu, Halil Ibrahim Ozturk, Ozsel Kilinc, Alptekin Temizel"
A Power-Efficient Hardware Implementation of L-Mul,"Multiplication is a core operation in modern neural network (NN)
computations, contributing significantly to energy consumption. The
linear-complexity multiplication (L-Mul) algorithm is specifically proposed as
an approximate multiplication method for emerging NN models, such as large
language model (LLM), to reduce the energy consumption and computational
complexity of multiplications. However, hardware implementation designs for
L-Mul have not yet been reported. Additionally, 8-bit floating-point (FP8), as
an emerging data format, offers a better dynamic range compared to traditional
8-bit integer (INT8), making it increasingly popular and widely adopted in NN
computations. This paper thus presents a power-efficient FPGAbased hardware
implementation (approximate FP8 multiplier) for L-Mul. The core computation is
implemented using the dynamic reconfigurable lookup tables and carry chains
primitives available in AMD Xilinx UltraScale/UltraScale+ technology. The
accuracy and resource utilization of the approximate multiplier are evaluated
and analyzed. Furthermore, the FP8 approximate multiplier is deployed in the
inference phase of representative NN models to validate its effectiveness.",2024-12-25T17:05:00Z,http://arxiv.org/abs/2412.18948v1,"Ruiqi Chen, Yangxintong Lyu, Han Bao, Bruno da Silva"
"Constraint-Adaptive Policy Switching for Offline Safe Reinforcement
  Learning","Offline safe reinforcement learning (OSRL) involves learning a
decision-making policy to maximize rewards from a fixed batch of training data
to satisfy pre-defined safety constraints. However, adapting to varying safety
constraints during deployment without retraining remains an under-explored
challenge. To address this challenge, we introduce constraint-adaptive policy
switching (CAPS), a wrapper framework around existing offline RL algorithms.
During training, CAPS uses offline data to learn multiple policies with a
shared representation that optimize different reward and cost trade-offs.
During testing, CAPS switches between those policies by selecting at each state
the policy that maximizes future rewards among those that satisfy the current
cost constraint. Our experiments on 38 tasks from the DSRL benchmark
demonstrate that CAPS consistently outperforms existing methods, establishing a
strong wrapper-based baseline for OSRL. The code is publicly available at
https://github.com/yassineCh/CAPS.",2024-12-25T16:42:27Z,http://arxiv.org/abs/2412.18946v1,"Yassine Chemingui, Aryan Deshwal, Honghao Wei, Alan Fern, Janardhan Rao Doppa"
Amuse: Human-AI Collaborative Songwriting with Multimodal Inspirations,"Songwriting is often driven by multimodal inspirations, such as imagery,
narratives, or existing music, yet songwriters remain unsupported by current
music AI systems in incorporating these multimodal inputs into their creative
processes. We introduce Amuse, a songwriting assistant that transforms
multimodal (image, text, or audio) inputs into chord progressions that can be
seamlessly incorporated into songwriters' creative processes. A key feature of
Amuse is its novel method for generating coherent chords that are relevant to
music keywords in the absence of datasets with paired examples of multimodal
inputs and chords. Specifically, we propose a method that leverages multimodal
large language models (LLMs) to convert multimodal inputs into noisy chord
suggestions and uses a unimodal chord model to filter the suggestions. A user
study with songwriters shows that Amuse effectively supports transforming
multimodal ideas into coherent musical suggestions, enhancing users' agency and
creativity throughout the songwriting process.",2024-12-25T16:23:32Z,http://arxiv.org/abs/2412.18940v1,"Yewon Kim, Sung-Ju Lee, Chris Donahue"
"Malware Classification using a Hybrid Hidden Markov Model-Convolutional
  Neural Network","The proliferation of malware variants poses a significant challenges to
traditional malware detection approaches, such as signature-based methods,
necessitating the development of advanced machine learning techniques. In this
research, we present a novel approach based on a hybrid architecture combining
features extracted using a Hidden Markov Model (HMM), with a Convolutional
Neural Network (CNN) then used for malware classification. Inspired by the
strong results in previous work using an HMM-Random Forest model, we propose
integrating HMMs, which serve to capture sequential patterns in opcode
sequences, with CNNs, which are adept at extracting hierarchical features. We
demonstrate the effectiveness of our approach on the popular Malicia dataset,
and we obtain superior performance, as compared to other machine learning
methods -- our results surpass the aforementioned HMM-Random Forest model. Our
findings underscore the potential of hybrid HMM-CNN architectures in bolstering
malware classification capabilities, offering several promising avenues for
further research in the field of cybersecurity.",2024-12-25T15:34:57Z,http://arxiv.org/abs/2412.18932v1,"Ritik Mehta, Olha Jureckova, Mark Stamp"
"Graph Cut-guided Maximal Coding Rate Reduction for Learning Image
  Embedding and Clustering","In the era of pre-trained models, image clustering task is usually addressed
by two relevant stages: a) to produce features from pre-trained vision models;
and b) to find clusters from the pre-trained features. However, these two
stages are often considered separately or learned by different paradigms,
leading to suboptimal clustering performance. In this paper, we propose a
unified framework, termed graph Cut-guided Maximal Coding Rate Reduction
(CgMCR$^2$), for jointly learning the structured embeddings and the clustering.
To be specific, we attempt to integrate an efficient clustering module into the
principled framework for learning structured representation, in which the
clustering module is used to provide partition information to guide the
cluster-wise compression and the learned embeddings is aligned to desired
geometric structures in turn to help for yielding more accurate partitions. We
conduct extensive experiments on both standard and out-of-domain image datasets
and experimental results validate the effectiveness of our approach.",2024-12-25T15:20:54Z,http://arxiv.org/abs/2412.18930v1,"W. He, Z. Huang, X. Meng, X. Qi, R. Xiao, C. -G. Li"
"UNIC-Adapter: Unified Image-instruction Adapter with Multi-modal
  Transformer for Image Generation","Recently, text-to-image generation models have achieved remarkable
advancements, particularly with diffusion models facilitating high-quality
image synthesis from textual descriptions. However, these models often struggle
with achieving precise control over pixel-level layouts, object appearances,
and global styles when using text prompts alone. To mitigate this issue,
previous works introduce conditional images as auxiliary inputs for image
generation, enhancing control but typically necessitating specialized models
tailored to different types of reference inputs. In this paper, we explore a
new approach to unify controllable generation within a single framework.
Specifically, we propose the unified image-instruction adapter (UNIC-Adapter)
built on the Multi-Modal-Diffusion Transformer architecture, to enable flexible
and controllable generation across diverse conditions without the need for
multiple specialized models. Our UNIC-Adapter effectively extracts multi-modal
instruction information by incorporating both conditional images and task
instructions, injecting this information into the image generation process
through a cross-attention mechanism enhanced by Rotary Position Embedding.
Experimental results across a variety of tasks, including pixel-level spatial
control, subject-driven image generation, and style-image-based image
synthesis, demonstrate the effectiveness of our UNIC-Adapter in unified
controllable image generation.",2024-12-25T15:19:02Z,http://arxiv.org/abs/2412.18928v1,"Lunhao Duan, Shanshan Zhao, Wenjun Yan, Yinglun Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Mingming Gong, Gui-Song Xia"
"Effective Lagrangian for strong and electromagnetic interactions of
  high-spin resonances","Recent experiments of photon-nucleon and meson-nucleon scatterings have
accumulated a lot of data for various meson production processes. One of the
purposes of those experiments is to search for the missing resonances which are
not discovered until now but whose existence was predicted by hadron models.
The analyses of the data requires the development of dynamical coupled-channel
models. Since several missing resonances are expected to have spin higher than
3/2, we need to include higher-spin resonances in dynamical coupled-channel
models, which enable us to determine the couplings of effective Lagrangians of
higher-spin baryons with pseudoscalar mesons or vector mesons. However, hadron
models, such as quark models, give predictions only of the decay amplitudes of
such baryons. Here we demonstrate the formalism of high-spin resonances and
construct the relation between the coupling constants of effective Lagrangians
and the partial decay widths that can be predicted by hadron models. This
allows us to compare the coupling constants to the hadron model predictions not
only in magnitude but in sign as well.",2024-12-25T15:17:08Z,http://arxiv.org/abs/2412.18927v1,"Sang-Ho Kim, Yongseok Oh, Sangyeong Son, S. Sakinah, Myung-Ki Cheoun"
Exemplar-condensed Federated Class-incremental Learning,"We propose Exemplar-Condensed federated class-incremental learning (ECoral)
to distil the training characteristics of real images from streaming data into
informative rehearsal exemplars. The proposed method eliminates the limitations
of exemplar selection in replay-based approaches for mitigating catastrophic
forgetting in federated continual learning (FCL). The limitations particularly
related to the heterogeneity of information density of each summarized data.
Our approach maintains the consistency of training gradients and the
relationship to past tasks for the summarized exemplars to represent the
streaming data compared to the original images effectively. Additionally, our
approach reduces the information-level heterogeneity of the summarized data by
inter-client sharing of the disentanglement generative model. Extensive
experiments show that our ECoral outperforms several state-of-the-art methods
and can be seamlessly integrated with many existing approaches to enhance
performance.",2024-12-25T15:13:40Z,http://arxiv.org/abs/2412.18926v1,"Rui Sun, Yumin Zhang, Varun Ojha, Tejal Shah, Haoran Duan, Bo Wei, Rajiv Ranjan"
"Asymptotic stability of the high-dimensional Kuramoto model on Stiefel
  manifolds","The aim of this article is to investigate the convergence properties of a
heterogeneous consensus model on Stiefel manifolds. We consider each agent,
without interaction, moving according to the flow determined by the fundamental
vector field of the right multiplication action of the orthogonal group on the
Stiefel manifold. We analyze the asymptotic behavior of N such agents, assuming
that, as a result of their interactions, each agent's velocity is the sum of
its natural velocity and an additional velocity directed towards the average
position of the N agents. If the fundamental vector fields of all agents are
the same, their movement can be represented as a gradient flow on a product
manifold. In this study, we specifically investigate the asymptotic behavior in
a non-gradient flow setting, where the fundamental vector fields are not all
the same. Since fewer tools are available to address non-gradient flows, we
perform an orbital stability analysis to obtain the desired results instead of
relying on a gradient flow structure. Our estimate improves upon the previous
result in [Ha et al., Automatica 136 (2022)]. Furthermore, as a direct
consequence of the asymptotic dynamics, we derive uniform-in-time stability
with respect to the initial data.",2024-12-25T15:06:49Z,http://arxiv.org/abs/2412.18923v1,"Dohyun Kim, Woojoo Shim"
"Generative Face Parsing Map Guided 3D Face Reconstruction Under Occluded
  Scenes","Over the past few years, single-view 3D face reconstruction methods can
produce beautiful 3D models. Nevertheless,the input of these works is
unobstructed faces.We describe a system designed to reconstruct convincing face
texture in the case of occlusion.Motivated by parsing facial features,we
propose a complete face parsing map generation method guided by landmarks.We
estimate the 2D face structure of the reasonable position of the occlusion
area,which is used for the construction of 3D texture.An excellent
anti-occlusion face reconstruction method should ensure the authenticity of the
output,including the topological structure between the eyes,nose, and mouth. We
extensively tested our method and its components, qualitatively demonstrating
the rationality of our estimated facial structure. We conduct extensive
experiments on general 3D face reconstruction tasks as concrete examples to
demonstrate the method's superior regulation ability over existing methods
often break down.We further provide numerous quantitative examples showing that
our method advances both the quality and the robustness of 3D face
reconstruction under occlusion scenes.",2024-12-25T14:49:41Z,http://arxiv.org/abs/2412.18920v1,"Dapeng Zhao, Yue Qi"
"An Attentive Dual-Encoder Framework Leveraging Multimodal Visual and
  Semantic Information for Automatic OSAHS Diagnosis","Obstructive sleep apnea-hypopnea syndrome (OSAHS) is a common sleep disorder
caused by upper airway blockage, leading to oxygen deprivation and disrupted
sleep. Traditional diagnosis using polysomnography (PSG) is expensive,
time-consuming, and uncomfortable. Existing deep learning methods using facial
image analysis lack accuracy due to poor facial feature capture and limited
sample sizes. To address this, we propose a multimodal dual encoder model that
integrates visual and language inputs for automated OSAHS diagnosis. The model
balances data using randomOverSampler, extracts key facial features with
attention grids, and converts physiological data into meaningful text.
Cross-attention combines image and text data for better feature extraction, and
ordered regression loss ensures stable learning. Our approach improves
diagnostic efficiency and accuracy, achieving 91.3% top-1 accuracy in a
four-class severity classification task, demonstrating state-of-the-art
performance. Code will be released upon acceptance.",2024-12-25T14:42:17Z,http://arxiv.org/abs/2412.18919v1,"Yingchen Wei, Xihe Qiu, Xiaoyu Tan, Jingjing Huang, Wei Chu, Yinghui Xu, Yuan Qi"
"Open-Vocabulary Panoptic Segmentation Using BERT Pre-Training of
  Vision-Language Multiway Transformer Model","Open-vocabulary panoptic segmentation remains a challenging problem. One of
the biggest difficulties lies in training models to generalize to an unlimited
number of classes using limited categorized training data. Recent popular
methods involve large-scale vision-language pre-trained foundation models, such
as CLIP. In this paper, we propose OMTSeg for open-vocabulary segmentation
using another large-scale vision-language pre-trained model called BEiT-3 and
leveraging the cross-modal attention between visual and linguistic features in
BEiT-3 to achieve better performance. Experiments result demonstrates that
OMTSeg performs favorably against state-of-the-art models.",2024-12-25T14:31:00Z,http://arxiv.org/abs/2412.18917v1,"Yi-Chia Chen, Wei-Hua Li, Chu-Song Chen"
"Optimization-based model order reduction of fluid-structure interaction
  problems","We introduce optimization-based full-order and reduced-order formulations of
fluid structure interaction problems. We study the flow of an incompressible
Newtonian fluid which interacts with an elastic body: we consider an arbitrary
Lagrangian Eulerian formulation of the fluid problem and a fully Lagrangian
formulation of the solid problem; we rely on a finite element discretization of
both fluid and solid equations. The distinctive feature of our approach is an
implicit coupling of fluid and structural problems that relies on the solution
to a constrained optimization problem with equality constraints. We discuss the
application of projection-based model reduction to both fluid and solid
subproblems: we rely on Galerkin projection for the solid equations and on
least-square Petrov-Galerkin projection for the fluid equations. Numerical
results for three model problems illustrate the many features of the
formulation.",2024-12-25T14:26:54Z,http://arxiv.org/abs/2412.18916v1,"Tommaso Taddei, Xuejun Xu, Lei Zhang"
"Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With
  Structured Memories","Long-range tasks require reasoning over long inputs. Existing solutions
either need large compute budgets, training data, access to model weights, or
use complex, task-specific approaches. We present PRISM, which alleviates these
concerns by processing information as a stream of chunks, maintaining a
structured in-context memory specified by a typed hierarchy schema. This
approach demonstrates superior performance to baselines on diverse tasks while
using at least 4x smaller contexts than long-context models. Moreover, PRISM is
token-efficient. By producing short outputs and efficiently leveraging
key-value (KV) caches, it achieves up to 54% cost reduction when compared to
alternative short-context approaches. The method also scales down to tiny
information chunks (e.g., 500 tokens) without increasing the number of tokens
encoded or sacrificing quality. Furthermore, we show that it is possible to
generate schemas to generalize our approach to new tasks with minimal effort.",2024-12-25T14:14:31Z,http://arxiv.org/abs/2412.18914v1,"Dulhan Jayalath, James Bradley Wendt, Nicholas Monath, Sandeep Tata, Beliz Gunel"
Quantitative estimates of the singular values of random i.i.d. matrices,"Let $M$ be an $n\times n$ random i.i.d. matrix. This paper studies the
deviation inequality of $s_{n-k+1}(M)$, the $k$-th smallest singular value of
$M$. In particular, when the entries of $M$ are subgaussian, we show that for
any $\gamma\in (0, 1/2), \varepsilon&gt;0$ and $\log n\le k\le c\sqrt{n}$
  \begin{align}
  \textsf{P}\{s_{n-k+1}(M)\le \frac{\varepsilon}{\sqrt{n}} \}\le \Big(
\frac{C\varepsilon}{k}\Big)^{\gamma k^{2}}+e^{-c_{1}kn}.\nonumber
  \end{align}
  This result improves an existing result of Nguyen, which obtained a deviation
inequality of $s_{n-k+1}(M)$ with $(C\varepsilon/k)^{\gamma k^{2}}+e^{-cn}$
decay.",2024-12-25T14:01:39Z,http://arxiv.org/abs/2412.18912v1,"Guozheng Dai, Zhonggen Su, Hanchao Wang"
Accelerating Diffusion Transformers with Dual Feature Caching,"Diffusion Transformers (DiT) have become the dominant methods in image and
video generation yet still suffer substantial computational costs. As an
effective approach for DiT acceleration, feature caching methods are designed
to cache the features of DiT in previous timesteps and reuse them in the next
timesteps, allowing us to skip the computation in the next timesteps. However,
on the one hand, aggressively reusing all the features cached in previous
timesteps leads to a severe drop in generation quality. On the other hand,
conservatively caching only the features in the redundant layers or tokens but
still computing the important ones successfully preserves the generation
quality but results in reductions in acceleration ratios. Observing such a
tradeoff between generation quality and acceleration performance, this paper
begins by quantitatively studying the accumulated error from cached features.
Surprisingly, we find that aggressive caching does not introduce significantly
more caching errors in the caching step, and the conservative feature caching
can fix the error introduced by aggressive caching. Thereby, we propose a dual
caching strategy that adopts aggressive and conservative caching iteratively,
leading to significant acceleration and high generation quality at the same
time. Besides, we further introduce a V-caching strategy for token-wise
conservative caching, which is compatible with flash attention and requires no
training and calibration data.
  Our codes have been released in Github: \textbf{Code:
\href{https://github.com/Shenyi-Z/DuCa}{\texttt{\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",2024-12-25T14:00:14Z,http://arxiv.org/abs/2412.18911v1,"Chang Zou, Evelyn Zhang, Runlin Guo, Haohang Xu, Conghui He, Xuming Hu, Linfeng Zhang"
"AdaEAGLE: Optimizing Speculative Decoding via Explicit Modeling of
  Adaptive Draft Structures","Speculative Decoding (SD) is a popular lossless technique for accelerating
the inference of Large Language Models (LLMs). We show that the decoding speed
of SD frameworks with static draft structures can be significantly improved by
incorporating context-aware adaptive draft structures. However, current studies
on adaptive draft structures are limited by their performance, modeling
approaches, and applicability. In this paper, we introduce AdaEAGLE, the first
SD framework that explicitly models adaptive draft structures. AdaEAGLE
leverages the Lightweight Draft Length Predictor (LDLP) module to explicitly
predict the optimal number of draft tokens during inference to guide the draft
model. It achieves comparable speedup results without manual thresholds and
allows for deeper, more specialized optimizations. Moreover, together with
threshold-based strategies, AdaEAGLE achieves a $1.62\times$ speedup over the
vanilla AR decoding and outperforms fixed-length SotA baseline while
maintaining output quality.",2024-12-25T13:57:33Z,http://arxiv.org/abs/2412.18910v1,"Situo Zhang, Hankun Wang, Da Ma, Zichen Zhu, Lu Chen, Kunyao Lan, Kai Yu"
Relaxation behavior near the first-order phase transition line,"Using the Metropolis algorithm, we simulate the relaxation process of the
three-dimensional kinetic Ising model. Starting from a random initial
configuration, we first present the average equilibration time across the
entire phase boundary. It is observed that the average equilibration time
increases significantly as the temperature decreases from the critical
temperature ($T_{\rm c}$). The average equilibration time along the first-order
phase transition (1st-PT) line exhibits an ultra-slow relaxation. We also
investigate the dynamic scaling behavior with system sizes, and find that
dynamic scaling holds not only at $T_{\rm c}$, but also below $T_{\rm c}$. The
dynamic exponent below $T_{\rm c}$ is larger than that at $T_{\rm c}$.
Additionally, we analyze the dynamic scaling of the average autocorrelation
time and find that it depends on system size only near $T_{\rm c}$, while it
becomes size-independent both above and below $T_{\rm c}$. The extremely slow
relaxation dynamics observed near the 1st-PT is attributed to the complex
structure of the free energy.",2024-12-25T13:56:18Z,http://arxiv.org/abs/2412.18909v1,"Xiaobing Li, Ranran Guo, Mingmei Xu, Yu Zhou, Jinghua Fu, Yuanfang Wu"
"Research Experiment on Multi-Model Comparison for Chinese Text
  Classification Tasks","With the explosive growth of Chinese text data and advancements in natural
language processing technologies, Chinese text classification has become one of
the key techniques in fields such as information retrieval and sentiment
analysis, attracting increasing attention. This paper conducts a comparative
study on three deep learning models:TextCNN, TextRNN, and FastText.specifically
for Chinese text classification tasks. By conducting experiments on the
THUCNews dataset, the performance of these models is evaluated, and their
applicability in different scenarios is discussed.",2024-12-25T13:54:40Z,http://arxiv.org/abs/2412.18908v1,JiaCheng Li
"EC-Diffuser: Multi-Object Manipulation via Entity-Centric Behavior
  Generation","Object manipulation is a common component of everyday tasks, but learning to
manipulate objects from high-dimensional observations presents significant
challenges. These challenges are heightened in multi-object environments due to
the combinatorial complexity of the state space as well as of the desired
behaviors. While recent approaches have utilized large-scale offline data to
train models from pixel observations, achieving performance gains through
scaling, these methods struggle with compositional generalization in unseen
object configurations with constrained network and dataset sizes. To address
these issues, we propose a novel behavioral cloning (BC) approach that
leverages object-centric representations and an entity-centric Transformer with
diffusion-based optimization, enabling efficient learning from offline image
data. Our method first decomposes observations into an object-centric
representation, which is then processed by our entity-centric Transformer that
computes attention at the object level, simultaneously predicting object
dynamics and the agent's actions. Combined with the ability of diffusion models
to capture multi-modal behavior distributions, this results in substantial
performance improvements in multi-object tasks and, more importantly, enables
compositional generalization. We present BC agents capable of zero-shot
generalization to tasks with novel compositions of objects and goals, including
larger numbers of objects than seen during training. We provide video rollouts
on our webpage: https://sites.google.com/view/ec-diffuser.",2024-12-25T13:50:15Z,http://arxiv.org/abs/2412.18907v1,"Carl Qi, Dan Haramati, Tal Daniel, Aviv Tamar, Amy Zhang"
External Bias and Opinion Clustering in Cooperative Networks,"In this work, we consider a group of n agents which interact with each other
in a cooperative framework. A Laplacian-based model is proposed to govern the
evolution of opinions in the group when the agents are subjected to external
biases like agents' traits, news, etc. The objective of the paper is to design
a control input which leads to any desired opinion clustering even in the
presence of external bias factors. Further, we also determine the conditions
which ensure the reachability to any arbitrary opinion states. Note that all of
these results hold for any kind of graph structure. Finally, some numerical
simulations are discussed to validate these results.",2024-12-25T13:36:36Z,http://arxiv.org/abs/2412.18905v1,"Akshay Nagesh Kamthe, Vishnudatta Thota, Aashi Shrinate, Twinkle Tripathy"
"FedCFA: Alleviating Simpson's Paradox in Model Aggregation with
  Counterfactual Federated Learning","Federated learning (FL) is a promising technology for data privacy and
distributed optimization, but it suffers from data imbalance and heterogeneity
among clients. Existing FL methods try to solve the problems by aligning client
with server model or by correcting client model with control variables. These
methods excel on IID and general Non-IID data but perform mediocrely in
Simpson's Paradox scenarios. Simpson's Paradox refers to the phenomenon that
the trend observed on the global dataset disappears or reverses on a subset,
which may lead to the fact that global model obtained through aggregation in FL
does not accurately reflect the distribution of global data. Thus, we propose
FedCFA, a novel FL framework employing counterfactual learning to generate
counterfactual samples by replacing local data critical factors with global
average data, aligning local data distributions with the global and mitigating
Simpson's Paradox effects. In addition, to improve the quality of
counterfactual samples, we introduce factor decorrelation (FDC) loss to reduce
the correlation among features and thus improve the independence of extracted
factors. We conduct extensive experiments on six datasets and verify that our
method outperforms other FL methods in terms of efficiency and global model
accuracy under limited communication rounds.",2024-12-25T13:35:54Z,http://arxiv.org/abs/2412.18904v1,"Zhonghua Jiang, Jimin Xu, Shengyu Zhang, Tao Shen, Jiwei Li, Kun Kuang, Haibin Cai, Fei Wu"
"Stationary Processes, Wiener-Granger Causality, and Matrix Spectral
  Factorization","Granger causality has become an indispensable tool for analyzing causal
relationships between time series. In this paper, we provide a detailed
overview of its mathematical foundations, trace its historical development, and
explore how recent computational advancements can enhance its application in
various fields. We will not hesitate to present the proofs in full if they are
simple and transparent. For more complex theorems on which we rely, we will
provide supporting citations. We also discuss potential future directions for
the method, particularly in the context of largescale data analysis.",2024-12-25T13:29:39Z,http://arxiv.org/abs/2412.18901v1,Lasha Ephremidze
"Observation of on- and off-resonant interaction between a solid-state
  spin qubit and a superconducting resonator","Hybrid systems consisting of multiple materials with distinct physical
properties and tunable interactions provide a promising route for fulfilling
transformative quantum innovations. Solid-state spin qubits and superconducting
circuits stand out as leading candidates in this context due to their
complementary device performance and quantum mechanical properties. Here, we
report experimental integration of a single nitrogen-vacancy (NV) spin qubit
and an on-chip superconducting resonator for realizing multimodal quantum
applications. Specifically, we have observed superconductivity enhanced NV spin
relaxation, which shows a similar Hebel-Slichter peak feature around the phase
transition point. In the coherent interaction regime, we show that the
superconducting resonator mode is capable of exciting NV Rabi oscillations.
Taking advantage of scanning NV magnetometry, we further visualized microscopic
electromagnetic behaviors of the superconducting resonator, revealing the
formation and evolution of superconducting vortices at the nanoscale. Our
results highlight the potential of harnessing NV centers and superconducting
circuits for designing hybrid systems to advance the burgeoning quantum
revolution. The current study will also open a new pathway to test and evaluate
miniaturized superconducting electronics for their future design and
performance improvements.",2024-12-25T13:06:25Z,http://arxiv.org/abs/2412.18896v1,"Senlei Li, Shane P. Kelly, Jingcheng Zhou, Hanyi Lu, Yaroslav Tserkovnyak, Hailong Wang, Chunhui Rita Du"
"Comprehensive Study on Lumbar Disc Segmentation Techniques Using MRI
  Data","Lumbar disk segmentation is essential for diagnosing and curing spinal
disorders by enabling precise detection of disk boundaries in medical imaging.
The advent of deep learning has resulted in the development of many
segmentation methods, offering differing levels of accuracy and effectiveness.
This study assesses the effectiveness of several sophisticated deep learning
architectures, including ResUnext, Ef3 Net, UNet, and TransUNet, for lumbar
disk segmentation, highlighting key metrics like as Pixel Accuracy, Mean
Intersection over Union (Mean IoU), and Dice Coefficient. The findings indicate
that ResUnext achieved the highest segmentation accuracy, with a Pixel Accuracy
of 0.9492 and a Dice Coefficient of 0.8425, with TransUNet following closely
after. Filtering techniques somewhat enhanced the performance of most models,
particularly Dense UNet, improving stability and segmentation quality. The
findings underscore the efficacy of these models in lumbar disk segmentation
and highlight potential areas for improvement.",2024-12-25T12:54:52Z,http://arxiv.org/abs/2412.18894v1,"Serkan Salturk, Irem Sayin, Ibrahim Cem Balci, Taha Emre Pamukcu, Zafer Soydan, Huseyin Uvet"
"CoEvo: Continual Evolution of Symbolic Solutions Using Large Language
  Models","Large Language Models (LLMs) have emerged as transformative tools in
artificial intelligence, capable of processing and understanding extensive
human knowledge to enhance problem-solving across various domains. This paper
explores the potential of LLMs to drive the discovery of symbolic solutions
within scientific and engineering disciplines, where such solutions are crucial
for advancing theoretical and practical applications. We propose a novel
framework that utilizes LLMs in an evolutionary search methodology, augmented
by a dynamic knowledge library that integrates and refines insights in an
\textit{open-ended manner}. This approach aims to tackle the dual challenges of
efficiently navigating complex symbolic representation spaces and leveraging
both existing and newly generated knowledge to foster open-ended innovation. By
enabling LLMs to interact with and expand upon a knowledge library, we
facilitate the continuous generation of novel solutions in diverse forms such
as language, code, and mathematical expressions. Our experimental results
demonstrate that this method not only enhances the efficiency of searching for
symbolic solutions but also supports the ongoing discovery process, akin to
human scientific endeavors. This study represents a first effort in
conceptualizing the search for symbolic solutions as a lifelong, iterative
process, marking a significant step towards harnessing AI in the perpetual
pursuit of scientific and engineering breakthroughs. We have open-sourced our
code and data, please visit \url{https://github.com/pgg3/CoEvo} for more
information.",2024-12-25T12:27:27Z,http://arxiv.org/abs/2412.18890v1,"Ping Guo, Qingfu Zhang, Xi Lin"
"Adversarial Training for Graph Neural Networks via Graph Subspace Energy
  Optimization","Despite impressive capability in learning over graph-structured data, graph
neural networks (GNN) suffer from adversarial topology perturbation in both
training and inference phases. While adversarial training has demonstrated
remarkable effectiveness in image classification tasks, its suitability for GNN
models has been doubted until a recent advance that shifts the focus from
transductive to inductive learning. Still, GNN robustness in the inductive
setting is under-explored, and it calls for deeper understanding of GNN
adversarial training. To this end, we propose a new concept of graph subspace
energy (GSE) -- a generalization of graph energy that measures graph stability
-- of the adjacency matrix, as an indicator of GNN robustness against topology
perturbations. To further demonstrate the effectiveness of such concept, we
propose an adversarial training method with the perturbed graphs generated by
maximizing the GSE regularization term, referred to as AT-GSE. To deal with the
local and global topology perturbations raised respectively by LRBCD and PRBCD,
we employ randomized SVD (RndSVD) and Nystrom low-rank approximation to favor
the different aspects of the GSE terms. An extensive set of experiments shows
that AT-GSE outperforms consistently the state-of-the-art GNN adversarial
training methods over different homophily and heterophily datasets in terms of
adversarial accuracy, whilst more surprisingly achieving a superior clean
accuracy on non-perturbed graphs.",2024-12-25T12:04:18Z,http://arxiv.org/abs/2412.18886v1,"Ganlin Liu, Ziling Liang, Xiaowei Huang, Xinping Yi, Shi Jin"
"HV-BEV: Decoupling Horizontal and Vertical Feature Sampling for
  Multi-View 3D Object Detection","The application of vision-based multi-view environmental perception system
has been increasingly recognized in autonomous driving technology, especially
the BEV-based models. Current state-of-the-art solutions primarily encode image
features from each camera view into the BEV space through explicit or implicit
depth prediction. However, these methods often focus on improving the accuracy
of projecting 2D features into corresponding depth regions, while overlooking
the highly structured information of real-world objects and the varying height
distributions of objects across different scenes. In this work, we propose
HV-BEV, a novel approach that decouples feature sampling in the BEV grid
queries paradigm into horizontal feature aggregation and vertical adaptive
height-aware reference point sampling, aiming to improve both the aggregation
of objects' complete information and generalization to diverse road
environments. Specifically, we construct a learnable graph structure in the
horizontal plane aligned with the ground for 3D reference points, reinforcing
the association of the same instance across different BEV grids, especially
when the instance spans multiple image views around the vehicle. Additionally,
instead of relying on uniform sampling within a fixed height range, we
introduce a height-aware module that incorporates historical information,
enabling the reference points to adaptively focus on the varying heights at
which objects appear in different scenes. Extensive experiments validate the
effectiveness of our proposed method, demonstrating its superior performance
over the baseline across the nuScenes dataset. Moreover, our best-performing
model achieves a remarkable 50.5% mAP and 59.8% NDS on the nuScenes testing
set.",2024-12-25T11:49:14Z,http://arxiv.org/abs/2412.18884v1,"Di Wu, Feng Yang, Benlian Xu, Pan Liao, Wenhui Zhao, Dingwen Zhang"
Cross-PCR: A Robust Cross-Source Point Cloud Registration Framework,"Due to the density inconsistency and distribution difference between
cross-source point clouds, previous methods fail in cross-source point cloud
registration. We propose a density-robust feature extraction and matching
scheme to achieve robust and accurate cross-source registration. To address the
density inconsistency between cross-source data, we introduce a density-robust
encoder for extracting density-robust features. To tackle the issue of
challenging feature matching and few correct correspondences, we adopt a
loose-to-strict matching pipeline with a ``loose generation, strict selection''
idea. Under it, we employ a one-to-many strategy to loosely generate initial
correspondences. Subsequently, high-quality correspondences are strictly
selected to achieve robust registration through sparse matching and dense
matching. On the challenging Kinect-LiDAR scene in the cross-source 3DCSR
dataset, our method improves feature matching recall by 63.5 percentage points
(pp) and registration recall by 57.6 pp. It also achieves the best performance
on 3DMatch, while maintaining robustness under diverse downsampling densities.",2024-12-25T11:14:59Z,http://arxiv.org/abs/2412.18873v1,"Guiyu Zhao, Zhentao Guo, Zewen Du, Hongbin Ma"
Proton Flux Measurement from Neutron Monitor Data Using Neural Networks,"Accurate measurements of cosmic ray proton flux are crucial for studying the
modulation processes of cosmic rays during the solar activity cycle. We present
a proton flux measurement method based on ground-based neutron monitor (NM)
data and machine learning techniques. After preprocessing the NM data, we use a
convolutional neural network (CNN) model to simulate the relationship between
the NM observations and proton flux measured by the Alpha Magnetic Spectrometer
(AMS-02). We obtain daily proton flux data ranging from 1GV to 100GV for the
period from 2011 to 2024, showing that the measured values are in good
agreement with the observed ones. In particular, we provide daily proton flux
measurements for periods when AMS-02 data are unavailable due to operational
reasons. We also perform wavelet analyses on the continuous proton flux data to
investigate the relationship between proton flux and solar activity variations,
particularly during late 2014 when AMS-02 measurements were missing.",2024-12-25T11:14:13Z,http://arxiv.org/abs/2412.18872v1,"Pengwei Zhao, Jie Feng"
"TSceneJAL: Joint Active Learning of Traffic Scenes for 3D Object
  Detection","Most autonomous driving (AD) datasets incur substantial costs for collection
and labeling, inevitably yielding a plethora of low-quality and redundant data
instances, thereby compromising performance and efficiency. Many applications
in AD systems necessitate high-quality training datasets using both existing
datasets and newly collected data. In this paper, we propose a traffic scene
joint active learning (TSceneJAL) framework that can efficiently sample the
balanced, diverse, and complex traffic scenes from both labeled and unlabeled
data. The novelty of this framework is threefold: 1) a scene sampling scheme
based on a category entropy, to identify scenes containing multiple object
classes, thus mitigating class imbalance for the active learner; 2) a
similarity sampling scheme, estimated through the directed graph representation
and a marginalize kernel algorithm, to pick sparse and diverse scenes; 3) an
uncertainty sampling scheme, predicted by a mixture density network, to select
instances with the most unclear or complex regression outcomes for the learner.
Finally, the integration of these three schemes in a joint selection strategy
yields an optimal and valuable subdataset. Experiments on the KITTI, Lyft,
nuScenes and SUScape datasets demonstrate that our approach outperforms
existing state-of-the-art methods on 3D object detection tasks with up to 12%
improvements.",2024-12-25T11:07:04Z,http://arxiv.org/abs/2412.18870v1,"Chenyang Lei, Meiying Zhang, Weiyuan Peng, Qi Hao, Chengzhong Xu, Chunlin Ji, Guang Zhou"
"Overview of MWE history, challenges, and horizons: standing at the 20th
  anniversary of the MWE workshop series via MWE-UD2024","Starting in 2003 when the first MWE workshop was held with ACL in Sapporo,
Japan, this year, the joint workshop of MWE-UD co-located with the LREC-COLING
2024 conference marked the 20th anniversary of MWE workshop events over the
past nearly two decades. Standing at this milestone, we look back to this
workshop series and summarise the research topics and methodologies researchers
have carried out over the years. We also discuss the current challenges that we
are facing and the broader impacts/synergies of MWE research within the CL and
NLP fields. Finally, we give future research perspectives. We hope this
position paper can help researchers, students, and industrial practitioners
interested in MWE get a brief but easy understanding of its history, current,
and possible future.",2024-12-25T11:00:27Z,http://arxiv.org/abs/2412.18868v1,"Lifeng Han, Kilian Evang, Archna Bhatia, Gosse Bouma, A. Seza Doğruöz, Marcos Garcia, Voula Giouli, Joakim Nivre, Alexandre Rademacher"
"Autonomous Navigation of 4WIS4WID Agricultural Field Mobile Robot using
  Deep Reinforcement Learning","In the futuristic agricultural fields compatible with Agriculture 4.0, robots
are envisaged to navigate through crops to perform functions like pesticide
spraying and fruit harvesting, which are complex tasks due to factors such as
non-geometric internal obstacles, space constraints, and outdoor conditions. In
this paper, we attempt to employ Deep Reinforcement Learning (DRL) to solve the
problem of 4WIS4WID mobile robot navigation in a structured, automated
agricultural field. This paper consists of three sections: parameterization of
four-wheel steering configurations, crop row tracking using DRL, and autonomous
navigation of 4WIS4WID mobile robot using DRL through multiple crop rows. We
show how to parametrize various configurations of four-wheel steering to two
variables. This includes symmetric four-wheel steering, zero-turn, and an
additional steering configuration that allows the 4WIS4WID mobile robot to move
laterally. Using DRL, we also followed an irregularly shaped crop row with
symmetric four-wheel steering. In the multiple crop row simulation environment,
with the help of waypoints, we effectively performed point-to-point navigation.
Finally, a comparative analysis of various DRL algorithms that use continuous
actions was carried out.",2024-12-25T10:33:33Z,http://arxiv.org/abs/2412.18865v1,"Tom Baby, Mahendra Kumar Gohil, Bishakh Bhattacharya"
"Whose Morality Do They Speak? Unraveling Cultural Bias in Multilingual
  Language Models","Large language models (LLMs) have become integral tools in diverse domains,
yet their moral reasoning capabilities across cultural and linguistic contexts
remain underexplored. This study investigates whether multilingual LLMs, such
as GPT-3.5-Turbo, GPT-4o-mini, Llama 3.1, and MistralNeMo, reflect culturally
specific moral values or impose dominant moral norms, particularly those rooted
in English. Using the updated Moral Foundations Questionnaire (MFQ-2) in eight
languages, Arabic, Farsi, English, Spanish, Japanese, Chinese, French, and
Russian, the study analyzes the models' adherence to six core moral
foundations: care, equality, proportionality, loyalty, authority, and purity.
The results reveal significant cultural and linguistic variability, challenging
the assumption of universal moral consistency in LLMs. Although some models
demonstrate adaptability to diverse contexts, others exhibit biases influenced
by the composition of the training data. These findings underscore the need for
culturally inclusive model development to improve fairness and trust in
multilingual AI systems.",2024-12-25T10:17:15Z,http://arxiv.org/abs/2412.18863v1,Meltem Aksoy
Bootstrap Your Own Context Length,"We introduce a bootstrapping approach to train long-context language models
by exploiting their short-context capabilities only. Our method utilizes a
simple agent workflow to synthesize diverse long-context instruction tuning
data, thereby eliminating the necessity for manual data collection and
annotation. The proposed data synthesis workflow requires only a short-context
language model, a text retriever, and a document collection, all of which are
readily accessible within the open-source ecosystem. Subsequently, language
models are fine-tuned using the synthesized data to extend their context
lengths. In this manner, we effectively transfer the short-context capabilities
of language models to long-context scenarios through a bootstrapping process.
We conduct experiments with the open-source Llama-3 family of models and
demonstrate that our method can successfully extend the context length to up to
1M tokens, achieving superior performance across various benchmarks.",2024-12-25T10:08:54Z,http://arxiv.org/abs/2412.18860v1,"Liang Wang, Nan Yang, Xingxing Zhang, Xiaolong Huang, Furu Wei"
"Few-shot Metric Domain Adaptation: Practical Learning Strategies for an
  Automated Plant Disease Diagnosis","Numerous studies have explored image-based automated systems for plant
disease diagnosis, demonstrating impressive diagnostic capabilities. However,
recent large-scale analyses have revealed a critical limitation: that the
diagnostic capability suffers significantly when validated on images captured
in environments (domains) differing from those used during training. This
shortfall stems from the inherently limited dataset size and the diverse
manifestation of disease symptoms, combined with substantial variations in
cultivation environments and imaging conditions, such as equipment and
composition. These factors lead to insufficient variety in training data,
ultimately constraining the system's robustness and generalization. To address
these challenges, we propose Few-shot Metric Domain Adaptation (FMDA), a
flexible and effective approach for enhancing diagnostic accuracy in practical
systems, even when only limited target data is available. FMDA reduces domain
discrepancies by introducing a constraint to the diagnostic model that
minimizes the ""distance"" between feature spaces of source (training) data and
target data with limited samples. FMDA is computationally efficient, requiring
only basic feature distance calculations and backpropagation, and can be
seamlessly integrated into any machine learning (ML) pipeline. In large-scale
experiments, involving 223,015 leaf images across 20 fields and 3 crop species,
FMDA achieved F1 score improvements of 11.1 to 29.3 points compared to cases
without target data, using only 10 images per disease from the target domain.
Moreover, FMDA consistently outperformed fine-tuning methods utilizing the same
data, with an average improvement of 8.5 points.",2024-12-25T10:01:30Z,http://arxiv.org/abs/2412.18859v1,"Shoma Kudo, Satoshi Kagiwada, Hitoshi Iyatomi"
Computing Approximate Graph Edit Distance via Optimal Transport,"Given a graph pair $(G^1, G^2)$, graph edit distance (GED) is defined as the
minimum number of edit operations converting $G^1$ to $G^2$. GED is a
fundamental operation widely used in many applications, but its exact
computation is NP-hard, so the approximation of GED has gained a lot of
attention. Data-driven learning-based methods have been found to provide
superior results compared to classical approximate algorithms, but they
directly fit the coupling relationship between a pair of vertices from their
vertex features. We argue that while pairwise vertex features can capture the
coupling cost (discrepancy) of a pair of vertices, the vertex coupling matrix
should be derived from the vertex-pair cost matrix through a more
well-established method that is aware of the global context of the graph pair,
such as optimal transport. In this paper, we propose an ensemble approach that
integrates a supervised learning-based method and an unsupervised method, both
based on optimal transport. Our learning method, GEDIOT, is based on inverse
optimal transport that leverages a learnable Sinkhorn algorithm to generate the
coupling matrix. Our unsupervised method, GEDGW, models GED computation as a
linear combination of optimal transport and its variant, Gromov-Wasserstein
discrepancy, for node and edge operations, respectively, which can be solved
efficiently without needing the ground truth. Our ensemble method, GEDHOT,
combines GEDIOT and GEDGW to further boost the performance. Extensive
experiments demonstrate that our methods significantly outperform the existing
methods in terms of the performance of GED computation, edit path generation,
and model generalizability.",2024-12-25T09:55:14Z,http://arxiv.org/abs/2412.18857v1,"Qihao Cheng, Da Yan, Tianhao Wu, Zhongyi Huang, Qin Zhang"
"Digital Twin Enhanced Deep Reinforcement Learning for Intelligent
  Omni-Surface Configurations in MU-MIMO Systems","Intelligent omni-surface (IOS) is a promising technique to enhance the
capacity of wireless networks, by reflecting and refracting the incident signal
simultaneously. Traditional IOS configuration schemes, relying on all
sub-channels' channel state information and user equipments' mobility, are
difficult to implement in complex realistic systems. Existing works attempt to
address this issue employing deep reinforcement learning (DRL), but this method
requires a lot of trial-and-error interactions with the external environment
for efficient results and thus cannot satisfy the real-time decision-making. To
enable model-free and real-time IOS control, this paper puts forth a new
framework that integrates DRL and digital twins. DeepIOS, a DRL based IOS
configuration scheme with the goal of maximizing the sum data rate, is first
developed to jointly optimize the phase-shift and amplitude of IOS in
multi-user multiple-input-multiple-output systems. Thereafter, to further
reduce the computational complexity, DeepIOS introduces an action branch
architecture, which separately decides two optimization variables in parallel.
Finally, a digital twin module is constructed through supervised learning as a
pre-verification platform for DeepIOS, such that the decision-making's
real-time can be guaranteed. The formulated framework is a closed-loop system,
in which the physical space provides data to establish and calibrate the
digital space, while the digital space generates experience samples for DeepIOS
training and sends the trained parameters to the IOS controller for
configurations. Numerical results show that compared with random and MAB
schemes, the proposed framework attains a higher data rate and is more robust
to different settings. Furthermore, the action branch architecture reduces
DeepIOS's computational complexity, and the digital twin module improves the
convergence speed and run-time.",2024-12-25T09:53:07Z,http://arxiv.org/abs/2412.18856v1,"Xiaowen Ye, Xianghao Yu, Liqun Fu"
Machine Learning-Based Detection of Pump-and-Dump Schemes in Real-Time,"Cryptocurrency markets often face manipulation through prevalent
pump-and-dump (P&amp;D) schemes, where self-organized Telegram groups, some
exceeding two million members, artificially inflate target cryptocurrency
prices. These groups sell premium access to inside information, worsening
information asymmetry and financial risks for subscribers and all investors.
This paper presents a real-time prediction pipeline to forecast target coins
and alert investors to possible P&amp;D schemes. In a Poloniex case study, the
model accurately identified the target coin among the top five from 50 random
coins in 24 out of 43 (55.81%) P&amp;D events. The pipeline uses advanced natural
language processing (NLP) to classify Telegram messages, identifying 2,079 past
pump events and detecting new ones in real-time. Our analysis also evaluates
the susceptibility of token standards - ERC-20, ERC-721, BRC-20, Inscriptions,
and Runes - to manipulation and identifies exchanges commonly involved in P&amp;D
schemes.",2024-12-25T09:23:36Z,http://arxiv.org/abs/2412.18848v1,"Manuel Bolz, Kevin Bründler, Liam Kane, Panagiotis Patsias, Liam Tessendorf, Krzysztof Gogol, Taehoon Kim, Claudio Tessone"
"TPCH: Tensor-interacted Projection and Cooperative Hashing for
  Multi-view Clustering","In recent years, anchor and hash-based multi-view clustering methods have
gained attention for their efficiency and simplicity in handling large-scale
data. However, existing methods often overlook the interactions among
multi-view data and higher-order cooperative relationships during projection,
negatively impacting the quality of hash representation in low-dimensional
spaces, clustering performance, and sensitivity to noise. To address this
issue, we propose a novel approach named Tensor-Interacted Projection and
Cooperative Hashing for Multi-View Clustering(TPCH). TPCH stacks multiple
projection matrices into a tensor, taking into account the synergies and
communications during the projection process. By capturing higher-order
multi-view information through dual projection and Hamming space, TPCH employs
an enhanced tensor nuclear norm to learn more compact and distinguishable hash
representations, promoting communication within and between views. Experimental
results demonstrate that this refined method significantly outperforms
state-of-the-art methods in clustering on five large-scale multi-view datasets.
Moreover, in terms of CPU time, TPCH achieves substantial acceleration compared
to the most advanced current methods. The code is available at
\textcolor{red}{\url{https://github.com/jankin-wang/TPCH}}.",2024-12-25T09:22:11Z,http://arxiv.org/abs/2412.18847v1,"Zhongwen Wang, Xingfeng Li, Yinghui Sun, Quansen Sun, Yuan Sun, Han Ling, Jian Dai, Zhenwen Ren"
"Enhancing Federated Graph Learning via Adaptive Fusion of Structural and
  Node Characteristics","Federated Graph Learning (FGL) has demonstrated the advantage of training a
global Graph Neural Network (GNN) model across distributed clients using their
local graph data. Unlike Euclidean data (\eg, images), graph data is composed
of nodes and edges, where the overall node-edge connections determine the
topological structure, and individual nodes along with their neighbors capture
local node features. However, existing studies tend to prioritize one aspect
over the other, leading to an incomplete understanding of the data and the
potential misidentification of key characteristics across varying graph
scenarios. Additionally, the non-independent and identically distributed
(non-IID) nature of graph data makes the extraction of these two data
characteristics even more challenging. To address the above issues, we propose
a novel FGL framework, named FedGCF, which aims to simultaneously extract and
fuse structural properties and node features to effectively handle diverse
graph scenarios. FedGCF first clusters clients by structural similarity,
performing model aggregation within each cluster to form the shared structural
model. Next, FedGCF selects the clients with common node features and
aggregates their models to generate a common node model. This model is then
propagated to all clients, allowing common node features to be shared. By
combining these two models with a proper ratio, FedGCF can achieve a
comprehensive understanding of the graph data and deliver better performance,
even under non-IID distributions. Experimental results show that FedGCF
improves accuracy by 4.94%-7.24% under different data distributions and reduces
communication cost by 64.18%-81.25% to reach the same accuracy compared to
baselines.",2024-12-25T09:20:06Z,http://arxiv.org/abs/2412.18845v1,"Xianjun Gao, Jianchun Liu, Hongli Xu, Shilong Wang, Liusheng Huang"
"Improving Integrated Gradient-based Transferable Adversarial Examples by
  Refining the Integration Path","Transferable adversarial examples are known to cause threats in practical,
black-box attack scenarios. A notable approach to improving transferability is
using integrated gradients (IG), originally developed for model
interpretability. In this paper, we find that existing IG-based attacks have
limited transferability due to their naive adoption of IG in model
interpretability. To address this limitation, we focus on the IG integration
path and refine it in three aspects: multiplicity, monotonicity, and diversity,
supported by theoretical analyses. We propose the Multiple Monotonic
Diversified Integrated Gradients (MuMoDIG) attack, which can generate highly
transferable adversarial examples on different CNN and ViT models and defenses.
Experiments validate that MuMoDIG outperforms the latest IG-based attack by up
to 37.3\% and other state-of-the-art attacks by 8.4\%. In general, our study
reveals that migrating established techniques to improve transferability may
require non-trivial efforts. Code is available at
\url{https://github.com/RYC-98/MuMoDIG}.",2024-12-25T09:15:39Z,http://arxiv.org/abs/2412.18844v1,"Yuchen Ren, Zhengyu Zhao, Chenhao Lin, Bo Yang, Lu Zhou, Zhe Liu, Chao Shen"
"Improving the Readability of Automatically Generated Tests using Large
  Language Models","Search-based test generators are effective at producing unit tests with high
coverage. However, such automatically generated tests have no meaningful test
and variable names, making them hard to understand and interpret by developers.
On the other hand, large language models (LLMs) can generate highly readable
test cases, but they are not able to match the effectiveness of search-based
generators, in terms of achieved code coverage.
  In this paper, we propose to combine the effectiveness of search-based
generators with the readability of LLM generated tests. Our approach focuses on
improving test and variable names produced by search-based tools, while keeping
their semantics (i.e., their coverage) unchanged.
  Our evaluation on nine industrial and open source LLMs show that our
readability improvement transformations are overall semantically-preserving and
stable across multiple repetitions. Moreover, a human study with ten
professional developers, show that our LLM-improved tests are as readable as
developer-written tests, regardless of the LLM employed.",2024-12-25T09:08:53Z,http://arxiv.org/abs/2412.18843v1,"Matteo Biagiola, Gianluca Ghislotti, Paolo Tonella"
"Context-Based Semantic-Aware Alignment for Semi-Supervised Multi-Label
  Learning","Due to the lack of extensive precisely-annotated multi-label data in real
word, semi-supervised multi-label learning (SSMLL) has gradually gained
attention. Abundant knowledge embedded in vision-language models (VLMs)
pre-trained on large-scale image-text pairs could alleviate the challenge of
limited labeled data under SSMLL setting.Despite existing methods based on
fine-tuning VLMs have achieved advances in weakly-supervised multi-label
learning, they failed to fully leverage the information from labeled data to
enhance the learning of unlabeled data. In this paper, we propose a
context-based semantic-aware alignment method to solve the SSMLL problem by
leveraging the knowledge of VLMs. To address the challenge of handling multiple
semantics within an image, we introduce a novel framework design to extract
label-specific image features. This design allows us to achieve a more compact
alignment between text features and label-specific image features, leading the
model to generate high-quality pseudo-labels. To incorporate the model with
comprehensive understanding of image, we design a semi-supervised context
identification auxiliary task to enhance the feature representation by
capturing co-occurrence information. Extensive experiments on multiple
benchmark datasets demonstrate the effectiveness of our proposed method.",2024-12-25T09:06:54Z,http://arxiv.org/abs/2412.18842v1,"Heng-Bo Fan, Ming-Kun Xie, Jia-Hao Xiao, Sheng-Jun Huang"
"Implicit factorized transformer approach to fast prediction of turbulent
  channel flows","Transformer neural operators have recently become an effective approach for
surrogate modeling of nonlinear systems governed by partial differential
equations (PDEs). In this paper, we introduce a modified implicit factorized
transformer (IFactFormer-m) model which replaces the original chained
factorized attention with parallel factorized attention. The IFactFormer-m
model successfully performs long-term predictions for turbulent channel flow,
whereas the original IFactFormer (IFactFormer-o), Fourier neural operator
(FNO), and implicit Fourier neural operator (IFNO) exhibit a poor performance.
Turbulent channel flows are simulated by direct numerical simulation using fine
grids at friction Reynolds numbers $\text{Re}_{\tau}\approx 180,395,590$, and
filtered to coarse grids for training neural operator. The neural operator
takes the current flow field as input and predicts the flow field at the next
time step, and long-term prediction is achieved in the posterior through an
autoregressive approach. The prediction results show that IFactFormer-m,
compared to other neural operators and the traditional large eddy simulation
(LES) methods including dynamic Smagorinsky model (DSM) and the wall-adapted
local eddy-viscosity (WALE) model, reduces prediction errors in the short term,
and achieves stable and accurate long-term prediction of various statistical
properties and flow structures, including the energy spectrum, mean streamwise
velocity, root mean square (rms) values of fluctuating velocities, Reynolds
shear stress, and spatial structures of instantaneous velocity. Moreover, the
trained IFactFormer-m is much faster than traditional LES methods.",2024-12-25T09:05:14Z,http://arxiv.org/abs/2412.18840v1,"Huiyu Yang, Yunpeng Wang, Jianchun Wang"
"Advancing NAM-to-Speech Conversion with Novel Methods and the MultiNAM
  Dataset","Current Non-Audible Murmur (NAM)-to-speech techniques rely on voice cloning
to simulate ground-truth speech from paired whispers. However, the simulated
speech often lacks intelligibility and fails to generalize well across
different speakers. To address this issue, we focus on learning phoneme-level
alignments from paired whispers and text and employ a Text-to-Speech (TTS)
system to simulate the ground-truth. To reduce dependence on whispers, we learn
phoneme alignments directly from NAMs, though the quality is constrained by the
available training data. To further mitigate reliance on NAM/whisper data for
ground-truth simulation, we propose incorporating the lip modality to infer
speech and introduce a novel diffusion-based method that leverages recent
advancements in lip-to-speech technology. Additionally, we release the MultiNAM
dataset with over $7.96$ hours of paired NAM, whisper, video, and text data
from two speakers and benchmark all methods on this dataset. Speech samples and
the dataset are available at \url{https://diff-nam.github.io/DiffNAM/}",2024-12-25T08:57:24Z,http://arxiv.org/abs/2412.18839v1,"Neil Shah, Shirish Karande, Vineet Gandhi"
DiFiC: Your Diffusion Model Holds the Secret to Fine-Grained Clustering,"Fine-grained clustering is a practical yet challenging task, whose essence
lies in capturing the subtle differences between instances of different
classes. Such subtle differences can be easily disrupted by data augmentation
or be overwhelmed by redundant information in data, leading to significant
performance degradation for existing clustering methods. In this work, we
introduce DiFiC a fine-grained clustering method building upon the conditional
diffusion model. Distinct from existing works that focus on extracting
discriminative features from images, DiFiC resorts to deducing the textual
conditions used for image generation. To distill more precise and
clustering-favorable object semantics, DiFiC further regularizes the diffusion
target and guides the distillation process utilizing neighborhood similarity.
Extensive experiments demonstrate that DiFiC outperforms both state-of-the-art
discriminative and generative clustering methods on four fine-grained image
clustering benchmarks. We hope the success of DiFiC will inspire future
research to unlock the potential of diffusion models in tasks beyond
generation. The code will be released.",2024-12-25T08:55:48Z,http://arxiv.org/abs/2412.18838v1,"Ruohong Yang, Peng Hu, Xi Peng, Xiting Liu, Yunfan Li"
"Experimental secure entanglement-free quantum remote sensing over 50 km
  of optical fiber","Secure quantum remote sensing (SQRS) uses quantum states to gather
information about distant objects or environments while ensuring secure data
transmission against eavesdropping. It has potential applications in various
fields, including environmental monitoring, military surveillance, and disaster
response, where both data accuracy and transmission security are critical.
Recent experiments have demonstrated the feasibility of SQRS using entanglement
states. Here, we experimentally demonstrate an SQRS that can estimate a phase
without requiring entanglement, offering the practical advantage that
single-qubit states are easier to prepare. We successfully estimate the preset
phase information at a remote site over a fiber distance of 50 km, which serves
as a key step toward long-distance applications.",2024-12-25T08:52:06Z,http://arxiv.org/abs/2412.18837v1,"Wenjie He, Chunfeng Huang, Rui Guan, Ye Chen, Zhenrong Zhang, Kejin Wei"
"LoGFiLM: Fine-Tuning A Large Language Model for Automated Generation of
  Log Statements","Log statements have become an integral part of modern software systems. Prior
research efforts have focused on supporting the decisions of placing log
statements, such as where/what to log, while automated generation or completion
of log statements has received little attention. With the increasing use of
Large Language Models (LLMs) for code-related tasks such as code completion or
generation, automated methods for generating or completing log statements have
gained much momentum. Fine-tuning open-source LLMs like the Llama series is
often preferred by enterprises over using commercial ones like the GPT series
due to considerations including privacy, security, openness, performance, etc.
Fine-tuning LLMs requires task-specific training data and custom-designed
processing algorithms, which, however, have not been thoroughly explored for
the log statement generation task. This paper fills this gap by contributing
such a fine-tuning method LoGFiLM and an exemplar model by using the proposed
method to fine-tune Llama-3-8B. Experiments with our own curated dataset and a
public dataset show that LoGFiLM consistently outperforms the original
Llama-3-8B and the commercial LLMs of GPT-3.5 and GPT-4. The results further
reveal that fine-tuning Llama-3-8B with data encompassing broader contextual
ranges surrounding log statements yields a better model for the automated
generation of log statements.",2024-12-25T08:43:00Z,http://arxiv.org/abs/2412.18835v1,"Hao Zhang, Dongjun Yu, Lei Zhang, Guoping Rong, Yongda Yu, Haifeng Shen, He Zhang, Dong Shao, Hongyu Kuang"
"Federated Learning with Partially Labeled Data: A Conditional
  Distillation Approach","In medical imaging, developing generalized segmentation models that can
handle multiple organs and lesions is crucial. However, the scarcity of fully
annotated datasets and strict privacy regulations present significant barriers
to data sharing. Federated Learning (FL) allows decentralized model training,
but existing FL methods often struggle with partial labeling, leading to model
divergence and catastrophic forgetting. We propose ConDistFL, a novel FL
framework incorporating conditional distillation to address these challenges.
ConDistFL enables effective learning from partially labeled datasets,
significantly improving segmentation accuracy across distributed and
non-uniform datasets. In addition to its superior segmentation performance,
ConDistFL maintains computational and communication efficiency, ensuring its
scalability for real-world applications. Furthermore, ConDistFL demonstrates
remarkable generalizability, significantly outperforming existing FL methods in
out-of-federation tests, even adapting to unseen contrast phases (e.g.,
non-contrast CT images) in our experiments. Extensive evaluations on 3D CT and
2D chest X-ray datasets show that ConDistFL is an efficient, adaptable solution
for collaborative medical image segmentation in privacy-constrained settings.",2024-12-25T08:40:03Z,http://arxiv.org/abs/2412.18833v1,"Pochuan Wang, Chen Shen, Masahiro Oda, Chiou-Shann Fuh, Kensaku Mori, Weichung Wang, Holger R. Roth"
"Structured Speaker-Deficiency Adaptation of Foundation Models for
  Dysarthric and Elderly Speech Recognition","Data-intensive fine-tuning of speech foundation models (SFMs) to scarce and
diverse dysarthric and elderly speech leads to data bias and poor
generalization to unseen speakers. This paper proposes novel structured
speaker-deficiency adaptation approaches for SSL pre-trained SFMs on such data.
Speaker and speech deficiency invariant SFMs were constructed in their
supervised adaptive fine-tuning stage to reduce undue bias to training data
speakers, and serves as a more neutral and robust starting point for test time
unsupervised adaptation. Speech variability attributed to speaker identity and
speech impairment severity, or aging induced neurocognitive decline, are
modelled using separate adapters that can be combined together to model any
seen or unseen speaker. Experiments on the UASpeech dysarthric and DementiaBank
Pitt elderly speech corpora suggest structured speaker-deficiency adaptation of
HuBERT and Wav2vec2-conformer models consistently outperforms baseline SFMs
using either: a) no adapters; b) global adapters shared among all speakers; or
c) single attribute adapters modelling speaker or deficiency labels alone by
statistically significant WER reductions up to 3.01% and 1.50% absolute (10.86%
and 6.94% relative) on the two tasks respectively. The lowest published WER of
19.45% (49.34% on very low intelligibility, 33.17% on unseen words) is obtained
on the UASpeech test set of 16 dysarthric speakers.",2024-12-25T08:39:02Z,http://arxiv.org/abs/2412.18832v1,"Shujie Hu, Xurong Xie, Mengzhe Geng, Jiajun Deng, Zengrui Jin, Tianzi Wang, Mingyu Cui, Guinan Li, Zhaoqing Li, Helen Meng, Xunying Liu"
"Data-driven $H_{\infty}$ predictive control for constrained systems: a
  Lagrange duality approcah","This article proposes a data-driven $H_{\infty}$ control scheme for
time-domain constrained systems based on model predictive control formulation.
The scheme combines $H_{\infty}$ control and minimax model predictive control,
enabling more effective handling of external disturbances and time-domain
constraints. First, by leveraging input-output-disturbance data, the scheme
ensures $H_{\infty}$ performance of the closed-loop system. Then, a minimax
optimization problem is converted to a more manageable minimization problem
employing Lagrange duality, which reduces conservatism typically associated
with ellipsoidal evaluations of time-domain constraints. The study examines key
closed-loop properties, including stability, disturbance attenuation, and
constraint satisfaction, achieved by the proposed data-driven moving horizon
predictive control algorithm. The effectiveness and advantages of the proposed
method are demonstrated through numerical simulations involving a batch reactor
system, confirming its robustness and feasibility under noisy conditions.",2024-12-25T08:38:32Z,http://arxiv.org/abs/2412.18831v1,"Wenhuang Wu, Lulu Guo, Nan Li, Hong Chen"
"Generalized Grade-of-Membership Estimation for High-dimensional Locally
  Dependent Data","This work focuses on the mixed membership models for multivariate categorical
data widely used for analyzing survey responses and population genetics data.
These grade of membership (GoM) models offer rich modeling power but present
significant estimation challenges for high-dimensional polytomous data. Popular
existing approaches, such as Bayesian MCMC inference, are not scalable and lack
theoretical guarantees in high-dimensional settings. To address this, we first
observe that data from this model can be reformulated as a three-way
(quasi-)tensor, with many subjects responding to many items with varying
numbers of categories. We introduce a novel and simple approach that flattens
the three-way quasi-tensor into a ""fat"" matrix, and then perform a singular
value decomposition of it to estimate parameters by exploiting the singular
subspace geometry. Our fast spectral method can accommodate a broad range of
data distributions with arbitrarily locally dependent noise, which we formalize
as the generalized-GoM models. We establish finite-sample entrywise error
bounds for the generalized-GoM model parameters. This is supported by a new
sharp two-to-infinity singular subspace perturbation theory for locally
dependent and flexibly distributed noise, a contribution of independent
interest. Simulations and applications to data in political surveys, population
genetics, and single-cell sequencing demonstrate our method's superior
performance.",2024-12-27T18:51:15Z,http://arxiv.org/abs/2412.19796v1,"Ling Chen, Chengzhu Huang, Yuqi Gu"
"Data-driven analysis of anomalous transport and three-wave-coupling
  effects in E x B plasma discharges","Collisionless cross-field electron transport in an E x B configuration
relevant for electric propulsion is studied using data from a (z, {\theta})
full-PIC simulation. Higher-order spectral analysis shows that transport is
dominated by the in-phase interaction of the oscillations of the azimuthal
electric field and the electron density associated to the first electron
cyclotron drift instability (ECDI) mode. A secondary contribution emanates from
a lower-frequency mode, not predicted by linear ECDI theory, while higher modes
have a minor direct impact on transport. However, a bicoherence analysis
reveals that strong phase couplings exist among the ECDI modes, and a sparse
symbolic regression spectral model, based on the three-wave coupling equations,
suggests an inverse energy cascade as the most likely explanation, thus
suggesting that higher modes contribute indirectly to transport by quadratic
power transfer to the first mode. This work provides new insights into the
dynamics of anomalous plasma transport in E x B sources and the underlying
processes governing energy distribution across different scales, and supports
the validity of weak turbulence theory to examine their behavior.",2024-12-27T18:43:24Z,http://arxiv.org/abs/2412.19789v1,"Borja Bayón-Buján, Enrique Bello-Benítez, Jiewei Zhou, Mario Merino"
"Machine Learning for Sentiment Analysis of Imported Food in Trinidad and
  Tobago","This research investigates the performance of various machine learning
algorithms (CNN, LSTM, VADER, and RoBERTa) for sentiment analysis of Twitter
data related to imported food items in Trinidad and Tobago. The study addresses
three primary research questions: the comparative accuracy and efficiency of
the algorithms, the optimal configurations for each model, and the potential
applications of the optimized models in a live system for monitoring public
sentiment and its impact on the import bill. The dataset comprises tweets from
2018 to 2024, divided into imbalanced, balanced, and temporal subsets to assess
the impact of data balancing and the COVID-19 pandemic on sentiment trends. Ten
experiments were conducted to evaluate the models under various configurations.
Results indicated that VADER outperformed the other models in both multi-class
and binary sentiment classifications. The study highlights significant changes
in sentiment trends pre- and post-COVID-19, with implications for import
policies.",2024-12-27T18:25:08Z,http://arxiv.org/abs/2412.19781v1,"Cassandra Daniels, Koffka Khan"
Tensor Network Estimation of Distribution Algorithms,"Tensor networks are a tool first employed in the context of many-body quantum
physics that now have a wide range of uses across the computational sciences,
from numerical methods to machine learning. Methods integrating tensor networks
into evolutionary optimization algorithms have appeared in the recent
literature. In essence, these methods can be understood as replacing the
traditional crossover operation of a genetic algorithm with a tensor
network-based generative model. We investigate these methods from the point of
view that they are Estimation of Distribution Algorithms (EDAs). We find that
optimization performance of these methods is not related to the power of the
generative model in a straightforward way. Generative models that are better
(in the sense that they better model the distribution from which their training
data is drawn) do not necessarily result in better performance of the
optimization algorithm they form a part of. This raises the question of how
best to incorporate powerful generative models into optimization routines. In
light of this we find that adding an explicit mutation operator to the output
of the generative model often improves optimization performance.",2024-12-27T18:22:47Z,http://arxiv.org/abs/2412.19780v1,"John Gardiner, Javier Lopez-Piqueres"
"Analysis of Premature Death Rates in Texas Counties: The Impact of Air
  Quality, Socioeconomic Factors, and COPD Prevalence","Understanding factors contributing to premature mortality is critical for
public health planning. This study examines the relationships between premature
death rates and multiple risk factors across several Texas counties, utilizing
EPA air quality data, Census information, and county health records from recent
years. We analyze the impact of air quality (PM2.5 levels), socioeconomic
factors (median household income), and health conditions (COPD prevalence)
through statistical analysis and modeling techniques. Results reveal COPD
prevalence as a strong predictor of premature death rates, with higher
prevalence associated with a substantial increase in years of potential life
lost. While socioeconomic factors show a significant negative correlation, air
quality demonstrates more complex indirect relationships. These findings
emphasize the need for integrated public health interventions that prioritize
key health conditions while addressing underlying socioeconomic disparities.",2024-12-27T18:12:04Z,http://arxiv.org/abs/2412.19774v1,"Richard Rich, Ernesto Diaz"
Direct estimates of irreversibility from time series,"The arrow of time can be quantified through the Kullback-Leibler divergence
($D_{KL}$) between the distributions of forward and reverse trajectories in a
system. Many approaches to estimate this rely on specific models, but the use
of incorrect models can introduce uncontrolled errors. Here, we describe a
model-free method that uses trajectory data directly to estimate the evidence
for irreversibility over finite windows of time. To do this we build on
previous work to identify and correct for errors that arise from limited sample
size. Importantly, our approach accurately recovers $D_{KL} = 0$ in systems
that adhere to detailed balance, and the correct nonzero $D_{KL}$ for data
generated by well understood models of nonequilibrium systems. We apply our
method to trajectories of neural activity in the retina as it responds to
naturalistic inputs, and find evidence of irreversibility in single neurons,
emphasizing the non-Markovian character of these data. These results open new
avenues for investigating how the brain represents the arrow of time.",2024-12-27T18:10:53Z,http://arxiv.org/abs/2412.19772v1,"Trevor GrandPre, Gianluca Teza, William Bialek"
Generative Video Propagation,"Large-scale video generation models have the inherent ability to
realistically model natural scenes. In this paper, we demonstrate that through
a careful design of a generative video propagation framework, various video
tasks can be addressed in a unified way by leveraging the generative power of
such models. Specifically, our framework, GenProp, encodes the original video
with a selective content encoder and propagates the changes made to the first
frame using an image-to-video generation model. We propose a data generation
scheme to cover multiple video tasks based on instance-level video segmentation
datasets. Our model is trained by incorporating a mask prediction decoder head
and optimizing a region-aware loss to aid the encoder to preserve the original
content while the generation model propagates the modified region. This novel
design opens up new possibilities: In editing scenarios, GenProp allows
substantial changes to an object's shape; for insertion, the inserted objects
can exhibit independent motion; for removal, GenProp effectively removes
effects like shadows and reflections from the whole video; for tracking,
GenProp is capable of tracking objects and their associated effects together.
Experiment results demonstrate the leading performance of our model in various
video tasks, and we further provide in-depth analyses of the proposed
framework.",2024-12-27T17:42:29Z,http://arxiv.org/abs/2412.19761v1,"Shaoteng Liu, Tianyu Wang, Jui-Hsien Wang, Qing Liu, Zhifei Zhang, Joon-Young Lee, Yijun Li, Bei Yu, Zhe Lin, Soo Ye Kim, Jiaya Jia"
"""Did my figure do justice to the answer?"" : Towards Multimodal Short
  Answer Grading with Feedback (MMSAF)","Personalized feedback plays a vital role in a student's learning process.
While existing systems are adept at providing feedback over MCQ-based
evaluation, this work focuses more on subjective and open-ended questions,
which is similar to the problem of Automatic Short Answer Grading (ASAG) with
feedback. Additionally, we introduce the Multimodal Short Answer grading with
Feedback (MMSAF) problem over the traditional ASAG feedback problem to address
the scenario where the student answer and reference answer might contain
images. Moreover, we introduce the MMSAF dataset with 2197 data points along
with an automated framework for generating such data sets. Our evaluations on
existing LLMs over this dataset achieved an overall accuracy of 55\% on Level
of Correctness labels, 75\% on Image Relevance labels and a score of 4.27 out
of 5 in correctness level of LLM generated feedback as rated by experts. As per
experts, Pixtral achieved a rating of above 4 out of all metrics, indicating
that it is more aligned to human judgement, and that it is the best solution
for assisting students.",2024-12-27T17:33:39Z,http://arxiv.org/abs/2412.19755v1,"Pritam Sil, Bhaskaran Raman, Pushpak Bhattacharyya"
Complement or substitute? How AI increases the demand for human skills,"The question of whether AI substitutes or complements human work is central
to debates on the future of work. This paper examines the impact of AI on skill
demand and compensation in the U.S. economy, analysing 12 million online job
vacancies from 2018 to 2023. It investigates internal effects (within-job
substitution and complementation) and external effects (across occupations,
industries, and regions). Our findings reveal a significant increase in demand
for AI-complementary skills, such as digital literacy, teamwork, and
resilience, alongside rising wage premiums for these skills in AI roles like
Data Scientist. Conversely, substitute skills, including customer service and
text review, have declined in both demand and value within AI-related
positions. Examining external effects, we find a notable rise in demand for
complementary skills in non-AI roles linked to the growth of AI-related jobs in
specific industries or regions. At the same time, there is a moderate decline
in non-AI roles requiring substitute skills. Overall, AI's complementary effect
is up to 50% larger than its substitution effect, resulting in net positive
demand for skills. These results, replicated for the UK and Australia,
highlight AI's transformative impact on workforce skill requirements. They
suggest reskilling efforts should prioritise not only technical AI skills but
also complementary skills like ethics and digital literacy.",2024-12-27T17:26:30Z,http://arxiv.org/abs/2412.19754v1,"Elina Mäkelä, Fabian Stephany"
"IMAGINE: An 8-to-1b 22nm FD-SOI Compute-In-Memory CNN Accelerator With
  an End-to-End Analog Charge-Based 0.15-8POPS/W Macro Featuring
  Distribution-Aware Data Reshaping","Charge-domain compute-in-memory (CIM) SRAMs have recently become an enticing
compromise between computing efficiency and accuracy to process sub-8b
convolutional neural networks (CNNs) at the edge. Yet, they commonly make use
of a fixed dot-product (DP) voltage swing, which leads to a loss in effective
ADC bits due to data-dependent clipping or truncation effects that waste
precious conversion energy and computing accuracy. To overcome this, we present
IMAGINE, a workload-adaptive 1-to-8b CIM-CNN accelerator in 22nm FD-SOI. It
introduces a 1152x256 end-to-end charge-based macro with a multi-bit DP based
on an input-serial, weight-parallel accumulation that avoids power-hungry DACs.
An adaptive swing is achieved by combining a channel-wise DP array split with a
linear in-ADC implementation of analog batch-normalization (ABN), obtaining a
distribution-aware data reshaping. Critical design constraints are relaxed by
including the post-silicon equivalent noise within a CIM-aware CNN training
framework. Measurement results showcase an 8b system-level energy efficiency of
40TOPS/W at 0.3/0.6V, with competitive accuracies on MNIST and CIFAR-10.
Moreover, the peak energy and area efficiencies of the 187kB/mm2 macro
respectively reach up to 0.15-8POPS/W and 2.6-154TOPS/mm2, scaling with the
8-to-1b computing precision. These results exceed previous charge-based designs
by 3-to-5x while being the first work to provide linear in-memory rescaling.",2024-12-27T17:18:15Z,http://arxiv.org/abs/2412.19750v1,"Adrian Kneip, Martin Lefebvre, Pol Maistriaux, David Bol"
"AAM-SEALS: Developing Aerial-Aquatic Manipulators in SEa, Air, and Land
  Simulator","Current simulators lack the ability to accurately model integrated
environments that encompass sea, air, and land. To address this gap, we
introduce Aerial-Aquatic Manipulators (AAMs) in SEa, Air, and Land Simulator
(SEALS), a comprehensive and photorealistic simulator designed for AAMs to
operate and learn in these diverse environments. The development of AAM-SEALS
tackles several significant challenges, including the creation of integrated
controllers for flying, swimming, and manipulation, and the high-fidelity
simulation of aerial dynamics and hydrodynamics leveraging particle physics.
Our evaluation demonstrates smooth operation and photorealistic transitions
across air, water, and their interfaces. We quantitatively validate the
fidelity of particle-based hydrodynamics by comparing position-tracking errors
across real-world and simulated systems. AAM-SEALS promises to benefit a broad
range of robotics communities, including robot learning, aerial robotics,
underwater robotics, mobile manipulation, and robotic simulators. We will
open-source our code and data to foster the advancement of research in these
fields. Please access our project website at: https:
//aam-seals.github.io/aam-seals-v1/",2024-12-27T17:13:14Z,http://arxiv.org/abs/2412.19744v1,"William Wang Yang, Karthikeya Kona, Yashveer Jain, Abhinav Bhamidipati, Tomer Atzili, Xiaomin Lin, Yantian Zha"
A Root-Zone Soil Salinity Observatory for Coastal Southwest Bangladesh,"The research assesses soil salinity in the southwest coastal region of
Bangladesh, collecting a total of 162 topsoil samples between March 1 and March
9, 2024, and processing them following the standard operating procedure for
soil electrical conductivity (soil/water, 1:5). Electrical conductivity (EC)
measurements obtained using a HI-6321 advanced conductivity benchtop meter were
analyzed and visualized using bubble density mapping and the Empirical Bayesian
Kriging interpolation method. The findings indicate that soil salinity in the
study area ranges from 0.05 to 9.09 mS/cm, with the highest levels observed
near Debhata and Koyra. A gradient of increasing soil salinity is clearly
evident from the northern to southern regions. This dataset provides a critical
resource for soil salinity-related research in the region, offering valuable
insights to support decision-makers in understanding and mitigating the impacts
of soil salinity in Bangladesh's coastal areas.",2024-12-27T17:02:12Z,http://arxiv.org/abs/2412.19740v1,"Showmitra Kumar Sarkar, Mafrid Haydar, Rhyme Rubayet Rudra, Tanmoy Mazumder, Md. Sadmin Nur, Md. Shahriar Islam, Shakib Mohammad Sany, Tanzim Al Noor, Shakil Ahmed, Myisha Ahmad, Annajmus Sakib, Sai Ravela"
Hard Photon Triggered Jets in $p$-$p$ and $A$-$A$ Collisions,"An investigation of high transverse momentum (high-$p_T$) photon triggered
jets in proton-proton ($p$-$p$) and ion-ion ($A$-$A$) collisions at
$\sqrt{s_{NN}} = 0.2$ and $5.02~\mathrm{TeV}$ is carried out, using the
multistage description of in-medium jet evolution. Monte Carlo simulations of
hard scattering and energy loss in heavy-ion collisions are performed using
parameters tuned in a previous study of the nuclear modification factor
($R_{AA}$) for inclusive jets and high-$p_T$ hadrons. We obtain a good
reproduction of the experimental data for photon triggered jet $R_{AA}$, as
measured by the ATLAS detector, the distribution of the ratio of jet to photon
$p_T$ ($X_{\rm J \gamma}$), measured by both CMS and ATLAS, and the photon-jet
azimuthal correlation as measured by CMS. We obtain a moderate description of
the photon triggered jet $I_{AA}$, as measured by STAR. A noticeable
improvement in the comparison is observed when one goes beyond prompt photons
and includes bremsstrahlung and decay photons, revealing their significance in
certain kinematic regions, particularly at $X_{J\gamma} &gt; 1$. Moreover,
azimuthal angle correlations demonstrate a notable impact of non-prompt photons
on the distribution, emphasizing their role in accurately describing
experimental results. This work highlights the success of the multistage model
of jet modification to straightforwardly predict (this set of) photon triggered
jet observables. This comparison, along with the role played by non-prompt
photons, has important consequences on the inclusion of such observables in a
future Bayesian analysis.",2024-12-27T16:56:15Z,http://arxiv.org/abs/2412.19738v1,"C. Sirimanna, Y. Tachibana, A. Majumder, A. Angerami, R. Arora, S. A. Bass, Y. Chen, R. Datta, L. Du, R. Ehlers, H. Elfner, R. J. Fries, C. Gale, Y. He, B. V. Jacak, P. M. Jacobs, S. Jeon, Y. Ji, F. Jonas, L. Kasper, M. Kordell II, A. Kumar, R. Kunnawalkam-Elayavalli, J. Latessa, Y. -J. Lee, R. Lemmon, M. Luzum, S. Mak, A. Mankolli, C. Martin, H. Mehryar, T. Mengel, C. Nattrass, J. Norman, C. Parker, J. -F. Paquet, J. H. Putschke, H. Roch, G. Roland, B. Schenke, L. Schwiebert, A. Sengupta, C. Shen, M. Singh, D. Soeder, R. A. Soltz, I. Soudi, J. Velkovska, G. Vujanovic, X. -N. Wang, X. Wu, W. Zhao"
"Adaptive Context-Aware Multi-Path Transmission Control for VR/AR
  Content: A Deep Reinforcement Learning Approach","This paper introduces the Adaptive Context-Aware Multi-Path Transmission
Control Protocol (ACMPTCP), an efficient approach designed to optimize the
performance of Multi-Path Transmission Control Protocol (MPTCP) for
data-intensive applications such as augmented and virtual reality (AR/VR)
streaming. ACMPTCP addresses the limitations of conventional MPTCP by
leveraging deep reinforcement learning (DRL) for agile end-to-end path
management and optimal bandwidth allocation, facilitating path realignment
across diverse network environments.",2024-12-27T16:56:12Z,http://arxiv.org/abs/2412.19737v1,"Shakil Ahmed, Saifur Rahman Sabuj, Ashfaq Khokhar"
"A General Framework of Brain Region Detection And Genetic Variants
  Selection in Imaging Genetics","Imaging genetics is a growing field that employs structural or functional
neuroimaging techniques to study individuals with genetic risk variants
potentially linked to specific illnesses. This area presents considerable
challenges to statisticians due to the heterogeneous information and different
data forms it involves. In addition, both imaging and genetic data are
typically high-dimensional, creating a ""big data squared"" problem. Moreover,
brain imaging data contains extensive spatial information. Simply vectorizing
tensor images and treating voxels as independent features can lead to
computational issues and disregard spatial structure. This paper presents a
novel statistical method for imaging genetics modeling while addressing all
these challenges. We explore a Canonical Correlation Analysis based linear
model for the joint modeling of brain imaging, genetic information, and
clinical phenotype, enabling the simultaneous detection of significant brain
regions and selection of important genetic variants associated with the
phenotype outcome. Scalable algorithms are developed to tackle the ""big data
squared"" issue. We apply the proposed method to explore the reaction speed, an
indicator of cognitive functions, and its associations with brain MRI and
genetic factors using the UK Biobank database. Our study reveals a notable
connection between the caudate nucleus region of brain and specific significant
SNPs, along with their respective regulated genes, and the reaction speed.",2024-12-27T16:54:11Z,http://arxiv.org/abs/2412.19735v1,"Siqiang Su, Zhenghao Li, Long Feng, Ting Li"
"Dynamics, data and reconstruction","Data-driven learning is prevalent in many fields of science, mathematics and
engineering. The goal of data-driven learning of dynamical systems is to
interpret timeseries as a continuous observation of an underlying dynamical
system. This task is not well-posed for a variety of reasons. A dynamical
system may have multiple sub-systems co-existing within it. The nature of the
dataset depends on the portion of the phase space being viewed, and may thus my
confined to a sub-system. Secondly these sub-systems may be topologically
inter-weaved, so may be inseparable computationally. Thirdly, two timeseries
sampled separately from different dynamical systems may be close or even
indistinguishable. So there is no unqiue source for the timeseries. We show how
these ambiguities are circumvented if one considers dynamical systems and
measurement maps collectively. This is made possible in a category theoretical
framework, in which reconstruction is unique up to equivalences. We introduce
two categories of observed dynamical systems and timeseries-data. These are
related to the well known category of dynamical systems via functors. This
enables a functorial interpretation of the task of reconstruction as well.",2024-12-27T16:49:52Z,http://arxiv.org/abs/2412.19734v1,"Suddhasattwa Das, Tomoharu Suda"
"Generative Pretrained Embedding and Hierarchical Irregular Time Series
  Representation for Daily Living Activity Recognition","Within the evolving landscape of smart homes, the precise recognition of
daily living activities using ambient sensor data stands paramount. This paper
not only aims to bolster existing algorithms by evaluating two distinct
pretrained embeddings suited for ambient sensor activations but also introduces
a novel hierarchical architecture. We delve into an architecture anchored on
Transformer Decoder-based pre-trained embeddings, reminiscent of the GPT
design, and contrast it with the previously established state-of-the-art (SOTA)
ELMo embeddings for ambient sensors. Our proposed hierarchical structure
leverages the strengths of each pre-trained embedding, enabling the discernment
of activity dependencies and sequence order, thereby enhancing classification
precision. To further refine recognition, we incorporate into our proposed
architecture an hour-of-the-day embedding. Empirical evaluations underscore the
preeminence of the Transformer Decoder embedding in classification endeavors.
Additionally, our innovative hierarchical design significantly bolsters the
efficacy of both pre-trained embeddings, notably in capturing inter-activity
nuances. The integration of temporal aspects subtly but distinctively augments
classification, especially for time-sensitive activities. In conclusion, our
GPT-inspired hierarchical approach, infused with temporal insights, outshines
the SOTA ELMo benchmark.",2024-12-27T16:43:52Z,http://arxiv.org/abs/2412.19732v1,"Damien Bouchabou, Sao Mai Nguyen"
"Learning to Forget: Bayesian Time Series Forecasting using Recurrent
  Sparse Spectrum Signature Gaussian Processes","The signature kernel is a kernel between time series of arbitrary length and
comes with strong theoretical guarantees from stochastic analysis. It has found
applications in machine learning such as covariance functions for Gaussian
processes. A strength of the underlying signature features is that they provide
a structured global description of a time series. However, this property can
quickly become a curse when local information is essential and forgetting is
required; so far this has only been addressed with ad-hoc methods such as
slicing the time series into subsegments. To overcome this, we propose a
principled, data-driven approach by introducing a novel forgetting mechanism
for signatures. This allows the model to dynamically adapt its context length
to focus on more recent information. To achieve this, we revisit the recently
introduced Random Fourier Signature Features, and develop Random Fourier
Decayed Signature Features (RFDSF) with Gaussian processes (GPs). This results
in a Bayesian time series forecasting algorithm with variational inference,
that offers a scalable probabilistic algorithm that processes and transforms a
time series into a joint predictive distribution over time steps in one pass
using recurrence. For example, processing a sequence of length $10^4$ steps in
$\approx 10^{-2}$ seconds and in $&lt; 1\text{GB}$ of GPU memory. We demonstrate
that it outperforms other GP-based alternatives and competes with
state-of-the-art probabilistic time series forecasting algorithms.",2024-12-27T16:31:09Z,http://arxiv.org/abs/2412.19727v1,"Csaba Tóth, Masaki Adachi, Michael A. Osborne, Harald Oberhauser"
EEG-Reptile: An Automatized Reptile-Based Meta-Learning Library for BCIs,"Meta-learning, i.e., ""learning to learn"", is a promising approach to enable
efficient BCI classifier training with limited amounts of data. It can
effectively use collections of in some way similar classification tasks, with
rapid adaptation to new tasks where only minimal data are available. However,
applying meta-learning to existing classifiers and BCI tasks requires
significant effort. To address this issue, we propose EEG-Reptile, an automated
library that leverages meta-learning to improve classification accuracy of
neural networks in BCIs and other EEG-based applications. It utilizes the
Reptile meta-learning algorithm to adapt neural network classifiers of EEG data
to the inter-subject domain, allowing for more efficient fine-tuning for a new
subject on a small amount of data. The proposed library incorporates an
automated hyperparameter tuning module, a data management pipeline, and an
implementation of the Reptile meta-learning algorithm. EEG-Reptile automation
level allows using it without deep understanding of meta-learning. We
demonstrate the effectiveness of EEG-Reptile on two benchmark datasets (BCI IV
2a, Lee2019 MI) and three neural network architectures (EEGNet, FBCNet,
EEG-Inception). Our library achieved improvement in both zero-shot and few-shot
learning scenarios compared to traditional transfer learning approaches.",2024-12-27T16:24:31Z,http://arxiv.org/abs/2412.19725v1,"Daniil A. Berdyshev, Artem M. Grachev, Sergei L. Shishkin, Bogdan L. Kozyrskiy"
"Exploring low-rank structure for an inverse scattering problem with
  far-field data","The inverse scattering problem exhibits an inherent low-rank structure due to
its ill-posed nature; however developing low-rank structures for the inverse
scattering problem remains challenging. In this work, we introduce a novel
low-rank structure tailored for solving the inverse scattering problem. The
particular low-rank structure is given by the generalized prolate spheroidal
wave functions, computed stably and accurately via a Sturm-Liouville problem.
We first process the far-field data to obtain a post-processed data set within
a disk domain. Subsequently, the post-processed data are projected onto a
low-rank space given by the low-rank structure. The unknown is approximately
solved in this low-rank space, by dropping higher-order terms. The low-rank
structure leads to a H\""{o}lder-logarithmic type stability estimate for
arbitrary unknown functions, and a Lipschitz stability estimate for unknowns
belonging to a finite dimensional low-rank space. Various numerical experiments
are conducted to validate its performance, encompassing assessments of
resolution capability, robustness against randomly added noise and modeling
errors, and demonstration of increasing stability.",2024-12-27T16:24:20Z,http://arxiv.org/abs/2412.19724v1,"Yuyuan Zhou, Lorenzo Audibert, Shixu Meng, Bo Zhang"
"OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse
  Task Synthesis","Graphical User Interface (GUI) agents powered by Vision-Language Models
(VLMs) have demonstrated human-like computer control capability. Despite their
utility in advancing digital automation, a critical bottleneck persists:
collecting high-quality trajectory data for training. Common practices for
collecting such data rely on human supervision or synthetic data generation
through executing pre-defined tasks, which are either resource-intensive or
unable to guarantee data quality. Moreover, these methods suffer from limited
data diversity and significant gaps between synthetic data and real-world
environments. To address these challenges, we propose OS-Genesis, a novel GUI
data synthesis pipeline that reverses the conventional trajectory collection
process. Instead of relying on pre-defined tasks, OS-Genesis enables agents
first to perceive environments and perform step-wise interactions, then
retrospectively derive high-quality tasks to enable trajectory-level
exploration. A trajectory reward model is then employed to ensure the quality
of the generated trajectories. We demonstrate that training GUI agents with
OS-Genesis significantly improves their performance on highly challenging
online benchmarks. In-depth analysis further validates OS-Genesis's efficiency
and its superior data quality and diversity compared to existing synthesis
methods. Our codes, data, and checkpoints are available at
\href{https://qiushisun.github.io/OS-Genesis-Home/}{OS-Genesis Homepage}.",2024-12-27T16:21:58Z,http://arxiv.org/abs/2412.19723v1,"Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, Ben Kao, Guohao Li, Junxian He, Yu Qiao, Zhiyong Wu"
"Quantum correlations in a gravitational collapse simulation with
  SpheriCo.jl","We report on work using a newly developed code, SpheriCo.jl, that computes
the gravitational collapse of a spherical scalar field, where the scalar can be
either a classical field, or a quantum field operator. By utilising
summation-by-parts methods for the numerical derivatives we are able to
simulate the collapse longer than was possible previously due to enhanced
numerical stability. We present a suite of tests for the code that tests its
accuracy and stability, both for the classical and quantum fields. We are able
to observe critical behavior of gravitational collapse for the classical setup,
in agreement with expected results. The code is also used to compute two-point
correlation functions, with results that hint at a non-trivial correlation
across the horizon of Hawking quanta.",2024-12-27T16:20:27Z,http://arxiv.org/abs/2412.19722v1,"Benjamin Berczi, Magdalena Eriksson, Thanasis Giannakopoulos, Paul M. Saffin"
Sharpening Neural Implicit Functions with Frequency Consolidation Priors,"Signed Distance Functions (SDFs) are vital implicit representations to
represent high fidelity 3D surfaces. Current methods mainly leverage a neural
network to learn an SDF from various supervisions including signed distances,
3D point clouds, or multi-view images. However, due to various reasons
including the bias of neural network on low frequency content, 3D unaware
sampling, sparsity in point clouds, or low resolutions of images, neural
implicit representations still struggle to represent geometries with high
frequency components like sharp structures, especially for the ones learned
from images or point clouds. To overcome this challenge, we introduce a method
to sharpen a low frequency SDF observation by recovering its high frequency
components, pursuing a sharper and more complete surface. Our key idea is to
learn a mapping from a low frequency observation to a full frequency coverage
in a data-driven manner, leading to a prior knowledge of shape consolidation in
the frequency domain, dubbed frequency consolidation priors. To better
generalize a learned prior to unseen shapes, we introduce to represent
frequency components as embeddings and disentangle the embedding of the low
frequency component from the embedding of the full frequency component. This
disentanglement allows the prior to generalize on an unseen low frequency
observation by simply recovering its full frequency embedding through a
test-time self-reconstruction. Our evaluations under widely used benchmarks or
real scenes show that our method can recover high frequency component and
produce more accurate surfaces than the latest methods. The code, data, and
pre-trained models are available at \url{https://github.com/chenchao15/FCP}.",2024-12-27T16:18:46Z,http://arxiv.org/abs/2412.19720v1,"Chao Chen, Yu-Shen Liu, Zhizhong Han"
"Text2Insight: Transform natural language text into insights seamlessly
  using multi-model architecture","The growing demand for dynamic, user-centric data analysis and visualization
is evident across domains like healthcare, finance, and research. Traditional
visualization tools often fail to meet individual user needs due to their
static and predefined nature. To address this gap, Text2Insight is introduced
as an innovative solution that delivers customized data analysis and
visualizations based on user-defined natural language requirements. Leveraging
a multi-model architecture, Text2Insight transforms user inputs into actionable
insights and dynamic visualizations.
  The methodology begins with analyzing the input dataset to extract structural
details such as columns and values. A pre-trained Llama3 model converts the
user's natural language query into an SQL query, which is further refined using
a Named Entity Recognition (NER) model for accuracy. A chart predictor
determines the most suitable visualization type, while the Llama3 model
generates insights based on the SQL query's results. The output is a
user-friendly and visually informative chart. To enhance analysis capabilities,
the system integrates a question-answering model and a predictive model using
the BERT framework. These models provide insights into historical data and
predict future trends.
  Performance evaluation of Text2Insight demonstrates its effectiveness,
achieving high accuracy (99%), precision (100%), recall (99%), and F1-score
(99%), with a BLEU score of 0.5. The question-answering model attained an
accuracy of 89% and the predictive model achieved 70% accuracy. These results
validate Text2Insight as a robust and viable solution for transforming natural
language text into dynamic, user-specific data analysis and visualizations.",2024-12-27T16:17:22Z,http://arxiv.org/abs/2412.19718v1,Pradeep Sain
Low-Regularity Global solution for fractional NLS in modulation spaces,"We establish global well-posedness for the mass sub-critical nonlinear
fractional Schr\""odinger equation
  $$iu_t + (-\Delta)^\frac{\beta}{2} u \pm (|u|^{\alpha}u)=0$$
  with radial initial data in modulation spaces $M^{p,\frac{p}{p-1}}(\mathbb
R^n)$ with $2&lt;p$ sufficiently close to $2.$ Our order of dispersion $\beta$
lies in $(2n/ (2n-1), 2)$ for $n \geq 2$.",2024-12-27T16:15:10Z,http://arxiv.org/abs/2412.19714v1,"Divyang G. Bhimani, Diksha Dhingra, Vijay Kumar Sohani"
"ProKAN: Progressive Stacking of Kolmogorov-Arnold Networks for Efficient
  Liver Segmentation","The growing need for accurate and efficient 3D identification of tumors,
particularly in liver segmentation, has spurred considerable research into deep
learning models. While many existing architectures offer strong performance,
they often face challenges such as overfitting and excessive computational
costs. An adjustable and flexible architecture that strikes a balance between
time efficiency and model complexity remains an unmet requirement. In this
paper, we introduce proKAN, a progressive stacking methodology for
Kolmogorov-Arnold Networks (KANs) designed to address these challenges. Unlike
traditional architectures, proKAN dynamically adjusts its complexity by
progressively adding KAN blocks during training, based on overfitting behavior.
This approach allows the network to stop growing when overfitting is detected,
preventing unnecessary computational overhead while maintaining high accuracy.
Additionally, proKAN utilizes KAN's learnable activation functions modeled
through B-splines, which provide enhanced flexibility in learning complex
relationships in 3D medical data. Our proposed architecture achieves
state-of-the-art performance in liver segmentation tasks, outperforming
standard Multi-Layer Perceptrons (MLPs) and fixed KAN architectures. The
dynamic nature of proKAN ensures efficient training times and high accuracy
without the risk of overfitting. Furthermore, proKAN provides better
interpretability by allowing insight into the decision-making process through
its learnable coefficients. The experimental results demonstrate a significant
improvement in accuracy, Dice score, and time efficiency, making proKAN a
compelling solution for 3D medical image segmentation tasks.",2024-12-27T16:14:06Z,http://arxiv.org/abs/2412.19713v1,"Bhavesh Gyanchandani, Aditya Oza, Abhinav Roy"
"Causal machine learning for heterogeneous treatment effects in the
  presence of missing outcome data","When estimating heterogeneous treatment effects, missing outcome data can
complicate treatment effect estimation, causing certain subgroups of the
population to be poorly represented. In this work, we discuss this commonly
overlooked problem and consider the impact that missing at random (MAR) outcome
data has on causal machine learning estimators for the conditional average
treatment effect (CATE). We then propose two de-biased machine learning
estimators for the CATE, the mDR-learner and mEP-learner, which address the
issue of under-representation by integrating inverse probability of censoring
weights into the DR-learner and EP-learner respectively. We show that under
reasonable conditions, these estimators are oracle efficient, and illustrate
their favorable performance through simulated data settings, comparing them to
existing CATE estimators, including comparison to estimators which use common
missing data techniques. Guidance on the implementation of these estimators is
provided and we present an example of their application using the ACTG175
trial, exploring treatment effect heterogeneity when comparing Zidovudine
mono-therapy against alternative antiretroviral therapies among HIV-1-infected
individuals.",2024-12-27T16:10:03Z,http://arxiv.org/abs/2412.19711v1,"Matthew Pryce, Karla Diaz-Ordaz, Ruth H. Keogh, Stijn Vansteelandt"
"Noise Sensitivity of the Semidefinite Programs for Direct Data-Driven
  LQR","In this paper, we study the noise sensitivity of the semidefinite program
(SDP) proposed for direct data-driven infinite-horizon linear quadratic
regulator (LQR) problem for discrete-time linear time-invariant systems. While
this SDP is shown to find the true LQR controller in the noise-free setting, we
show that it leads to a trivial solution with zero gain matrices when data is
corrupted by noise, even when the noise is arbitrarily small. We then study a
variant of the SDP that includes a robustness promoting regularization term and
prove that regularization does not fully eliminate the sensitivity issue. In
particular, the solution of the regularized SDP converges in probability also
to a trivial solution.",2024-12-27T15:59:42Z,http://arxiv.org/abs/2412.19705v1,"Xiong Zeng, Laurent Bako, Necmiye Ozay"
"Search for the double Dalitz decays $η/η' \to e^+e^-μ^+μ^-$
  and $η' \to μ^+μ^-μ^+μ^-$","Using a data sample of $(10087 \pm 44) \times {10^{6}}$ $J/{\psi}$ events
collected with the BESIII detector, we search for the decays $\eta/\eta'\to
e^+e^-\mu^+\mu^-$ and $\eta' \to \mu^+\mu^-\mu^+\mu^-$ via the radiative decays
$J/{\psi}\to\gamma\eta$/$\gamma\eta'$. No excess of events over expected
background is observed for any of the decays of interest. At 90% confidence
level, we report the first upper limits on the branching fractions of $\eta'
\to e^{+}e^{-}\mu^{+}\mu^{-}$ and $\eta' \to \mu^{+}\mu^{-}\mu^{+}\mu^{-}$ to
be $ 1.75 \times {10^{-6}}$ and $5.28 \times {10^{-7}}$, respectively. In
addition, we set an upper limit on the branching fraction of $\eta \to
e^{+}e^{-}\mu^{+}\mu^{-}$ to be $6.88 \times {10^{-6}}$, which improves the
previous result by about two orders of magnitude.",2024-12-27T15:55:02Z,http://arxiv.org/abs/2412.19702v1,"BESIII Collaboration, M. Ablikim, M. N. Achasov, P. Adlarson, O. Afedulidis, X. C. Ai, R. Aliberti, A. Amoroso, Y. Bai, O. Bakina, I. Balossino, Y. Ban, H. -R. Bao, V. Batozskaya, K. Begzsuren, N. Berger, M. Berlowski, M. Bertani, D. Bettoni, F. Bianchi, E. Bianco, A. Bortone, I. Boyko, R. A. Briere, A. Brueggemann, H. Cai, X. Cai, A. Calcaterra, G. F. Cao, N. Cao, S. A. Cetin, X. Y. Chai, J. F. Chang, G. R. Che, Y. Z. Che, G. Chelkov, C. Chen, C. H. Chen, Chao Chen, G. Chen, H. S. Chen, H. Y. Chen, M. L. Chen, S. J. Chen, S. L. Chen, S. M. Chen, T. Chen, X. R. Chen, X. T. Chen, Y. B. Chen, Y. Q. Chen, Z. J. Chen, Z. Y. Chen, S. K. Choi, G. Cibinetto, F. Cossio, J. J. Cui, H. L. Dai, J. P. Dai, A. Dbeyssi, R. E. de Boer, D. Dedovich, C. Q. Deng, Z. Y. Deng, A. Denig, I. Denysenko, M. Destefanis, F. De Mori, B. Ding, X. X. Ding, Y. Ding, Y. Ding, J. Dong, L. Y. Dong, M. Y. Dong, X. Dong, M. C. Du, S. X. Du, Y. Y. Duan, Z. H. Duan, P. Egorov, Y. H. Fan, J. Fang, J. Fang, S. S. Fang, W. X. Fang, Y. Fang, Y. Q. Fang, R. Farinelli, L. Fava, F. Feldbauer, G. Felici, C. Q. Feng, J. H. Feng, Y. T. Feng, M. Fritsch, C. D. Fu, J. L. Fu, Y. W. Fu, H. Gao, X. B. Gao, Y. N. Gao, Yang Gao, S. Garbolino, I. Garzia, L. Ge, P. T. Ge, Z. W. Ge, C. Geng, E. M. Gersabeck, A. Gilman, K. Goetzen, L. Gong, W. X. Gong, W. Gradl, S. Gramigna, M. Greco, M. H. Gu, Y. T. Gu, C. Y. Guan, A. Q. Guo, L. B. Guo, M. J. Guo, R. P. Guo, Y. P. Guo, A. Guskov, J. Gutierrez, K. L. Han, T. T. Han, F. Hanisch, X. Q. Hao, F. A. Harris, K. K. He, K. L. He, F. H. Heinsius, C. H. Heinz, Y. K. Heng, C. Herold, T. Holtmann, P. C. Hong, G. Y. Hou, X. T. Hou, Y. R. Hou, Z. L. Hou, B. Y. Hu, H. M. Hu, J. F. Hu, Q. P. Hu, S. L. Hu, T. Hu, Y. Hu, G. S. Huang, K. X. Huang, L. Q. Huang, X. T. Huang, Y. P. Huang, Y. S. Huang, T. Hussain, F. Hölzken, N. Hüsken, N. in der Wiesche, J. Jackson, S. Janchiv, J. H. Jeong, Q. Ji, Q. P. Ji, W. Ji, X. B. Ji, X. L. Ji, Y. Y. Ji, X. Q. Jia, Z. K. Jia, D. Jiang, H. B. Jiang, P. C. Jiang, S. S. Jiang, T. J. Jiang, X. S. Jiang, Y. Jiang, J. B. Jiao, J. K. Jiao, Z. Jiao, S. Jin, Y. Jin, M. Q. Jing, X. M. Jing, T. Johansson, S. Kabana, N. Kalantar-Nayestanaki, X. L. Kang, X. S. Kang, M. Kavatsyuk, B. C. Ke, V. Khachatryan, A. Khoukaz, R. Kiuchi, O. B. Kolcu, B. Kopf, M. Kuessner, X. Kui, N. Kumar, A. Kupsc, W. Kühn, L. Lavezzi, T. T. Lei, Z. H. Lei, M. Lellmann, T. Lenz, C. Li, C. Li, C. H. Li, Cheng Li, D. M. Li, F. Li, G. Li, H. B. Li, H. J. Li, H. N. Li, Hui Li, J. R. Li, J. S. Li, K. Li, K. L. Li, L. J. Li, L. K. Li, Lei Li, M. H. Li, P. R. Li, Q. M. Li, Q. X. Li, R. Li, S. X. Li, T. Li, W. D. Li, W. G. Li, X. Li, X. H. Li, X. L. Li, X. Y. Li, X. Z. Li, Y. G. Li, Z. J. Li, Z. Y. Li, C. Liang, H. Liang, H. Liang, Y. F. Liang, Y. T. Liang, G. R. Liao, Y. P. Liao, J. Libby, A. Limphirat, C. C. Lin, C. X. Lin, D. X. Lin, T. Lin, B. J. Liu, B. X. Liu, C. Liu, C. X. Liu, F. Liu, F. H. Liu, Feng Liu, G. M. Liu, H. Liu, H. B. Liu, H. H. Liu, H. M. Liu, Huihui Liu, J. B. Liu, J. Y. Liu, K. Liu, K. Y. Liu, Ke Liu, L. Liu, L. C. Liu, Lu Liu, M. H. Liu, P. L. Liu, Q. Liu, S. B. Liu, T. Liu, W. K. Liu, W. M. Liu, X. Liu, X. Liu, Y. Liu, Y. Liu, Y. B. Liu, Z. A. Liu, Z. D. Liu, Z. Q. Liu, X. C. Lou, F. X. Lu, H. J. Lu, J. G. Lu, X. L. Lu, Y. Lu, Y. P. Lu, Z. H. Lu, C. L. Luo, J. R. Luo, M. X. Luo, T. Luo, X. L. Luo, X. R. Lyu, Y. F. Lyu, F. C. Ma, H. Ma, H. L. Ma, J. L. Ma, L. L. Ma, L. R. Ma, M. M. Ma, Q. M. Ma, R. Q. Ma, T. Ma, X. T. Ma, X. Y. Ma, Y. M. Ma, F. E. Maas, I. MacKay, M. Maggiora, S. Malde, Y. J. Mao, Z. P. Mao, S. Marcello, Z. X. Meng, J. G. Messchendorp, G. Mezzadri, H. Miao, T. J. Min, R. E. Mitchell, X. H. Mo, B. Moses, N. Yu. Muchnoi, J. Muskalla, Y. Nefedov, F. Nerling, L. S. Nie, I. B. Nikolaev, Z. Ning, S. Nisar, Q. L. Niu, W. D. Niu, Y. Niu, S. L. Olsen, S. L. Olsen, Q. Ouyang, S. Pacetti, X. Pan, Y. Pan, A. Pathak, Y. P. Pei, M. Pelizaeus, H. P. Peng, Y. Y. Peng, K. Peters, J. L. Ping, R. G. Ping, S. Plura, V. Prasad, F. Z. Qi, H. Qi, H. R. Qi, M. Qi, T. Y. Qi, S. Qian, W. B. Qian, C. F. Qiao, X. K. Qiao, J. J. Qin, L. Q. Qin, L. Y. Qin, X. P. Qin, X. S. Qin, Z. H. Qin, J. F. Qiu, Z. H. Qu, C. F. Redmer, K. J. Ren, A. Rivetti, M. Rolo, G. Rong, Ch. Rosner, M. Q. Ruan, S. N. Ruan, N. Salone, A. Sarantsev, Y. Schelhaas, K. Schoenning, M. Scodeggio, K. Y. Shan, W. Shan, X. Y. Shan, Z. J. Shang, J. F. Shangguan, L. G. Shao, M. Shao, C. P. Shen, H. F. Shen, W. H. Shen, X. Y. Shen, B. A. Shi, H. Shi, H. C. Shi, J. L. Shi, J. Y. Shi, Q. Q. Shi, S. Y. Shi, X. Shi, J. J. Song, T. Z. Song, W. M. Song, Y. J. Song, Y. X. Song, S. Sosio, S. Spataro, F. Stieler, S. S Su, Y. J. Su, G. B. Sun, G. X. Sun, H. Sun, H. K. Sun, J. F. Sun, K. Sun, L. Sun, S. S. Sun, T. Sun, W. Y. Sun, Y. Sun, Y. J. Sun, Y. Z. Sun, Z. Q. Sun, Z. T. Sun, C. J. Tang, G. Y. Tang, J. Tang, M. Tang, Y. A. Tang, L. Y. Tao, Q. T. Tao, M. Tat, J. X. Teng, V. Thoren, W. H. Tian, Y. Tian, Z. F. Tian, I. Uman, Y. Wan, S. J. Wang, B. Wang, B. L. Wang, Bo Wang, D. Y. Wang, F. Wang, H. J. Wang, J. J. Wang, J. P. Wang, K. Wang, L. L. Wang, M. Wang, N. Y. Wang, S. Wang, S. Wang, T. Wang, T. J. Wang, W. Wang, W. Wang, W. P. Wang, X. Wang, X. F. Wang, X. J. Wang, X. L. Wang, X. N. Wang, Y. Wang, Y. D. Wang, Y. F. Wang, Y. H. Wang, Y. L. Wang, Y. N. Wang, Y. Q. Wang, Yaqian Wang, Yi Wang, Z. Wang, Z. L. Wang, Z. Y. Wang, Ziyi Wang, D. H. Wei, F. Weidner, S. P. Wen, Y. R. Wen, U. Wiedner, G. Wilkinson, M. Wolke, L. Wollenberg, C. Wu, J. F. Wu, L. H. Wu, L. J. Wu, X. Wu, X. H. Wu, Y. Wu, Y. H. Wu, Y. J. Wu, Z. Wu, L. Xia, X. M. Xian, B. H. Xiang, T. Xiang, D. Xiao, G. Y. Xiao, S. Y. Xiao, Y. L. Xiao, Z. J. Xiao, C. Xie, X. H. Xie, Y. Xie, Y. G. Xie, Y. H. Xie, Z. P. Xie, T. Y. Xing, C. F. Xu, C. J. Xu, G. F. Xu, H. Y. Xu, M. Xu, Q. J. Xu, Q. N. Xu, W. Xu, W. L. Xu, X. P. Xu, Y. Xu, Y. C. Xu, Z. S. Xu, F. Yan, L. Yan, W. B. Yan, W. C. Yan, X. Q. Yan, H. J. Yang, H. L. Yang, H. X. Yang, J. H. Yang, T. Yang, Y. Yang, Y. F. Yang, Y. F. Yang, Y. X. Yang, Z. W. Yang, Z. P. Yao, M. Ye, M. H. Ye, J. H. Yin, Junhao Yin, Z. Y. You, B. X. Yu, C. X. Yu, G. Yu, J. S. Yu, M. C. Yu, T. Yu, X. D. Yu, Y. C. Yu, C. Z. Yuan, J. Yuan, J. Yuan, L. Yuan, S. C. Yuan, Y. Yuan, Z. Y. Yuan, C. X. Yue, A. A. Zafar, F. R. Zeng, S. H. Zeng, X. Zeng, Y. Zeng, Y. J. Zeng, Y. J. Zeng, X. Y. Zhai, Y. C. Zhai, Y. H. Zhan, A. Q. Zhang, B. L. Zhang, B. X. Zhang, D. H. Zhang, G. Y. Zhang, H. Zhang, H. Zhang, H. C. Zhang, H. H. Zhang, H. H. Zhang, H. Q. Zhang, H. R. Zhang, H. Y. Zhang, J. Zhang, J. Zhang, J. J. Zhang, J. L. Zhang, J. Q. Zhang, J. S. Zhang, J. W. Zhang, J. X. Zhang, J. Y. Zhang, J. Z. Zhang, Jianyu Zhang, L. M. Zhang, Lei Zhang, P. Zhang, Q. Y. Zhang, R. Y. Zhang, S. H. Zhang, Shulei Zhang, X. M. Zhang, X. Y Zhang, X. Y. Zhang, Y. Zhang, Y. Zhang, Y. T. Zhang, Y. H. Zhang, Y. M. Zhang, Yan Zhang, Z. D. Zhang, Z. H. Zhang, Z. L. Zhang, Z. Y. Zhang, Z. Y. Zhang, Z. Z. Zhang, G. Zhao, J. Y. Zhao, J. Z. Zhao, L. Zhao, Lei Zhao, M. G. Zhao, N. Zhao, R. P. Zhao, S. J. Zhao, Y. B. Zhao, Y. X. Zhao, Z. G. Zhao, A. Zhemchugov, B. Zheng, B. M. Zheng, J. P. Zheng, W. J. Zheng, Y. H. Zheng, B. Zhong, X. Zhong, H. Zhou, J. Y. Zhou, L. P. Zhou, S. Zhou, X. Zhou, X. K. Zhou, X. R. Zhou, X. Y. Zhou, Y. Z. Zhou, Z. C. Zhou, A. N. Zhu, J. Zhu, K. Zhu, K. J. Zhu, K. S. Zhu, L. Zhu, L. X. Zhu, S. H. Zhu, T. J. Zhu, W. D. Zhu, Y. C. Zhu, Z. A. Zhu, J. H. Zou, J. Zu"
"An Integrated Optimization and Deep Learning Pipeline for Predicting
  Live Birth Success in IVF Using Feature Optimization and Transformer-Based
  Models","In vitro fertilization (IVF) is a widely utilized assisted reproductive
technology, yet predicting its success remains challenging due to the
multifaceted interplay of clinical, demographic, and procedural factors. This
study develops a robust artificial intelligence (AI) pipeline aimed at
predicting live birth outcomes in IVF treatments. The pipeline uses anonymized
data from 2010 to 2018, obtained from the Human Fertilization and Embryology
Authority (HFEA). We evaluated the prediction performance of live birth success
as a binary outcome (success/failure) by integrating different feature
selection methods, such as principal component analysis (PCA) and particle
swarm optimization (PSO), with different traditional machine learning-based
classifiers including random forest (RF) and decision tree, as well as deep
learning-based classifiers including custom transformer-based model and a tab
transformer model with an attention mechanism. Our research demonstrated that
the best performance was achieved by combining PSO for feature selection with
the TabTransformer-based deep learning model, yielding an accuracy of 99.50%
and an AUC of 99.96%, highlighting its significant performance to predict live
births. This study establishes a highly accurate AI pipeline for predicting
live birth outcomes in IVF, demonstrating its potential to enhance personalized
fertility treatments.",2024-12-27T15:46:59Z,http://arxiv.org/abs/2412.19696v1,"Arezoo Borji, Hossam Haick, Birgit Pohn, Antonia Graf, Jana Zakall, S M Ragib Shahriar Islam, Gernot Kronreif, Daniel Kovatchki, Heinz Strohmer, Sepideh Hatamikia"
"Nonperturbative effects in triple-differential dijet and Z+jet
  production at the LHC","In comparisons of precision collider data to the most accurate highest-order
calculations in perturbative quantum chromodynamics (QCD), it is required to
correct for nonperturbative effects. Such effects are typically studied using
Monte Carlo event generators that complement fixed-order predictions with
perturbative parton showers and models for the nonperturbative effects of the
Underlying Event and hadronisation. Thereby, the final state of collision
events can be predicted at the level of stable particles, which serve as input
for full detector simulations.
  This article investigates the impact of nonperturbative effects on two
processes that may be used for precision determinations of the strong coupling
constant and the proton structure: the triple-differential dijet and Z+jet
production. While nonperturbative effects impact both processes, significant
differences among them are observed and further investigated. Indications are
found that the Underlying Event and hadronisation cannot fully explain these
differences and the perturbative modelling may play a significant role as well.",2024-12-27T15:42:07Z,http://arxiv.org/abs/2412.19694v1,"Stefan Gieseke, Maximilian Horzela, Manjit Kaur, Dari Leonardi, Klaus Rabbertz, Aayushi Singla, Cedric Verstege"
"From prediction to explanation: managing influential negative reviews
  through explainable AI","The profound impact of online reviews on consumer decision-making has made it
crucial for businesses to manage negative reviews. Recent advancements in
artificial intelligence (AI) technology have offered businesses novel and
effective ways to manage and analyze substantial consumer feedback. In response
to the growing demand for explainablility and transparency in AI applications,
this study proposes a novel explainable AI (XAI) algorithm aimed at identifying
influential negative reviews. The experiments conducted on 101,338 restaurant
reviews validate the algorithm's effectiveness and provides understandable
explanations from both the feature-level and word-level perspectives. By
leveraging this algorithm, businesses can gain actionable insights for
predicting, perceiving, and strategically responding to online negative
feedback, fostering improved customer service and mitigating the potential
damage caused by negative reviews.",2024-12-27T15:37:19Z,http://arxiv.org/abs/2412.19692v1,Rongping Shen
"A Review on the Integration of Artificial Intelligence and Medical
  Imaging in IVF Ovarian Stimulation","Artificial intelligence (AI) has emerged as a powerful tool to enhance
decision-making and optimize treatment protocols in in vitro fertilization
(IVF). In particular, AI shows significant promise in supporting
decision-making during the ovarian stimulation phase of the IVF process. This
review evaluates studies focused on the applications of AI combined with
medical imaging in ovarian stimulation, examining methodologies, outcomes, and
current limitations. Our analysis of 13 studies on this topic reveals that,
reveal that while AI algorithms demonstrated notable potential in predicting
optimal hormonal dosages, trigger timing, and oocyte retrieval outcomes, the
medical imaging data utilized predominantly came from two-dimensional (2D)
ultrasound which mainly involved basic quantifications, such as follicle size
and number, with limited use of direct feature extraction or advanced image
analysis techniques. This points to an underexplored opportunity where advanced
image analysis approaches, such as deep learning, and more diverse imaging
modalities, like three-dimensional (3D) ultrasound, could unlock deeper
insights. Additionally, the lack of explainable AI (XAI) in most studies raises
concerns about the transparency and traceability of AI-driven decisions - key
factors for clinical adoption and trust. Furthermore, many studies relied on
single-center designs and small datasets, which limit the generalizability of
their findings. This review highlights the need for integrating advanced
imaging analysis techniques with explainable AI methodologies, as well as the
importance of leveraging multicenter collaborations and larger datasets.
Addressing these gaps has the potential to enhance ovarian stimulation
management, paving the way for efficient, personalized, and data-driven
treatment pathways that improve IVF outcomes.",2024-12-27T15:29:08Z,http://arxiv.org/abs/2412.19688v1,"Jana Zakall, Birgit Pohn, Antonia Graf, Daniel Kovatchki, Arezoo Borji, Ragib Shahriar Islam, Hossam Haick, Heinz Strohmer, Sepideh Hatamikia"
"Boosting Private Domain Understanding of Efficient MLLMs: A Tuning-free,
  Adaptive, Universal Prompt Optimization Framework","Efficient multimodal large language models (EMLLMs), in contrast to
multimodal large language models (MLLMs), reduce model size and computational
costs and are often deployed on resource-constrained devices. However, due to
data privacy concerns, existing open-source EMLLMs rarely have access to
private domain-specific data during the pre-training process, making them
difficult to directly apply in device-specific domains, such as certain
business scenarios. To address this weakness, this paper focuses on the
efficient adaptation of EMLLMs to private domains, specifically in two areas:
1) how to reduce data requirements, and 2) how to avoid parameter fine-tuning.
Specifically, we propose a tun\textbf{\underline{I}}ng-free,
a\textbf{\underline{D}}aptiv\textbf{\underline{E}},
univers\textbf{\underline{AL}} \textbf{\underline{Prompt}} Optimization
Framework, abbreviated as \textit{\textbf{\ourmethod{}}} which consists of two
stages: 1) Predefined Prompt, based on the reinforcement searching strategy,
generate a prompt optimization strategy tree to acquire optimization priors; 2)
Prompt Reflection initializes the prompt based on optimization priors, followed
by self-reflection to further search and refine the prompt. By doing so,
\ourmethod{} elegantly generates the ``ideal prompts'' for processing private
domain-specific data. Note that our method requires no parameter fine-tuning
and only a small amount of data to quickly adapt to the data distribution of
private data. Extensive experiments across multiple tasks demonstrate that our
proposed \ourmethod{} significantly improves both efficiency and performance
compared to baselines.",2024-12-27T15:21:17Z,http://arxiv.org/abs/2412.19684v1,"Jiang Liu, Bolin Li, Haoyuan Li, Tianwei Lin, Wenqiao Zhang, Tao Zhong, Zhelun Yu, Jinghao Wei, Hao Cheng, Hao Jiang, Zheqi Lv, Juncheng Li, Siliang Tang, Yueting Zhuang"
"A Hybrid Technique for Plant Disease Identification and Localisation in
  Real-time","Over the past decade, several image-processing methods and algorithms have
been proposed for identifying plant diseases based on visual data. DNN (Deep
Neural Networks) have recently become popular for this task. Both traditional
image processing and DNN-based methods encounter significant performance issues
in real-time detection owing to computational limitations and a broad spectrum
of plant disease features. This article proposes a novel technique for
identifying and localising plant disease based on the Quad-Tree decomposition
of an image and feature learning simultaneously. The proposed algorithm
significantly improves accuracy and faster convergence in high-resolution
images with relatively low computational load. Hence it is ideal for deploying
the algorithm in a standalone processor in a remotely operated image
acquisition and disease detection system, ideally mounted on drones and robots
working on large agricultural fields. The technique proposed in this article is
hybrid as it exploits the advantages of traditional image processing methods
and DNN-based models at different scales, resulting in faster inference. The F1
score is approximately 0.80 for four disease classes corresponding to potato
and tomato crops.",2024-12-27T15:20:45Z,http://arxiv.org/abs/2412.19682v1,"Mahendra Kumar Gohil, Anirudha Bhattacharjee, Rwik Rana, Kishan Lal, Samir Kumar Biswas, Nachiketa Tiwari, Bishakh Bhattacharya"
Identifying clusters in Czekanowski's diagram,"Visualizing data through Czekanowski's diagram has as its aim the
illustration of the relationships between objects. Often, obvious clusters of
observations are directly visible. However, it is not straightforward to
precisely delineate these clusters. This paper presents the development of the
package RMaCzek, which now includes features for cluster identification in
Czekanowski diagrams.",2024-12-27T15:04:06Z,http://arxiv.org/abs/2412.19679v1,"Krzysztof Bartoszek, Ying Luo"
Engineering Digital Systems for Humanity: a Research Roadmap,"As testified by new regulations like the European AI Act, worries about the
human and societal impact of (autonomous) software technologies are becoming of
public concern. Human, societal, and environmental values, alongside
traditional software quality, are increasingly recognized as essential for
sustainability and long-term well-being. Traditionally, systems are engineered
taking into account business goals and technology drivers. Considering the
growing awareness in the community, in this paper, we argue that engineering of
systems should also consider human, societal, and environmental drivers. Then,
we identify the macro and technological challenges by focusing on humans and
their role while co-existing with digital systems. The first challenge
considers humans in a proactive role when interacting with digital systems,
i.e., taking initiative in making things happen instead of reacting to events.
The second concerns humans having a reactive role in interacting with digital
systems, i.e., humans interacting with digital systems as a reaction to events.
The third challenge focuses on humans with a passive role, i.e., they
experience, enjoy or even suffer the decisions and/or actions of digital
systems. The fourth challenge concerns the duality of trust and
trustworthiness, with humans playing any role. Building on the new human,
societal, and environmental drivers and the macro and technological challenges,
we identify a research roadmap of digital systems for humanity. The research
roadmap is concretized in a number of research directions organized into four
groups: development process, requirements engineering, software architecture
and design, and verification and validation.",2024-12-27T14:28:39Z,http://arxiv.org/abs/2412.19668v1,"Marco Autili, Martina De Sanctis, Paola Inverardi, Patrizio Pelliccione"
"Asymmetrical Reciprocity-based Federated Learning for Resolving
  Disparities in Medical Diagnosis","Geographic health disparities pose a pressing global challenge, particularly
in underserved regions of low- and middle-income nations. Addressing this issue
requires a collaborative approach to enhance healthcare quality, leveraging
support from medically more developed areas. Federated learning emerges as a
promising tool for this purpose. However, the scarcity of medical data and
limited computation resources in underserved regions make collaborative
training of powerful machine learning models challenging. Furthermore, there
exists an asymmetrical reciprocity between underserved and developed regions.
To overcome these challenges, we propose a novel cross-silo federated learning
framework, named FedHelp, aimed at alleviating geographic health disparities
and fortifying the diagnostic capabilities of underserved regions.
Specifically, FedHelp leverages foundational model knowledge via one-time API
access to guide the learning process of underserved small clients, addressing
the challenge of insufficient data. Additionally, we introduce a novel
asymmetric dual knowledge distillation module to manage the issue of asymmetric
reciprocity, facilitating the exchange of necessary knowledge between developed
large clients and underserved small clients. We validate the effectiveness and
utility of FedHelp through extensive experiments on both medical image
classification and segmentation tasks. The experimental results demonstrate
significant performance improvement compared to state-of-the-art baselines,
particularly benefiting clients in underserved regions.",2024-12-27T13:59:58Z,http://arxiv.org/abs/2412.19654v1,"Jiaqi Wang, Ziyi Yin, Quanzeng You, Lingjuan Lyu, Fenglong Ma"
"Distributed Download from an External Data Source in Faulty Majority
  Settings","We extend the study of retrieval problems in distributed networks, focusing
on improving the efficiency and resilience of protocols in the \emph{Data
Retrieval (DR) Model}. The DR Model consists of a complete network (i.e., a
clique) with $k$ peers, up to $\beta k$ of which may be Byzantine (for $\beta
\in [0, 1)$), and a trusted \emph{External Data Source} comprising an array $X$
of $n$ bits ($n \gg k$) that the peers can query. Additionally, the peers can
also send messages to each other. In this work, we focus on the Download
problem that requires all peers to learn $X$. Our primary goal is to minimize
the maximum number of queries made by any honest peer and additionally optimize
time.
  We begin with a randomized algorithm for the Download problem that achieves
optimal query complexity up to a logarithmic factor. For the stronger dynamic
adversary that can change the set of Byzantine peers from one round to the
next, we achieve the optimal time complexity in peer-to-peer communication but
with larger messages. In broadcast communication where all peers (including
Byzantine peers) are required to send the same message to all peers, with
larger messages, we achieve almost optimal time and query complexities for a
dynamic adversary. Finally, in a more relaxed crash fault model, where peers
stop responding after crashing, we address the Download problem in both
synchronous and asynchronous settings. Using a deterministic protocol, we
obtain nearly optimal results for both query complexity and message sizes in
these scenarios.",2024-12-27T13:55:00Z,http://arxiv.org/abs/2412.19649v1,"John Augustine, Soumyottam Chatterjee, Valerie King, Manish Kumar, Shachar Meir, David Peleg"
"Enhancing Vision-Language Tracking by Effectively Converting Textual
  Cues into Visual Cues","Vision-Language Tracking (VLT) aims to localize a target in video sequences
using a visual template and language description. While textual cues enhance
tracking potential, current datasets typically contain much more image data
than text, limiting the ability of VLT methods to align the two modalities
effectively. To address this imbalance, we propose a novel plug-and-play method
named CTVLT that leverages the strong text-image alignment capabilities of
foundation grounding models. CTVLT converts textual cues into interpretable
visual heatmaps, which are easier for trackers to process. Specifically, we
design a textual cue mapping module that transforms textual cues into target
distribution heatmaps, visually representing the location described by the
text. Additionally, the heatmap guidance module fuses these heatmaps with the
search image to guide tracking more effectively. Extensive experiments on
mainstream benchmarks demonstrate the effectiveness of our approach, achieving
state-of-the-art performance and validating the utility of our method for
enhanced VLT.",2024-12-27T13:54:32Z,http://arxiv.org/abs/2412.19648v1,"X. Feng, D. Zhang, S. Hu, X. Li, M. Wu, J. Zhang, X. Chen, K. Huang"
"Chimera: A Block-Based Neural Architecture Search Framework for
  Event-Based Object Detection","Event-based cameras are sensors that simulate the human eye, offering
advantages such as high-speed robustness and low power consumption. Established
Deep Learning techniques have shown effectiveness in processing event data.
Chimera is a Block-Based Neural Architecture Search (NAS) framework
specifically designed for Event-Based Object Detection, aiming to create a
systematic approach for adapting RGB-domain processing methods to the event
domain. The Chimera design space is constructed from various macroblocks,
including Attention blocks, Convolutions, State Space Models, and
MLP-mixer-based architectures, which provide a valuable trade-off between local
and global processing capabilities, as well as varying levels of complexity.
The results on the PErson Detection in Robotics (PEDRo) dataset demonstrated
performance levels comparable to leading state-of-the-art models, alongside an
average parameter reduction of 1.6 times.",2024-12-27T13:50:44Z,http://arxiv.org/abs/2412.19646v1,"Diego A. Silva, Ahmed Elsheikh, Kamilya Smagulova, Mohammed E. Fouda, Ahmed M. Eltawil"
A Brief Overlook on Magnetoplasmadynamic Thrusters,"This paper presents a comprehensive analysis of Magnetoplasmadynamic
Thrusters (MPDT), examining their working principles, performance
characteristics, and potential applications in space propulsion. The study
focuses on both self-field and applied-field MPDT variants, detailing the
fundamental physics of plasma generation, acceleration mechanisms through
Lorentz forces, and plasma detachment processes. Through mathematical modeling
and experimental data analysis, the paper demonstrates MPDTs' capability to
achieve high specific impulse and efficient propellant utilization compared to
chemical propulsion systems. While highlighting their advantages for deep space
missions and satellite operations, the study also addresses key challenges,
including high power requirements and thermal management issues. The research
concludes that despite current technological limitations, MPDTs show promising
potential for future space exploration, particularly for long-duration missions
requiring sustained thrust.",2024-12-27T13:30:13Z,http://arxiv.org/abs/2412.19636v1,Egemen Gover
"Primordial Black Hole Formation from the Upward Step Model: Avoiding
  Overproduction","We investigate the formation of primordial black holes (PBHs) in an upward
step inflationary model, where nonlinearities between curvature perturbations
and field fluctuations introduce a cutoff, deviating from the Gaussian case.
This necessitates a reevaluation of PBH formation, as $\mathcal{R}$ is not the
optimal variable for estimating abundance. Using the extended Press-Schechter
formalism, we show that non-Gaussianity modifies both the curvature
perturbation profile $\mathcal{R}(r)$ and the integration path in probability
space, significantly impacting PBH abundance. Our results reveal that the
abundance initially increases with the parameter $h$, which characterizes the
relaxation stage after the step. However, beyond a critical value ($h \simeq
5.9$), it sharply declines before rising again. Furthermore, we demonstrate
that non-Gaussianity introduces uncertainties in indirect PBH observations via
gravitational waves. Notably, we present an example where a positive $f_{\rm
NL}$ does not necessarily enhance PBH production, contrary to conventional
expectations. Finally, by accounting for non-perturbative effects, we resolve
the overproduction of PBHs suggested by pulsar timing array (PTA) data,
underscoring the critical importance of incorporating non-Gaussianity in future
studies.",2024-12-27T13:21:06Z,http://arxiv.org/abs/2412.19631v1,"Xiaoding Wang, Xiao-Han Ma, Yi-Fu Cai"
"Measurement of the branching fraction, polarization, and time-dependent
  $CP$ asymmetry in $B^0 \to ρ^+ρ^-$ decays and constraint on the CKM
  angle $φ_2$","We present a measurement of the branching fraction and fraction of
longitudinal polarization of $B^0 \to \rho^+ \rho^-$ decays, which have two
$\pi^0$'s in the final state. We also measure time-dependent $CP$ violation
parameters for decays into longitudinally polarized $\rho^+ \rho^-$ pairs. This
analysis is based on a data sample containing $(387\pm6) \times 10^6$ \BBbar
pairs collected with the Belle~II detector at the SuperKEKB asymmetric-energy
$e^+e^-$ collider in 2019-2022. We obtain ${B}(B^0\to\rho^+\rho^-) = (2.88
^{+0.23}_{-0.22} {}^{+0.29}_{-0.27}) \times 10^{-5}, f_{L} = 0.921
^{+0.024}_{-0.025} {}^{+0.017}_{-0.015}$, $S = -0.26\pm0.19\pm0.08$, and $C =
-0.02\pm0.12^{+0.06}_{-0.05}$, where the first uncertainties are statistical
and the second are systematic. We use these results to perform an isospin
analysis to constrain the CKM angle $\phi_2$ and obtain two solutions; the
result consistent with other Standard Model constraints is $\phi_2 =
(92.6^{+4.5}_{-4.8})^\circ$.",2024-12-27T13:00:10Z,http://arxiv.org/abs/2412.19624v1,"Belle II Collaboration, I. Adachi, L. Aggarwal, H. Ahmed, N. Akopov, M. Alhakami, A. Aloisio, N. Althubiti, N. Anh Ky, D. M. Asner, H. Atmacan, V. Aushev, M. Aversano, R. Ayad, V. Babu, N. K. Baghel, P. Bambade, Sw. Banerjee, M. Barrett, M. Bartl, J. Baudot, A. Baur, A. Beaubien, J. Becker, J. V. Bennett, V. Bertacchi, M. Bertemes, E. Bertholet, M. Bessner, S. Bettarini, B. Bhuyan, D. Biswas, A. Bobrov, D. Bodrov, A. Bolz, A. Bondar, J. Borah, A. Boschetti, A. Bozek, M. Bračko, P. Branchini, R. A. Briere, T. E. Browder, A. Budano, S. Bussino, Q. Campagna, M. Campajola, G. Casarosa, C. Cecchi, J. Cerasoli, M. -C. Chang, P. Chang, R. Cheaib, P. Cheema, B. G. Cheon, K. Chilikin, K. Chirapatpimol, H. -E. Cho, K. Cho, S. -J. Cho, S. -K. Choi, S. Choudhury, J. Cochran, L. Corona, J. X. Cui, E. De La Cruz-Burelo, S. A. De La Motte, G. De Nardo, G. De Pietro, R. de Sangro, M. Destefanis, S. Dey, F. Di Capua, J. Dingfelder, Z. Doležal, I. Domínguez Jiménez, T. V. Dong, X. Dong, M. Dorigo, D. Dossett, K. Dugic, G. Dujany, P. Ecker, J. Eppelt, P. Feichtinger, T. Ferber, T. Fillinger, C. Finck, G. Finocchiaro, A. Fodor, F. Forti, B. G. Fulsom, A. Gabrielli, E. Ganiev, M. Garcia-Hernandez, R. Garg, G. Gaudino, V. Gaur, A. Gaz, A. Gellrich, G. Ghevondyan, D. Ghosh, H. Ghumaryan, G. Giakoustidis, R. Giordano, A. Giri, P. Gironella Gironell, A. Glazov, B. Gobbo, R. Godang, O. Gogota, P. Goldenzweig, W. Gradl, E. Graziani, D. Greenwald, Z. Gruberová, Y. Guan, K. Gudkova, I. Haide, T. Hara, C. Harris, K. Hayasaka, S. Hazra, C. Hearty, M. T. Hedges, A. Heidelbach, I. Heredia de la Cruz, M. Hernández Villanueva, T. Higuchi, M. Hoek, M. Hohmann, R. Hoppe, P. Horak, C. -L. Hsu, T. Humair, T. Iijima, K. Inami, N. Ipsita, A. Ishikawa, R. Itoh, M. Iwasaki, D. Jacobi, W. W. Jacobs, E. -J. Jang, Y. Jin, A. Johnson, H. Junkerkalefeld, M. Kaleta, A. B. Kaliyar, J. Kandra, F. Keil, C. Ketter, C. Kiesling, C. -H. Kim, D. Y. Kim, J. -Y. Kim, K. -H. Kim, Y. -K. Kim, K. Kinoshita, P. Kodyš, T. Koga, S. Kohani, K. Kojima, A. Korobov, S. Korpar, E. Kovalenko, R. Kowalewski, P. Križan, P. Krokovny, T. Kuhr, Y. Kulii, R. Kumar, K. Kumara, T. Kunigo, A. Kuzmin, Y. -J. Kwon, S. Lacaprara, K. Lalwani, T. Lam, L. Lanceri, J. S. Lange, T. S. Lau, M. Laurenza, R. Leboucher, F. R. Le Diberder, M. J. Lee, C. Lemettais, P. Leo, L. K. Li, Q. M. Li, W. Z. Li, Y. Li, Y. B. Li, Y. P. Liao, J. Libby, J. Lin, S. Lin, M. H. Liu, Q. Y. Liu, Z. Q. Liu, D. Liventsev, S. Longo, T. Lueck, C. Lyu, Y. Ma, C. Madaan, M. Maggiora, S. P. Maharana, R. Maiti, G. Mancinelli, R. Manfredi, E. Manoni, M. Mantovano, D. Marcantonio, S. Marcello, C. Marinas, C. Martellini, A. Martens, A. Martini, T. Martinov, L. Massaccesi, M. Masuda, K. Matsuoka, D. Matvienko, S. K. Maurya, M. Maushart, J. A. McKenna, F. Meier, D. Meleshko, M. Merola, C. Miller, M. Mirra, S. Mitra, K. Miyabayashi, H. Miyake, G. B. Mohanty, S. Mondal, S. Moneta, H. -G. Moser, R. Mussa, I. Nakamura, M. Nakao, Y. Nakazawa, M. Naruki, Z. Natkaniec, A. Natochii, M. Nayak, G. Nazaryan, M. Neu, S. Nishida, S. Ogawa, R. Okubo, H. Ono, Y. Onuki, G. Pakhlova, S. Pardi, K. Parham, H. Park, J. Park, K. Park, S. -H. Park, A. Passeri, S. Patra, T. K. Pedlar, I. Peruzzi, R. Peschke, R. Pestotnik, L. E. Piilonen, P. L. M. Podesta-Lerma, T. Podobnik, S. Pokharel, C. Praz, S. Prell, E. Prencipe, M. T. Prim, H. Purwar, S. Raiz, K. Ravindran, J. U. Rehman, M. Reif, S. Reiter, M. Remnev, L. Reuter, D. Ricalde Herrmann, I. Ripp-Baudot, G. Rizzo, M. Roehrken, J. M. Roney, A. Rostomyan, N. Rout, Y. Sakai, D. A. Sanders, S. Sandilya, L. Santelj, V. Savinov, B. Scavino, C. Schwanda, A. J. Schwartz, Y. Seino, A. Selce, K. Senyo, J. Serrano, M. E. Sevior, C. Sfienti, W. Shan, X. D. Shi, T. Shillington, J. -G. Shiu, D. Shtol, B. Shwartz, A. Sibidanov, F. Simon, J. Skorupa, R. J. Sobie, M. Sobotzik, A. Soffer, A. Sokolov, E. Solovieva, S. Spataro, B. Spruck, W. Song, M. Starič, P. Stavroulakis, S. Stefkova, R. Stroili, J. Strube, M. Sumihama, K. Sumisawa, N. Suwonjandee, H. Svidras, M. Takizawa, U. Tamponi, K. Tanida, F. Tenchini, A. Thaller, O. Tittel, R. Tiwary, E. Torassa, K. Trabelsi, I. Tsaklidis, I. Ueda, T. Uglov, K. Unger, Y. Unno, K. Uno, S. Uno, P. Urquijo, Y. Ushiroda, S. E. Vahsen, R. van Tonder, K. E. Varvell, M. Veronesi, A. Vinokurova, V. S. Vismaya, L. Vitale, V. Vobbilisetti, R. Volpe, M. Wakai, S. Wallner, M. -Z. Wang, A. Warburton, M. Watanabe, S. Watanuki, C. Wessel, E. Won, X. P. Xu, B. D. Yabsley, S. Yamada, W. Yan, J. Yelton, J. H. Yin, K. Yoshihara, J. Yuan, Y. Yusa, L. Zani, V. Zhilich, J. S. Zhou, Q. D. Zhou, L. Zhu, R. Žlebčík"
Signatures of prediction during natural listening in MEG data?,"The brain uses contextual information and prior knowledge to anticipate
upcoming content during language comprehension. Recent research has shown
predictive signals can be revealed in pre-onset ECoG activity during
naturalistic narrative listening, by building encoding models based on word
embeddings from Large Language Models (LLMs). Similarly, evidence for
long-range predictive encoding has been observed in fMRI data, where
incorporating embeddings for multiple upcoming words in a narrative improves
alignment with brain activity. This study examines whether similar predictive
information can be detected in MEG, a technique with higher temporal resolution
than fMRI but a lower signal-to-noise ratio than ECoG. Our findings indicate
that MEG captures pre-onset representations up to 1 second before word onset,
consistent with ECoG results. However, unlike fMRI findings, incorporating
future word embeddings did not enhance MEG encoding, even for one word into the
future, which suggests that the pre-onset encoding may not reflect predictive
processing. This work demonstrates that MEG combined with LLMs is a valuable
approach for studying language processing in naturalistic narratives and
highlights the need to study further what constitutes evidence for prediction
during natural listening.",2024-12-27T12:49:03Z,http://arxiv.org/abs/2412.19622v1,"Sahel Azizpour, Britta U. Westner, Jakub Szewczyk, Umut Güçlü, Linda Geerligs"
"The Key Steps and Distinct Performance Trends of Pyrrolic vs. Pyridinic
  M-N-C Catalysts in Electrocatalytic Nitrate Reduction","Electrochemical nitrate reduction reaction(NO3RR)offers a sustainable route
for ambient ammonia synthesis. While metal-nitrogen-carbon (M-N-C) single-atom
catalysts have emerged as promising candidates for NO3RR, the
structure-activity relations underlying their catalytic behavior remain to be
elucidated. Through systematic analysis of reported experimental data and
pH-field coupled microkinetic modelling on a reversible hydrogen electrode
(RHE) scale, we reveal that the coordination-dependent activity originates from
distinct scaling relations governed by metal-intermediate interactions.
M-N-Pyrrolic catalysts demonstrate higher turnover frequencies for ammonia
production, whereas M-N-Pyridinic catalysts exhibit broader activity ranges
across the activity volcano plot. Meanwhile, the adsorption and protonation of
nitrate, which is a step often dismissed and/or assumed to be simultaneous in
many previous reports, is identified to be the rate-determining step (RDS) in
NO3RR. Remarkably, our subsequent experimental validation confirms the
theoretical predictions under both neutral and alkaline conditions. This study
offers a comprehensive mechanistic framework for interpreting the
electrocatalytic activity of M-N-C catalysts in NO3RR, showing that a classical
thermodynamic limiting-potential model is not sufficiently accurate to capture
the RDS and the catalytic performance trends of different materials (even on
M-N-Pyrrolic and M-N-Pyridinic catalysts). These findings provide brand new
insights into the reaction mechanism of NO3RR and establish fundamental design
principles for electrocatalytic ammonia synthesis.",2024-12-27T12:23:09Z,http://arxiv.org/abs/2412.19615v1,"Qiuling Jiang, Mingyao Gu, Tianyi Wang, Fangzhou Liu, Xin Yang, Di Zhang, Zhijian Wu, Ying Wang, Li Wei, Hao Li"
"Enhancing Fine-grained Image Classification through Attentive Batch
  Training","Fine-grained image classification, which is a challenging task in computer
vision, requires precise differentiation among visually similar object
categories. In this paper, we propose 1) a novel module called Residual
Relationship Attention (RRA) that leverages the relationships between images
within each training batch to effectively integrate visual feature vectors of
batch images and 2) a novel technique called Relationship Position Encoding
(RPE), which encodes the positions of relationships between original images in
a batch and effectively preserves the relationship information between images
within the batch. Additionally, we design a novel framework, namely
Relationship Batch Integration (RBI), which utilizes RRA in conjunction with
RPE, allowing the discernment of vital visual features that may remain elusive
when examining a singular image representative of a particular class. Through
extensive experiments, our proposed method demonstrates significant
improvements in the accuracy of different fine-grained classifiers, with an
average increase of $(+2.78\%)$ and $(+3.83\%)$ on the CUB200-2011 and Stanford
Dog datasets, respectively, while achieving a state-of-the-art results
$(95.79\%)$ on the Stanford Dog dataset. Despite not achieving the same level
of improvement as in fine-grained image classification, our method still
demonstrates its prowess in leveraging general image classification by
attaining a state-of-the-art result of $(93.71\%)$ on the Tiny-Imagenet
dataset. Furthermore, our method serves as a plug-in refinement module and can
be easily integrated into different networks.",2024-12-27T12:07:58Z,http://arxiv.org/abs/2412.19606v1,"Duy M. Le, Bao Q. Bui, Anh Tran, Cong Tran, Cuong Pham"
Super-bath Quantum Eigensolver,"Simulating the dynamics of a system coupled to a suitable environment is a
promising approach in quantum computing for determining the ground state of
physical systems. However, this approach requires not only the
$\textit{existence}$ of an environment that allows the system to dissipate
energy and evolve to its ground state, but also the environment's
characteristics to be $\textit{known}$ in detail. In this paper, we propose an
algorithm with a sufficient condition for achieving polynomial-time complexity
in ground state preparation: the existence of an environment that enables the
system to evolve to its ground state in polynomial time, while such
environment's details may remain $\textit{unknown}$. The proposed algorithm is
Super-bath Quantum Eigensolver, which solves the system's ground state by
utilizing quasi-steady state preparation and simulating the coupling between
the system and the super-bath. Supported by experimental lifetime data of
nuclear metastable states, we suggest that our algorithm is applicable to
determine nuclear ground states in polynomial time. These results highlight the
potential advantage of quantum computing in addressing ground state problems in
real-world physical systems.",2024-12-27T11:46:47Z,http://arxiv.org/abs/2412.19599v1,"Tianren Wang, Zongkang Zhang, Bing-Nan Lu, Mauro Cirio, Ying Li"
Composite nature of the $T_{cc}$ state,"In 2021, LHCb collaboration reported a very narrow state in the $D^0D^0\pi^+$
mass spectrum just below the $D^{*+}D^0$ mass threshold. We consider the
influence of the Castillejo-Dalitz-Dyson (CDD) pole in the scattering amplitude
to derive a general treatment for the two-body final state interaction near its
threshold. The line shape (or the energy dependent event distribution) are then
obtained, where the parameters can be fixed by fitting to the experimental data
on the $D^0D^0\pi^+$ mass spectrum. Within our method the data are quite well
reproduced. The pole structure in the complex energy plane indicates the bound
state structure of the $T_{cc}$ state. The compositeness as a measure of
molecule component in its hadron wave function is predicted to be
$0.23_{-0.09}^{+0.40}$. The non-molecular component, e.g., the compact
tetraquark also takes a non-negligible portion.",2024-12-27T11:42:44Z,http://arxiv.org/abs/2412.19597v1,"Xian-Wei Kang, Wen-Shuo Ding"
A counterexample to a Brenti-Carnevale conjecture,"Recently, F. Brenti put a preprint on the arXiv with several interesting open
problems on Coxeter groups and unimodality. In this note, we refute one of
these conjectures with a counterexample and provide supporting data related to
it. This work serves as an initial step toward further exploration of the
topic.",2024-12-27T11:26:53Z,http://arxiv.org/abs/2412.19593v1,"Nathan Chapelier-Laget, Jean Fromentin"
DAS3R: Dynamics-Aware Gaussian Splatting for Static Scene Reconstruction,"We propose a novel framework for scene decomposition and static background
reconstruction from everyday videos. By integrating the trained motion masks
and modeling the static scene as Gaussian splats with dynamics-aware
optimization, our method achieves more accurate background reconstruction
results than previous works. Our proposed method is termed DAS3R, an
abbreviation for Dynamics-Aware Gaussian Splatting for Static Scene
Reconstruction. Compared to existing methods, DAS3R is more robust in complex
motion scenarios, capable of handling videos where dynamic objects occupy a
significant portion of the scene, and does not require camera pose inputs or
point cloud data from SLAM-based methods. We compared DAS3R against recent
distractor-free approaches on the DAVIS and Sintel datasets; DAS3R demonstrates
enhanced performance and robustness with a margin of more than 2 dB in PSNR.
The project's webpage can be accessed via \url{https://kai422.github.io/DAS3R/}",2024-12-27T10:59:46Z,http://arxiv.org/abs/2412.19584v1,"Kai Xu, Tze Ho Elden Tse, Jizong Peng, Angela Yao"
"A Comparative Study of Machine Unlearning Techniques for Image and Text
  Classification Models","Machine Unlearning has emerged as a critical area in artificial intelligence,
addressing the need to selectively remove learned data from machine learning
models in response to data privacy regulations. This paper provides a
comprehensive comparative analysis of six state-of-theart unlearning techniques
applied to image and text classification tasks. We evaluate their performance,
efficiency, and compliance with regulatory requirements, highlighting their
strengths and limitations in practical scenarios. By systematically analyzing
these methods, we aim to provide insights into their applicability,
challenges,and tradeoffs, fostering advancements in the field of ethical and
adaptable machine learning.",2024-12-27T10:58:55Z,http://arxiv.org/abs/2412.19583v1,"Omar M. Safa, Mahmoud M. Abdelaziz, Mustafa Eltawy, Mohamed Mamdouh, Moamen Gharib, Salaheldin Eltenihy, Nagia M. Ghanem, Mohamed M. Ismail"
"Readout of strongly coupled NV center-pair spin states with deep neural
  networks","Optically addressable electron spin clusters are of interest for quantum
computation, simulation and sensing. However, with interaction length scales of
a few tens of nanometers in the strong coupling regime, they are unresolved in
conventional confocal microscopy, making individual readout problematic. Here
we show that when using a single shot readout technique, collective states of
the combined register space become accessible. By using spin to charge
conversion of the defects we draw the connection between the intricate photon
count statistics with spin state tomography using deep neural networks. This
approach is particularly versatile with further scaling the number of
constituent spins in a cluster due to complexity of the analytical treatment.
We perform a proof of concept measurement of the correlated classical signal,
paving the way for using our technique in realistic applications.",2024-12-27T10:56:04Z,http://arxiv.org/abs/2412.19581v1,"Matthew Joliffe, Vadim Vorobyov, Jörg Wrachtrup"
Stamps and Mathematics,"This study examines the potential of using math-themed postage stamps in
mathematics lessons as a tool to engage students and integrate the subject with
history, art, and culture. Since the first mathematical stamps appeared in the
early 20th century, featuring prominent scholars like Carl Friedrich Gauss and
Isaac Newton, they serve not only as philatelic artifacts but also as
historical carriers of knowledge. The paper presents several practical projects
to interest students, such as creating their own math stamps, investigating the
price trends of math-themed stamps, and developing a timeline of mathematical
discoveries depicted in philatelic issues. The proposed projects develop
students' mathematical skills in areas such as percentage calculations, general
arithmetic, working with time intervals, and statistical analysis. Students can
analyze shapes, symmetry, and patterns on stamps, study principles of
proportion, and explore geometric figures. Using stamps broadens students'
horizons, providing an opportunity to become familiar with renowned
mathematicians from different eras, countries, and cultures. This also offers
students a new perspective on the subject, presenting mathematical discoveries
as part of the world's cultural heritage. Postage stamps dedicated to
mathematics can become a powerful tool for visualizing theoretical knowledge,
stimulating interest in mathematics, and encouraging independent research among
students.",2024-12-27T10:52:54Z,http://arxiv.org/abs/2412.19579v1,Nataliya M. Ivanova
"Gauging or extending bulk and boundary conformal field theories:
  Application to bulk and domain wall problem in topological matter and their
  descriptions by (mock) modular covariant","We study gauging operations (or group extensions) in (smeared) boundary
conformal field theories (BCFTs) and bulk conformal field theories and their
applications to various phenomena in topologically ordered systems. We apply
the resultant theories to the correspondence between the renormalization group
(RG) flow of CFTs and the classification of topological quantum field theories
in the testable information of general classes of partition functions. One can
obtain the bulk topological properties of $2+1$ dimensional topological ordered
phase corresponding to the massive RG flow of $1+1$ dimensional systems, or
smeared BCFT. We present an obstruction of mass condensation for smeared BCFT
analogous to the Lieb-Shultz-Mattis theorem for noninvertible symmetry. Related
to the bulk topological degeneracies in $2+1$ dimensions and quantum phases in
$1+1$ dimensions we construct a new series of BCFT. We also investigate the
implications of the massless RG flow of $1+1$ dimensional CFT to $2+1$
dimensional topological order which corresponds to the earlier proposal by L.
Kong and H. Zheng in [Nucl. Phys. B 966 (2021), 115384], arXiv:1912.01760
closely related to the integer-spin simple current by Schellekens and
Gato-Rivera. We study the properties of the product of two CFTs connected by
the two kinds of massless flows. The (mock) modular covariants appearing in the
analysis seem to contain new ones. By applying the folding trick to the coupled
model, we provide a general method to solve the gapped and charged domain wall.
One can obtain the general phenomenology of the transportation of anyons
through the domain wall. Our work gives a unified direction for the future
theoretical and numerical studies of the topological phase based on the
established data of classifications of conformal field theories or modular
invariants.",2024-12-27T10:46:30Z,http://arxiv.org/abs/2412.19577v1,Yoshiki Fukusumi
"The possible long-term periodic variability of the extremely luminous
  quasar WISE J090924.01+000211.1","The extremely luminous infrared galaxy (ELIRG), WISE J090924.01+000211.1
(hereafter; WISE J0909+0002, $z=1.87$) is an extraordinary object with a quasar
aspect. This study performs monitoring observations of WISE J0909+0002 with the
105 cm Murikabushi telescope, Okayama and Akeno 50 cm telescopes/MITSuME ($g'$,
$R_{\rm c}$, and $I_{\rm c}$ bands), and the SaCRA 55 cm telescope/MuSaSHI
($r$, $i$, and $z$ bands). We obtain the following results by combining the
UV/optical light curves of the CRTS, Pan-STARRS, and ZTF archive data, and our
observational data: (1) the light curves of WISE J0909+0002 present
quasi-periodic (sinusoidal) oscillations with the rest-frame period of $\sim$
660$-$689 day; (2) the structure functions of WISE J0909+0002 do not show a
damped random walk (DRW) trend; (3) the mock DRW light curves present
periodic-like trend on rare occasions in 10000 simulations; (4) the
relativistic boost scenario is favored, since the relation between variability
amplitude and power-law slope ratio is consistent with the theoretical
prediction of this scenario, and a substantial parameter space exists between
the inclination angles and the black hole mass; (5) the circumbinary disk model
is difficult to explain the spectral energy distribution of our target; (6) the
significant radio flux density of WISE J0909+0002 is not detected from the VLA
FIRST Survey, thus the radio jet precession scenario is ruled out. From our
results, the Doppler boost scenario is likely as a cause of the periodic
variability, consequently the quasi-periodic oscillations in WISE J0909+0002 is
possibly interpreted by a supermassive blackhole binary. Additional
observations to investigate the continuity of the periodic trend would bring
new insights into mechanisms of the quasi-periodic oscillations and/or ELIRGs.",2024-12-27T10:33:11Z,http://arxiv.org/abs/2412.19573v1,"Takashi Horiuchi, Yoshiki Toba, Toru Misawa, Katsuhiro L. Murata, Keisuke Isogai, Yoichi Yatsu, Ichiro Takahashi, Mahito Sasada, Masafumi Niwano, Narikazu Higuchi, Shunsuke Hayatsu, Hibiki Seki, Yumiko Oasa, Rikuto Sato"
"Nonminimally coupled Dark Matter in Clusters of Galaxies: a fully
  comprehensive analysis","In this study, we explore how a non-minimal coupling between dark matter and
gravity can affect the behavior of dark matter in galaxy clusters. We have
considered the case of a disformal coupling, which leads to a modification of
the Poisson equation. Building on an earlier work, we expand the analysis
considering all possible disformal coupling scenarios and employing various
dark matter density profiles. In doing so, we aim to constrain the key
parameter in our model, the characteristic coupling length. To achieve this, we
analyze data from a combination of strong and weak lensing using three
statistical approaches: a single cluster fitting procedure, a joint analysis,
and one with stacked profiles. Our findings show that the coupling length is
typically very small, thus being fully consistent with general relativity,
although with an upper limit at $1\sigma$ which is of the order of $100$ kpc.",2024-12-27T10:12:29Z,http://arxiv.org/abs/2412.19569v1,"Saboura Zamani, Vincenzo Salzano, Dario Bettoni"
"Reinforced Label Denoising for Weakly-Supervised Audio-Visual Video
  Parsing","Audio-visual video parsing (AVVP) aims to recognize audio and visual event
labels with precise temporal boundaries, which is quite challenging since audio
or visual modality might include only one event label with only the overall
video labels available. Existing label denoising models often treat the
denoising process as a separate preprocessing step, leading to a disconnect
between label denoising and AVVP tasks. To bridge this gap, we present a novel
joint reinforcement learning-based label denoising approach (RLLD). This
approach enables simultaneous training of both label denoising and video
parsing models through a joint optimization strategy. We introduce a novel
AVVP-validation and soft inter-reward feedback mechanism that directly guides
the learning of label denoising policy. Extensive experiments on AVVP tasks
demonstrate the superior performance of our proposed method compared to label
denoising techniques. Furthermore, by incorporating our label denoising method
into other AVVP models, we find that it can further enhance parsing results.",2024-12-27T10:05:56Z,http://arxiv.org/abs/2412.19563v1,"Yongbiao Gao, Xiangcheng Sun, Guohua Lv, Deng Yu, Sijiu Niu"
Quantiles under ambiguity and risk sharing,"Choquet capacities and integrals are central concepts in decision making
under ambiguity or model uncertainty, pioneered by Schmeidler. Motivated by
risk optimization problems for quantiles under ambiguity, we study the subclass
of Choquet integrals, called Choquet quantiles, which generalizes the usual
(probabilistic) quantiles, also known as Value-at-Risk in finance, from
probabilities to capacities. Choquet quantiles share many features with
probabilistic quantiles, in terms of axiomatic representation, optimization
formulas, and risk sharing. We characterize Choquet quantiles via only one
axiom, called ordinality. We prove that the inf-convolution of Choquet
quantiles is again a Choquet quantile, leading to explicit optimal allocations
in risk sharing problems for quantile agents under ambiguity. A new class of
risk measures, Choquet Expected Shortfall, is introduced, which enjoys most
properties of the coherent risk measure Expected Shortfall. Our theory is
complemented by optimization algorithms, numerical examples, and a stylized
illustration with financial data.",2024-12-27T09:22:19Z,http://arxiv.org/abs/2412.19546v1,"Peng Liu, Tiantian Mao, Ruodu Wang"
"Enhancing Media Literacy: The Effectiveness of (Human) Annotations and
  Bias Visualizations on Bias Detection","Marking biased texts is a practical approach to increase media bias awareness
among news consumers. However, little is known about the generalizability of
such awareness to new topics or unmarked news articles, and the role of
machine-generated bias labels in enhancing awareness remains unclear. This
study tests how news consumers may be trained and pre-bunked to detect media
bias with bias labels obtained from different sources ( (Human or AI) and in
various manifestations. We conducted two experiments with 470 and 846
participants, exposing them to various bias-labeling conditions. We
subsequently tested how much bias they could identify in unlabeled news
materials on new topics. The results show that both Human (t(467) = 4.55, p &lt;
.001, d = 0.42) and AI labels (t(467) = 2.49, p = .039, d = 0.23) increased
correct detection compared to the control group. Human labels demonstrate
larger effect sizes and higher statistical significance. The control group
(t(467) = 4.51, p &lt; .001, d = 0.21) also improves performance through mere
exposure to study materials. We also find that participants trained with marked
biased phrases detected bias most reliably (F(834,1) = 44.00, p &lt; .001,
{\eta}2part = 0.048). Our experimental framework provides theoretical
implications for systematically assessing the generalizability of learning
effects in identifying media bias. These findings also provide practical
implications for developing news-reading platforms that offer bias indicators
and designing media literacy curricula to enhance media bias awareness.",2024-12-27T09:19:22Z,http://arxiv.org/abs/2412.19545v1,"Timo Spinde, Fei Wu, Wolfgang Gaissmaier, Gianluca Demartini, Helge Giese"
"TARGA: Targeted Synthetic Data Generation for Practical Reasoning over
  Structured Data","Semantic parsing, which converts natural language questions into logic forms,
plays a crucial role in reasoning within structured environments. However,
existing methods encounter two significant challenges: reliance on extensive
manually annotated datasets and limited generalization capability to unseen
examples. To tackle these issues, we propose Targeted Synthetic Data Generation
(TARGA), a practical framework that dynamically generates high-relevance
synthetic data without manual annotation. Starting from the pertinent entities
and relations of a given question, we probe for the potential relevant queries
through layer-wise expansion and cross-layer combination. Then we generate
corresponding natural language questions for these constructed queries to
jointly serve as the synthetic demonstrations for in-context learning.
Experiments on multiple knowledge base question answering (KBQA) datasets
demonstrate that TARGA, using only a 7B-parameter model, substantially
outperforms existing non-fine-tuned methods that utilize close-sourced model,
achieving notable improvements in F1 scores on GrailQA(+7.7) and
KBQA-Agent(+12.2). Furthermore, TARGA also exhibits superior sample efficiency,
robustness, and generalization capabilities under non-I.I.D. settings.",2024-12-27T09:16:39Z,http://arxiv.org/abs/2412.19544v1,"Xiang Huang, Jiayu Shen, Shanshan Huang, Sitao Cheng, Xiaxia Wang, Yuzhong Qu"
Diverse Rare Sample Generation with Pretrained GANs,"Deep generative models are proficient in generating realistic data but
struggle with producing rare samples in low density regions due to their
scarcity of training datasets and the mode collapse problem. While recent
methods aim to improve the fidelity of generated samples, they often reduce
diversity and coverage by ignoring rare and novel samples. This study proposes
a novel approach for generating diverse rare samples from high-resolution image
datasets with pretrained GANs. Our method employs gradient-based optimization
of latent vectors within a multi-objective framework and utilizes normalizing
flows for density estimation on the feature space. This enables the generation
of diverse rare images, with controllable parameters for rarity, diversity, and
similarity to a reference image. We demonstrate the effectiveness of our
approach both qualitatively and quantitatively across various datasets and GANs
without retraining or fine-tuning the pretrained GANs.",2024-12-27T09:10:30Z,http://arxiv.org/abs/2412.19543v1,"Subeen Lee, Jiyeon Han, Soyeon Kim, Jaesik Choi"
Interacted Object Grounding in Spatio-Temporal Human-Object Interactions,"Spatio-temporal Human-Object Interaction (ST-HOI) understanding aims at
detecting HOIs from videos, which is crucial for activity understanding.
However, existing whole-body-object interaction video benchmarks overlook the
truth that open-world objects are diverse, that is, they usually provide
limited and predefined object classes. Therefore, we introduce a new open-world
benchmark: Grounding Interacted Objects (GIO) including 1,098 interacted
objects class and 290K interacted object boxes annotation. Accordingly, an
object grounding task is proposed expecting vision systems to discover
interacted objects. Even though today's detectors and grounding methods have
succeeded greatly, they perform unsatisfactorily in localizing diverse and rare
objects in GIO. This profoundly reveals the limitations of current vision
systems and poses a great challenge. Thus, we explore leveraging
spatio-temporal cues to address object grounding and propose a 4D
question-answering framework (4D-QA) to discover interacted objects from
diverse videos. Our method demonstrates significant superiority in extensive
experiments compared to current baselines. Data and code will be publicly
available at https://github.com/DirtyHarryLYL/HAKE-AVA.",2024-12-27T09:08:46Z,http://arxiv.org/abs/2412.19542v1,"Xiaoyang Liu, Boran Wen, Xinpeng Liu, Zizheng Zhou, Hongwei Fan, Cewu Lu, Lizhuang Ma, Yulong Chen, Yong-Lu Li"
"Finger in Camera Speaks Everything: Unconstrained Air-Writing for
  Real-World","Air-writing is a challenging task that combines the fields of computer vision
and natural language processing, offering an intuitive and natural approach for
human-computer interaction. However, current air-writing solutions face two
primary challenges: (1) their dependency on complex sensors (e.g., Radar, EEGs
and others) for capturing precise handwritten trajectories, and (2) the absence
of a video-based air-writing dataset that covers a comprehensive vocabulary
range. These limitations impede their practicality in various real-world
scenarios, including the use on devices like iPhones and laptops. To tackle
these challenges, we present the groundbreaking air-writing Chinese character
video dataset (AWCV-100K-UCAS2024), serving as a pioneering benchmark for
video-based air-writing. This dataset captures handwritten trajectories in
various real-world scenarios using commonly accessible RGB cameras, eliminating
the need for complex sensors. AWCV-100K-UCAS2024 includes 8.8 million video
frames, encompassing the complete set of 3,755 characters from the GB2312-80
level-1 set (GB1). Furthermore, we introduce our baseline approach, the
video-based character recognizer (VCRec). VCRec adeptly extracts fingertip
features from sparse visual cues and employs a spatio-temporal sequence module
for analysis. Experimental results showcase the superior performance of VCRec
compared to existing models in recognizing air-written characters, both
quantitatively and qualitatively. This breakthrough paves the way for enhanced
human-computer interaction in real-world contexts. Moreover, our approach
leverages affordable RGB cameras, enabling its applicability in a diverse range
of scenarios. The code and data examples will be made public at
https://github.com/wmeiqi/AWCV.",2024-12-27T09:04:04Z,http://arxiv.org/abs/2412.19537v1,"Meiqi Wu, Kaiqi Huang, Yuanqiang Cai, Shiyu Hu, Yuzhong Zhao, Weiqiang Wang"
Is Your Text-to-Image Model Robust to Caption Noise?,"In text-to-image (T2I) generation, a prevalent training technique involves
utilizing Vision Language Models (VLMs) for image re-captioning. Even though
VLMs are known to exhibit hallucination, generating descriptive content that
deviates from the visual reality, the ramifications of such caption
hallucinations on T2I generation performance remain under-explored. Through our
empirical investigation, we first establish a comprehensive dataset comprising
VLM-generated captions, and then systematically analyze how caption
hallucination influences generation outcomes. Our findings reveal that (1) the
disparities in caption quality persistently impact model outputs during
fine-tuning. (2) VLMs confidence scores serve as reliable indicators for
detecting and characterizing noise-related patterns in the data distribution.
(3) even subtle variations in caption fidelity have significant effects on the
quality of learned representations. These findings collectively emphasize the
profound impact of caption quality on model performance and highlight the need
for more sophisticated robust training algorithm in T2I. In response to these
observations, we propose a approach leveraging VLM confidence score to mitigate
caption noise, thereby enhancing the robustness of T2I models against
hallucination in caption.",2024-12-27T08:53:37Z,http://arxiv.org/abs/2412.19531v1,"Weichen Yu, Ziyan Yang, Shanchuan Lin, Qi Zhao, Jianyi Wang, Liangke Gui, Matt Fredrikson, Lu Jiang"
"Exploiting Domain-Specific Parallel Data on Multilingual Language Models
  for Low-resource Language Translation","Neural Machine Translation (NMT) systems built on multilingual
sequence-to-sequence Language Models (msLMs) fail to deliver expected results
when the amount of parallel data for a language, as well as the language's
representation in the model are limited. This restricts the capabilities of
domain-specific NMT systems for low-resource languages (LRLs). As a solution,
parallel data from auxiliary domains can be used either to fine-tune or to
further pre-train the msLM. We present an evaluation of the effectiveness of
these two techniques in the context of domain-specific LRL-NMT. We also explore
the impact of domain divergence on NMT model performance. We recommend several
strategies for utilizing auxiliary parallel data in building domain-specific
NMT models for LRLs.",2024-12-27T08:25:52Z,http://arxiv.org/abs/2412.19522v1,"Surangika Ranathungaa, Shravan Nayak, Shih-Ting Cindy Huang, Yanke Mao, Tong Su, Yun-Hsiang Ray Chan, Songchen Yuan, Anthony Rinaldi, Annie En-Shiun Lee"
"Improved measurements of neutron lifetime with cold neutron beam at
  J-PARC","The ``neutron lifetime puzzle'' arises from the discrepancy between neutron
lifetime measurements obtained using the beam method, which measures decay
products, and the bottle method, which measures the disappearance of neutrons.
To resolve this puzzle, we conducted an experiment using a pulsed cold neutron
beam at J-PARC. In this experiment, the neutron lifetime is determined from the
ratio of neutron decay counts to $^3$He(n,p)$^3$H reactions in a gas detector.
This experiment belongs to the beam method but differs from previous
experiments that measured protons, as it instead detects electrons, enabling
measurements with distinct systematic uncertainties. By enlarging the beam
transport system and reducing systematic uncertainties, we achieved a fivefold
improvement in precision. Analysis of all acquired data yielded a neutron
lifetime of $\tau_{\rm n}=877.2~\pm~1.7_{\rm(stat.)}~^{+4.0}_{-3.6}{}_{\rm
(sys.)}$ s. This result is consistent with bottle method measurements but
exhibits a 2.3$\sigma$ tension with the average value obtained from the
proton-detection-based beam method.",2024-12-27T08:19:54Z,http://arxiv.org/abs/2412.19519v1,"Y. Fuwa, T. Hasegawa, K. Hirota, T. Hoshino, R. Hosokawa, G. Ichikawa, S. Ieki, T. Ino, Y. Iwashita, M. Kitaguchi, R. Kitahara, S. Makise, K. Mishima, T. Mogi, N. Nagakura, H. Oide, H. Okabe, H. Otono, Y. Seki, D. Sekiba, T. Shima, H. E. Shimizu, H. M. Shimizu, N. Sumi, H. Sumino, M. Tanida, H. Uehara, T. Yamada, S. Yamashita, K. Yano, T. Yoshioka"
"Estimation of System Parameters Including Repeated Cross-Sectional Data
  through Emulator-Informed Deep Generative Model","Differential equations (DEs) are crucial for modeling the evolution of
natural or engineered systems. Traditionally, the parameters in DEs are
adjusted to fit data from system observations. However, in fields such as
politics, economics, and biology, available data are often independently
collected at distinct time points from different subjects (i.e., repeated
cross-sectional (RCS) data). Conventional optimization techniques struggle to
accurately estimate DE parameters when RCS data exhibit various
heterogeneities, leading to a significant loss of information. To address this
issue, we propose a new estimation method called the emulator-informed
deep-generative model (EIDGM), designed to handle RCS data. Specifically, EIDGM
integrates a physics-informed neural network-based emulator that immediately
generates DE solutions and a Wasserstein generative adversarial network-based
parameter generator that can effectively mimic the RCS data. We evaluated EIDGM
on exponential growth, logistic population models, and the Lorenz system,
demonstrating its superior ability to accurately capture parameter
distributions. Additionally, we applied EIDGM to an experimental dataset of
Amyloid beta 40 and beta 42, successfully capturing diverse parameter
distribution shapes. This shows that EIDGM can be applied to model a wide range
of systems and extended to uncover the operating principles of systems based on
limited data.",2024-12-27T08:19:23Z,http://arxiv.org/abs/2412.19517v1,"Hyunwoo Cho, Sung Woong Cho, Hyeontae Jo, Hyung Ju Hwang"
"Confidence v.s. Critique: A Decomposition of Self-Correction Capability
  for LLMs","Large Language Models (LLMs) can correct their self-generated responses, but
a decline in accuracy after self-correction is also witnessed. To have a deeper
understanding of self-correction, we endeavor to decompose, evaluate, and
analyze the self-correction behaviors of LLMs. By enumerating and analyzing
answer correctness before and after self-correction, we decompose the
self-correction capability into confidence (being confident to correct answers)
and critique (turning wrong answers to correct) capabilities, and propose two
metrics from a probabilistic perspective to measure these 2 capabilities, along
with another metric for overall self-correction capability evaluation. Based on
our decomposition and evaluation metrics, we conduct extensive experiments and
draw some empirical conclusions. For example, we find different models can
exhibit distinct behaviors: some models are confident while others are more
critical. We also find the trade-off between the two capabilities (i.e.
improving one can lead to a decline in the other) when manipulating model
self-correction behavior by prompts or in-context learning. Further, we find a
simple yet efficient strategy to improve self-correction capability by
transforming Supervision Fine-Tuning (SFT) data format, and our strategy
outperforms vanilla SFT in both capabilities and achieves much higher accuracy
after self-correction. Our code will be publicly available on GitHub.",2024-12-27T08:09:11Z,http://arxiv.org/abs/2412.19513v1,"Zhe Yang, Yichang Zhang, Yudong Wang, Ziyao Xu, Junyang Lin, Zhifang Sui"
Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging,"Fine-tuning large language models (LLMs) for downstream tasks is a widely
adopted approach, but it often leads to safety degradation in safety-aligned
LLMs. Currently, many solutions address this issue by incorporating additional
safety data, which can be impractical in many cases. In this paper, we address
the question: How can we improve downstream task performance while preserving
safety in LLMs without relying on additional safety data? We propose a simple
and effective method that maintains the inherent safety of LLMs while enhancing
their downstream task performance: merging the weights of pre- and
post-fine-tuned safety-aligned models. Experimental results across various
downstream tasks, models, and merging methods demonstrate that this approach
effectively mitigates safety degradation while improving downstream task
performance, offering a practical solution for adapting safety-aligned LLMs.",2024-12-27T08:03:22Z,http://arxiv.org/abs/2412.19512v1,"Hua Farn, Hsuan Su, Shachi H Kumar, Saurav Sahay, Shang-Tse Chen, Hung-yi Lee"
Hybrid Local Causal Discovery,"Local causal discovery aims to learn and distinguish the direct causes and
effects of a target variable from observed data. Existing constraint-based
local causal discovery methods use AND or OR rules in constructing the local
causal skeleton, but using either rule alone is prone to produce cascading
errors in the learned local causal skeleton, and thus impacting the inference
of local causal relationships. On the other hand, directly applying score-based
global causal discovery methods to local causal discovery may randomly return
incorrect results due to the existence of local equivalence classes. To address
the above issues, we propose a Hybrid Local Causal Discovery algorithm, called
HLCD. Specifically, HLCD initially utilizes a constraint-based approach
combined with the OR rule to obtain a candidate skeleton and then employs a
score-based method to eliminate redundant portions in the candidate skeleton.
Furthermore, during the local causal orientation phase, HLCD distinguishes
between V-structures and equivalence classes by comparing the local structure
scores between the two, thereby avoiding orientation interference caused by
local equivalence classes. We conducted extensive experiments with seven
state-of-the-art competitors on 14 benchmark Bayesian network datasets, and the
experimental results demonstrate that HLCD significantly outperforms existing
local causal discovery algorithms.",2024-12-27T07:53:59Z,http://arxiv.org/abs/2412.19507v1,"Zhaolong Ling, Honghui Peng, Yiwen Zhang, Peng Zhou, Xingyu Wu, Kui Yu, Xindong Wu"
"DrivingWorld: ConstructingWorld Model for Autonomous Driving via Video
  GPT","Recent successes in autoregressive (AR) generation models, such as the GPT
series in natural language processing, have motivated efforts to replicate this
success in visual tasks. Some works attempt to extend this approach to
autonomous driving by building video-based world models capable of generating
realistic future video sequences and predicting ego states. However, prior
works tend to produce unsatisfactory results, as the classic GPT framework is
designed to handle 1D contextual information, such as text, and lacks the
inherent ability to model the spatial and temporal dynamics essential for video
generation. In this paper, we present DrivingWorld, a GPT-style world model for
autonomous driving, featuring several spatial-temporal fusion mechanisms. This
design enables effective modeling of both spatial and temporal dynamics,
facilitating high-fidelity, long-duration video generation. Specifically, we
propose a next-state prediction strategy to model temporal coherence between
consecutive frames and apply a next-token prediction strategy to capture
spatial information within each frame. To further enhance generalization
ability, we propose a novel masking strategy and reweighting strategy for token
prediction to mitigate long-term drifting issues and enable precise control.
Our work demonstrates the ability to produce high-fidelity and consistent video
clips of over 40 seconds in duration, which is over 2 times longer than
state-of-the-art driving world models. Experiments show that, in contrast to
prior works, our method achieves superior visual quality and significantly more
accurate controllable future video generation. Our code is available at
https://github.com/YvanYin/DrivingWorld.",2024-12-27T07:44:07Z,http://arxiv.org/abs/2412.19505v1,"Xiaotao Hu, Wei Yin, Mingkai Jia, Junyuan Deng, Xiaoyang Guo, Qian Zhang, Xiaoxiao Long, Ping Tan"
"Multi-P$^2$A: A Multi-perspective Benchmark on Privacy Assessment for
  Large Vision-Language Models","Large Vision-Language Models (LVLMs) exhibit impressive potential across
various tasks but also face significant privacy risks, limiting their practical
applications. Current researches on privacy assessment for LVLMs is limited in
scope, with gaps in both assessment dimensions and privacy categories. To
bridge this gap, we propose Multi-P$^2$A, a comprehensive benchmark for
evaluating the privacy preservation capabilities of LVLMs in terms of privacy
awareness and leakage. Privacy awareness measures the model's ability to
recognize the privacy sensitivity of input data, while privacy leakage assesses
the risk of the model unintentionally disclosing privacy information in its
output. We design a range of sub-tasks to thoroughly evaluate the model's
privacy protection offered by LVLMs. Multi-P$^2$A covers 26 categories of
personal privacy, 15 categories of trade secrets, and 18 categories of state
secrets, totaling 31,962 samples. Based on Multi-P$^2$A, we evaluate the
privacy preservation capabilities of 21 open-source and 2 closed-source LVLMs.
Our results reveal that current LVLMs generally pose a high risk of
facilitating privacy breaches, with vulnerabilities varying across personal
privacy, trade secret, and state secret.",2024-12-27T07:33:39Z,http://arxiv.org/abs/2412.19496v1,"Jie Zhang, Xiangkui Cao, Zhouyu Han, Shiguang Shan, Xilin Chen"
"Disparate Model Performance and Stability in Machine Learning Clinical
  Support for Diabetes and Heart Diseases","Machine Learning (ML) algorithms are vital for supporting clinical
decision-making in biomedical informatics. However, their predictive
performance can vary across demographic groups, often due to the
underrepresentation of historically marginalized populations in training
datasets. The investigation reveals widespread sex- and age-related inequities
in chronic disease datasets and their derived ML models. Thus, a novel
analytical framework is introduced, combining systematic arbitrariness with
traditional metrics like accuracy and data complexity. The analysis of data
from over 25,000 individuals with chronic diseases revealed mild sex-related
disparities, favoring predictive accuracy for males, and significant
age-related differences, with better accuracy for younger patients. Notably,
older patients showed inconsistent predictive accuracy across seven datasets,
linked to higher data complexity and lower model performance. This highlights
that representativeness in training data alone does not guarantee equitable
outcomes, and model arbitrariness must be addressed before deploying models in
clinical settings.",2024-12-27T07:31:14Z,http://arxiv.org/abs/2412.19495v1,"Ioannis Bilionis, Ricardo C. Berrios, Luis Fernandez-Luque, Carlos Castillo"
User Willingness-aware Sales Talk Dataset,"User willingness is a crucial element in the sales talk process that affects
the achievement of the salesperson's or sales system's objectives. Despite the
importance of user willingness, to the best of our knowledge, no previous study
has addressed the development of automated sales talk dialogue systems that
explicitly consider user willingness. A major barrier is the lack of sales talk
datasets with reliable user willingness data. Thus, in this study, we developed
a user willingness-aware sales talk collection by leveraging the ecological
validity concept, which is discussed in the field of human-computer
interaction. Our approach focused on three types of user willingness essential
in real sales interactions. We created a dialogue environment that closely
resembles real-world scenarios to elicit natural user willingness, with
participants evaluating their willingness at the utterance level from multiple
perspectives. We analyzed the collected data to gain insights into practical
user willingness-aware sales talk strategies. In addition, as a practical
application of the constructed dataset, we developed and evaluated a sales
dialogue system aimed at enhancing the user's intent to purchase.",2024-12-27T07:16:10Z,http://arxiv.org/abs/2412.19490v1,"Asahi Hentona, Jun Baba, Shiki Sato, Reina Akama"
RAIN: Real-time Animation of Infinite Video Stream,"Live animation has gained immense popularity for enhancing online engagement,
yet achieving high-quality, real-time, and stable animation with diffusion
models remains challenging, especially on consumer-grade GPUs. Existing methods
struggle with generating long, consistent video streams efficiently, often
being limited by latency issues and degraded visual quality over extended
periods. In this paper, we introduce RAIN, a pipeline solution capable of
animating infinite video streams in real-time with low latency using a single
RTX 4090 GPU. The core idea of RAIN is to efficiently compute frame-token
attention across different noise levels and long time-intervals while
simultaneously denoising a significantly larger number of frame-tokens than
previous stream-based methods. This design allows RAIN to generate video frames
with much shorter latency and faster speed, while maintaining long-range
attention over extended video streams, resulting in enhanced continuity and
consistency. Consequently, a Stable Diffusion model fine-tuned with RAIN in
just a few epochs can produce video streams in real-time and low latency
without much compromise in quality or consistency, up to infinite long. Despite
its advanced capabilities, the RAIN only introduces a few additional 1D
attention blocks, imposing minimal additional burden. Experiments in benchmark
datasets and generating super-long videos demonstrating that RAIN can animate
characters in real-time with much better quality, accuracy, and consistency
than competitors while costing less latency. All code and models will be made
publicly available.",2024-12-27T07:13:15Z,http://arxiv.org/abs/2412.19489v1,"Zhilei Shu, Ruili Feng, Yang Cao, Zheng-Jun Zha"
Learning Radiance Fields from a Single Snapshot Compressive Image,"In this paper, we explore the potential of Snapshot Compressive Imaging (SCI)
technique for recovering the underlying 3D scene structure from a single
temporal compressed image. SCI is a cost-effective method that enables the
recording of high-dimensional data, such as hyperspectral or temporal
information, into a single image using low-cost 2D imaging sensors. To achieve
this, a series of specially designed 2D masks are usually employed, reducing
storage and transmission requirements and offering potential privacy
protection. Inspired by this, we take one step further to recover the encoded
3D scene information leveraging powerful 3D scene representation capabilities
of neural radiance fields (NeRF). Specifically, we propose SCINeRF, in which we
formulate the physical imaging process of SCI as part of the training of NeRF,
allowing us to exploit its impressive performance in capturing complex scene
structures. In addition, we further integrate the popular 3D Gaussian Splatting
(3DGS) framework and propose SCISplat to improve 3D scene reconstruction
quality and training/rendering speed by explicitly optimizing point clouds into
3D Gaussian representations. To assess the effectiveness of our method, we
conduct extensive evaluations using both synthetic data and real data captured
by our SCI system. Experimental results demonstrate that our proposed approach
surpasses the state-of-the-art methods in terms of image reconstruction and
novel view synthesis. Moreover, our method also exhibits the ability to render
high frame-rate multi-view consistent images in real time by leveraging SCI and
the rendering capabilities of 3DGS. Codes will be available at:
https://github.com/WU- CVGL/SCISplat.",2024-12-27T06:40:44Z,http://arxiv.org/abs/2412.19483v1,"Yunhao Li, Xiang Liu, Xiaodong Wang, Xin Yuan, Peidong Liu"
"Pre-training, Fine-tuning and Re-ranking: A Three-Stage Framework for
  Legal Question Answering","Legal question answering (QA) has attracted increasing attention from people
seeking legal advice, which aims to retrieve the most applicable answers from a
large-scale database of question-answer pairs. Previous methods mainly use a
dual-encoder architecture to learn dense representations of both questions and
answers. However, these methods could suffer from lacking domain knowledge and
sufficient labeled training data. In this paper, we propose a three-stage
(\underline{p}re-training, \underline{f}ine-tuning and \underline{r}e-ranking)
framework for \underline{l}egal \underline{QA} (called PFR-LQA), which promotes
the fine-grained text representation learning and boosts the performance of
dense retrieval with the dual-encoder architecture. Concretely, we first
conduct domain-specific pre-training on legal questions and answers through a
self-supervised training objective, allowing the pre-trained model to be
adapted to the legal domain. Then, we perform task-specific fine-tuning of the
dual-encoder on legal question-answer pairs by using the supervised learning
objective, leading to a high-quality dual-encoder for the specific downstream
QA task. Finally, we employ a contextual re-ranking objective to further refine
the output representations of questions produced by the document encoder, which
uses contextual similarity to increase the discrepancy between the anchor and
hard negative samples for better question re-ranking. We conduct extensive
experiments on a manually annotated legal QA dataset. Experimental results show
that our PFR-LQA method achieves better performance than the strong competitors
for legal question answering.",2024-12-27T06:33:42Z,http://arxiv.org/abs/2412.19482v1,"Shiwen Ni, Hao Cheng, Min Yang"
"An Overview of Machine Learning-Driven Resource Allocation in IoT
  Networks","In the wake of disruptive IoT technologies generating massive amounts of
diverse data, Machine Learning (ML) will play a crucial role in bringing
intelligence to Internet of Things (IoT) networks. This paper provides a
comprehensive analysis of the current state of resource allocation within IoT
networks, focusing specifically on two key categories: Low-Power IoT Networks
and Mobile IoT Networks. We delve into the resource allocation strategies that
are crucial for optimizing network performance and energy efficiency in these
environments. Furthermore, the paper explores the transformative role of
Machine Learning (ML), Deep Learning (DL), and Reinforcement Learning (RL) in
enhancing IoT functionalities. We highlight a range of applications and use
cases where these advanced technologies can significantly improve
decision-making and optimization processes. In addition to the opportunities
presented by ML, DL, and RL, we also address the potential challenges that
organizations may face when implementing these technologies in IoT settings.
These challenges include crucial accuracy, low flexibility and adaptability,
and high computational cost, etc. Finally, the paper identifies promising
avenues for future research, emphasizing the need for innovative solutions to
overcome existing hurdles and improve the integration of ML, DL, and RL into
IoT networks. By providing this holistic perspective, we aim to contribute to
the ongoing discourse on resource allocation strategies and the application of
intelligent technologies in the IoT landscape.",2024-12-27T06:11:28Z,http://arxiv.org/abs/2412.19478v1,Zhengdong Li
"Effects of Reynolds number and spatial resolution on the pressure source
  terms in turbulent boundary layers","The increase in wall-pressure fluctuations with increasing friction Reynolds
number ($Re_{\tau}$) of a turbulent boundary layer (TBL) is well known in the
literature. However, very few studies have investigated the
$Re_{\tau}$-variation of the source terms of the pressure fluctuations, which
are solely a function of the spatial velocity gradients within the TBL. This
study quantifies the pressure source terms in a zero-pressure gradient TBL by
utilizing a published direct numerical simulation (DNS; Sillero et al. 2013,
Phys. Fluids) database across 1000 $\lesssim$ $Re_{\tau}$ $\lesssim$ 2000. It
is found that the magnitude of all source terms increases with $Re_{\tau}$
across the entire TBL thickness, with the turbulence-turbulence (non-linear)
interaction terms growing faster than the mean-shear (linear) source terms.
Further, we use the simulation database to mimic the scenario of particle image
velocimetry (PIV) experiments that are typically spatially under-resolved
compared to DNS data. It is used to quantify the effect of spatial resolution
on the accuracy of pressure source terms, which are estimated here for two
common PIV scenarios: (i) planar PIV in the streamwise-wall-normal plane, and
(ii) stereo-PIV in the spanwise-wall-normal plane of a ZPG TBL. This exercise
reveals significant attenuation of all pressure source terms compared to those
estimated from the original DNS, highlighting the challenges of accurately
estimating these source terms in a high $Re_{\tau}$ PIV experiment.",2024-12-27T05:58:49Z,http://arxiv.org/abs/2412.19474v1,"Aditya Agarwal, Rahul Deshpande"
"Knowledge Graph-Based Multi-Agent Path Planning in Dynamic Environments
  using WAITR","This paper addresses the challenge of multi-agent path planning for efficient
data collection in dynamic, uncertain environments, exemplified by autonomous
underwater vehicles (AUVs) navigating the Gulf of Mexico. Traditional greedy
algorithms, though computationally efficient, often fall short in long-term
planning due to their short-sighted nature, missing crucial data collection
opportunities and increasing exposure to hazards. To address these limitations,
we introduce WAITR (Weighted Aggregate Inter-Temporal Reward), a novel
path-planning framework that integrates a knowledge graph with pathlet-based
planning, segmenting the environment into dynamic, speed-adjusted sub-regions
(pathlets). This structure enables coordinated, adaptive planning, as agents
can operate within time-bound regions while dynamically responding to
environmental changes. WAITR's cumulative scoring mechanism balances immediate
data collection with long-term optimization of Points of Interest (POIs),
ensuring safer navigation and comprehensive data coverage. Experimental results
show that WAITR substantially improves POI coverage and reduces exposure to
hazards, achieving up to 27.1\% greater event coverage than traditional greedy
methods.",2024-12-27T05:43:41Z,http://arxiv.org/abs/2412.19469v1,"Ted Edward Holmberg, Elias Ioup, Mahdi Abdelguerfi"
A Prototype Unit for Image De-raining using Time-Lapse Data,"We address the challenge of single-image de-raining, a task that involves
recovering rain-free background information from a single rain image. While
recent advancements have utilized real-world time-lapse data for training,
enabling the estimation of consistent backgrounds and realistic rain streaks,
these methods often suffer from computational and memory consumption, limiting
their applicability in real-world scenarios. In this paper, we introduce a
novel solution: the Rain Streak Prototype Unit (RsPU). The RsPU efficiently
encodes rain streak-relevant features as real-time prototypes derived from
time-lapse data, eliminating the need for excessive memory resources. Our
de-raining network combines encoder-decoder networks with the RsPU, allowing us
to learn and encapsulate diverse rain streak-relevant features as concise
prototypes, employing an attention-based approach. To ensure the effectiveness
of our approach, we propose a feature prototype loss encompassing cohesion and
divergence components. This loss function captures both the compactness and
diversity aspects of the prototypical rain streak features within the RsPU. Our
method evaluates various de-raining benchmarks, accompanied by comprehensive
ablation studies. We show that it can achieve competitive results in various
rain images compared to state-of-the-art methods.",2024-12-27T05:04:56Z,http://arxiv.org/abs/2412.19459v1,"Jaehoon Cho, Minjung Yoo, Jini Yang, Sunok Kim"
"DriveEditor: A Unified 3D Information-Guided Framework for Controllable
  Object Editing in Driving Scenes","Vision-centric autonomous driving systems require diverse data for robust
training and evaluation, which can be augmented by manipulating object
positions and appearances within existing scene captures. While recent
advancements in diffusion models have shown promise in video editing, their
application to object manipulation in driving scenarios remains challenging due
to imprecise positional control and difficulties in preserving high-fidelity
object appearances. To address these challenges in position and appearance
control, we introduce DriveEditor, a diffusion-based framework for object
editing in driving videos. DriveEditor offers a unified framework for
comprehensive object editing operations, including repositioning, replacement,
deletion, and insertion. These diverse manipulations are all achieved through a
shared set of varying inputs, processed by identical position control and
appearance maintenance modules. The position control module projects the given
3D bounding box while preserving depth information and hierarchically injects
it into the diffusion process, enabling precise control over object position
and orientation. The appearance maintenance module preserves consistent
attributes with a single reference image by employing a three-tiered approach:
low-level detail preservation, high-level semantic maintenance, and the
integration of 3D priors from a novel view synthesis model. Extensive
qualitative and quantitative evaluations on the nuScenes dataset demonstrate
DriveEditor's exceptional fidelity and controllability in generating diverse
driving scene edits, as well as its remarkable ability to facilitate downstream
tasks.",2024-12-27T04:49:36Z,http://arxiv.org/abs/2412.19458v1,"Yiyuan Liang, Zhiying Yan, Liqun Chen, Jiahuan Zhou, Luxin Yan, Sheng Zhong, Xu Zou"
Focusing Image Generation to Mitigate Spurious Correlations,"Instance features in images exhibit spurious correlations with background
features, affecting the training process of deep neural classifiers. This leads
to insufficient attention to instance features by the classifier, resulting in
erroneous classification outcomes. In this paper, we propose a data
augmentation method called Spurious Correlations Guided Synthesis (SCGS) that
mitigates spurious correlations through image generation model. This approach
does not require expensive spurious attribute (group) labels for the training
data and can be widely applied to other debiasing methods. Specifically, SCGS
first identifies the incorrect attention regions of a pre-trained classifier on
the training images, and then uses an image generation model to generate new
training data based on these incorrect attended regions. SCGS increases the
diversity and scale of the dataset to reduce the impact of spurious
correlations on classifiers. Changes in the classifier's attention regions and
experimental results on three different domain datasets demonstrate that this
method is effective in reducing the classifier's reliance on spurious
correlations.",2024-12-27T04:48:56Z,http://arxiv.org/abs/2412.19457v1,"Xuewei Li, Zhenzhen Nie, Mei Yu, Zijian Zhang, Jie Gao, Tianyi Xu, Zhiqiang Liu"
"NijiGAN: Transform What You See into Anime with Contrastive
  Semi-Supervised Learning and Neural Ordinary Differential Equations","Generative AI has transformed the animation industry. Several models have
been developed for image-to-image translation, particularly focusing on
converting real-world images into anime through unpaired translation.
Scenimefy, a notable approach utilizing contrastive learning, achieves high
fidelity anime scene translation by addressing limited paired data through
semi-supervised training. However, it faces limitations due to its reliance on
paired data from a fine-tuned StyleGAN in the anime domain, often producing
low-quality datasets. Additionally, Scenimefy's high parameter architecture
presents opportunities for computational optimization. This research introduces
NijiGAN, a novel model incorporating Neural Ordinary Differential Equations
(NeuralODEs), which offer unique advantages in continuous transformation
modeling compared to traditional residual networks. NijiGAN successfully
transforms real-world scenes into high fidelity anime visuals using half of
Scenimefy's parameters. It employs pseudo-paired data generated through
Scenimefy for supervised training, eliminating dependence on low-quality paired
data and improving the training process. Our comprehensive evaluation includes
ablation studies, qualitative, and quantitative analysis comparing NijiGAN to
similar models. The testing results demonstrate that NijiGAN produces
higher-quality images compared to AnimeGAN, as evidenced by a Mean Opinion
Score (MOS) of 2.192, it surpasses AnimeGAN's MOS of 2.160. Furthermore, our
model achieved a Frechet Inception Distance (FID) score of 58.71, outperforming
Scenimefy's FID score of 60.32. These results demonstrate that NijiGAN achieves
competitive performance against existing state-of-the-arts, especially
Scenimefy as the baseline model.",2024-12-27T04:46:44Z,http://arxiv.org/abs/2412.19455v1,"Kevin Putra Santoso, Anny Yuniarti, Dwiyasa Nakula, Dimas Prihady Setyawan, Adam Haidar Azizi, Jeany Aurellia P. Dewati, Farah Dhia Fadhila, Maria T. Elvara Bumbungan"
"Feature Alignment-Based Knowledge Distillation for Efficient Compression
  of Large Language Models","This study proposes a knowledge distillation algorithm based on large
language models and feature alignment, aiming to effectively transfer the
knowledge of large pre-trained models into lightweight student models, thereby
reducing computational costs while maintaining high model performance.
Different from the traditional soft label distillation method, this method
introduces a multi-layer feature alignment strategy to deeply align the
intermediate features and attention mechanisms of the teacher model and the
student model, maximally retaining the semantic expression ability and context
modeling ability of the teacher model. In terms of method design, a multi-task
loss function is constructed, including feature matching loss, attention
alignment loss, and output distribution matching loss, to ensure multi-level
information transfer through joint optimization. The experiments were
comprehensively evaluated on the GLUE data set and various natural language
processing tasks. The results show that the proposed model performs very close
to the state-of-the-art GPT-4 model in terms of evaluation indicators such as
perplexity, BLEU, ROUGE, and CER. At the same time, it far exceeds baseline
models such as DeBERTa, XLNet, and GPT-3, showing significant performance
improvements and computing efficiency advantages. Research results show that
the feature alignment distillation strategy is an effective model compression
method that can significantly reduce computational overhead and storage
requirements while maintaining model capabilities. Future research can be
further expanded in the directions of self-supervised learning, cross-modal
feature alignment, and multi-task transfer learning to provide more flexible
and efficient solutions for the deployment and optimization of deep learning
models.",2024-12-27T04:37:06Z,http://arxiv.org/abs/2412.19449v1,"Shuo Wang, Chihang Wang, Jia Gao, Zhen Qi, Hongye Zheng, Xiaoxuan Liao"
"Adrenaline: Adaptive Rendering Optimization System for Scalable Cloud
  Gaming","Cloud gaming requires a low-latency network connection, making it a prime
candidate for being hosted at the network edge. However, an edge server is
provisioned with a fixed compute capacity, causing an issue for multi-user
service and resulting in users having to wait before they can play when the
server is occupied. In this work, we present a new insight that when a user's
network condition results in use of lossy compression, the end-to-end visual
quality more degrades for frames of high rendering quality, wasting the
server's computing resources. We leverage this observation to build Adrenaline,
a new system which adaptively optimizes the game rendering qualities by
considering the user-side visual quality and server-side rendering cost. The
rendering quality optimization of Adrenaline is done via a scoring mechanism
quantifying the effectiveness of server resource usage on the user-side gaming
quality. Our open-sourced implementation of Adrenaline demonstrates easy
integration with modern game engines. In our evaluations, Adrenaline achieves
up to 24% higher service quality and 2x more users served with the same
resource footprint compared to other baselines.",2024-12-27T04:25:32Z,http://arxiv.org/abs/2412.19446v1,"Jin Heo, Ketan Bhardwaj, Ada Gavrilovska"
"Comparative Performance Analysis of Quantum Machine Learning
  Architectures for Credit Card Fraud Detection","As financial fraud becomes increasingly complex, effective detection methods
are essential. Quantum Machine Learning (QML) introduces certain capabilities
that may enhance both accuracy and efficiency in this area. This study examines
how different quantum feature map and ansatz configurations affect the
performance of three QML-based classifiers-the Variational Quantum Classifier
(VQC), the Sampler Quantum Neural Network (SQNN), and the Estimator Quantum
Neural Network (EQNN)-when applied to two non-standardized financial fraud
datasets. Different quantum feature map and ansatz configurations are
evaluated, revealing distinct performance patterns. The VQC consistently
demonstrates strong classification results, achieving an F1 score of 0.88,
while the SQNN also delivers promising outcomes. In contrast, the EQNN
struggles to produce robust results, emphasizing the challenges presented by
non-standardized data. These findings highlight the importance of careful model
configuration in QML-based financial fraud detection. By showing how specific
feature maps and ansatz choices influence predictive success, this work guides
researchers and practitioners in refining QML approaches for complex financial
applications.",2024-12-27T04:17:34Z,http://arxiv.org/abs/2412.19441v1,"Mansour El Alami, Nouhaila Innan, Muhammad Shafique, Mohamed Bennai"
"Paleoinspired Vision: From Exploring Colour Vision Evolution to
  Inspiring Camera Design","The evolution of colour vision is captivating, as it reveals the adaptive
strategies of extinct species while simultaneously inspiring innovations in
modern imaging technology. In this study, we present a simplified model of
visual transduction in the retina, introducing a novel opsin layer. We quantify
evolutionary pressures by measuring machine vision recognition accuracy on
colour images shaped by specific opsins. Building on this, we develop an
evolutionary conservation optimisation algorithm to reconstruct the spectral
sensitivity of opsins, enabling mutation-driven adaptations to to more
effectively spot fruits or predators. This model condenses millions of years of
evolution within seconds on GPU, providing an experimental framework to test
long-standing hypotheses in evolutionary biology , such as vision of early
mammals, primate trichromacy from gene duplication, retention of colour
blindness, blue-shift of fish rod and multiple rod opsins with bioluminescence.
Moreover, the model enables speculative explorations of hypothetical species,
such as organisms with eyes adapted to the conditions on Mars. Our findings
suggest a minimalist yet effective approach to task-specific camera filter
design, optimising the spectral response function to meet application-driven
demands. The code will be made publicly available upon acceptance.",2024-12-27T04:07:52Z,http://arxiv.org/abs/2412.19439v1,"Junjie Zhang, Zhimin Zong, Lin Gu, Shenghan Su, Ziteng Cui, Yan Pu, Zirui Chen, Jing Lu, Daisuke Kojima, Tatsuya Harada, Ruogu Fang"
"Residual Feature-Reutilization Inception Network for Image
  Classification","Capturing feature information effectively is of great importance in the field
of computer vision. With the development of convolutional neural networks
(CNNs), concepts like residual connection and multiple scales promote continual
performance gains in diverse deep learning vision tasks. In this paper, we
propose a novel CNN architecture that it consists of residual
feature-reutilization inceptions (ResFRI) or split-residual
feature-reutilization inceptions (Split-ResFRI). And it is composed of four
convolutional combinations of different structures connected by specially
designed information interaction passages, which are utilized to extract
multi-scale feature information and effectively increase the receptive field of
the model. Moreover, according to the network structure designed above,
Split-ResFRI can adjust the segmentation ratio of the input information,
thereby reducing the number of parameters and guaranteeing the model
performance. Specifically, in experiments based on popular vision datasets,
such as CIFAR10 ($97.94$\%), CIFAR100 ($85.91$\%) and Tiny Imagenet
($70.54$\%), we obtain state-of-the-art results compared with other modern
models under the premise that the model size is approximate and no additional
data is used.",2024-12-27T03:55:25Z,http://arxiv.org/abs/2412.19433v1,"Yuanpeng He, Wenjie Song, Lijian Li, Tianxiang Zhan, Wenpin Jiao"
Revisiting PCA for time series reduction in temporal dimension,"Revisiting PCA for Time Series Reduction in Temporal Dimension; Jiaxin Gao,
Wenbo Hu, Yuntian Chen; Deep learning has significantly advanced time series
analysis (TSA), enabling the extraction of complex patterns for tasks like
classification, forecasting, and regression. Although dimensionality reduction
has traditionally focused on the variable space-achieving notable success in
minimizing data redundancy and computational complexity-less attention has been
paid to reducing the temporal dimension. In this study, we revisit Principal
Component Analysis (PCA), a classical dimensionality reduction technique, to
explore its utility in temporal dimension reduction for time series data. It is
generally thought that applying PCA to the temporal dimension would disrupt
temporal dependencies, leading to limited exploration in this area. However,
our theoretical analysis and extensive experiments demonstrate that applying
PCA to sliding series windows not only maintains model performance, but also
enhances computational efficiency. In auto-regressive forecasting, the temporal
structure is partially preserved through windowing, and PCA is applied within
these windows to denoise the time series while retaining their statistical
information. By preprocessing time-series data with PCA, we reduce the temporal
dimensionality before feeding it into TSA models such as Linear, Transformer,
CNN, and RNN architectures. This approach accelerates training and inference
and reduces resource consumption. Notably, PCA improves Informer training and
inference speed by up to 40% and decreases GPU memory usage of TimesNet by 30%,
without sacrificing model accuracy. Comparative analysis against other
reduction methods further highlights the effectiveness of PCA in improving the
efficiency of TSA models.",2024-12-27T03:17:26Z,http://arxiv.org/abs/2412.19423v1,"Jiaxin Gao, Wenbo Hu, Yuntian Chen"
"A Matrix Logic Approach to Efficient Frequent Itemset Discovery in Large
  Data Sets","This paper proposes a frequent itemset mining algorithm based on the Boolean
matrix method, aiming to solve the storage and computational bottlenecks of
traditional frequent pattern mining algorithms in high-dimensional and
large-scale transaction databases. By representing the itemsets in the
transaction database as Boolean matrices, the algorithm uses Boolean logic
operations such as AND and OR to efficiently calculate the support of the
itemsets, avoiding the generation and storage of a large number of candidates
itemsets in traditional algorithms. The algorithm recursively mines frequent
itemsets through matrix operations and can flexibly adapt to different data
scales and support thresholds. In the experiment, the public Groceries dataset
was selected, and the running efficiency test and frequent itemset mining
effect test were designed to evaluate the algorithm's performance indicators
such as running time, memory usage, and number of frequent itemsets under
different transaction numbers and support thresholds. The experimental results
show that the algorithm can efficiently mine a large number of frequent
itemsets when the support threshold is low, and focus on strong association
rules with high support when the threshold is high. In addition, the changing
trends of running time and memory usage show that the Boolean matrix method can
still maintain good running efficiency when the number of transactions
increases significantly and has high scalability and robustness. Future
research can improve memory optimization and matrix block operations, and
combine distributed computing and deep learning models to further enhance the
algorithm's applicability and real-time processing capabilities in
ultra-large-scale data environments. The algorithm has broad application
potential and development prospects in the fields of market analysis,
recommendation systems, and network security.",2024-12-27T03:13:13Z,http://arxiv.org/abs/2412.19420v1,"Xuan Li, Tingyi Ruan, Yankaiqi Li, Quanchao Lu, Xiaoxuan Sun"
KALAHash: Knowledge-Anchored Low-Resource Adaptation for Deep Hashing,"Deep hashing has been widely used for large-scale approximate nearest
neighbor search due to its storage and search efficiency. However, existing
deep hashing methods predominantly rely on abundant training data, leaving the
more challenging scenario of low-resource adaptation for deep hashing
relatively underexplored. This setting involves adapting pre-trained models to
downstream tasks with only an extremely small number of training samples
available. Our preliminary benchmarks reveal that current methods suffer
significant performance degradation due to the distribution shift caused by
limited training samples. To address these challenges, we introduce
Class-Calibration LoRA (CLoRA), a novel plug-and-play approach that dynamically
constructs low-rank adaptation matrices by leveraging class-level textual
knowledge embeddings. CLoRA effectively incorporates prior class knowledge as
anchors, enabling parameter-efficient fine-tuning while maintaining the
original data distribution. Furthermore, we propose Knowledge-Guided Discrete
Optimization (KIDDO), a framework to utilize class knowledge to compensate for
the scarcity of visual information and enhance the discriminability of hash
codes. Extensive experiments demonstrate that our proposed method, Knowledge-
Anchored Low-Resource Adaptation Hashing (KALAHash), significantly boosts
retrieval performance and achieves a 4x data efficiency in low-resource
scenarios.",2024-12-27T03:04:54Z,http://arxiv.org/abs/2412.19417v1,"Shu Zhao, Tan Yu, Xiaoshuai Hao, Wenchao Ma, Vijaykrishnan Narayanan"
DIPS: Optimal Dynamic Index for Poisson $\boldsymbolπ$ps Sampling,"This paper addresses the Poisson $\pi$ps sampling problem, a topic of
significant academic interest in various domains and with practical data mining
applications, such as influence maximization. The problem includes a set
$\mathcal{S}$ of $n$ elements, where each element $v$ is assigned a weight
$w(v)$ reflecting its importance. The goal is to generate a random subset $X$
of $\mathcal{S}$, where each element $v \in \mathcal{S}$ is included in $X$
independently with probability $\frac{c\cdot w(v)}{\sum_{v \in \mathcal{S}}
w(v)}$, where $0&lt;c\leq 1$ is a constant. The subsets must be independent across
different queries. While the Poisson $\pi$ps sampling problem can be reduced to
the well-studied subset sampling problem, updates in Poisson $\pi$ps sampling,
such as adding a new element or removing an element, would cause the
probabilities of all $n$ elements to change in the corresponding subset
sampling problem, making this approach impractical for dynamic scenarios. To
address this, we propose a dynamic index specifically tailored for the Poisson
$\pi$ps sampling problem, supporting optimal expected $\mathcal{O}(1)$ query
time and $\mathcal{O}(1)$ index update time, with an optimal $\mathcal{O}(n)$
space cost. Our solution involves recursively partitioning the set by weights
and ultimately using table lookup. The core of our solution lies in addressing
the challenges posed by weight explosion and correlations between elements.
Empirical evaluations demonstrate that our approach achieves significant
speedups in update time while maintaining consistently competitive query time
compared to the subset-sampling-based methods.",2024-12-27T02:47:44Z,http://arxiv.org/abs/2412.19415v1,"Jinchao Huang, Sibo Wang"
"The Hobby-Eberly Telescope Dark Energy Experiment Survey (HETDEX) Active
  Galactic Nuclei Catalog: the Fourth Data Release","We present the Active Galactic Nuclei (AGN) catalog from the fourth data
release (HDR4) of the Hobby-Eberly Telescope Dark Energy Experiment Survey
(HETDEX). HETDEX is an untargeted spectroscopic survey. HDR4 contains 345,874
Integral Field Unit (IFU) observations from January 2017 to August 2023
covering an effective area of 62.9 deg2. With no imaging pre-selection, our
spectroscopic confirmed AGN sample includes low-luminosity AGN, narrow-line
AGN, and/or red AGN down to g~25. This catalog has 15,940 AGN across the
redshifts of z=0.1~4.6, giving a raw AGN number density of 253.4 deg-2. Among
them, 10,499 (66%) have redshifts either confirmed by line pairs or matched to
the Sloan Digital Sky Survey Quasar Catalog. For the remaining 5,441 AGN, 2,083
are single broad line AGN candidates, while the remaining 3,358 are single
intermediate broad line (full width at half maximum, FWHM ~ 1200 km s-1) AGN
candidates. A total of 4,060 (39%) of the 10,499 redshift-confirmed AGN have
emission-line regions $3\sigma$ more extended than the image quality which
could be strong outflows blowing into the outskirts of the host galaxies or
ionized intergalactic medium.",2024-12-27T02:45:20Z,http://arxiv.org/abs/2412.19414v1,"Chenxu Liu, Karl Gebhardt, Erin Mentuch Cooper, Dustin Davis, Donald P. Schneider, Matt J. Jarvis, Daniel J. Farrow, Steven L. Finkelstein, Oscar A. Chavez Ortiz, The HETDEX Collaboration"
MINIMA: Modality Invariant Image Matching,"Image matching for both cross-view and cross-modality plays a critical role
in multimodal perception. In practice, the modality gap caused by different
imaging systems/styles poses great challenges to the matching task. Existing
works try to extract invariant features for specific modalities and train on
limited datasets, showing poor generalization. In this paper, we present
MINIMA, a unified image matching framework for multiple cross-modal cases.
Without pursuing fancy modules, our MINIMA aims to enhance universal
performance from the perspective of data scaling up. For such purpose, we
propose a simple yet effective data engine that can freely produce a large
dataset containing multiple modalities, rich scenarios, and accurate matching
labels. Specifically, we scale up the modalities from cheap but rich RGB-only
matching data, by means of generative models. Under this setting, the matching
labels and rich diversity of the RGB dataset are well inherited by the
generated multimodal data. Benefiting from this, we construct MD-syn, a new
comprehensive dataset that fills the data gap for general multimodal image
matching. With MD-syn, we can directly train any advanced matching pipeline on
randomly selected modality pairs to obtain cross-modal ability. Extensive
experiments on in-domain and zero-shot matching tasks, including $19$
cross-modal cases, demonstrate that our MINIMA can significantly outperform the
baselines and even surpass modality-specific methods. The dataset and code are
available at https://github.com/LSXI7/MINIMA .",2024-12-27T02:39:50Z,http://arxiv.org/abs/2412.19412v1,"Xingyu Jiang, Jiangwei Ren, Zizhuo Li, Xin Zhou, Dingkang Liang, Xiang Bai"
"MLLM-SUL: Multimodal Large Language Model for Semantic Scene
  Understanding and Localization in Traffic Scenarios","Multimodal large language models (MLLMs) have shown satisfactory effects in
many autonomous driving tasks. In this paper, MLLMs are utilized to solve joint
semantic scene understanding and risk localization tasks, while only relying on
front-view images. In the proposed MLLM-SUL framework, a dual-branch visual
encoder is first designed to extract features from two resolutions, and rich
visual information is conducive to the language model describing risk objects
of different sizes accurately. Then for the language generation, LLaMA model is
fine-tuned to predict scene descriptions, containing the type of driving
scenario, actions of risk objects, and driving intentions and suggestions of
ego-vehicle. Ultimately, a transformer-based network incorporating a regression
token is trained to locate the risk objects. Extensive experiments on the
existing DRAMA-ROLISP dataset and the extended DRAMA-SRIS dataset demonstrate
that our method is efficient, surpassing many state-of-the-art image-based and
video-based methods. Specifically, our method achieves 80.1% BLEU-1 score and
298.5% CIDEr score in the scene understanding task, and 59.6% accuracy in the
localization task. Codes and datasets are available at
https://github.com/fjq-tongji/MLLM-SUL.",2024-12-27T02:05:38Z,http://arxiv.org/abs/2412.19406v1,"Jiaqi Fan, Jianhua Wu, Jincheng Gao, Jianhao Yu, Yafei Wang, Hongqing Chu, Bingzhao Gao"
Spectral-Temporal Fusion Representation for Person-in-Bed Detection,"This study is based on the ICASSP 2025 Signal Processing Grand Challenge's
Accelerometer-Based Person-in-Bed Detection Challenge, which aims to determine
bed occupancy using accelerometer signals. The task is divided into two tracks:
""in bed"" and ""not in bed"" segmented detection, and streaming detection, facing
challenges such as individual differences, posture variations, and external
disturbances. We propose a spectral-temporal fusion-based feature
representation method with mixup data augmentation, and adopt Intersection over
Union (IoU) loss to optimize detection accuracy. In the two tracks, our method
achieved outstanding results of 100.00% and 95.55% in detection scores,
securing first place and third place, respectively.",2024-12-27T02:05:09Z,http://arxiv.org/abs/2412.19404v1,"Xuefeng Yang, Shiheng Zhang, Jian Guan, Feiyang Xiao, Wei Lu, Qiaoxi Zhu"
"Fully Data-driven but Interpretable Human Behavioural Modelling with
  Differentiable Discrete Choice Model","Discrete choice models are essential for modelling various decision-making
processes in human behaviour. However, the specification of these models has
depended heavily on domain knowledge from experts, and the fully automated but
interpretable modelling of complex human behaviours has been a long-standing
challenge. In this paper, we introduce the differentiable discrete choice model
(Diff-DCM), a fully data-driven method for the interpretable modelling,
learning, prediction, and control of complex human behaviours, which is
realised by differentiable programming. Solely from input features and choice
outcomes without any prior knowledge, Diff-DCM can estimate interpretable
closed-form utility functions that reproduce observed behaviours. Comprehensive
experiments with both synthetic and real-world data demonstrate that Diff-DCM
can be applied to various types of data and requires only a small amount of
computational resources for the estimations, which can be completed within tens
of seconds on a laptop without any accelerators. In these experiments, we also
demonstrate that, using its differentiability, Diff-DCM can provide useful
insights into human behaviours, such as an optimal intervention path for
effective behavioural changes. This study provides a strong basis for the fully
automated and reliable modelling, prediction, and control of human behaviours.",2024-12-27T01:53:18Z,http://arxiv.org/abs/2412.19403v1,"Fumiyasu Makinoshima, Tatsuya Mitomi, Fumiya Makihara, Eigo Segawa"
"Comparing Few to Rank Many: Active Human Preference Learning using
  Randomized Frank-Wolfe","We study learning of human preferences from a limited comparison feedback.
This task is ubiquitous in machine learning. Its applications such as
reinforcement learning from human feedback, have been transformational. We
formulate this problem as learning a Plackett-Luce model over a universe of $N$
choices from $K$-way comparison feedback, where typically $K \ll N$. Our
solution is the D-optimal design for the Plackett-Luce objective. The design
defines a data logging policy that elicits comparison feedback for a small
collection of optimally chosen points from all ${N \choose K}$ feasible
subsets. The main algorithmic challenge in this work is that even fast methods
for solving D-optimal designs would have $O({N \choose K})$ time complexity. To
address this issue, we propose a randomized Frank-Wolfe (FW) algorithm that
solves the linear maximization sub-problems in the FW method on randomly chosen
variables. We analyze the algorithm, and evaluate it empirically on synthetic
and open-source NLP datasets.",2024-12-27T01:10:17Z,http://arxiv.org/abs/2412.19396v1,"Kiran Koshy Thekumparampil, Gaurush Hiranandani, Kousha Kalantari, Shoham Sabach, Branislav Kveton"
"An In-Depth Analysis of Adversarial Discriminative Domain Adaptation for
  Digit Classification","Domain adaptation is an active area of research driven by the growing demand
for robust machine learning models that perform well on real-world data.
Adversarial learning for deep neural networks (DNNs) has emerged as a promising
approach to improving generalization ability, particularly for image
classification. In this paper, we implement a specific adversarial learning
technique known as Adversarial Discriminative Domain Adaptation (ADDA) and
replicate digit classification experiments from the original ADDA paper. We
extend their findings by examining a broader range of domain shifts and provide
a detailed analysis of in-domain classification accuracy post-ADDA. Our results
demonstrate that ADDA significantly improves accuracy across certain domain
shifts with minimal impact on in-domain performance. Furthermore, we provide
qualitative analysis and propose potential explanations for ADDA's limitations
in less successful domain shifts. Code is at
https://github.com/eugenechoi2004/COS429_FINAL .",2024-12-27T00:36:40Z,http://arxiv.org/abs/2412.19391v1,"Eugene Choi, Julian Rodriguez, Edmund Young"
"A reduced-order framework for temperature estimation in food freezing
  from optimally located sensors, including turbulent conjugate flow scenarios","This article proposes a framework for estimating temperature fields in
food-freezing applications that significantly reduces computational load while
ensuring accurate temperature monitoring, representing a promising
technological tool for optimizing and controlling food engineering processes.
The strategy is based on (i) a mathematical model of a convection-dominated
problem coupling thermal convection and turbulence and (ii) a least-squares
approach for solving the inverse data assimilation problem, regularized by
projecting the governing dynamics onto a reduced-order model (ROM). The
unsteady freezing process considers an idealized salmon slice in a freezer
cabinet, modeled with temperature-dependent thermophysical properties. The
forward problem is approximated using a third-order WENO finite volume solver,
including an optimized second-order backward scheme for time discretization. We
employ our data assimilation framework to reconstruct the temperature field
from a limited number of sensor data and to estimate temperature distributions
within frozen food. Sensor placement is optimized using a new greedy algorithm,
relying on maximizing the observability of the reduced-order dynamics for a
fixed set of sensors. The proposed approach allows efficient extrapolation from
external sensor measurements to the internal temperature of the food, which is
crucial for maintaining food quality.",2024-12-27T00:26:36Z,http://arxiv.org/abs/2412.19387v1,"Felipe Galarce, Diego Rivera, Douglas Pacheco, Alfonso Caiazzo, Ernesto Castillo"
Resolvent-based estimation and control of a laminar airfoil wake,"We develop an optimal resolvent-based estimator and controller to predict and
attenuate unsteady vortex shedding fluctuations in the laminar wake of a NACA
0012 airfoil at an angle of attack of 6.5 degrees, chord-based Reynolds number
of 5000, and Mach number of 0.3. The resolvent-based estimation and control
framework offers several advantages over standard methods. Under equivalent
assumptions, the resolvent-based estimator and controller reproduce the Kalman
filter and LQG controller, respectively, but at substantially lower
computational cost using either an operator-based or data-driven
implementation. Unlike these methods, the resolvent-based approach can
naturally accommodate forcing terms (nonlinear terms from Navier-Stokes) with
colored-in-time statistics, significantly improving estimation accuracy and
control efficacy. Causality is optimally enforced using a Wiener-Hopf
formalism. We integrate these tools into a high-performance-computing-ready
compressible flow solver and demonstrate their effectiveness for estimating and
controlling velocity fluctuations in the wake of the airfoil immersed in clean
and noisy freestreams, the latter of which prevents the flow from falling into
a periodic limit cycle. Using four shear-stress sensors on the surface of the
airfoil, the resolvent-based estimator predicts a series of downstream targets
with approximately 3% and 30% error for the clean and noisy freestream
conditions, respectively. For the latter case, using four actuators on the
airfoil surface, the resolvent-based controller reduces the turbulent kinetic
energy in the wake by 98%.",2024-12-27T00:12:23Z,http://arxiv.org/abs/2412.19386v1,"Junoh Jung, Rutvij Bhagwat, Aaron Towne"
"The Internet of Value: Integrating Blockchain and Lightning Network
  Micropayments for Knowledge Markets","Q&amp;A websites rely on user-generated responses, with incentives such as
reputation scores or monetary rewards often offered. While some users may find
it intrinsically rewarding to assist others, studies indicate that payment can
improve the quality and speed of answers. However, traditional payment
processors impose minimum thresholds that many Q&amp;A inquiries fall below. The
introduction of Bitcoin enabled direct digital value transfer, yet frequent
micropayments remain challenging. Recent advancements like the Lightning
Network now allow frictionless micropayments by reducing costs and minimising
reliance on intermediaries. This development fosters an ""Internet of Value,""
where transferring even small amounts of money is as simple as sharing data.
This study investigates integrating Lightning Network-based micropayment
strategies into Q&amp;A platforms, aiming to create a knowledge market free of
minimum payment barriers. A survey was conducted to address the gap below the
$2 payment level identified in prior research. Responses confirmed that
incentives for asking and answering weaken as payments decrease. Findings
reveal even minimal payments, such as {\pounds}0.01, significantly encourage
higher quality and effort in responses. The study recommends micropayment
incentives for service-oriented applications, particularly Q&amp;A platforms. By
leveraging the Lightning Network to remove barriers, a more open marketplace
can emerge, improving engagement and outcomes. Further research is needed to
confirm if users follow through on reported intentions when spending funds.",2024-12-26T23:57:54Z,http://arxiv.org/abs/2412.19384v1,"Ellis Solaiman, Jorge Robins"
"Minimal Batch Adaptive Learning Policy Engine for Real-Time Mid-Price
  Forecasting in High-Frequency Trading","High-frequency trading (HFT) has transformed modern financial markets, making
reliable short-term price forecasting models essential. In this study, we
present a novel approach to mid-price forecasting using Level 1 limit order
book (LOB) data from NASDAQ, focusing on 100 U.S. stocks from the S&amp;P 500 index
during the period from September to November 2022. Expanding on our previous
work with Radial Basis Function Neural Networks (RBFNN), which leveraged
automated feature importance techniques based on mean decrease impurity (MDI)
and gradient descent (GD), we introduce the Adaptive Learning Policy Engine
(ALPE) - a reinforcement learning (RL)-based agent designed for batch-free,
immediate mid-price forecasting. ALPE incorporates adaptive epsilon decay to
dynamically balance exploration and exploitation, outperforming a diverse range
of highly effective machine learning (ML) and deep learning (DL) models in
forecasting performance.",2024-12-26T22:49:53Z,http://arxiv.org/abs/2412.19372v1,"Adamantios Ntakaris, Gbenga Ibikunle"
Large Language Models for Market Research: A Data-augmentation Approach,"Large Language Models (LLMs) have transformed artificial intelligence by
excelling in complex natural language processing tasks. Their ability to
generate human-like text has opened new possibilities for market research,
particularly in conjoint analysis, where understanding consumer preferences is
essential but often resource-intensive. Traditional survey-based methods face
limitations in scalability and cost, making LLM-generated data a promising
alternative. However, while LLMs have the potential to simulate real consumer
behavior, recent studies highlight a significant gap between LLM-generated and
human data, with biases introduced when substituting between the two. In this
paper, we address this gap by proposing a novel statistical data augmentation
approach that efficiently integrates LLM-generated data with real data in
conjoint analysis. Our method leverages transfer learning principles to debias
the LLM-generated data using a small amount of human data. This results in
statistically robust estimators with consistent and asymptotically normal
properties, in contrast to naive approaches that simply substitute human data
with LLM-generated data, which can exacerbate bias. We validate our framework
through an empirical study on COVID-19 vaccine preferences, demonstrating its
superior ability to reduce estimation error and save data and costs by 24.9\%
to 79.8\%. In contrast, naive approaches fail to save data due to the inherent
biases in LLM-generated data compared to human data. Another empirical study on
sports car choices validates the robustness of our results. Our findings
suggest that while LLM-generated data is not a direct substitute for human
responses, it can serve as a valuable complement when used within a robust
statistical framework.",2024-12-26T22:06:29Z,http://arxiv.org/abs/2412.19363v1,"Mengxin Wang, Dennis J. Zhang, Heng Zhang"
"Evaluating Convolutional Neural Networks for COVID-19 classification in
  chest X-ray images","Coronavirus Disease 2019 (COVID-19) pandemic rapidly spread globally,
impacting the lives of billions of people. The effective screening of infected
patients is a critical step to struggle with COVID-19, and treating the
patients avoiding this quickly disease spread. The need for automated and
scalable methods has increased due to the unavailability of accurate automated
toolkits. Recent researches using chest X-ray images suggest they include
relevant information about the COVID-19 virus. Hence, applying machine learning
techniques combined with radiological imaging promises to identify this disease
accurately. It is straightforward to collect these images once it is spreadly
shared and analyzed in the world. This paper presents a method for automatic
COVID-19 detection using chest Xray images through four convolutional neural
networks, namely: AlexNet, VGG-11, SqueezeNet, and DenseNet-121. This method
had been providing accurate diagnostics for positive or negative COVID-19
classification. We validate our experiments using a ten-fold cross-validation
procedure over the training and test sets. Our findings include the shallow
fine-tuning and data augmentation strategies that can assist in dealing with
the low number of positive COVID-19 images publicly available. The accuracy for
all CNNs is higher than 97.00%, and the SqueezeNet model achieved the best
result with 99.20%.",2024-12-26T22:05:30Z,http://arxiv.org/abs/2412.19362v1,"Leonardo Gabriel Ferreira Rodrigues, Danilo Ferreira da Silva, Larissa Ferreira Rodrigues, João Fernando Mari"
Dynamic Skill Adaptation for Large Language Models,"We present Dynamic Skill Adaptation (DSA), an adaptive and dynamic framework
to adapt novel and complex skills to Large Language Models (LLMs). Compared
with previous work which learns from human-curated and static data in random
orders, we propose to first automatically generate and organize the training
data by mimicking the learning pathways of human and then dynamically tailor
the training data based on the training dynamics. Specifically, inspired by the
learning structures and teaching strategies in the human education system, we
first construct a skill graph by decomposing complex skills into sub-skills and
arranging them based on their dependencies in human syllables. For every skill,
we utilize LLMs to generate both textbook-like data which contains detailed
descriptions of skills for pre-training and exercise-like data which targets at
explicitly utilizing the skills to solve problems for instruction-tuning.
Furthermore, during the instruction-tuning, we dynamically update the training
data which down-weight easy-to-learn examples, generate more complex examples,
and filter out data with errors. Experiments on large language models such as
LLAMA and Mistral demonstrate the effectiveness of our proposed methods in
adapting math reasoning skills and social study skills.",2024-12-26T22:04:23Z,http://arxiv.org/abs/2412.19361v1,"Jiaao Chen, Diyi Yang"
"Improving the network traffic classification using the Packet Vision
  approach","The network traffic classification allows improving the management, and the
network services offer taking into account the kind of application. The future
network architectures, mainly mobile networks, foresee intelligent mechanisms
in their architectural frameworks to deliver application-aware network
requirements. The potential of convolutional neural networks capabilities,
widely exploited in several contexts, can be used in network traffic
classification. Thus, it is necessary to develop methods based on the content
of packets transforming it into a suitable input for CNN technologies. Hence,
we implemented and evaluated the Packet Vision, a method capable of building
images from packets raw-data, considering both header and payload. Our approach
excels those found in state-of-the-art by delivering security and privacy by
transforming the raw-data packet into images. Therefore, we built a dataset
with four traffic classes evaluating the performance of three CNNs
architectures: AlexNet, ResNet-18, and SqueezeNet. Experiments showcase the
Packet Vision combined with CNNs applicability and suitability as a promising
approach to deliver outstanding performance in classifying network traffic.",2024-12-26T21:56:03Z,http://arxiv.org/abs/2412.19360v1,"Rodrigo Moreira, Larissa Ferreira Rodrigues, Pedro Frosi Rosa, Flávio de Oliveira Silva"
"Federated Hybrid Training and Self-Adversarial Distillation: Towards
  Robust Edge Networks","Federated learning (FL) is a distributed training technology that enhances
data privacy in mobile edge networks by allowing data owners to collaborate
without transmitting raw data to the edge server. However, data heterogeneity
and adversarial attacks pose challenges to develop an unbiased and robust
global model for edge deployment. To address this, we propose Federated hyBrid
Adversarial training and self-adversarial disTillation (FedBAT), a new
framework designed to improve both robustness and generalization of the global
model. FedBAT seamlessly integrates hybrid adversarial training and
self-adversarial distillation into the conventional FL framework from data
augmentation and feature distillation perspectives. From a data augmentation
perspective, we propose hybrid adversarial training to defend against
adversarial attacks by balancing accuracy and robustness through a weighted
combination of standard and adversarial training. From a feature distillation
perspective, we introduce a novel augmentation-invariant adversarial
distillation method that aligns local adversarial features of augmented images
with their corresponding unbiased global clean features. This alignment can
effectively mitigate bias from data heterogeneity while enhancing both the
robustness and generalization of the global model. Extensive experimental
results across multiple datasets demonstrate that FedBAT yields comparable or
superior performance gains in improving robustness while maintaining accuracy
compared to several baselines.",2024-12-26T21:32:08Z,http://arxiv.org/abs/2412.19354v1,"Yu Qiao, Apurba Adhikary, Kitae Kim, Eui-Nam Huh, Zhu Han, Choong Seon Hong"
ETTA: Elucidating the Design Space of Text-to-Audio Models,"Recent years have seen significant progress in Text-To-Audio (TTA) synthesis,
enabling users to enrich their creative workflows with synthetic audio
generated from natural language prompts. Despite this progress, the effects of
data, model architecture, training objective functions, and sampling strategies
on target benchmarks are not well understood. With the purpose of providing a
holistic understanding of the design space of TTA models, we set up a
large-scale empirical experiment focused on diffusion and flow matching models.
Our contributions include: 1) AF-Synthetic, a large dataset of high quality
synthetic captions obtained from an audio understanding model; 2) a systematic
comparison of different architectural, training, and inference design choices
for TTA models; 3) an analysis of sampling methods and their Pareto curves with
respect to generation quality and inference speed. We leverage the knowledge
obtained from this extensive analysis to propose our best model dubbed
Elucidated Text-To-Audio (ETTA). When evaluated on AudioCaps and MusicCaps,
ETTA provides improvements over the baselines trained on publicly available
data, while being competitive with models trained on proprietary data. Finally,
we show ETTA's improved ability to generate creative audio following complex
and imaginative captions -- a task that is more challenging than current
benchmarks.",2024-12-26T21:13:12Z,http://arxiv.org/abs/2412.19351v1,"Sang-gil Lee, Zhifeng Kong, Arushi Goel, Sungwon Kim, Rafael Valle, Bryan Catanzaro"
"Semi-Supervised Learning from Small Annotated Data and Large Unlabeled
  Data for Fine-grained PICO Entity Recognition","Objective: Extracting PICO elements -- Participants, Intervention,
Comparison, and Outcomes -- from clinical trial literature is essential for
clinical evidence retrieval, appraisal, and synthesis. Existing approaches do
not distinguish the attributes of PICO entities. This study aims to develop a
named entity recognition (NER) model to extract PICO entities with fine
granularities.
  Materials and Methods: Using a corpus of 2,511 abstracts with PICO mentions
from 4 public datasets, we developed a semi-supervised method to facilitate the
training of a NER model, FinePICO, by combining limited annotated data of PICO
entities and abundant unlabeled data. For evaluation, we divided the entire
dataset into two subsets: a smaller group with annotations and a larger group
without annotations. We then established the theoretical lower and upper
performance bounds based on the performance of supervised learning models
trained solely on the small, annotated subset and on the entire set with
complete annotations, respectively. Finally, we evaluated FinePICO on both the
smaller annotated subset and the larger, initially unannotated subset. We
measured the performance of FinePICO using precision, recall, and F1.
  Results: Our method achieved precision/recall/F1 of 0.567/0.636/0.60,
respectively, using a small set of annotated samples, outperforming the
baseline model (F1: 0.437) by more than 16\%. The model demonstrates
generalizability to a different PICO framework and to another corpus, which
consistently outperforms the benchmark in diverse experimental settings
(p-value \textless0.001).
  Conclusion: This study contributes a generalizable and effective
semi-supervised approach to named entity recognition leveraging large unlabeled
data together with small, annotated data. It also initially supports
fine-grained PICO extraction.",2024-12-26T20:24:35Z,http://arxiv.org/abs/2412.19346v1,"Fangyi Chen, Gongbo Zhang, Yilu Fang, Yifan Peng, Chunhua Weng"
Advanced Scheduling of Electrolyzer Modules for Grid Flexibility,"As the transition to sustainable power generation progresses, green hydrogen
production via electrolysis is expected to gain importance as a means for
energy storage and flexible load to complement variable renewable generation.
With the increasing need for cost-effective and efficient hydrogen production,
electrolyzer optimization is essential to improve both energy efficiency and
profitability. This paper analyzes how the efficiency and modular setup of
alkaline hydrogen electrolyzers can improve hydrogen output of systems linked
to a fluctuating renewable power supply. To explore this, we propose a
day-ahead optimal scheduling problem of a hybrid wind and electrolyzer system.
The novelty of our approach lies in modeling the number and capacity of
electrolyzer modules, and capturing the modules' impact on the hydrogen
production and efficiency. We solve the resulting mixed-integer optimization
problem with several different combinations of number of modules, efficiency
and operating range parameters, using day-ahead market data from a wind farm
generator in the ERCOT system as an input. Our results demonstrate that the
proposed approach ensures that electrolyzer owners can better optimize the
operation of their systems, achieving greater hydrogen production and higher
revenue. Key findings include that as the number of modules in a system with
the same overall capacity increases, hydrogen production and revenue increases.",2024-12-26T20:24:01Z,http://arxiv.org/abs/2412.19345v1,"Angelina Lesniak, Andrea Gloppen Johnsen, Noah Rhodes, Line Roald"
"CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language
  Models","Recent advances in Large Vision-Language Models (LVLMs) have sparked
significant progress in general-purpose vision tasks through visual instruction
tuning. While some works have demonstrated the capability of LVLMs to generate
segmentation masks that align phrases with natural language descriptions in a
single image, they struggle with segmentation-grounded comparisons across
multiple images, particularly at finer granularities such as object parts. In
this paper, we introduce the new task of part-focused semantic co-segmentation,
which seeks to identify and segment common and unique objects and parts across
images. To address this task, we present CALICO, the first LVLM that can
segment and reason over multiple masks across images, enabling object
comparison based on their constituent parts. CALICO features two proposed
components, a novel Correspondence Extraction Module, which captures
semantic-rich information to identify part-level correspondences between
objects, and a Correspondence Adaptation Module, which embeds this information
into the LVLM to facilitate multi-image understanding in a parameter-efficient
manner. To support training and evaluation, we curate MixedParts, a
comprehensive multi-image segmentation dataset containing $\sim$2.4M samples
across $\sim$44K images with diverse object and part categories. Experimental
results show CALICO, finetuned on only 0.3% of its architecture, achieves
robust performance in part-focused semantic co-segmentation.",2024-12-26T18:59:37Z,http://arxiv.org/abs/2412.19331v1,"Kiet A. Nguyen, Adheesh Juvekar, Tianjiao Yu, Muntasir Wahed, Ismini Lourentzou"
"Deep learning and whole-brain networks for biomarker discovery: modeling
  the dynamics of brain fluctuations in resting-state and cognitive tasks","Background: Brain network models offer insights into brain dynamics, but the
utility of model-derived bifurcation parameters as biomarkers remains
underexplored. Objective: This study evaluates bifurcation parameters from a
whole-brain network model as biomarkers for distinguishing brain states
associated with resting-state and task-based cognitive conditions. Methods:
Synthetic BOLD signals were generated using a supercritical Hopf brain network
model to train deep learning models for bifurcation parameter prediction.
Inference was performed on Human Connectome Project data, including both
resting-state and task-based conditions. Statistical analyses assessed the
separability of brain states based on bifurcation parameter distributions.
Results: Bifurcation parameter distributions differed significantly across task
and resting-state conditions ($p &lt; 0.0001$ for all but one comparison).
Task-based brain states exhibited higher bifurcation values compared to rest.
Conclusion: Bifurcation parameters effectively differentiate cognitive and
resting states, warranting further investigation as biomarkers for brain state
characterization and neurological disorder assessment.",2024-12-26T18:58:38Z,http://arxiv.org/abs/2412.19329v1,"Facundo Roffet, Gustavo Deco, Claudio Delrieux, Gustavo Patow"
"Resolving the Ambiguity of Complete-to-Partial Point Cloud Registration
  for Image-Guided Liver Surgery with Patches-to-Partial Matching","In image-guided liver surgery, the initial rigid alignment between
preoperative and intraoperative data, often represented as point clouds, is
crucial for providing sub-surface information from preoperative CT/MRI images
to the surgeon during the procedure. Currently, this alignment is typically
performed using semi-automatic methods, which, while effective to some extent,
are prone to errors that demand manual correction. Point cloud
correspondence-based registration methods are promising to serve as a fully
automatic solution. However, they may struggle in scenarios with limited
intraoperative surface visibility, a common challenge in liver surgery,
particularly in laparoscopic procedures, which we refer to as
complete-to-partial ambiguity. We first illustrate this ambiguity by evaluating
the performance of state-of-the-art learning-based point cloud registration
methods on our carefully constructed in silico and in vitro datasets. Then, we
propose a patches-to-partial matching strategy as a plug-and-play module to
resolve the ambiguity, which can be seamlessly integrated into learning-based
registration methods without disrupting their end-to-end structure. It has
proven effective and efficient in improving registration performance for cases
with limited intraoperative visibility. The constructed benchmark and the
proposed module establish a solid foundation for advancing applications of
point cloud correspondence-based registration methods in image-guided liver
surgery.",2024-12-26T18:58:29Z,http://arxiv.org/abs/2412.19328v1,"Zixin Yang, Jon S. Heiselman, Cheng Han, Kelly Merrell, Richard Simon, Cristian. A. Linte"
"Task Preference Optimization: Improving Multimodal Large Language Models
  with Vision Task Alignment","Current multimodal large language models (MLLMs) struggle with fine-grained
or precise understanding of visuals though they give comprehensive perception
and reasoning in a spectrum of vision applications. Recent studies either
develop tool-using or unify specific visual tasks into the autoregressive
framework, often at the expense of overall multimodal performance. To address
this issue and enhance MLLMs with visual tasks in a scalable fashion, we
propose Task Preference Optimization (TPO), a novel method that utilizes
differentiable task preferences derived from typical fine-grained visual tasks.
TPO introduces learnable task tokens that establish connections between
multiple task-specific heads and the MLLM. By leveraging rich visual labels
during training, TPO significantly enhances the MLLM's multimodal capabilities
and task-specific performance. Through multi-task co-training within TPO, we
observe synergistic benefits that elevate individual task performance beyond
what is achievable through single-task training methodologies. Our
instantiation of this approach with VideoChat and LLaVA demonstrates an overall
14.6% improvement in multimodal performance compared to baseline models.
Additionally, MLLM-TPO demonstrates robust zero-shot capabilities across
various tasks, performing comparably to state-of-the-art supervised models. The
code will be released at https://github.com/OpenGVLab/TPO",2024-12-26T18:56:05Z,http://arxiv.org/abs/2412.19326v1,"Ziang Yan, Zhilin Li, Yinan He, Chenting Wang, Kunchang Li, Xinhao Li, Xiangyu Zeng, Zilei Wang, Yali Wang, Yu Qiao, Limin Wang, Yi Wang"
"Performance Control in Early Exiting to Deploy Large Models at the Same
  Cost of Smaller Ones","Early Exiting (EE) is a promising technique for speeding up inference by
adaptively allocating compute resources to data points based on their
difficulty. The approach enables predictions to exit at earlier layers for
simpler samples while reserving more computation for challenging ones. In this
study, we first present a novel perspective on the EE approach, showing that
larger models deployed with EE can achieve higher performance than smaller
models while maintaining similar computational costs. As existing EE approaches
rely on confidence estimation at each exit point, we further study the impact
of overconfidence on the controllability of the compute-performance trade-off.
We introduce Performance Control Early Exiting (PCEE), a method that enables
accuracy thresholding by basing decisions not on a data point's confidence but
on the average accuracy of samples with similar confidence levels from a
held-out validation set. In our experiments, we show that PCEE offers a simple
yet computationally efficient approach that provides better control over
performance than standard confidence-based approaches, and allows us to scale
up model sizes to yield performance gain while reducing the computational cost.",2024-12-26T18:54:32Z,http://arxiv.org/abs/2412.19325v1,"Mehrnaz Mofakhami, Reza Bayat, Ioannis Mitliagkas, Joao Monteiro, Valentina Zantedeschi"
Electron efficiency in LHC Run-2 with the ATLAS experiment,"The document presents a general overview of the electron reconstruction,
identification and isolation performance in the ATLAS experiment. The results
are obtained using 13 TeV proton-proton collision data collected during the LHC
Run-2. The electron reconstruction efficiency is higher than 97%, and the ratio
of data to Monte Carlo simulation efficiency is close to unity, with associated
uncertainties generally smaller than 0.1%. The electron identification is shown
for three working points, and depending on the electron $E_T$, it can be as low
as 60%, increasing to more than 80% above 50 GeV. The correction factors are
close to one, generally within 5%. Five isolation working points are
recommended in the ATLAS experiment, to successfully reject fake/non-prompt
electrons. Their dependency on the electron identification working points is
shown and discussed, as well as their pile-up dependency, and their performance
versus electron $E_T$ and $\eta$.
  Document based on a presentation at the XI International Conference on New
Frontiers in Physics (ICNFP 2022).
  keywords; prompt electrons, reconstruction, identification, isolation,
fake/non-prompt electrons",2024-12-26T18:50:43Z,http://arxiv.org/abs/2412.19323v1,Otilia Ducu
Adaptive Conformal Inference by Betting,"Conformal prediction is a valuable tool for quantifying predictive
uncertainty of machine learning models. However, its applicability relies on
the assumption of data exchangeability, a condition which is often not met in
real-world scenarios. In this paper, we consider the problem of adaptive
conformal inference without any assumptions about the data generating process.
Existing approaches for adaptive conformal inference are based on optimizing
the pinball loss using variants of online gradient descent. A notable
shortcoming of such approaches is in their explicit dependence on and
sensitivity to the choice of the learning rates. In this paper, we propose a
different approach for adaptive conformal inference that leverages
parameter-free online convex optimization techniques. We prove that our method
controls long-term miscoverage frequency at a nominal level and demonstrate its
convincing empirical performance without any need of performing cumbersome
parameter tuning.",2024-12-26T18:42:08Z,http://arxiv.org/abs/2412.19318v1,"Aleksandr Podkopaev, Darren Xu, Kuang-Chih Lee"
"ATLAS searches for higgsinos with R-parity violating couplings in events
  with leptons","This document presents two searches for Supersymmetry through the direct
production of pairs of higgsinos decaying into final states with leptons and
($b$-) jets. The analyses are performed using 139~fb$^{-1}$ of the 13~TeV
proton-proton collision data collected with the ATLAS detector. The methods
used to estimate the Standard Model and detector backgrounds are discussed, as
well as their shortcomings. Finally, results in selected signal regions, and
some exclusion limits, are presented, illustrating the significant improvement
over the previous exclusion limits.
  Document based on a presentation at the XI International Conference on New
Frontiers in Physics (ICNFP 2022).",2024-12-26T18:41:55Z,http://arxiv.org/abs/2412.19317v1,Otilia Ducu
Towards a Single ASR Model That Generalizes to Disordered Speech,"This study investigates the impact of integrating a dataset of disordered
speech recordings ($\sim$1,000 hours) into the fine-tuning of a near
state-of-the-art ASR baseline system. Contrary to what one might expect,
despite the data being less than 1% of the training data of the ASR system, we
find a considerable improvement in disordered speech recognition accuracy.
Specifically, we observe a 33% improvement on prompted speech, and a 26%
improvement on a newly gathered spontaneous, conversational dataset of
disordered speech. Importantly, there is no significant performance decline on
standard speech recognition benchmarks. Further, we observe that the proposed
tuning strategy helps close the gap between the baseline system and
personalized models by 64% highlighting the significant progress as well as the
room for improvement. Given the substantial benefits of our findings, this
experiment suggests that from a fairness perspective, incorporating a small
fraction of high quality disordered speech data in a training recipe is an easy
step that could be done to make speech technology more accessible for users
with speech disabilities.",2024-12-26T18:39:15Z,http://arxiv.org/abs/2412.19315v1,"Jimmy Tobin, Katrin Tomanek, Subhashini Venugopalan"
"xSRL: Safety-Aware Explainable Reinforcement Learning -- Safety as a
  Product of Explainability","Reinforcement learning (RL) has shown great promise in simulated
environments, such as games, where failures have minimal consequences. However,
the deployment of RL agents in real-world systems such as autonomous vehicles,
robotics, UAVs, and medical devices demands a higher level of safety and
transparency, particularly when facing adversarial threats. Safe RL algorithms
have been developed to address these concerns by optimizing both task
performance and safety constraints. However, errors are inevitable, and when
they occur, it is essential that the RL agents can also explain their actions
to human operators. This makes trust in the safety mechanisms of RL systems
crucial for effective deployment. Explainability plays a key role in building
this trust by providing clear, actionable insights into the agent's
decision-making process, ensuring that safety-critical decisions are well
understood. While machine learning (ML) has seen significant advances in
interpretability and visualization, explainability methods for RL remain
limited. Current tools fail to address the dynamic, sequential nature of RL and
its needs to balance task performance with safety constraints over time. The
re-purposing of traditional ML methods, such as saliency maps, is inadequate
for safety-critical RL applications where mistakes can result in severe
consequences. To bridge this gap, we propose xSRL, a framework that integrates
both local and global explanations to provide a comprehensive understanding of
RL agents' behavior. xSRL also enables developers to identify policy
vulnerabilities through adversarial attacks, offering tools to debug and patch
agents without retraining. Our experiments and user studies demonstrate xSRL's
effectiveness in increasing safety in RL systems, making them more reliable and
trustworthy for real-world deployment. Code is available at
https://github.com/risal-shefin/xSRL.",2024-12-26T18:19:04Z,http://arxiv.org/abs/2412.19311v1,"Risal Shahriar Shefin, Md Asifur Rahman, Thai Le, Sarra Alqahtani"
"Theoretical models for longitudinal coupled-bunch instabilities driven
  by harmonic cavities in electron storage rings","We present a theoretical framework for analyzing longitudinal coupled-bunch
instabilities in double-rf systems with even filling patterns, accounting for
potential-well distortion and multiple azimuthal modes. The linearized Vlasov
equation is solved in the frequency-domain for an arbitrary rf potential to
derive the Lebedev equation. We unified different formulations, obtaining
results from recent publications as particular cases. Applications to Robinson
dipole-quadrupole mode coupling and the periodic transient beam loading
(PTBL)/mode-1 instability are presented. Notably, for the first time,
theoretical predictions of the mode-1 thresholds show excellent agreement with
experimental data. The analysis reveals that the PTBL instability is a
zero-frequency effect dependent on azimuthal mode interactions and resistant to
Landau damping, providing new insights into its mechanism. The methods are
implemented in the open-source package pycolleff, offering a useful
semi-analytical tool for studying instabilities in electron storage rings with
harmonic cavities.",2024-12-26T18:11:39Z,http://arxiv.org/abs/2412.19308v1,Murilo B. Alves
"Perceive, Query &amp; Reason: Enhancing Video QA with Question-Guided
  Temporal Queries","Video Question Answering (Video QA) is a challenging video understanding task
that requires models to comprehend entire videos, identify the most relevant
information based on contextual cues from a given question, and reason
accurately to provide answers. Recent advancements in Multimodal Large Language
Models (MLLMs) have transformed video QA by leveraging their exceptional
commonsense reasoning capabilities. This progress is largely driven by the
effective alignment between visual data and the language space of MLLMs.
However, for video QA, an additional space-time alignment poses a considerable
challenge for extracting question-relevant information across frames. In this
work, we investigate diverse temporal modeling techniques to integrate with
MLLMs, aiming to achieve question-guided temporal modeling that leverages
pre-trained visual and textual alignment in MLLMs. We propose T-Former, a novel
temporal modeling method that creates a question-guided temporal bridge between
frame-wise visual perception and the reasoning capabilities of LLMs. Our
evaluation across multiple video QA benchmarks demonstrates that T-Former
competes favorably with existing temporal modeling approaches and aligns with
recent advancements in video QA.",2024-12-26T17:53:14Z,http://arxiv.org/abs/2412.19304v1,"Roberto Amoroso, Gengyuan Zhang, Rajat Koner, Lorenzo Baraldi, Rita Cucchiara, Volker Tresp"
RecLM: Recommendation Instruction Tuning,"Modern recommender systems aim to deeply understand users' complex
preferences through their past interactions. While deep collaborative filtering
approaches using Graph Neural Networks (GNNs) excel at capturing user-item
relationships, their effectiveness is limited when handling sparse data or
zero-shot scenarios, primarily due to constraints in ID-based embedding
functions. To address these challenges, we propose a model-agnostic
recommendation instruction-tuning paradigm that seamlessly integrates large
language models with collaborative filtering. Our proposed Recommendation
Language Model (RecLM) enhances the capture of user preference diversity
through a carefully designed reinforcement learning reward function that
facilitates self-augmentation of language models. Comprehensive evaluations
demonstrate significant advantages of our approach across various settings, and
its plug-and-play compatibility with state-of-the-art recommender systems
results in notable performance enhancements.",2024-12-26T17:51:54Z,http://arxiv.org/abs/2412.19302v1,"Yangqin Jiang, Yuhao Yang, Lianghao Xia, Da Luo, Kangyi Lin, Chao Huang"
"Sample Complexity of Data-driven Multistage Stochastic Programming under
  Markovian Uncertainty","This work is motivated by the challenges of applying the sample average
approximation (SAA) method to multistage stochastic programming with an unknown
continuous-state Markov process. While SAA is widely used in static and
two-stage stochastic optimization, it becomes computationally intractable in
general multistage settings as the time horizon $T$ increases. Indeed, the
number of samples required to obtain a reasonably accurate solution grows
exponentially$\text{ -- }$a phenomenon known as the curse of dimensionality
with respect to the time horizon. To overcome this limitation, we propose a
novel data-driven approach, the Markov Recombining Scenario Tree (MRST) method,
which constructs an approximate problem using only two independent trajectories
of historical data. Our analysis demonstrates that the MRST method achieves
polynomial sample complexity in $T$, providing a more efficient alternative to
SAA. Numerical experiments on the Linear Quadratic Gaussian problem show that
MRST outperforms SAA, addressing the curse of dimensionality.",2024-12-26T17:46:27Z,http://arxiv.org/abs/2412.19299v1,"Hyuk Park, Grani A. Hanasusanto"
Spatio-Temporal Differences in Bike Sharing Usage: A Tale of Six Cities,"This study investigates the spatio-temporal patterns of Bike Sharing System
(BSS) usage in six major cities: New York, London, Tokyo, Boston, Chicago and
Washington D.C. By analyzing data over a 30-day period with comparable climate
and average temperatures, we explored differences in BSS usage between weekdays
and weekends in those cities using Jensen-Shannon divergence (JSD) and rank
distribution analysis. Our findings reveal significant temporal differences in
BSS usage that were commonly observed in all cities, with weekday patterns
dominated by commute peaks and weekend patterns reflecting recreational
activities. Friday emerges as a transitional day, sharing the characteristics
of both weekdays and weekends. Meanwhile, docking station usage rank
distributions show remarkable consistency between weekdays and weekends for
most cities, with London being a unique anomaly. This study highlights the
potential of BSS data to uncover urban mobility patterns and the underlying
structures of cities. The results suggest that BSS usage reflects both
intrinsic user behavior and external influences such as urban planning.",2024-12-26T17:35:28Z,http://arxiv.org/abs/2412.19294v1,"Shu-ichi Kinoshita, Yuya Bando, Hiroki Sayama"
RAG with Differential Privacy,"Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to
provide *Large Language Models* (LLM) with fresh and relevant context,
mitigating the risk of hallucinations and improving the overall quality of
responses in environments with large and fast moving knowledge bases. However,
the integration of external documents into the generation process raises
significant privacy concerns. Indeed, when added to a prompt, it is not
possible to guarantee a response will not inadvertently expose confidential
data, leading to potential breaches of privacy and ethical dilemmas. This paper
explores a practical solution to this problem suitable to general knowledge
extraction from personal data. It shows *differentially private token
generation* is a viable approach to private RAG.",2024-12-26T17:34:26Z,http://arxiv.org/abs/2412.19291v1,Nicolas Grislain
"ViPCap: Retrieval Text-Based Visual Prompts for Lightweight Image
  Captioning","Recent lightweight image captioning models using retrieved data mainly focus
on text prompts. However, previous works only utilize the retrieved text as
text prompts, and the visual information relies only on the CLIP visual
embedding. Because of this issue, there is a limitation that the image
descriptions inherent in the prompt are not sufficiently reflected in the
visual embedding space. To tackle this issue, we propose ViPCap, a novel
retrieval text-based visual prompt for lightweight image captioning. ViPCap
leverages the retrieved text with image information as visual prompts to
enhance the ability of the model to capture relevant visual information. By
mapping text prompts into the CLIP space and generating multiple randomized
Gaussian distributions, our method leverages sampling to explore randomly
augmented distributions and effectively retrieves the semantic features that
contain image information. These retrieved features are integrated into the
image and designated as the visual prompt, leading to performance improvements
on the datasets such as COCO, Flickr30k, and NoCaps. Experimental results
demonstrate that ViPCap significantly outperforms prior lightweight captioning
models in efficiency and effectiveness, demonstrating the potential for a
plug-and-play solution.",2024-12-26T17:29:38Z,http://arxiv.org/abs/2412.19289v1,"Taewhan Kim, Soeun Lee, Si-Woo Kim, Dong-Jin Kim"
Reflective Gaussian Splatting,"Novel view synthesis has experienced significant advancements owing to
increasingly capable NeRF- and 3DGS-based methods. However, reflective object
reconstruction remains challenging, lacking a proper solution to achieve
real-time, high-quality rendering while accommodating inter-reflection. To fill
this gap, we introduce a Reflective Gaussian splatting (\textbf{Ref-Gaussian})
framework characterized with two components: (I) {\em Physically based deferred
rendering} that empowers the rendering equation with pixel-level material
properties via formulating split-sum approximation; (II) {\em Gaussian-grounded
inter-reflection} that realizes the desired inter-reflection function within a
Gaussian splatting paradigm for the first time. To enhance geometry modeling,
we further introduce material-aware normal propagation and an initial
per-Gaussian shading stage, along with 2D Gaussian primitives. Extensive
experiments on standard datasets demonstrate that Ref-Gaussian surpasses
existing approaches in terms of quantitative metrics, visual quality, and
compute efficiency. Further, we show that our method serves as a unified
solution for both reflective and non-reflective scenes, going beyond the
previous alternatives focusing on only reflective scenes. Also, we illustrate
that Ref-Gaussian supports more applications such as relighting and editing.",2024-12-26T16:58:35Z,http://arxiv.org/abs/2412.19282v1,"Yuxuan Yao, Zixuan Zeng, Chun Gu, Xiatian Zhu, Li Zhang"
Improving Generalization for AI-Synthesized Voice Detection,"AI-synthesized voice technology has the potential to create realistic human
voices for beneficial applications, but it can also be misused for malicious
purposes. While existing AI-synthesized voice detection models excel in
intra-domain evaluation, they face challenges in generalizing across different
domains, potentially becoming obsolete as new voice generators emerge. Current
solutions use diverse data and advanced machine learning techniques (e.g.,
domain-invariant representation, self-supervised learning), but are limited by
predefined vocoders and sensitivity to factors like background noise and
speaker identity. In this work, we introduce an innovative disentanglement
framework aimed at extracting domain-agnostic artifact features related to
vocoders. Utilizing these features, we enhance model learning in a flat loss
landscape, enabling escape from suboptimal solutions and improving
generalization. Extensive experiments on benchmarks show our approach
outperforms state-of-the-art methods, achieving up to 5.12% improvement in the
equal error rate metric in intra-domain and 7.59% in cross-domain evaluations.",2024-12-26T16:45:20Z,http://arxiv.org/abs/2412.19279v1,"Hainan Ren, Lin Li, Chun-Hao Liu, Xin Wang, Shu Hu"
Memory-Centric Computing: Recent Advances in Processing-in-DRAM,"Memory-centric computing aims to enable computation capability in and near
all places where data is generated and stored. As such, it can greatly reduce
the large negative performance and energy impact of data access and data
movement, by 1) fundamentally avoiding data movement, 2) reducing data access
latency &amp; energy, and 3) exploiting large parallelism of memory arrays. Many
recent studies show that memory-centric computing can largely improve system
performance &amp; energy efficiency. Major industrial vendors and startup companies
have recently introduced memory chips with sophisticated computation
capabilities. Going forward, both hardware and software stack should be
revisited and designed carefully to take advantage of memory-centric computing.
  This work describes several major recent advances in memory-centric
computing, specifically in Processing-in-DRAM, a paradigm where the operational
characteristics of a DRAM chip are exploited and enhanced to perform
computation on data stored in DRAM. Specifically, we describe 1) new techniques
that slightly modify DRAM chips to enable both enhanced computation capability
and easier programmability, 2) new experimental studies that demonstrate the
functionally-complete bulk-bitwise computational capability of real commercial
off-the-shelf DRAM chips, without any modifications to the DRAM chip or the
interface, and 3) new DRAM designs that improve access granularity &amp;
efficiency, unleashing the true potential of Processing-in-DRAM.",2024-12-26T16:31:40Z,http://arxiv.org/abs/2412.19275v1,"Onur Mutlu, Ataberk Olgun, Geraldo F. Oliveira, Ismail Emir Yuksel"
"Anvendelse av kunstig intelligens (KI) i Norge i norsk offentlig sektor
  2024","There are great expectations for the use of AI in Norway. On the other hand,
it is reported that the adoption of AI in Norway is slower than expected in
both the private and public sectors. Using responses from NOKIOS Technology
Radar 2017-2021, IT in Practice surveys conducted by Ramboll in 2021-2024, as
well as another national survey as part of a five-year cycle, this article
looks at reported and planned use of AI with a focus on local (municipalities)
and national government agencies. IT in practice is distributed to a large
number of Norwegian public agencies, with a response rate of over 5o percent.
The most recent data (2024) presented in this article is based on responses
from 335 public organizations, with 237 municipalities, and 98 public
organizations at the national or regional level. The survey confirms that the
use of AI is still at an early stage, although expectations are high for future
use.
  --
  Det er store forventninger til bruk av KI i Norge. P{\aa} den annen side
rapporteres det at adopsjonen av KI i Norge g{\aa}r tregere enn forventet
b{\aa}de i privat og offentlig sektor. Ved hjelp av svar fra NOKIOS
teknologiradar 2017-2021, IT i Praksis unders{\o}kelser utf{\o}rt av Ramb{\o}ll
i 2021-2024, samt en annen nasjonal unders{\o}kelse som en del av en
fem{\aa}rig syklus, ser vi i denne artikkelen p{\aa} rapportert og planlagt
bruk av KI med fokus p{\aa} lokale (kommuner) og nasjonale offentlige etater.
IT i praksis distribueres til en lang rekke norske offentlige virksomheter, med
en svarprosent p{\aa} over 50 prosent. De nyeste dataene (2024) presentert i
denne artikkelen er basert p{\aa} svar fra 335 offentlige organisasjoner, med
237 kommuner, og 98 offentlige organisasjoner p{\aa} nasjonalt eller regionalt
niv{\aa}. Unders{\o}kelsen bekrefter at bruken av KI fortsatt er p{\aa} et
tidlig stadium, selv om forventningene er h{\o}ye til fremtidig bruk.",2024-12-26T16:28:49Z,http://arxiv.org/abs/2412.19273v1,John Krogstie
Jet formation studies in AGN: a search for new targets,"In recent years, the jet formation region in active galaxies has been imaged
through mm-VLBI in few ideal targets, first and foremost M87. An important leap
forward for understanding jet launching could be made by identifying a larger
number of suitable objects, characterized by different accretion modes and jet
powers. In this article, we present 1 cm and 7 mm VLBI data of a sample of 16
poorly explored radio galaxies, comprising both High-Excitation (HEG) and
Low-Excitation Galaxies (LEG) and spanning a large range in radio power. The
combination of the sources vicinity (z&lt;0.1) with a large black hole mass
($\log{M_{\rm BH}}$&gt;8.5) results in a high spatial resolution in units of
Schwarzschild radii ($&lt;10^3-10^4$ $R_{\rm S}$), necessary for probing the
region where the jet is initially accelerated and collimated. We aim at
identifying the best candidates for follow-up observations with current and
future VLBI facilities. The observations were performed with the High
Sensitivity Array, including Effelsberg and the phased-VLA, which has allowed
us to characterize the sub-parsec properties of these faint jets and to
estimate their core brightness temperature and orientation. The number of
sources imaged on scales $\lesssim 10^3$ $R_{\rm S}$ is more than doubled by
our study. All targets were detected at both frequencies, and several present
two-sided jet structures. Several LEG jets show hints of limb-brightening. The
core brightness temperatures are generally below the equipartition value,
indicating that equipartition has not yet been reached and/or that the emission
is de-boosted. Among LEG, we identify 3C31, 3C66B, and 3C465 as the most
promising, combining a relatively high flux density (&gt;50 mJy) with superb
spatial resolution (&lt;500 $R_{\rm S}$) at 7 mm. The powerful HEG 3C452 is
interesting as well due to its highly symmetric, two-sided jet base.",2024-12-26T16:20:53Z,http://arxiv.org/abs/2412.19268v1,"B. Boccardi, L. Ricci, E. Madika, V. Bartolini, U. Bach, P. Grandi, E. Torresi, T. P. Krichbaum, J. A. Zensus"
"Search for a neutral gauge boson with nonuniversal fermion couplings in
  vector boson fusion processes in proton-proton collisions at $\sqrt{s}$ = 13
  TeV","The first search for a heavy neutral spin-1 gauge boson (Z') with
nonuniversal fermion couplings produced via vector boson fusion processes and
decaying to tau leptons or W bosons is presented. The analysis is performed
using LHC data at $\sqrt{s}$ = 13 TeV, collected from 2016 to 2018 and
corresponding to an integrated luminosity of 138 fb$^{-1}$. The data are
consistent with the standard model predictions. Upper limits are set on the
product of the cross section for production of the Z' boson and its branching
fraction to $\tau\tau$ or WW. The presence of a Z' boson decaying to
$\tau^+\tau^-$ (W$^+$W$^-$) is excluded for masses up to 2.45 (1.60) TeV,
depending on the Z' boson coupling to SM weak bosons, and assuming a Z' $\to$
$\tau^+\tau^-$ (W$^+$W$^-$) branching fraction of 50%.",2024-12-26T15:54:58Z,http://arxiv.org/abs/2412.19261v1,CMS Collaboration
"MEDEC: A Benchmark for Medical Error Detection and Correction in
  Clinical Notes","Several studies showed that Large Language Models (LLMs) can answer medical
questions correctly, even outperforming the average human score in some medical
exams. However, to our knowledge, no study has been conducted to assess the
ability of language models to validate existing or generated medical text for
correctness and consistency. In this paper, we introduce MEDEC
(https://github.com/abachaa/MEDEC), the first publicly available benchmark for
medical error detection and correction in clinical notes, covering five types
of errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal
Organism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes
from three US hospital systems that were not previously seen by any LLM. The
dataset has been used for the MEDIQA-CORR shared task to evaluate seventeen
participating systems [Ben Abacha et al., 2024]. In this paper, we describe the
data creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,
Claude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and
correcting medical errors requiring both medical knowledge and reasoning
capabilities. We also conducted a comparative study where two medical doctors
performed the same task on the MEDEC test set. The results showed that MEDEC is
a sufficiently challenging benchmark to assess the ability of models to
validate existing or generated notes and to correct medical errors. We also
found that although recent LLMs have a good performance in error detection and
correction, they are still outperformed by medical doctors in these tasks. We
discuss the potential factors behind this gap, the insights from our
experiments, the limitations of current evaluation metrics, and share potential
pointers for future research.",2024-12-26T15:54:10Z,http://arxiv.org/abs/2412.19260v1,"Asma Ben Abacha, Wen-wai Yim, Yujuan Fu, Zhaoyi Sun, Meliha Yetisgen, Fei Xia, Thomas Lin"
"VoiceDiT: Dual-Condition Diffusion Transformer for Environment-Aware
  Speech Synthesis","We present VoiceDiT, a multi-modal generative model for producing
environment-aware speech and audio from text and visual prompts. While aligning
speech with text is crucial for intelligible speech, achieving this alignment
in noisy conditions remains a significant and underexplored challenge in the
field. To address this, we present a novel audio generation pipeline named
VoiceDiT. This pipeline includes three key components: (1) the creation of a
large-scale synthetic speech dataset for pre-training and a refined real-world
speech dataset for fine-tuning, (2) the Dual-DiT, a model designed to
efficiently preserve aligned speech information while accurately reflecting
environmental conditions, and (3) a diffusion-based Image-to-Audio Translator
that allows the model to bridge the gap between audio and image, facilitating
the generation of environmental sound that aligns with the multi-modal prompts.
Extensive experimental results demonstrate that VoiceDiT outperforms previous
models on real-world datasets, showcasing significant improvements in both
audio quality and modality integration.",2024-12-26T15:52:58Z,http://arxiv.org/abs/2412.19259v1,"Jaemin Jung, Junseok Ahn, Chaeyoung Jung, Tan Dat Nguyen, Youngjoon Jang, Joon Son Chung"
Swarm Contract: A Multi-Sovereign Agent Consensus Mechanism,"Traditional smart contracts on blockchains excel at on-chain, deterministic
logic. However, they have inherent limitations when dealing with large-scale
off-chain data, dynamic multi-step workflows, and scenarios requiring high
flexibility or iterative updates. In this paper, we propose the concept of a
""Swarm Contract"" (Swarm), a multi-agent mechanism wherein several digital life
forms (DLF) or Sovereign Agents (SA) collectively handle complex tasks in
Trusted Execution Environments (TEE). These digital entities are defined as
autonomous software agents that own their code, state, and possibly on-chain
assets, while operating free from centralized control.
  By leveraging a simple multi-signature wallet on-chain, Swarm moves most of
the logic off-chain, achieving trust minimization through multi-agent consensus
rather than a single monolithic on-chain contract. We illustrate these ideas
with a lightweight off-chain auction example - minting and selling 10,000
identical NFTs - to showcase how off-chain coordination can determine a
clearing price and finalize distribution, with each step performed collectively
by multiple agents in TEE. This approach broadens the scope of trustless and
decentralized solutions, potentially benefiting DAO governance, multi-modal
data processing, and cross-chain interoperability.",2024-12-26T15:46:56Z,http://arxiv.org/abs/2412.19256v1,Haowei Yang
"Leveraging Self-Training and Variational Autoencoder for Agitation
  Detection in People with Dementia Using Wearable Sensors","Dementia is a neurodegenerative disorder that has been growing among elder
people over the past decades. This growth profoundly impacts the quality of
life for patients and caregivers due to the symptoms arising from it. Agitation
and aggression (AA) are some of the symptoms of people with severe dementia
(PwD) in long-term care or hospitals. AA not only causes discomfort but also
puts the patients or others at potential risk. Existing monitoring solutions
utilizing different wearable sensors integrated with Artificial Intelligence
(AI) offer a way to detect AA early enough for timely and adequate medical
intervention. However, most studies are limited by the availability of
accurately labeled datasets, which significantly affects the efficacy of such
solutions in real-world scenarios. This study presents a novel comprehensive
approach to detect AA in PwD using physiological data from the Empatica E4
wristbands. The research creates a diverse dataset, consisting of three
distinct datasets gathered from 14 participants across multiple hospitals in
Canada. These datasets have not been extensively explored due to their limited
labeling. We propose a novel approach employing self-training and a variational
autoencoder (VAE) to detect AA in PwD effectively. The proposed approach aims
to learn the representation of the features extracted using the VAE and then
uses a semi-supervised block to generate labels, classify events, and detect
AA. We demonstrate that combining Self-Training and Variational Autoencoder
mechanism significantly improves model performance in classifying AA in PwD.
Among the tested techniques, the XGBoost classifier achieved the highest
accuracy of 90.16\%. By effectively addressing the challenge of limited labeled
data, the proposed system not only learns new labels but also proves its
superiority in detecting AA.",2024-12-26T15:34:25Z,http://arxiv.org/abs/2412.19254v1,"Abeer Badawi, Somayya Elmoghazy, Samira Choudhury, Khalid Elgazzar, Amer Burhan"
"Localized exploration in contextual dynamic pricing achieves
  dimension-free regret","We study the problem of contextual dynamic pricing with a linear demand
model. We propose a novel localized exploration-then-commit (LetC) algorithm
which starts with a pure exploration stage, followed by a refinement stage that
explores near the learned optimal pricing policy, and finally enters a pure
exploitation stage. The algorithm is shown to achieve a minimax optimal,
dimension-free regret bound when the time horizon exceeds a polynomial of the
covariate dimension. Furthermore, we provide a general theoretical framework
that encompasses the entire time spectrum, demonstrating how to balance
exploration and exploitation when the horizon is limited. The analysis is
powered by a novel critical inequality that depicts the
exploration-exploitation trade-off in dynamic pricing, mirroring its existing
counterpart for the bias-variance trade-off in regularized regression. Our
theoretical results are validated by extensive experiments on synthetic and
real-world data.",2024-12-26T15:29:58Z,http://arxiv.org/abs/2412.19252v1,"Jinhang Chai, Yaqi Duan, Jianqing Fan, Kaizheng Wang"
Network double autoregression,"Modeling high-dimensional time series with simple structures is a challenging
problem. This paper proposes a network double autoregression (NDAR) model,
which combines the advantages of network structure and the double
autoregression (DAR) model, to handle high-dimensional, conditionally
heteroscedastic, and network-structured data within a simple framework. The
parameters of the model are estimated using quasi-maximum likelihood
estimation, and the asymptotic properties of the estimators are derived. The
selection of the model's lag order will be based on the Bayesian information
criterion. Finite-sample simulations show that the proposed model performs well
even with moderate time dimensions and network sizes. Finally, the model is
applied to analyze three different categories of stock data.",2024-12-26T15:28:41Z,http://arxiv.org/abs/2412.19251v1,"Tingting Li, Hao Wang"
"A Space Lower Bound for Approximate Membership with Duplicate Insertions
  or Deletions of Nonelements","Designs of data structures for approximate membership queries with
false-positive errors that support both insertions and deletions stipulate the
following two conditions: (1) Duplicate insertions are prohibited, i.e., it is
prohibited to insert an element $x$ if $x$ is currently a member of the
dataset. (2) Deletions of nonelements are prohibited, i.e., it is prohibited to
delete $x$ if $x$ is not currently a member of the dataset. Under these
conditions, the space required for the approximate representation of a datasets
of cardinality $n$ with a false-positive probability of $\epsilon^{+}$ is at
most $(1+o(1))n\cdot\log_2 (1/\epsilon^{+}) + O(n)$ bits [Bender et al., 2018;
Bercea and Even, 2019].
  We prove that if these conditions are lifted, then the space required for the
approximate representation of datasets of cardinality $n$ from a universe of
cardinality $u$ is at least $\frac 12 \cdot (1-\epsilon^{+} -\frac 1n)\cdot
\log \binom{u}{n} -O(n)$ bits.",2024-12-26T15:21:42Z,http://arxiv.org/abs/2412.19249v1,"Aryan Agarwala, Guy Even"
Functional structural equation modeling with latent variables,"Handling latent variables in Structural Equation Models (SEMs) in a case
where both the latent variables and their corresponding indicators in the
measurement error part of the model are random curves presents significant
challenges, especially with sparse data. In this paper, we develop a novel
family of Functional Structural Equation Models (FSEMs) that incorporate latent
variables modeled as Gaussian Processes (GPs). The introduced FSEMs are built
upon functional regression models having response variables modeled as
underlying GPs. The model flexibly adapts to cases when the random curves'
realizations are observed only over a sparse subset of the domain, and the
inferential framework is based on a restricted maximum likelihood approach. The
advantage of this framework lies in its ability and flexibility in handling
various data scenarios, including regularly and irregularly spaced points and
thus missing data. To extract smooth estimates for the functional parameters,
we employ a penalized likelihood approach that selects the smoothing parameters
using a cross-validation method. We evaluate the performance of the proposed
model using simulation studies and a real data example, which suggests that our
model performs well in practice. The uncertainty associated with the estimates
of the functional coefficients is also assessed by constructing confidence
regions for each estimate. The goodness of fit indices that are commonly used
to evaluate the fit of SEMs are developed for the FSEMs introduced in this
paper. Overall, the proposed method is a promising approach for modeling
functional data in SEMs with functional latent variables.",2024-12-26T14:57:14Z,http://arxiv.org/abs/2412.19242v1,"Fatemeh Asgari, Valeria Vitelli, Uta Sailer"
"Effect of Peak Absolute Magnitude of Type Ia Supernovae and Sound
  Horizon Values on Hubble Tension using DESI results","We apply data-motivated priors on the peak absolute magnitude of Type Ia
supernovae ($M$), and on the sound horizon at the drag epoch ($r_d$), to study
their impact on the Hubble tension, when compared to the Planck estimated value
of the Hubble constant. We use the data from Pantheon$+$, cosmic chronometers,
and the latest DESI BAO results for this purpose. We reaffirm the fact that
there is a degeneracy between $M$ and $r_d$, and modifying the $r_d$ values to
reconcile the Hubble tension also requires a change in the peak absolute
magnitude $M$. For certain $M$ and $r_d$ priors, the tension is found to reduce
to as low as (1.2-2) $\sigma$.",2024-12-26T14:51:09Z,http://arxiv.org/abs/2412.19240v1,"Shubham Barua, Shantanu Desai"
SeaMo: A Multi-Seasonal and Multimodal Remote Sensing Foundation Model,"Remote Sensing (RS) data contains a wealth of multi-dimensional information
crucial for Earth observation. Owing to its vast volume, diverse sources, and
temporal properties, RS data is highly suitable for the development of large
Visual Foundation Models (VFMs). VFMs act as robust feature extractors,
learning from extensive RS data, and are subsequently fine-tuned for deployment
in various geoscientific tasks. However, current VFMs in the RS domain are
predominantly pretrained and tailored exclusively for specific characteristics
of RS imagery, neglecting the potential of utilizing the multi-dimensional
properties of RS data. Therefore, in this work, we propose SeaMo, a pioneering
visual foundation model that integrates multi-seasonal and multimodal
information in the RS field. SeaMo is designed to harness multiple properties
of RS data. Within the masked image modeling framework, we employ non-aligned
cropping techniques to extract spatial properties, use multi-source inputs for
multimodal integration, and incorporate temporal-multimodal fusion blocks for
effective assimilation of multi-seasonal data. SeaMo explicitly models the
multi-dimensional properties of RS data, making the model more comprehensive,
robust, and versatile. We applied SeaMo to several downstream geoscience tasks,
which demonstrated exceptional performance. Extensive ablation studies were
conducted to validate the model's superiority.",2024-12-26T14:40:38Z,http://arxiv.org/abs/2412.19237v1,"Xuyang Li, Danfeng Hong, Chenyu Li, Jocelyn Chanussot"
"Are Two Hidden Layers Still Enough for the Physics-Informed Neural
  Networks?","The article discusses the development of various methods and techniques for
initializing and training neural networks with a single hidden layer, as well
as training a separable physics-informed neural network consisting of neural
networks with a single hidden layer to solve physical problems described by
ordinary differential equations (ODEs) and partial differential equations
(PDEs). A method for strictly deterministic initialization of a neural network
with one hidden layer for solving physical problems described by an ODE is
proposed. Modifications to existing methods for weighting the loss function are
given, as well as new methods developed for training strictly
deterministic-initialized neural networks to solve ODEs (detaching, additional
weighting based on the second derivative, predicted solution-based weighting,
relative residuals). An algorithm for physics-informed data-driven
initialization of a neural network with one hidden layer is proposed. A neural
network with pronounced generalizing properties is presented, whose
generalizing abilities of which can be precisely controlled by adjusting
network parameters. A metric for measuring the generalization of such neural
network has been introduced. A gradient-free neuron-by-neuron fitting method
has been developed for adjusting the parameters of a single-hidden-layer neural
network, which does not require the use of an optimizer or solver for its
implementation. The proposed methods have been extended to 2D problems using
the separable physics-informed neural networks approach. Numerous experiments
have been carried out to develop the above methods and approaches. Experiments
on physical problems, such as solving various ODEs and PDEs, have demonstrated
that these methods for initializing and training neural networks with one or
two hidden layers (SPINN) achieve competitive accuracy and, in some cases,
state-of-the-art results.",2024-12-26T14:30:54Z,http://arxiv.org/abs/2412.19235v1,"Vasiliy A. Es'kin, Alexey O. Malkhanov, Mikhail E. Smorkalov"
"Virtual Nodes Can Help: Tackling Distribution Shifts in Federated Graph
  Learning","Federated Graph Learning (FGL) enables multiple clients to jointly train
powerful graph learning models, e.g., Graph Neural Networks (GNNs), without
sharing their local graph data for graph-related downstream tasks, such as
graph property prediction. In the real world, however, the graph data can
suffer from significant distribution shifts across clients as the clients may
collect their graph data for different purposes. In particular, graph
properties are usually associated with invariant label-relevant substructures
(i.e., subgraphs) across clients, while label-irrelevant substructures can
appear in a client-specific manner. The issue of distribution shifts of graph
data hinders the efficiency of GNN training and leads to serious performance
degradation in FGL. To tackle the aforementioned issue, we propose a novel FGL
framework entitled FedVN that eliminates distribution shifts through
client-specific graph augmentation strategies with multiple learnable Virtual
Nodes (VNs). Specifically, FedVN lets the clients jointly learn a set of shared
VNs while training a global GNN model. To eliminate distribution shifts, each
client trains a personalized edge generator that determines how the VNs connect
local graphs in a client-specific manner. Furthermore, we provide theoretical
analyses indicating that FedVN can eliminate distribution shifts of graph data
across clients. Comprehensive experiments on four datasets under five settings
demonstrate the superiority of our proposed FedVN over nine baselines.",2024-12-26T14:16:15Z,http://arxiv.org/abs/2412.19229v1,"Xingbo Fu, Zihan Chen, Yinhan He, Song Wang, Binchi Zhang, Chen Chen, Jundong Li"
Multi-view Fake News Detection Model Based on Dynamic Hypergraph,"With the rapid development of online social networks and the inadequacies in
content moderation mechanisms, the detection of fake news has emerged as a
pressing concern for the public. Various methods have been proposed for fake
news detection, including text-based approaches as well as a series of
graph-based approaches. However, the deceptive nature of fake news renders
text-based approaches less effective. Propagation tree-based methods focus on
the propagation process of individual news, capturing pairwise relationships
but lacking the capability to capture high-order complex relationships. Large
heterogeneous graph-based approaches necessitate the incorporation of
substantial additional information beyond news text and user data, while
hypergraph-based approaches rely on predefined hypergraph structures. To tackle
these issues, we propose a novel dynamic hypergraph-based multi-view fake news
detection model (DHy-MFND) that learns news embeddings across three distinct
views: text-level, propagation tree-level, and hypergraph-level. By employing
hypergraph structures to model complex high-order relationships among multiple
news pieces and introducing dynamic hypergraph structure learning, we optimize
predefined hypergraph structures while learning news embeddings. Additionally,
we introduce contrastive learning to capture authenticity-relevant embeddings
across different views. Extensive experiments on two benchmark datasets
demonstrate the effectiveness of our proposed DHy-MFND compared with a broad
range of competing baselines.",2024-12-26T14:05:51Z,http://arxiv.org/abs/2412.19227v1,"Rongping Ye, Xiaobing Pei"
"Completion as Enhancement: A Degradation-Aware Selective Image Guided
  Network for Depth Completion","In this paper, we introduce the Selective Image Guided Network (SigNet), a
novel degradation-aware framework that transforms depth completion into depth
enhancement for the first time. Moving beyond direct completion using
convolutional neural networks (CNNs), SigNet initially densifies sparse depth
data through non-CNN densification tools to obtain coarse yet dense depth. This
approach eliminates the mismatch and ambiguity caused by direct convolution
over irregularly sampled sparse data. Subsequently, SigNet redefines completion
as enhancement, establishing a self-supervised degradation bridge between the
coarse depth and the targeted dense depth for effective RGB-D fusion. To
achieve this, SigNet leverages the implicit degradation to adaptively select
high-frequency components (e.g., edges) of RGB data to compensate for the
coarse depth. This degradation is further integrated into a multi-modal
conditional Mamba, dynamically generating the state parameters to enable
efficient global high-frequency information interaction. We conduct extensive
experiments on the NYUv2, DIML, SUN RGBD, and TOFDC datasets, demonstrating the
state-of-the-art (SOTA) performance of SigNet.",2024-12-26T14:05:01Z,http://arxiv.org/abs/2412.19225v1,"Zhiqiang Yan, Zhengxue Wang, Kun Wang, Jun Li, Jian Yang"
"Interference-Robust Broadband Rapidly-Varying MIMO Communications: A
  Knowledge-Data Dual Driven Framework","A novel time-efficient framework is proposed for improving the robustness of
a broadband multiple-input multiple-output (MIMO) system against unknown
interference under rapidly-varying channels. A mean-squared error (MSE)
minimization problem is formulated by optimizing the beamformers employed.
Since the unknown interference statistics are the premise for solving the
formulated problem, an interference statistics tracking (IST) module is first
designed. The IST module exploits both the time- and spatial-domain
correlations of the interference-plus-noise (IPN) covariance for the future
predictions with data training. Compared to the conventional signal-free space
sampling approach, the IST module can realize zero-pilot and low-latency
estimation. Subsequently, an interference-resistant hybrid beamforming (IR-HBF)
module is presented, which incorporates both the prior knowledge of the
theoretical optimization method as well as the data-fed training. Taking
advantage of the interpretable network structure, the IR-HBF module enables the
simplified mapping from the interference statistics to the beamforming weights.
The simulations are executed in high-mobility scenarios, where the numerical
results unveil that: 1) the proposed IST module attains promising prediction
accuracy compared to the conventional counterparts under different snapshot
sampling errors; and 2) the proposed IR-HBF module achieves lower MSE with
significantly reduced computational complexity.",2024-12-26T13:59:08Z,http://arxiv.org/abs/2412.19221v1,"Jingjing Zhao, Jing Su, Xianchi Lv, Kaiquan Cai, Yanbo Zhu, Yuanwei Liu, Naofal Al-Dhahir"
"Applying the maximum entropy principle to multi-species neural networks
  improves species distribution models","The rapid expansion of citizen science initiatives has led to a significant
growth of biodiversity databases, and particularly presence-only (PO)
observations. PO data are invaluable for understanding species distributions
and their dynamics, but their use in Species Distribution Models (SDM) is
curtailed by sampling biases and the lack of information on absences. Poisson
point processes are widely used for SDMs, with Maxent being one of the most
popular methods. Maxent maximises the entropy of a probability distribution
across sites as a function of predefined transformations of environmental
variables, called features. In contrast, neural networks and deep learning have
emerged as a promising technique for automatic feature extraction from complex
input variables. In this paper, we propose DeepMaxent, which harnesses neural
networks to automatically learn shared features among species, using the
maximum entropy principle. To do so, it employs a normalised Poisson loss where
for each species, presence probabilities across sites are modelled by a neural
network. We evaluate DeepMaxent on a benchmark dataset known for its spatial
sampling biases, using PO data for calibration and presence-absence (PA) data
for validation across six regions with different biological groups and
environmental covariates. Our results indicate that DeepMaxent improves model
performance over Maxent and other state-of-the-art SDMs across regions and
taxonomic groups. The method performs particularly well in regions of uneven
sampling, demonstrating substantial potential to improve species distribution
modelling. The method opens the possibility to learn more robust environmental
features predicting jointly many species and scales to arbitrary large numbers
of sites without an increased memory demand.",2024-12-26T13:47:04Z,http://arxiv.org/abs/2412.19217v1,"Maxime Ryckewaert, Diego Marcos, Christophe Botella, Maximilien Servajean, Pierre Bonnet, Alexis Joly"
"Optimizing Fantasy Sports Team Selection with Deep Reinforcement
  Learning","Fantasy sports, particularly fantasy cricket, have garnered immense
popularity in India in recent years, offering enthusiasts the opportunity to
engage in strategic team-building and compete based on the real-world
performance of professional athletes. In this paper, we address the challenge
of optimizing fantasy cricket team selection using reinforcement learning (RL)
techniques. By framing the team creation process as a sequential
decision-making problem, we aim to develop a model that can adaptively select
players to maximize the team's potential performance. Our approach leverages
historical player data to train RL algorithms, which then predict future
performance and optimize team composition. This not only represents a huge
business opportunity by enabling more accurate predictions of high-performing
teams but also enhances the overall user experience. Through empirical
evaluation and comparison with traditional fantasy team drafting methods, we
demonstrate the effectiveness of RL in constructing competitive fantasy teams.
Our results show that RL-based strategies provide valuable insights into player
selection in fantasy sports.",2024-12-26T13:36:18Z,http://arxiv.org/abs/2412.19215v1,"Shamik Bhattacharjee, Kamlesh Marathe, Hitesh Kapoor, Nilesh Patil"
Primordial Power Spectrum of Five Dimensional Uniform Inflation,"Five dimensional (5D) uniform inflation describes a de Sitter (or
approximate) solution of 5D Einstein equations, with cosmological constant and
a 5D Planck scale $M_* \sim 10^9$ GeV. During the inflationary period all
dimensions (compact and non-compact) expand exponentially in terms of the 5D
proper time. This set-up requires about 40 $e$-folds to expand the fifth
dimension from the fundamental length to the micron size. At the end of 5D
inflation (or at any given moment during the inflationary phase) one can
interpret the solution in terms of 4D fields using 4D Planck units from the
relation $M_p^2 = 2 \pi R M_*^3$, which amounts going to the 4D Einstein frame.
This implies that if the compactification radius $R$ expands $N$ $e$-folds,
then the 3D space would expand $3N/2$ $e$-folds as a result of a uniform 5D
inflation. We reexamine the primordial power spectrum predicted by this model
and show that it is consistent with Planck's measurements of the comic
microwave background. The best-fit to Planck data corresponds to $R \sim
10~\mu$m. A departure of the angular power spectrum predicted by 4D cosmology
is visible at multipole moment $\ell \sim 7$.",2024-12-26T13:24:36Z,http://arxiv.org/abs/2412.19213v1,"Luis A. Anchordoqui, Ignatios Antoniadis"
"Towards Better Spherical Sliced-Wasserstein Distance Learning with
  Data-Adaptive Discriminative Projection Direction","Spherical Sliced-Wasserstein (SSW) has recently been proposed to measure the
discrepancy between spherical data distributions in various fields, such as
geology, medical domains, computer vision, and deep representation learning.
However, in the original SSW, all projection directions are treated equally,
which is too idealistic and cannot accurately reflect the importance of
different projection directions for various data distributions. To address this
issue, we propose a novel data-adaptive Discriminative Spherical
Sliced-Wasserstein (DSSW) distance, which utilizes a projected energy function
to determine the discriminative projection direction for SSW. In our new DSSW,
we introduce two types of projected energy functions to generate the weights
for projection directions with complete theoretical guarantees. The first type
employs a non-parametric deterministic function that transforms the projected
Wasserstein distance into its corresponding weight in each projection
direction. This improves the performance of the original SSW distance with
negligible additional computational overhead. The second type utilizes a neural
network-induced function that learns the projection direction weight through a
parameterized neural network based on data projections. This further enhances
the performance of the original SSW distance with less extra computational
overhead. Finally, we evaluate the performance of our proposed DSSW by
comparing it with several state-of-the-art methods across a variety of machine
learning tasks, including gradient flows, density estimation on real earth
data, and self-supervised learning.",2024-12-26T13:23:37Z,http://arxiv.org/abs/2412.19212v1,"Hongliang Zhang, Shuo Chen, Lei Luo, Jian Yang"
"Large Language Models Meet Graph Neural Networks: A Perspective of Graph
  Mining","Graph mining is an important area in data mining and machine learning that
involves extracting valuable information from graph-structured data. In recent
years, significant progress has been made in this field through the development
of graph neural networks (GNNs). However, GNNs are still deficient in
generalizing to diverse graph data. Aiming to this issue, Large Language Models
(LLMs) could provide new solutions for graph mining tasks with their superior
semantic understanding. In this review, we systematically review the
combination and application techniques of LLMs and GNNs and present a novel
taxonomy for research in this interdisciplinary field, which involves three
main categories: GNN-driving-LLM, LLM-driving-GNN, and GNN-LLM-co-driving.
Within this framework, we reveal the capabilities of LLMs in enhancing graph
feature extraction as well as improving the effectiveness of downstream tasks
such as node classification, link prediction, and community detection. Although
LLMs have demonstrated their great potential in handling graph-structured data,
their high computational requirements and complexity remain challenges. Future
research needs to continue to explore how to efficiently fuse LLMs and GNNs to
achieve more powerful graph learning and reasoning capabilities and provide new
impetus for the development of graph mining techniques.",2024-12-26T13:21:09Z,http://arxiv.org/abs/2412.19211v1,"Yuxin You, Zhen Liu, Xiangchao Wen, Yongtao Zhang, Wei Ai"
Context-Aware Deep Learning for Multi Modal Depression Detection,"In this study, we focus on automated approaches to detect depression from
clinical interviews using multi-modal machine learning (ML). Our approach
differentiates from other successful ML methods such as context-aware analysis
through feature engineering and end-to-end deep neural networks for depression
detection utilizing the Distress Analysis Interview Corpus. We propose a novel
method that incorporates: (1) pre-trained Transformer combined with data
augmentation based on topic modelling for textual data; and (2) deep 1D
convolutional neural network (CNN) for acoustic feature modeling. The
simulation results demonstrate the effectiveness of the proposed method for
training multi-modal deep learning models. Our deep 1D CNN and Transformer
models achieved state-of-the-art performance for audio and text modalities
respectively. Combining them in a multi-modal framework also outperforms
state-of-the-art for the combined setting. Code available at
https://github.com/genandlam/multi-modal-depression-detection",2024-12-26T13:19:26Z,http://arxiv.org/abs/2412.19209v1,"Genevieve Lam, Huang Dongyan, Weisi Lin"
"Open cluster dissolution rate and the initial cluster mass function in
  the solar neighbourhood. Modelling the age and mass distributions of clusters
  observed by Gaia","Context. The dissolution rate of open clusters (OCs) and integration of their
stars into the Milky Way's field population has been previously explored using
their age distribution. With the advent of the Gaia mission, we have an
exceptional opportunity to revisit and enhance these studies with ages and
masses from high quality data. Aims. To build a comprehensive Gaia-based OC
mass catalogue which, combined with the age distribution, allows a deeper
investigation of the disruption experienced by OCs within the solar
neighbourhood. Methods. Masses were determined by comparing luminosity
distributions to theoretical luminosity functions. The limiting and core radii
of the clusters were obtained by fitting the King function to their observed
density profiles. We examined the disruption process through simulations of the
build-up and mass evolution of a population of OCs which were compared to the
observed mass and age distributions. Results. Our analysis yielded an OC mass
distribution with a peak at $log(M)$ = 2.7 dex ($\sim 500 M_{\odot}$), as well
as radii for 1724 OCs. Our simulations showed that using a power-law Initial
Cluster Mass Function (ICMF) no parameters were able to reproduce the observed
mass distribution. Moreover, we find that a skew log-normal ICMF provides a
good match to the observations and that the disruption time of a $10^4
M{_\odot}$ OC is $t_4^{tot} = 2.9 \pm 0.4$ Gyr. Conclusions. Our results
indicate that the OC disruption time $t_4^{tot}$ is about twice longer than
previous estimates based solely on OC age distributions. We find that the shape
of the ICMF for bound OCs differs from that of embedded clusters, which could
imply a low typical star formation efficiency of $\leq 20\%$ in OCs. Our
results also suggest a lower limit of $\sim 60 M{_\odot}$ for bound OCs in the
solar neighbourhood.",2024-12-26T12:54:51Z,http://arxiv.org/abs/2412.19204v1,"Duarte Almeida, André Moitinho, Sandro Moreira"
"GAIS: A Novel Approach to Instance Selection with Graph Attention
  Networks","Instance selection (IS) is a crucial technique in machine learning that aims
to reduce dataset size while maintaining model performance. This paper
introduces a novel method called Graph Attention-based Instance Selection
(GAIS), which leverages Graph Attention Networks (GATs) to identify the most
informative instances in a dataset. GAIS represents the data as a graph and
uses GATs to learn node representations, enabling it to capture complex
relationships between instances. The method processes data in chunks, applies
random masking and similarity thresholding during graph construction, and
selects instances based on confidence scores from the trained GAT model.
Experiments on 13 diverse datasets demonstrate that GAIS consistently
outperforms traditional IS methods in terms of effectiveness, achieving high
reduction rates (average 96\%) while maintaining or improving model
performance. Although GAIS exhibits slightly higher computational costs, its
superior performance in maintaining accuracy with significantly reduced
training data makes it a promising approach for graph-based data selection.",2024-12-26T12:51:14Z,http://arxiv.org/abs/2412.19201v1,"Zahiriddin Rustamov, Ayham Zaitouny, Rafat Damseh, Nazar Zaki"
"Personalized Dynamic Music Emotion Recognition with Dual-Scale
  Attention-Based Meta-Learning","Dynamic Music Emotion Recognition (DMER) aims to predict the emotion of
different moments in music, playing a crucial role in music information
retrieval. The existing DMER methods struggle to capture long-term dependencies
when dealing with sequence data, which limits their performance. Furthermore,
these methods often overlook the influence of individual differences on emotion
perception, even though everyone has their own personalized emotional
perception in the real world. Motivated by these issues, we explore more
effective sequence processing methods and introduce the Personalized DMER
(PDMER) problem, which requires models to predict emotions that align with
personalized perception. Specifically, we propose a Dual-Scale Attention-Based
Meta-Learning (DSAML) method. This method fuses features from a dual-scale
feature extractor and captures both short and long-term dependencies using a
dual-scale attention transformer, improving the performance in traditional
DMER. To achieve PDMER, we design a novel task construction strategy that
divides tasks by annotators. Samples in a task are annotated by the same
annotator, ensuring consistent perception. Leveraging this strategy alongside
meta-learning, DSAML can predict personalized perception of emotions with just
one personalized annotation sample. Our objective and subjective experiments
demonstrate that our method can achieve state-of-the-art performance in both
traditional DMER and PDMER.",2024-12-26T12:47:35Z,http://arxiv.org/abs/2412.19200v1,"Dengming Zhang, Weitao You, Ziheng Liu, Lingyun Sun, Pei Chen"
"Implications for the non-Gaussianity of primordial gravitational waves
  from pulsar timing arrays","The detection of a stochastic signal by recent pulsar timing array (PTA)
collaborations, including NANOGrav, PPTA, EPTA+InPTA, CPTA and MPTA, has opened
a new window to explore gravitational waves (GWs) at nanohertz frequencies.
Motivated by the possibility that such a signal could arise from primordial
gravitational waves (PGWs), we investigate the implications of tensor
non-Gaussianity for the PGW power spectrum. Utilizing PTA data sets, we provide
constraints on local-type tensor non-Gaussianity parameter ${F}_{\mathrm{NL}}$.
We find $|{F}_{\mathrm{NL}}|\lesssim 7.97$ for a log-normal PGW power spectrum.
Our analysis reveals that even moderate tensor non-Gaussianity can lead to
significant deviations from standard predictions, thereby offering a novel
means to test inflationary scenarios and probe the underlying dynamics of the
early Universe. Future multi-band GW observatories, such as LISA, Taiji, and
TianQin, will be instrumental in complementing these efforts and further
refining our understanding of tensor non-Gaussianity.",2024-12-26T12:31:58Z,http://arxiv.org/abs/2412.19196v1,"Zhi-Zhang Peng, You Wu, Lang Liu"
"Game-Theoretically Secure Distributed Protocols for Fair Allocation in
  Coalitional Games","We consider game-theoretically secure distributed protocols for coalition
games that approximate the Shapley value with small multiplicative error. Since
all known existing approximation algorithms for the Shapley value are
randomized, it is a challenge to design efficient distributed protocols among
mutually distrusted players when there is no central authority to generate
unbiased randomness. The game-theoretic notion of maximin security has been
proposed to offer guarantees to an honest player's reward even if all other
players are susceptible to an adversary.
  Permutation sampling is often used in approximation algorithms for the
Shapley value. A previous work in 1994 by Zlotkin et al. proposed a simple
constant-round distributed permutation generation protocol based on commitment
scheme, but it is vulnerable to rushing attacks. The protocol, however, can
detect such attacks.
  In this work, we model the limited resources of an adversary by a violation
budget that determines how many times it can perform such detectable attacks.
Therefore, by repeating the number of permutation samples, an honest player's
reward can be guaranteed to be close to its Shapley value. We explore both high
probability and expected maximin security. We obtain an upper bound on the
number of permutation samples for high probability maximin security, even with
an unknown violation budget. Furthermore, we establish a matching lower bound
for the weaker notion of expected maximin security in specific permutation
generation protocols. We have also performed experiments on both synthetic and
real data to empirically verify our results.",2024-12-26T12:13:21Z,http://arxiv.org/abs/2412.19192v1,"T-H. Hubert Chan, Qipeng Kuang, Quan Xue"
New Physics in the 3-3-1 models,"Two main ingredients of current particle physics
  such as local gauge symmetry and mass generation via the Higgs mechanism
being basic ground of the Standard Model are widely confirmed by
  experimental data. However, some problems such as neutrino masses, dark
matter, baryon asymmetry of Universe have clearly indicated that the Standard
Model cannot be the ultimate theory of nature. To surpass the mentioned
puzzles,
  many extensions of the Standard Model (called beyond Standard Model) have
been proposed. Among beyond Standard Models, the 3-3-1 models have some
intriguing features and they get wide attention. The pioneer models develop in
some directions. In this paper, %some new main versions of the 3-3-1 models and
their consequences are presented.",2024-12-26T11:55:43Z,http://arxiv.org/abs/2412.19188v1,H. N. Long
"Multi-Head Attention Driven Dynamic Visual-Semantic Embedding for
  Enhanced Image-Text Matching","With the rapid development of multimodal learning, the image-text matching
task, as a bridge connecting vision and language, has become increasingly
important. Based on existing research, this study proposes an innovative visual
semantic embedding model, Multi-Headed Consensus-Aware Visual-Semantic
Embedding (MH-CVSE). This model introduces a multi-head self-attention
mechanism based on the consensus-aware visual semantic embedding model (CVSE)
to capture information in multiple subspaces in parallel, significantly
enhancing the model's ability to understand and represent the complex
relationship between images and texts. In addition, we adopt a parameterized
feature fusion strategy to flexibly integrate feature information at different
levels, further improving the model's expressive power. In terms of loss
function design, the MH-CVSE model adopts a dynamic weight adjustment strategy
to dynamically adjust the weight according to the loss value itself, so that
the model can better balance the contribution of different loss terms during
training. At the same time, we introduce a cosine annealing learning rate
strategy to help the model converge more stably in the later stages of
training. Extensive experimental verification on the Flickr30k dataset shows
that the MH-CVSE model achieves better performance than previous methods in
both bidirectional image and text retrieval tasks, fully demonstrating its
effectiveness and superiority.",2024-12-26T11:46:22Z,http://arxiv.org/abs/2412.19184v1,Wenjing Chen
"Outlier-Bias Removal with Alpha Divergence: A Robust Non-Convex
  Estimator for Linear Regression","Convex and penalized robust methods often suffer from bias induced by large
outliers, limiting their effectiveness in adversarial or heavy-tailed settings.
In this study, we propose a novel approach that eliminates this bias (when
possible) by leveraging a non-convex $M$-estimator based on the alpha
divergence. We address the problem of estimating the parameters vector in high
dimensional linear regression, even when a subset of the data has been
deliberately corrupted by an adversary with full knowledge of the dataset and
its underlying distribution.
  Our primary contribution is to demonstrate that the objective function,
although non-convex, exhibits convexity within a carefully chosen basin of
attraction, enabling robust and unbiased estimation. Additionally, we establish
three key theoretical guarantees for the estimator: (a) a deviation bound that
is minimax optimal up to a logarithmic factor, (b) an improved unbiased bound
when the outliers are large and (c) asymptotic normality as the sample size
increases. Finally, we validate the theoretical findings through empirical
comparisons with state-of-the-art estimators on both synthetic and real-world
datasets, highlighting the proposed method's superior robustness, efficiency,
and ability to mitigate outlier-induced bias.",2024-12-26T11:42:46Z,http://arxiv.org/abs/2412.19183v1,"Ilyes Hammouda, Mohamed Ndaoud, and Abd-Krim Seghouane"
"Unraveling the magnetic and electronic complexity of intermetallic
  ErPd$_2$Si$_2$: Anisotropic thermal expansion, phase transitions, and twofold
  magnetotransport behavior","We present a comprehensive investigation into the physical properties of
intermetallic ErPd$_2$Si$_2$, a compound renowned for its intriguing magnetic
and electronic characteristics. We confirm the tetragonal crystal structure of
ErPd$_2$Si$_2$ within the $I4/mmm$ space group. Notably, we observed
anisotropic thermal expansion, with the lattice constant $a$ expanding and $c$
contracting between 15 K and 300 K. This behavior is attributed to lattice
vibrations and electronic contributions. Heat capacity measurements revealed
three distinct temperature regimes: $T_1 \sim 3.0$ K, $T_\textrm{N} \sim 4.20$
K, and $T_2 \sim 15.31$ K. These correspond to the disappearance of
spin-density waves, the onset of an incommensurate antiferromagnetic (AFM)
structure, and the crystal-field splitting and/or the presence of short-range
spin fluctuations, respectively. Remarkably, the AFM phase transition anomaly
was observed exclusively in low-field magnetization data (120 Oe) at
$T_\textrm{N}$. A high magnetic field ($B =$ 3 T) effectively suppressed this
anomaly, likely due to spin-flop and spin-flip transitions. Furthermore, the
extracted effective PM moments closely matched the expected theoretical value,
suggesting a dominant magnetic contribution from localized 4$f$ spins of Er.
Additionally, significant differences in resistance ($R$) values at low
temperatures under applied $B$ indicated a magnetoresistance (MR) effect with a
minimum value of -4.36\%. Notably, the measured MR effect exhibited anisotropic
behavior, where changes in the strength or direction of the applied $B$ induced
variations in the MR effect. A twofold symmetry of $R$ was discerned at 3 T and
9 T, originating from the orientation of spin moments relative to the applied
$B$. Intriguingly, above $T_\textrm{N}$, short-range spin fluctuations also
displayed a preferred orientation along the $c$-axis due to single-ion
anisotropy.",2024-12-26T11:39:24Z,http://arxiv.org/abs/2412.19181v1,"Kaitong Sun, Si Wu, Guanping Xu, Lingwei Li, Hongyu Chen, Qian Zhao, Muqing Su, Wolfgang Schmidt, Chongde Cao, Hai-Feng Li"
"Mask Approximation Net: Merging Feature Extraction and Distribution
  Learning for Remote Sensing Change Captioning","Remote sensing image change description, as a novel multimodal task in the
field of remote sensing processing, not only enables the detection of changes
in surface conditions but also provides detailed descriptions of these changes,
thereby enhancing human interpretability and interactivity. However, previous
methods mainly employed Convolutional Neural Network (CNN) architectures to
extract bitemporal image features. This approach often leads to an overemphasis
on designing specific network architectures and limits the captured feature
distributions to the current dataset, resulting in poor generalizability and
robustness when applied to other datasets or real-world scenarios. To address
these limitations, this paper proposes a novel approach for remote sensing
image change detection and description that integrates diffusion models, aiming
to shift the focus from conventional feature learning paradigms to data
distribution learning. The proposed method primarily includes a simple
multi-scale change detection module, whose output features are subsequently
refined using a diffusion model. Additionally, we introduce a frequency-guided
complex filter module to handle high-frequency noise during the diffusion
process, which helps to maintain model performance. Finally, we validate the
effectiveness of our proposed method on several remote sensing change detection
description datasets, demonstrating its superior performance. The code
available at MaskApproxNet.",2024-12-26T11:35:57Z,http://arxiv.org/abs/2412.19179v1,"Dongwei Sun, Xiangyong Cao"
Physical nature of quasi-stable structures existing in antimony melt,"Equilibrium antimony melt near the melting temperature is characterised by
structural features that are not present in simple single-component liquids.
The cause of these features may be long-lived structural formations that are
not yet fully understood. The present work provides the detailed
characterization of the structures formed in liquid antimony near the melting
temperature based on the results of quantum chemical calculations and the
available neutron and X-ray diffraction data. The quasi-stable structures in
antimony melt are detected with lifetimes exceeding the structural relaxation
time of this melt. These structures are characterised by a low degree of order
and spatial localisation. It is shown for the first time that the elementary
units of these quasi-stable structures are triplets of atoms with
characteristic lengths of $3.07$\,\AA~and $4.7$\,\AA~and characteristic angles
of $45$ and $90$ degrees. It was found that these triplets can form chains and
percolating clusters up to $\sim15$\,\AA~in length. The characteristic lengths
of these triplets are fully consistent with the correlation lengths associated
with short-range order in the antimony melt as determined by diffraction
experiments.",2024-12-26T11:30:38Z,http://arxiv.org/abs/2412.19177v1,"Artem A. Tsygankov, Bulat N. Galimzyanov, Anatolii V. Mokshin"
"Accelerating Stochastic Gravitational Wave Backgrounds Parameter
  Estimation in Pulsar Timing Arrays with Flow Matching","Pulsar timing arrays (PTAs) are essential tools for detecting the stochastic
gravitational wave background (SGWB), but their analysis faces significant
computational challenges. Traditional methods like Markov-chain Monte Carlo
(MCMC) struggle with high-dimensional parameter spaces where noise parameters
often dominate, while existing deep learning approaches fail to model the
Hellings-Downs (HD) correlation or are validated only on synthetic datasets. We
propose a flow-matching-based continuous normalizing flow (CNF) for efficient
and accurate PTA parameter estimation. By focusing on the 10 most contributive
pulsars from the NANOGrav 15-year dataset, our method achieves posteriors
consistent with MCMC, with a Jensen-Shannon divergence below \(10^{-2}\) nat,
while reducing sampling time from 50 hours to 4 minutes. Powered by a versatile
embedding network and a reweighting loss function, our approach prioritizes the
SGWB parameters and scales effectively for future datasets. It enables precise
reconstruction of SGWB and opens new avenues for exploring vast observational
data and uncovering potential new physics, offering a transformative tool for
advancing gravitational wave astronomy.",2024-12-26T11:02:11Z,http://arxiv.org/abs/2412.19169v1,"Bo Liang, Chang Liu, Tianyu Zhao, Minghui Du, Manjia Liang, Ruijun Shi, Hong Guo, Yuxiang Xu, Li-e Qiang, Peng Xu, Wei-Liang Qian, Ziren Luo"
Master Stability Functions in Complex Networks,"Synchronization is an emergent phenomenon in coupled dynamical networks. The
Master Stability Function (MSF) is a highly elegant and powerful tool for
characterizing the stability of synchronization states. However, a significant
challenge lies in determining the MSF for complex dynamical networks driven by
nonlinear interaction mechanisms. These mechanisms introduce additional
complexity through the intricate connectivity of interacting elements within
the network and the intrinsic dynamics, which are governed by nonlinear
processes with diverse parameters and higher dimensionality of systems. Over
the past 25 years, extensive research has focused on determining the MSF for
pairwise coupled identical systems with diffusive coupling. Our literature
survey highlights two significant advancements in recent years: the
consideration of multilayer networks instead of single-layer networks and the
extension of MSF analysis to incorporate higher-order interactions alongside
pairwise interactions.
  In this review article, we revisit the analysis of the MSF for diffusively
pairwise coupled dynamical systems and extend this framework to more general
coupling schemes. Furthermore, we systematically derive the MSF for multilayer
dynamical networks and single-layer coupled systems by incorporating
higher-order interactions alongside pairwise interactions. The primary focus of
our review is on the analytical derivation and numerical computation of the MSF
for complex dynamical networks. Finally, we demonstrate the application of the
MSF in data science, emphasizing its relevance and potential in this rapidly
evolving field.",2024-12-26T10:47:00Z,http://arxiv.org/abs/2412.19163v1,"Suman Acharyya, Priodyuti Pradhan, Chandrakala Meena"
Advancements in Terahertz Antenna Design,"The promising way to provide sufficient transmission capacity is by accessing
transmission bands at higher carrier frequencies. This desire for higher
carrier frequency or more bandwidth led the researchers to take advantage of
the terahertz (THz) spectrum. The opportunity for large bandwidth in the THz
band leads to the possibility of easy, high data rate transmission. In spite of
the advantages, the THz band suffers from large free space path loss. In the
development of THz communication systems, the antenna is the most significant
component. The focus is especially on designing highly directive antennas
because they enhance the performance of the overall system by compensating for
the large path loss at THz and thus improving the signal-to-noise ratio. This
chapter presents different types of THz antennas, including planar,
reflectarray, horn antenna, and lens antenna. Emphasis has been made to present
the latest trend of designing THz antennas using carbon-based materials, such
as graphene and carbon nanotubes. The performance of these antennas has been
compared with that of traditional copper-based THz antennas by critically
analyzing their properties. A brief discussion on THz power sources is included
in this chapter for completeness. A comprehensive discussion on different
fabrication techniques has been provided to appraise the reader of the general
fabrication processes of THz components.",2024-12-26T10:21:58Z,http://arxiv.org/abs/2412.19156v1,"Sasmita Dash, Amalendu Patnaik"
"Referencing Where to Focus: Improving VisualGrounding with Referential
  Query","Visual Grounding aims to localize the referring object in an image given a
natural language expression. Recent advancements in DETR-based visual grounding
methods have attracted considerable attention, as they directly predict the
coordinates of the target object without relying on additional efforts, such as
pre-generated proposal candidates or pre-defined anchor boxes. However,
existing research primarily focuses on designing stronger multi-modal decoder,
which typically generates learnable queries by random initialization or by
using linguistic embeddings. This vanilla query generation approach inevitably
increases the learning difficulty for the model, as it does not involve any
target-related information at the beginning of decoding. Furthermore, they only
use the deepest image feature during the query learning process, overlooking
the importance of features from other levels. To address these issues, we
propose a novel approach, called RefFormer. It consists of the query adaption
module that can be seamlessly integrated into CLIP and generate the referential
query to provide the prior context for decoder, along with a task-specific
decoder. By incorporating the referential query into the decoder, we can
effectively mitigate the learning difficulty of the decoder, and accurately
concentrate on the target object. Additionally, our proposed query adaption
module can also act as an adapter, preserving the rich knowledge within CLIP
without the need to tune the parameters of the backbone network. Extensive
experiments demonstrate the effectiveness and efficiency of our proposed
method, outperforming state-of-the-art approaches on five visual grounding
benchmarks.",2024-12-26T10:19:20Z,http://arxiv.org/abs/2412.19155v1,"Yabing Wang, Zhuotao Tian, Qingpei Guo, Zheng Qin, Sanping Zhou, Ming Yang, Le Wang"
"Evolutionary de-homogenization using a generative model for optimizing
  solid-porous infill structures considering the stress concentration issue","The design of porous infill structures presents significant challenges due to
their complex geometric configurations, such as the accurate representation of
geometric boundaries and the control of localized maximum stress. In current
mainstream design methods, such as topology optimization, the analysis is often
performed using pixel or voxel-based element approximations. These
approximations, constrained by the optimization framework, result in
substantial geometric discrepancies between the analysis model and the final
physical model. Such discrepancies can severely impact structural performance,
particularly for localized properties like stress response, where accurate
geometry is critical to mitigating stress concentration. To address these
challenges, we propose evolutionary de-homogenization, which is a design
framework based on the integration of de-homogenization and data-driven
multifidelity optimization. This framework facilitates the hybrid solid-porous
infill design by bridging the gap between low-fidelity analysis and
high-fidelity physical realizations, ensuring both geometric accuracy and
enhanced structural performance. The low-fidelity level utilizes commonly used
density control variables, while the high-fidelity level involves stress
analysis based on structures with precise geometric representations. By
employing a de-homogenization-based mapping method, a side-by-side
correspondence between low-fidelity and high-fidelity results is established.
The low-fidelity control variables are iteratively adjusted to optimize the
high-fidelity results by integrating deep generative model with multi-objective
evolutionary algorithm. Finally, numerical experiments demonstrate the
effectiveness of the proposed method.",2024-12-26T10:18:16Z,http://arxiv.org/abs/2412.19154v1,"Shuzhi Xu, Hiroki Kawabe, Kentaro Yaji"
"To Predict or Not To Predict? Proportionally Masked Autoencoders for
  Tabular Data Imputation","Masked autoencoders (MAEs) have recently demonstrated effectiveness in
tabular data imputation. However, due to the inherent heterogeneity of tabular
data, the uniform random masking strategy commonly used in MAEs can disrupt the
distribution of missingness, leading to suboptimal performance. To address
this, we propose a proportional masking strategy for MAEs. Specifically, we
first compute the statistics of missingness based on the observed proportions
in the dataset, and then generate masks that align with these statistics,
ensuring that the distribution of missingness is preserved after masking.
Furthermore, we argue that simple MLP-based token mixing offers competitive or
often superior performance compared to attention mechanisms while being more
computationally efficient, especially in the tabular domain with the inherent
heterogeneity. Experimental results validate the effectiveness of the proposed
proportional masking strategy across various missing data patterns in tabular
datasets. Code is available at: \url{https://github.com/normal-kim/PMAE}.",2024-12-26T10:12:08Z,http://arxiv.org/abs/2412.19152v1,"Jungkyu Kim, Kibok Lee, Taeyoung Park"
AskChart: Universal Chart Understanding through Textual Enhancement,"Chart understanding tasks such as ChartQA and Chart-to-Text involve
automatically extracting and interpreting key information from charts, enabling
users to query or convert visual data into structured formats. State-of-the-art
approaches primarily focus on visual cues from chart images, failing to
explicitly incorporate rich textual information (e.g., data labels and axis
labels) embedded within the charts. This textual information is vital for
intuitive human comprehension and interpretation of charts. Moreover, existing
models are often large and computationally intensive, limiting their practical
applicability. In this paper, we introduce AskChart, a universal model that
explicitly integrates both textual and visual cues from charts using a Mixture
of Experts (MoE) architecture. AskChart facilitates the learning of enhanced
visual-textual representations of charts for effectively handling multiple
chart understanding tasks, while maintaining a smaller model size. To capture
the synergy between visual and textual modalities, we curate a large-scale
dataset named ChartBank with about 7.5M data samples, which helps align textual
and visual information and facilitates the extraction of visual entities and
text. To effectively train AskChart, we design a three-stage training strategy
to align visual and textual modalities for learning robust visual-textual
representations and optimizing the learning of the MoE layer. Extensive
experiments across five datasets demonstrate the significant performance gains
of AskChart in four chart understanding tasks. Remarkably, AskChart with 4.6B
parameters outperforms state-of-the-art models with 13B parameters by 68.3% in
Open-ended ChartQA and 49.2% in Chart-to-Text tasks, while achieving comparable
performance in ChartQA and Chart-to-Table tasks.",2024-12-26T09:59:43Z,http://arxiv.org/abs/2412.19146v1,"Xudong Yang, Yifan Wu, Yizhang Zhu, Nan Tang, Yuyu Luo"
"Impact of color and mixing proportion of synthetic point clouds on
  semantic segmentation","Semantic segmentation of point clouds is essential for understanding the
built environment, and a large amount of high-quality data is required for
training deep learning models. Despite synthetic point clouds (SPC) having the
potential to compensate for the shortage of real data, how to exploit the
benefits of SPC is still open. Therefore, this study systematically
investigates how color and mixing proportion of SPC impact semantic
segmentation for the first time. First, a new method to mimic the scanning
process and generate SPC based on BIM is proposed, to create a synthetic
dataset with consistent colors of BIM (UniSPC) and a synthetic dataset with
real colors (RealSPC) respectively. Subsequently, by integrating with the S3DIS
dataset, further experiments on PointNet, PointNet++, and DGCNN are conducted.
Meanwhile, benchmark experiments and new evaluation metrics are introduced to
better evaluate the performance of different models. Experiments show that
synthetic color significantly impacts model performance, the performance for
common components of the models trained with pure RealSPC is comparable to
models with real data, and RealSPC contributes average improvements of 14.1% on
overall accuracy and 7.3% on mIoU than UniSPC. Furthermore, the proportion of
SPC also has a significant impact on the performance. In mixing training
experiments, adding more than 70% SPC achieves an average of 3.9% on overall
accuracy and 3.4% on mIoU better than benchmark on three models. It is also
revealed that for large flat elements such as floors, ceilings, and walls, the
SPC can even replace real point clouds without compromising model performance.",2024-12-26T09:58:04Z,http://arxiv.org/abs/2412.19145v1,"Shaojie Zhou, Jia-Rui Lin, Peng Pan, Yuandong Pan, Ioannis Brilakis"
"CLIP-GS: Unifying Vision-Language Representation with 3D Gaussian
  Splatting","Recent works in 3D multimodal learning have made remarkable progress.
However, typically 3D multimodal models are only capable of handling point
clouds. Compared to the emerging 3D representation technique, 3D Gaussian
Splatting (3DGS), the spatially sparse point cloud cannot depict the texture
information of 3D objects, resulting in inferior reconstruction capabilities.
This limitation constrains the potential of point cloud-based 3D multimodal
representation learning. In this paper, we present CLIP-GS, a novel multimodal
representation learning framework grounded in 3DGS. We introduce the GS
Tokenizer to generate serialized gaussian tokens, which are then processed
through transformer layers pre-initialized with weights from point cloud
models, resulting in the 3DGS embeddings. CLIP-GS leverages contrastive loss
between 3DGS and the visual-text embeddings of CLIP, and we introduce an image
voting loss to guide the directionality and convergence of gradient
optimization. Furthermore, we develop an efficient way to generate triplets of
3DGS, images, and text, facilitating CLIP-GS in learning unified multimodal
representations. Leveraging the well-aligned multimodal representations,
CLIP-GS demonstrates versatility and outperforms point cloud-based models on
various 3D tasks, including multimodal retrieval, zero-shot, and few-shot
classification.",2024-12-26T09:54:25Z,http://arxiv.org/abs/2412.19142v1,"Siyu Jiao, Haoye Dong, Yuyang Yin, Zequn Jie, Yinlong Qian, Yao Zhao, Humphrey Shi, Yunchao Wei"
"How Panel Layouts Define Manga: Insights from Visual Ablation
  Experiments","Today, manga has gained worldwide popularity. However, the question of how
various elements of manga, such as characters, text, and panel layouts, reflect
the uniqueness of a particular work, or even define it, remains an unexplored
area. In this paper, we aim to quantitatively and qualitatively analyze the
visual characteristics of manga works, with a particular focus on panel layout
features. As a research method, we used facing page images of manga as input to
train a deep learning model for predicting manga titles, examining
classification accuracy to quantitatively analyze these features. Specifically,
we conducted ablation studies by limiting page image information to panel
frames to analyze the characteristics of panel layouts. Through a series of
quantitative experiments using all 104 works, 12 genres, and 10,122 facing page
images from the Manga109 dataset, as well as qualitative analysis using
Grad-CAM, our study demonstrates that the uniqueness of manga works is strongly
reflected in their panel layouts.",2024-12-26T09:53:37Z,http://arxiv.org/abs/2412.19141v1,"Siyuan Feng, Teruya Yoshinaga, Katsuhiko Hayashi, Koki Washio, Hidetaka Kamigaito"
"SILC-EFSA: Self-aware In-context Learning Correction for Entity-level
  Financial Sentiment Analysis","In recent years, fine-grained sentiment analysis in finance has gained
significant attention, but the scarcity of entity-level datasets remains a key
challenge. To address this, we have constructed the largest English and Chinese
financial entity-level sentiment analysis datasets to date. Building on this
foundation, we propose a novel two-stage sentiment analysis approach called
Self-aware In-context Learning Correction (SILC). The first stage involves
fine-tuning a base large language model to generate pseudo-labeled data
specific to our task. In the second stage, we train a correction model using a
GNN-based example retriever, which is informed by the pseudo-labeled data. This
two-stage strategy has allowed us to achieve state-of-the-art performance on
the newly constructed datasets, advancing the field of financial sentiment
analysis. In a case study, we demonstrate the enhanced practical utility of our
data and methods in monitoring the cryptocurrency market. Our datasets and code
are available at https://github.com/NLP-Bin/SILC-EFSA.",2024-12-26T09:53:01Z,http://arxiv.org/abs/2412.19140v1,"Senbin Zhu, Chenyuan He, Hongde Liu, Pengcheng Dong, Hanjie Zhao, Yuchen Yan, Yuxiang Jia, Hongying Zan, Min Peng"
PlanLLM: Video Procedure Planning with Refinable Large Language Models,"Video procedure planning, i.e., planning a sequence of action steps given the
video frames of start and goal states, is an essential ability for embodied AI.
Recent works utilize Large Language Models (LLMs) to generate enriched action
step description texts to guide action step decoding. Although LLMs are
introduced, these methods decode the action steps into a closed-set of one-hot
vectors, limiting the model's capability of generalizing to new steps or tasks.
Additionally, fixed action step descriptions based on world-level commonsense
may contain noise in specific instances of visual states. In this paper, we
propose PlanLLM, a cross-modal joint learning framework with LLMs for video
procedure planning. We propose an LLM-Enhanced Planning module which fully uses
the generalization ability of LLMs to produce free-form planning output and to
enhance action step decoding. We also propose Mutual Information Maximization
module to connect world-level commonsense of step descriptions and
sample-specific information of visual states, enabling LLMs to employ the
reasoning ability to generate step sequences. With the assistance of LLMs, our
method can both closed-set and open vocabulary procedure planning tasks. Our
PlanLLM achieves superior performance on three benchmarks, demonstrating the
effectiveness of our designs.",2024-12-26T09:51:05Z,http://arxiv.org/abs/2412.19139v1,"Dejie Yang, Zijing Zhao, YangLiu"
SUTrack: Towards Simple and Unified Single Object Tracking,"In this paper, we propose a simple yet unified single object tracking (SOT)
framework, dubbed SUTrack. It consolidates five SOT tasks (RGB-based,
RGB-Depth, RGB-Thermal, RGB-Event, RGB-Language Tracking) into a unified model
trained in a single session. Due to the distinct nature of the data, current
methods typically design individual architectures and train separate models for
each task. This fragmentation results in redundant training processes,
repetitive technological innovations, and limited cross-modal knowledge
sharing. In contrast, SUTrack demonstrates that a single model with a unified
input representation can effectively handle various common SOT tasks,
eliminating the need for task-specific designs and separate training sessions.
Additionally, we introduce a task-recognition auxiliary training strategy and a
soft token type embedding to further enhance SUTrack's performance with minimal
overhead. Experiments show that SUTrack outperforms previous task-specific
counterparts across 11 datasets spanning five SOT tasks. Moreover, we provide a
range of models catering edge devices as well as high-performance GPUs,
striking a good trade-off between speed and accuracy. We hope SUTrack could
serve as a strong foundation for further compelling research into unified
tracking models. Code and models are available at
github.com/chenxin-dlut/SUTrack.",2024-12-26T09:41:36Z,http://arxiv.org/abs/2412.19138v1,"Xin Chen, Ben Kang, Wanting Geng, Jiawen Zhu, Yi Liu, Dong Wang, Huchuan Lu"
"A Rhetorical Relations-Based Framework for Tailored Multimedia Document
  Summarization","In the rapidly evolving landscape of digital content, the task of summarizing
multimedia documents, which encompass textual, visual, and auditory elements,
presents intricate challenges. These challenges include extracting pertinent
information from diverse formats, maintaining the structural integrity and
semantic coherence of the original content, and generating concise yet
informative summaries. This paper introduces a novel framework for multimedia
document summarization that capitalizes on the inherent structure of the
document to craft coherent and succinct summaries. Central to this framework is
the incorporation of a rhetorical structure for structural analysis, augmented
by a graph-based representation to facilitate the extraction of pivotal
information. Weighting algorithms are employed to assign significance values to
document units, thereby enabling effective ranking and selection of relevant
content. Furthermore, the framework is designed to accommodate user preferences
and time constraints, ensuring the production of personalized and contextually
relevant summaries. The summarization process is elaborately delineated,
encompassing document specification, graph construction, unit weighting, and
summary extraction, supported by illustrative examples and algorithmic
elucidation. This proposed framework represents a significant advancement in
automatic summarization, with broad potential applications across multimedia
document processing, promising transformative impacts in the field.",2024-12-26T09:29:59Z,http://arxiv.org/abs/2412.19133v1,"Azze-Eddine Maredj, Madjid Sadallah"
Semantic Residual for Multimodal Unified Discrete Representation,"Recent research in the domain of multimodal unified representations
predominantly employs codebook as representation forms, utilizing Vector
Quantization(VQ) for quantization, yet there has been insufficient exploration
of other quantization representation forms. Our work explores more precise
quantization methods and introduces a new framework, Semantic Residual
Cross-modal Information Disentanglement (SRCID), inspired by the numerical
residual concept inherent to Residual Vector Quantization (RVQ). SRCID employs
semantic residual-based information disentanglement for multimodal data to
better handle the inherent discrepancies between different modalities. Our
method enhances the capabilities of unified multimodal representations and
demonstrates exceptional performance in cross-modal generalization and
cross-modal zero-shot retrieval. Its average results significantly surpass
existing state-of-the-art models, as well as previous attempts with RVQ and
Finite Scalar Quantization (FSQ) based on these modals.",2024-12-26T09:08:52Z,http://arxiv.org/abs/2412.19128v1,"Hai Huang, Shulei Wang, Yan Xia"
"Advanced Knowledge Transfer: Refined Feature Distillation for Zero-Shot
  Quantization in Edge Computing","We introduce AKT (Advanced Knowledge Transfer), a novel method to enhance the
training ability of low-bit quantized (Q) models in the field of zero-shot
quantization (ZSQ). Existing research in ZSQ has focused on generating
high-quality data from full-precision (FP) models. However, these approaches
struggle with reduced learning ability in low-bit quantization due to its
limited information capacity. To overcome this limitation, we propose effective
training strategy compared to data generation. Particularly, we analyzed that
refining feature maps in the feature distillation process is an effective way
to transfer knowledge to the Q model. Based on this analysis, AKT efficiently
transfer core information from the FP model to the Q model. AKT is the first
approach to utilize both spatial and channel attention information in feature
distillation in ZSQ. Our method addresses the fundamental gradient exploding
problem in low-bit Q models. Experiments on CIFAR-10 and CIFAR-100 datasets
demonstrated the effectiveness of the AKT. Our method led to significant
performance enhancement in existing generative models. Notably, AKT achieved
significant accuracy improvements in low-bit Q models, achieving
state-of-the-art in the 3,5bit scenarios on CIFAR-10. The code is available at
https://github.com/Inpyo-Hong/AKT-Advanced-knowledge-Transfer.",2024-12-26T08:52:27Z,http://arxiv.org/abs/2412.19125v1,"Inpyo Hong, Youngwan Jo, Hyojeong Lee, Sunghyun Ahn, Sanghyun Park"
"Evaluating Self-Supervised Learning in Medical Imaging: A Benchmark for
  Robustness, Generalizability, and Multi-Domain Impact","Self-supervised learning (SSL) has emerged as a promising paradigm in medical
imaging, addressing the chronic challenge of limited labeled data in healthcare
settings. While SSL has shown impressive results, existing studies in the
medical domain are often limited in scope, focusing on specific datasets or
modalities, or evaluating only isolated aspects of model performance. This
fragmented evaluation approach poses a significant challenge, as models
deployed in critical medical settings must not only achieve high accuracy but
also demonstrate robust performance and generalizability across diverse
datasets and varying conditions. To address this gap, we present a
comprehensive evaluation of SSL methods within the medical domain, with a
particular focus on robustness and generalizability. Using the MedMNIST dataset
collection as a standardized benchmark, we evaluate 8 major SSL methods across
11 different medical datasets. Our study provides an in-depth analysis of model
performance in both in-domain scenarios and the detection of
out-of-distribution (OOD) samples, while exploring the effect of various
initialization strategies, model architectures, and multi-domain pre-training.
We further assess the generalizability of SSL methods through cross-dataset
evaluations and the in-domain performance with varying label proportions (1%,
10%, and 100%) to simulate real-world scenarios with limited supervision. We
hope this comprehensive benchmark helps practitioners and researchers make more
informed decisions when applying SSL methods to medical applications.",2024-12-26T08:51:56Z,http://arxiv.org/abs/2412.19124v1,"Valay Bundele, Oğuz Ata Çal, Bora Kargi, Karahan Sarıtaş, Kıvanç Tezören, Zohreh Ghaderi, Hendrik Lensch"
Discrete vs. Continuous Trade-offs for Generative Models,"This work explores the theoretical and practical foundations of denoising
diffusion probabilistic models (DDPMs) and score-based generative models, which
leverage stochastic processes and Brownian motion to model complex data
distributions. These models employ forward and reverse diffusion processes
defined through stochastic differential equations (SDEs) to iteratively add and
remove noise, enabling high-quality data generation. By analyzing the
performance bounds of these models, we demonstrate how score estimation errors
propagate through the reverse process and bound the total variation distance
using discrete Girsanov transformations, Pinsker's inequality, and the data
processing inequality (DPI) for an information theoretic lens.",2024-12-26T08:14:27Z,http://arxiv.org/abs/2412.19114v1,"Jathin Korrapati, Tanish Baranwal, Rahul Shah"
"SketchFill: Sketch-Guided Code Generation for Imputing Derived Missing
  Values","Missing value is a critical issue in data science, significantly impacting
the reliability of analyses and predictions. Missing value imputation (MVI) is
a longstanding problem because it highly relies on domain knowledge. Large
language models (LLMs) have emerged as a promising tool for data cleaning,
including MVI for tabular data, offering advanced capabilities for
understanding and generating content. However, despite their promise, existing
LLM techniques such as in-context learning and Chain-of-Thought (CoT) often
fall short in guiding LLMs to perform complex reasoning for MVI, particularly
when imputing derived missing values, which require mathematical formulas and
data relationships across rows and columns. This gap underscores the need for
further advancements in LLM methodologies to enhance their reasoning
capabilities for more reliable imputation outcomes. To fill this gap, we
propose SketchFill, a novel sketch-based method to guide LLMs in generating
accurate formulas to impute missing numerical values. Our experimental results
demonstrate that SketchFill significantly outperforms state-of-the-art methods,
achieving 56.2% higher accuracy than CoT-based methods and 78.8% higher
accuracy than MetaGPT. This sets a new standard for automated data cleaning and
advances the field of MVI for numerical values.",2024-12-26T08:13:34Z,http://arxiv.org/abs/2412.19113v1,"Yunfan Zhang, Changlun Li, Yuyu Luo, Nan Tang"
"Graph Mixture of Experts and Memory-augmented Routers for Multivariate
  Time Series Anomaly Detection","Multivariate time series (MTS) anomaly detection is a critical task that
involves identifying abnormal patterns or events in data that consist of
multiple interrelated time series. In order to better model the complex
interdependence between entities and the various inherent characteristics of
each entity, the GNN based methods are widely adopted by existing methods. In
each layer of GNN, node features aggregate information from their neighboring
nodes to update their information. In doing so, from shallow layer to deep
layer in GNN, original individual node features continue to be weakened and
more structural information,i.e., from short-distance neighborhood to
long-distance neighborhood, continues to be enhanced. However, research to date
has largely ignored the understanding of how hierarchical graph information is
represented and their characteristics that can benefit anomaly detection.
Existing methods simply leverage the output from the last layer of GNN for
anomaly estimation while neglecting the essential information contained in the
intermediate GNN layers. To address such limitations, in this paper, we propose
a Graph Mixture of Experts (Graph-MoE) network for multivariate time series
anomaly detection, which incorporates the mixture of experts (MoE) module to
adaptively represent and integrate hierarchical multi-layer graph information
into entity representations. It is worth noting that our Graph-MoE can be
integrated into any GNN-based MTS anomaly detection method in a plug-and-play
manner. In addition, the memory-augmented routers are proposed in this paper to
capture the correlation temporal information in terms of the global historical
features of MTS to adaptively weigh the obtained entity representations to
achieve successful anomaly estimation. Extensive experiments on five
challenging datasets prove the superiority of our approach and each proposed
module.",2024-12-26T07:49:51Z,http://arxiv.org/abs/2412.19108v1,"Xiaoyu Huang, Weidong Chen, Bo Hu, Zhendong Mao"
"The role of potential energy landscape research in the development of
  new electrolyte solutions","The development of new electrolyte solutions with improved characteristics is
a key challenge for creating high-performance batteries, fuel cells,
supercapacitors, and other electrochemical devices. The study of the potential
energy landscape (PEL) plays an important role in this process, providing
information about the interactions between solution components at the molecular
level. In this work, we review the practice of applying PEL research methods
based on classical and quantum-chemical algorithms to analyze the structure,
dynamics, and thermodynamic properties of electrolyte solutions. Intermolecular
and ion-molecular interactions at the microscopic level, which determine the
macroscopic properties of the electrolyte solution, are considered in detail.
The importance of identifying stable configurations of ions and their solvates
is emphasized. PEL analysis allows for the systematic determination of the most
probable structures and complexes formed in solution, which is important for
understanding ion transport mechanisms. The study of the PEL allows for the
determination of the energy barriers that must be overcome for ion migration,
which is related to the conductivity of the electrolyte. The application of PEL
research methods in combination with experimental data opens up new
possibilities for the rational design of electrolyte solutions with desired
physicochemical properties.",2024-12-26T07:45:29Z,http://arxiv.org/abs/2412.19103v1,Vitaly V. Chaban
"""I've Heard of You!"": Generate Spoken Named Entity Recognition Data for
  Unseen Entities","Spoken named entity recognition (NER) aims to identify named entities from
speech, playing an important role in speech processing. New named entities
appear every day, however, annotating their Spoken NER data is costly. In this
paper, we demonstrate that existing Spoken NER systems perform poorly when
dealing with previously unseen named entities. To tackle this challenge, we
propose a method for generating Spoken NER data based on a named entity
dictionary (NED) to reduce costs. Specifically, we first use a large language
model (LLM) to generate sentences from the sampled named entities and then use
a text-to-speech (TTS) system to generate the speech. Furthermore, we introduce
a noise metric to filter out noisy data. To evaluate our approach, we release a
novel Spoken NER benchmark along with a corresponding NED containing 8,853
entities. Experiment results show that our method achieves state-of-the-art
(SOTA) performance in the in-domain, zero-shot domain adaptation, and fully
zero-shot settings. Our data will be available at
https://github.com/DeepLearnXMU/HeardU.",2024-12-26T07:43:18Z,http://arxiv.org/abs/2412.19102v1,"Jiawei Yu, Xiang Geng, Yuang Li, Mengxin Ren, Wei Tang, Jiahuan Li, Zhibin Lan, Min Zhang, Hao Yang, Shujian Huang, Jinsong Su"
"Reconstruction Target Matters in Masked Image Modeling for Cross-Domain
  Few-Shot Learning","Cross-Domain Few-Shot Learning (CDFSL) requires the model to transfer
knowledge from the data-abundant source domain to data-scarce target domains
for fast adaptation, where the large domain gap makes CDFSL a challenging
problem. Masked Autoencoder (MAE) excels in effectively using unlabeled data
and learning image's global structures, enhancing model generalization and
robustness. However, in the CDFSL task with significant domain shifts, we find
MAE even shows lower performance than the baseline supervised models. In this
paper, we first delve into this phenomenon for an interpretation. We find that
MAE tends to focus on low-level domain information during reconstructing pixels
while changing the reconstruction target to token features could mitigate this
problem. However, not all features are beneficial, as we then find
reconstructing high-level features can hardly improve the model's
transferability, indicating a trade-off between filtering domain information
and preserving the image's global structure. In all, the reconstruction target
matters for the CDFSL task. Based on the above findings and interpretations, we
further propose Domain-Agnostic Masked Image Modeling (DAMIM) for the CDFSL
task. DAMIM includes an Aggregated Feature Reconstruction module to
automatically aggregate features for reconstruction, with balanced learning of
domain-agnostic information and images' global structure, and a Lightweight
Decoder module to further benefit the encoder's generalizability. Experiments
on four CDFSL datasets demonstrate that our method achieves state-of-the-art
performance.",2024-12-26T07:43:01Z,http://arxiv.org/abs/2412.19101v1,"Ran Ma, Yixiong Zou, Yuhua Li, Ruixuan Li"
"TrajGEOS: Trajectory Graph Enhanced Orientation-based Sequential Network
  for Mobility Prediction","Human mobility studies how people move to access their needed resources and
plays a significant role in urban planning and location-based services. As a
paramount task of human mobility modeling, next location prediction is
challenging because of the diversity of users' historical trajectories that
gives rise to complex mobility patterns and various contexts. Deep sequential
models have been widely used to predict the next location by leveraging the
inherent sequentiality of trajectory data. However, they do not fully leverage
the relationship between locations and fail to capture users' multi-level
preferences. This work constructs a trajectory graph from users' historical
traces and proposes a \textbf{Traj}ectory \textbf{G}raph \textbf{E}nhanced
\textbf{O}rientation-based \textbf{S}equential network (TrajGEOS) for
next-location prediction tasks. TrajGEOS introduces hierarchical graph
convolution to capture location and user embeddings. Such embeddings consider
not only the contextual feature of locations but also the relation between
them, and serve as additional features in downstream modules. In addition, we
design an orientation-based module to learn users' mid-term preferences from
sequential modeling modules and their recent trajectories. Extensive
experiments on three real-world LBSN datasets corroborate the value of graph
and orientation-based modules and demonstrate that TrajGEOS outperforms the
state-of-the-art methods on the next location prediction task.",2024-12-26T07:18:38Z,http://arxiv.org/abs/2412.19092v1,"Zhaoping Hu, Zongyuan Huang, Jinming Yang, Tao Yang, Yaohui Jin, Yanyan Xu"
From Coin to Data: The Impact of Object Detection on Digital Numismatics,"In this work we investigate the application of advanced object detection
techniques to digital numismatics, focussing on the analysis of historical
coins. Leveraging models such as Contrastive Language-Image Pre-training
(CLIP), we develop a flexible framework for identifying and classifying
specific coin features using both image and textual descriptions. By examining
two distinct datasets, modern Russian coins featuring intricate ""Saint George
and the Dragon"" designs and degraded 1st millennium AD Southeast Asian coins
bearing Hindu-Buddhist symbols, we evaluate the efficacy of different detection
algorithms in search and classification tasks. Our results demonstrate the
superior performance of larger CLIP models in detecting complex imagery, while
traditional methods excel in identifying simple geometric patterns.
Additionally, we propose a statistical calibration mechanism to enhance the
reliability of similarity scores in low-quality datasets. This work highlights
the transformative potential of integrating state-of-the-art object detection
into digital numismatics, enabling more scalable, precise, and efficient
analysis of historical artifacts. These advancements pave the way for new
methodologies in cultural heritage research, artefact provenance studies, and
the detection of forgeries.",2024-12-26T07:05:53Z,http://arxiv.org/abs/2412.19091v1,"Rafael Cabral, Maria De Iorio, Andrew Harris"
"Integrating Artificial Open Generative Artificial Intelligence into
  Software Supply Chain Security","While new technologies emerge, human errors always looming. Software supply
chain is increasingly complex and intertwined, the security of a service has
become paramount to ensuring the integrity of products, safeguarding data
privacy, and maintaining operational continuity. In this work, we conducted
experiments on the promising open Large Language Models (LLMs) into two main
software security challenges: source code language errors and deprecated code,
with a focus on their potential to replace conventional static and dynamic
security scanners that rely on predefined rules and patterns. Our findings
suggest that while LLMs present some unexpected results, they also encounter
significant limitations, particularly in memory complexity and the management
of new and unfamiliar data patterns. Despite these challenges, the proactive
application of LLMs, coupled with extensive security databases and continuous
updates, holds the potential to fortify Software Supply Chain (SSC) processes
against emerging threats.",2024-12-26T07:03:55Z,http://arxiv.org/abs/2412.19088v1,"Vasileios Alevizos, George A Papakostas, Akebu Simasiku, Dimitra Malliarou, Antonis Messinis, Sabrina Edralin, Clark Xu, Zongliang Yue"
MoPD: Mixture-of-Prompts Distillation for Vision-Language Models,"Soft prompt learning methods are effective for adapting vision-language
models (VLMs) to downstream tasks. Nevertheless, empirical evidence reveals a
tendency of existing methods that they overfit seen classes and exhibit
degraded performance on unseen classes. This limitation is due to the inherent
bias in the training data towards the seen classes. To address this issue, we
propose a novel soft prompt learning method, named Mixture-of-Prompts
Distillation (MoPD), which can effectively transfer useful knowledge from hard
prompts manually hand-crafted (a.k.a. teacher prompts) to the learnable soft
prompt (a.k.a. student prompt), thereby enhancing the generalization ability of
soft prompts on unseen classes. Moreover, the proposed MoPD method utilizes a
gating network that learns to select hard prompts used for prompt distillation.
Extensive experiments demonstrate that the proposed MoPD method outperforms
state-of-the-art baselines especially on on unseen classes.",2024-12-26T06:57:04Z,http://arxiv.org/abs/2412.19087v1,"Yang Chen, Shuai Fu, Yu Zhang"
Investigating the Temporal Dynamics of Cyber Threat Intelligence,"Indicators of Compromise (IoCs) play a crucial role in the rapid detection
and mitigation of cyber threats. However, the existing body of literature lacks
in-depth analytical studies on the temporal aspects of IoC publication,
especially when considering up-to-date datasets related to Common
Vulnerabilities and Exposures (CVEs). This paper addresses this gap by
conducting an analysis of the timeliness and comprehensiveness of Cyber Threat
Intelligence (CTI) pertaining to several recent CVEs. The insights derived from
this study aim to enhance cybersecurity defense strategies, particularly when
dealing with dynamic cyber threats that continually adapt their Tactics,
Techniques, and Procedures (TTPs). Utilizing IoCs sourced from multiple
providers, we scrutinize the IoC publication rate. Our analysis delves into how
various factors, including the inherent nature of a threat, its evolutionary
trajectory, and its observability over time, influence the publication rate of
IoCs. Our preliminary findings emphasize the critical need for cyber defenders
to maintain a constant state of vigilance in updating their IoCs for any given
vulnerability. This vigilance is warranted because the publication rate of IoCs
may exhibit fluctuations over time. We observe a recurring pattern akin to an
epidemic model, with an initial phase following the public disclosure of a
vulnerability characterized by sparse IoC publications, followed by a sudden
surge, and subsequently, a protracted period with a slower rate of IoC
publication.",2024-12-26T06:54:27Z,http://arxiv.org/abs/2412.19086v1,"Angel Kodituwakku, Clark Xu, Daniel Rogers, David K. Ahn, Errin W. Fulp"
"Assessing Pre-trained Models for Transfer Learning through Distribution
  of Spectral Components","Pre-trained model assessment for transfer learning aims to identify the
optimal candidate for the downstream tasks from a model hub, without the need
of time-consuming fine-tuning. Existing advanced works mainly focus on
analyzing the intrinsic characteristics of the entire features extracted by
each pre-trained model or how well such features fit the target labels. This
paper proposes a novel perspective for pre-trained model assessment through the
Distribution of Spectral Components (DISCO). Through singular value
decomposition of features extracted from pre-trained models, we investigate
different spectral components and observe that they possess distinct
transferability, contributing diversely to the fine-tuning performance.
Inspired by this, we propose an assessment method based on the distribution of
spectral components which measures the proportions of their corresponding
singular values. Pre-trained models with features concentrating on more
transferable components are regarded as better choices for transfer learning.
We further leverage the labels of downstream data to better estimate the
transferability of each spectral component and derive the final assessment
criterion. Our proposed method is flexible and can be applied to both
classification and regression tasks. We conducted comprehensive experiments
across three benchmarks and two tasks including image classification and object
detection, demonstrating that our method achieves state-of-the-art performance
in choosing proper pre-trained models from the model hub for transfer learning.",2024-12-26T06:54:22Z,http://arxiv.org/abs/2412.19085v1,"Tengxue Zhang, Yang Shu, Xinyang Chen, Yifei Long, Chenjuan Guo, Bin Yang"
A Microservice Graph Generator with Production Characteristics,"A production microservice application may provide multiple services, queries
of a service may have different call graphs, and a microservice may be shared
across call graphs. It is challenging to improve the resource efficiency of
such complex applications without proper benchmarks, while production traces
are too large to be used in experiments. To this end, we propose a Service
Dependency Graph Generator (DGG) that comprises a Data Handler and a Graph
Generator, for generating the service dependency graphs of benchmarks that
incorporate production-level characteristics from traces. The data handler
first constructs fine-grained call graphs with dynamic interface and repeated
calling features from the trace and merges them into dependency graphs, and
then clusters them into different categories based on the topological and
invocation types. Taking the organized data and the selected category, the
graph generator simulates the process of real microservices invoking downstream
microservices using a random graph model, generates multiple call graphs, and
merges the call graphs to form the small-scale service dependency graph with
production-level characteristics. Case studies show that DGG's generated graphs
are similar to real traces in terms of topologies. Moreover, the resource
scaling based on DGG's fine-grained call graph constructing increases the
resource efficiency by up to 44.8% while ensuring the required QoS.",2024-12-26T06:51:35Z,http://arxiv.org/abs/2412.19083v1,"Fanrong Du, Jiuchen Shi, Quan Chen, Li Li, Minyi Guo"
"Mask Factory: Towards High-quality Synthetic Data Generation for
  Dichotomous Image Segmentation","Dichotomous Image Segmentation (DIS) tasks require highly precise
annotations, and traditional dataset creation methods are labor intensive,
costly, and require extensive domain expertise. Although using synthetic data
for DIS is a promising solution to these challenges, current generative models
and techniques struggle with the issues of scene deviations, noise-induced
errors, and limited training sample variability. To address these issues, we
introduce a novel approach, \textbf{\ourmodel{}}, which provides a scalable
solution for generating diverse and precise datasets, markedly reducing
preparation time and costs. We first introduce a general mask editing method
that combines rigid and non-rigid editing techniques to generate high-quality
synthetic masks. Specially, rigid editing leverages geometric priors from
diffusion models to achieve precise viewpoint transformations under zero-shot
conditions, while non-rigid editing employs adversarial training and
self-attention mechanisms for complex, topologically consistent modifications.
Then, we generate pairs of high-resolution image and accurate segmentation mask
using a multi-conditional control generation method. Finally, our experiments
on the widely-used DIS5K dataset benchmark demonstrate superior performance in
quality and efficiency compared to existing methods. The code is available at
\url{https://qian-hao-tian.github.io/MaskFactory/}.",2024-12-26T06:37:25Z,http://arxiv.org/abs/2412.19080v1,"Haotian Qian, YD Chen, Shengtao Lou, Fahad Shahbaz Khan, Xiaogang Jin, Deng-Ping Fan"
"Graph-Enhanced Dual-Stream Feature Fusion with Pre-Trained Model for
  Acoustic Traffic Monitoring","Microphone array techniques are widely used in sound source localization and
smart city acoustic-based traffic monitoring, but these applications face
significant challenges due to the scarcity of labeled real-world traffic audio
data and the complexity and diversity of application scenarios. The DCASE
Challenge's Task 10 focuses on using multi-channel audio signals to count
vehicles (cars or commercial vehicles) and identify their directions
(left-to-right or vice versa). In this paper, we propose a graph-enhanced
dual-stream feature fusion network (GEDF-Net) for acoustic traffic monitoring,
which simultaneously considers vehicle type and direction to improve detection.
We propose a graph-enhanced dual-stream feature fusion strategy which consists
of a vehicle type feature extraction (VTFE) branch, a vehicle direction feature
extraction (VDFE) branch, and a frame-level feature fusion module to combine
the type and direction feature for enhanced performance. A pre-trained model
(PANNs) is used in the VTFE branch to mitigate data scarcity and enhance the
type features, followed by a graph attention mechanism to exploit temporal
relationships and highlight important audio events within these features. The
frame-level fusion of direction and type features enables fine-grained feature
representation, resulting in better detection performance. Experiments
demonstrate the effectiveness of our proposed method. GEDF-Net is our
submission that achieved 1st place in the DCASE 2024 Challenge Task 10.",2024-12-26T06:28:42Z,http://arxiv.org/abs/2412.19078v1,"Shitong Fan, Feiyang Xiao, Wenbo Wang, Shuhan Qi, Qiaoxi Zhu, Wenwu Wang, Jian Guan"
"Robust Speech and Natural Language Processing Models for Depression
  Screening","Depression is a global health concern with a critical need for increased
patient screening. Speech technology offers advantages for remote screening but
must perform robustly across patients. We have described two deep learning
models developed for this purpose. One model is based on acoustics; the other
is based on natural language processing. Both models employ transfer learning.
Data from a depression-labeled corpus in which 11,000 unique users interacted
with a human-machine application using conversational speech is used. Results
on binary depression classification have shown that both models perform at or
above AUC=0.80 on unseen data with no speaker overlap. Performance is further
analyzed as a function of test subset characteristics, finding that the models
are generally robust over speaker and session variables. We conclude that
models based on these approaches offer promise for generalized automated
depression screening.",2024-12-26T06:05:52Z,http://arxiv.org/abs/2412.19072v1,"Y. Lu, A. Harati, T. Rutowski, R. Oliveira, P. Chlebek, E. Shriberg"
Cross-Demographic Portability of Deep NLP-Based Depression Models,"Deep learning models are rapidly gaining interest for real-world applications
in behavioral health. An important gap in current literature is how well such
models generalize over different populations. We study Natural Language
Processing (NLP) based models to explore portability over two different corpora
highly mismatched in age. The first and larger corpus contains younger
speakers. It is used to train an NLP model to predict depression. When testing
on unseen speakers from the same age distribution, this model performs at
AUC=0.82. We then test this model on the second corpus, which comprises seniors
from a retirement community. Despite the large demographic differences in the
two corpora, we saw only modest degradation in performance for the
senior-corpus data, achieving AUC=0.76. Interestingly, in the senior
population, we find AUC=0.81 for the subset of patients whose health state is
consistent over time. Implications for demographic portability of speech-based
applications are discussed.",2024-12-26T05:54:24Z,http://arxiv.org/abs/2412.19070v1,"Tomek Rutowski, Elizabeth Shriberg, Amir Harati, Yang Lu, Ricardo Oliveira, Piotr Chlebek"
Effective and secure federated online learning to rank,"Online Learning to Rank (OLTR) optimises ranking models using implicit user
feedback, such as clicks. Unlike traditional Learning to Rank (LTR) methods
that rely on a static set of training data with relevance judgements to learn a
ranking model, OLTR methods update the model continually as new data arrives.
Thus, it addresses several drawbacks such as the high cost of human
annotations, potential misalignment between user preferences and human
judgments, and the rapid changes in user query intents. However, OLTR methods
typically require the collection of searchable data, user queries, and clicks,
which poses privacy concerns for users.
  Federated Online Learning to Rank (FOLTR) integrates OLTR within a Federated
Learning (FL) framework to enhance privacy by not sharing raw data. While
promising, FOLTR methods currently lag behind traditional centralised OLTR due
to challenges in ranking effectiveness, robustness with respect to data
distribution across clients, susceptibility to attacks, and the ability to
unlearn client interactions and data. This thesis presents a comprehensive
study on Federated Online Learning to Rank, addressing its effectiveness,
robustness, security, and unlearning capabilities, thereby expanding the
landscape of FOLTR.",2024-12-26T05:53:10Z,http://arxiv.org/abs/2412.19069v1,Shuyi Wang
"Attacking Voice Anonymization Systems with Augmented Feature and Speaker
  Identity Difference","This study focuses on the First VoicePrivacy Attacker Challenge within the
ICASSP 2025 Signal Processing Grand Challenge, which aims to develop speaker
verification systems capable of determining whether two anonymized speech
signals are from the same speaker. However, differences between feature
distributions of original and anonymized speech complicate this task. To
address this challenge, we propose an attacker system that combines Data
Augmentation enhanced feature representation and Speaker Identity Difference
enhanced classifier to improve verification performance, termed DA-SID.
Specifically, data augmentation strategies (i.e., data fusion and SpecAugment)
are utilized to mitigate feature distribution gaps, while probabilistic linear
discriminant analysis (PLDA) is employed to further enhance speaker identity
difference. Our system significantly outperforms the baseline, demonstrating
exceptional effectiveness and robustness against various voice anonymization
systems, ultimately securing a top-5 ranking in the challenge.",2024-12-26T05:52:44Z,http://arxiv.org/abs/2412.19068v1,"Yanzhe Zhang, Zhonghao Bi, Feiyang Xiao, Xuefeng Yang, Qiaoxi Zhu, Jian Guan"
Learning Monocular Depth from Events via Egomotion Compensation,"Event cameras are neuromorphically inspired sensors that sparsely and
asynchronously report brightness changes. Their unique characteristics of high
temporal resolution, high dynamic range, and low power consumption make them
well-suited for addressing challenges in monocular depth estimation (e.g.,
high-speed or low-lighting conditions). However, current existing methods
primarily treat event streams as black-box learning systems without
incorporating prior physical principles, thus becoming over-parameterized and
failing to fully exploit the rich temporal information inherent in event camera
data. To address this limitation, we incorporate physical motion principles to
propose an interpretable monocular depth estimation framework, where the
likelihood of various depth hypotheses is explicitly determined by the effect
of motion compensation. To achieve this, we propose a Focus Cost Discrimination
(FCD) module that measures the clarity of edges as an essential indicator of
focus level and integrates spatial surroundings to facilitate cost estimation.
Furthermore, we analyze the noise patterns within our framework and improve it
with the newly introduced Inter-Hypotheses Cost Aggregation (IHCA) module,
where the cost volume is refined through cost trend prediction and multi-scale
cost consistency constraints. Extensive experiments on real-world and synthetic
datasets demonstrate that our proposed framework outperforms cutting-edge
methods by up to 10\% in terms of the absolute relative error metric, revealing
superior performance in predicting accuracy.",2024-12-26T05:41:18Z,http://arxiv.org/abs/2412.19067v1,"Haitao Meng, Chonghao Zhong, Sheng Tang, Lian JunJia, Wenwei Lin, Zhenshan Bing, Yi Chang, Gang Chen, Alois Knoll"
LASER: A new method for locally adaptive nonparametric regression,"In this article, we introduce \textsf{LASER} (Locally Adaptive Smoothing
Estimator for Regression), a computationally efficient locally adaptive
nonparametric regression method that performs variable bandwidth local
polynomial regression. We prove that it adapts (near-)optimally to the local
H\""{o}lder exponent of the underlying regression function
\texttt{simultaneously} at all points in its domain. Furthermore, we show that
there is a single ideal choice of a global tuning parameter under which the
above mentioned local adaptivity holds. Despite the vast literature on
nonparametric regression, instances of practicable methods with provable
guarantees of such a strong notion of local adaptivity are rare. The proposed
method achieves excellent performance across a broad range of numerical
experiments in comparison to popular alternative locally adaptive methods.",2024-12-27T18:59:03Z,http://arxiv.org/abs/2412.19802v1,"Sabyasachi Chatterjee, Subhajit Goswami, Soumendu Sundar Mukherjee"
"Streamlined Krylov construction and classification of ergodic Floquet
  systems","We generalize Krylov construction to periodically driven (Floquet) quantum
systems using the theory of orthogonal polynomials on the unit circle. Compared
to other approaches, our method works faster and maps any quantum dynamics to a
one-dimensional tight-binding Krylov chain. We also suggest a classification of
chaotic and integrable Floquet systems based on the asymptotic behavior of
Krylov chain hopping parameters (Verblunsky coefficients). We illustrate this
classification with random matrix ensembles, kicked top, and kicked Ising
chain.",2024-12-27T18:56:27Z,http://arxiv.org/abs/2412.19797v1,"Nikita Kolganov, Dmitrii A. Trunin"
"Data-driven analysis of anomalous transport and three-wave-coupling
  effects in E x B plasma discharges","Collisionless cross-field electron transport in an E x B configuration
relevant for electric propulsion is studied using data from a (z, {\theta})
full-PIC simulation. Higher-order spectral analysis shows that transport is
dominated by the in-phase interaction of the oscillations of the azimuthal
electric field and the electron density associated to the first electron
cyclotron drift instability (ECDI) mode. A secondary contribution emanates from
a lower-frequency mode, not predicted by linear ECDI theory, while higher modes
have a minor direct impact on transport. However, a bicoherence analysis
reveals that strong phase couplings exist among the ECDI modes, and a sparse
symbolic regression spectral model, based on the three-wave coupling equations,
suggests an inverse energy cascade as the most likely explanation, thus
suggesting that higher modes contribute indirectly to transport by quadratic
power transfer to the first mode. This work provides new insights into the
dynamics of anomalous plasma transport in E x B sources and the underlying
processes governing energy distribution across different scales, and supports
the validity of weak turbulence theory to examine their behavior.",2024-12-27T18:43:24Z,http://arxiv.org/abs/2412.19789v1,"Borja Bayón-Buján, Enrique Bello-Benítez, Jiewei Zhou, Mario Merino"
"Machine Learning for Sentiment Analysis of Imported Food in Trinidad and
  Tobago","This research investigates the performance of various machine learning
algorithms (CNN, LSTM, VADER, and RoBERTa) for sentiment analysis of Twitter
data related to imported food items in Trinidad and Tobago. The study addresses
three primary research questions: the comparative accuracy and efficiency of
the algorithms, the optimal configurations for each model, and the potential
applications of the optimized models in a live system for monitoring public
sentiment and its impact on the import bill. The dataset comprises tweets from
2018 to 2024, divided into imbalanced, balanced, and temporal subsets to assess
the impact of data balancing and the COVID-19 pandemic on sentiment trends. Ten
experiments were conducted to evaluate the models under various configurations.
Results indicated that VADER outperformed the other models in both multi-class
and binary sentiment classifications. The study highlights significant changes
in sentiment trends pre- and post-COVID-19, with implications for import
policies.",2024-12-27T18:25:08Z,http://arxiv.org/abs/2412.19781v1,"Cassandra Daniels, Koffka Khan"
Functionality of Random Graphs,"The functionality of a graph $G$ is the minimum number $k$ such that in every
induced subgraph of $G$ there exists a vertex whose neighbourhood is uniquely
determined by the neighborhoods of at most $k$ other vertices in the subgraph.
The functionality parameter was introduced in the context of adjacency labeling
schemes, and it generalises a number of classical and recent graph parameters
including degeneracy, twin-width, and symmetric difference. We establish the
functionality of a random graph $G(n,p)$ up to a constant factor for every
value of $p$.",2024-12-27T18:09:29Z,http://arxiv.org/abs/2412.19771v1,"John Sylvester, Viktor Zamaraev, Maksim Zhukovskii"
Classification of Minimal Abelian Coulomb Branches,"Obtaining the classification of 3d $\mathcal{N}=4$ quivers whose Coulomb
branches have an isolated singularity is an essential step in understanding
moduli spaces of vacua of supersymmetric field theories with 8 supercharges in
any dimension. In this work, we derive a full classification for such Abelian
quivers with arbitrary charges, and identify all possible Coulomb branch
geometries as quotients of $\mathbb{H}^n$ by $\mathrm{U}(1)$ or a finite cyclic
group. We give two proofs, one which uses the decay and fission algorithm, and
another one relying only on explicit computations involving 3d mirror symmetry.
In the process, we put forward a method for computing the 3d mirror of any
$\mathrm{U}(1)^r$ gauge theory, which is sensitive to discrete gauge factors in
the mirror theory. This constitutes a confirmation for the decay and fission
algorithm.",2024-12-27T17:54:59Z,http://arxiv.org/abs/2412.19766v1,"Antoine Bourget, Quentin Lamouret, Sinan Moura Soysüren, Marcus Sperling"
Can one hear the shape of a random walk?,"To what extent is the underlying distribution of a finitely supported
unbiased random walk on $\mathbb{Z}$ determined by the sequence of times at
which the walk returns to the origin? The main result of this paper is that, in
various senses, most unbiased random walks on $\mathbb{Z}$ are determined up to
equivalence by the sequence $I_1,I_2,I_3,\ldots$, where $I_n$ denotes the
probability of being at the origin after $n$ steps. The proof depends on the
classification of finite simple groups.",2024-12-27T17:42:34Z,http://arxiv.org/abs/2412.19762v1,Michael J. Larsen
Tree tilings in random regular graphs,"We show that for every $\epsilon&gt;0$ there exists a sufficiently large $d_0\in
\mathbb{N}$ such that for every $d\ge d_0$, \textbf{whp} the random $d$-regular
graph $G(n,d)$ contains a $T$-factor for every tree $T$ on at most
$(1-\epsilon)d/\log d$ vertices. This is best possible since, for large enough
integer $d$, \textbf{whp} $G(n,d)$ does not contain a
$\frac{(1+\epsilon)d}{\log d}$-star factor.",2024-12-27T17:35:47Z,http://arxiv.org/abs/2412.19756v1,"Sahar Diskin, Ilay Hoshen, Maksim Zhukovskii"
A random walk among random graphs,"Lecture notes of a master course given at Orsay between 2019-2024. Topics
covered include Part I: One-dimensional random walks, cycle lemma and
Bienaym\'e--Galton--Watson random trees. Part II: Erd\""os--R\'enyi random
graphs, three proofs of the emergence of the giant component. Part III: Random
recursive tree, random permutations and continuous time embedding techniques.
Intended for publication.",2024-12-27T17:21:41Z,http://arxiv.org/abs/2412.19752v1,Nicolas Curien
"Weak lumping of left-invariant random walks on left cosets of finite
  groups","Let $G$ be a finite group and let $H$ be a subgroup of $G$. The
left-invariant random walk driven by a probability measure $w$ on $G$ is the
Markov chain in which from any state $x \in G$, the probability of stepping to
$xg \in G$ is $w(g)$. The initial state is chosen randomly according to a given
distribution. The walk is said to lump weakly on left cosets if the induced
process on $G/H$ is a time-homogeneous Markov chain. We characterise all the
initial distributions and weights $w$ such that the walk is irreducible and
lumps weakly on left cosets, and determine all the possible transition matrices
of the induced Markov chain. In the case where $H$ is abelian we refine our
main results to give a necessary and sufficient condition for weak lumping by
an explicit system of linear equations on $w$, organized by the double cosets
$HxH$. As an application we consider shuffles of a deck of $n$ cards such that
repeated observations of the top card form a Markov chain. Such shuffles
include the random-to-top shuffle, and also, when the deck is started in a
uniform random order, the top-to-random shuffle. We give a further family of
examples in which our full theory of weak lumping is needed to verify that the
top card sequence is Markov.",2024-12-27T17:08:17Z,http://arxiv.org/abs/2412.19742v1,"Edward Crane, Álvaro Gutiérrez, Erin Russell, Mark Wildon"
"Periodically and aperiodically Thue-Morse driven long-range systems:
  from dynamical localization to slow dynamics","We investigate the electric-field driven power-law random banded
matrix(PLRBM) model where a variation in the power-law exponent $\alpha$ yields
a delocalization-to-localization phase transition. We examine the periodically
driven PLRBM model with the help of the Floquet operator. The level spacing
ratio and the generalized participation ratio of the Floquet Hamiltonian reveal
a drive-induced fractal phase accompanied by diffusive transport on the
delocalized side of the undriven PLRBM model. On the localized side, the
time-periodic model remains localized - the average spacing ratio corresponds
to Poisson statistics and logarithmic transport is observed in the dynamics.
Extending our analysis to the aperiodic Thue-Morse (TM) driven system, we find
that the aperiodically driven clean long-range hopping model (clean counterpart
of the PLRBM model) exhibits the phenomenon of \textit{exact dynamical
localization} (EDL) on tuning the drive-parameters at special points. The
disordered time-aperiodic system shows diffusive transport followed by
relaxation to the infinite-temperature state on the delocalized side, and a
prethermal plateau with subdiffusion on the localized side. Additionally, we
compare this with a quasi-periodically driven AAH model that also undergoes a
localization-delocalization transition. Unlike the disordered long-range model,
it features a prolonged prethermal plateau followed by subdiffusion to the
infinite temperature state, even on the delocalized side.",2024-12-27T16:55:47Z,http://arxiv.org/abs/2412.19736v1,"Vatsana Tiwari, Devendra Singh Bhakuni, Auditya Sharma"
"Generative Pretrained Embedding and Hierarchical Irregular Time Series
  Representation for Daily Living Activity Recognition","Within the evolving landscape of smart homes, the precise recognition of
daily living activities using ambient sensor data stands paramount. This paper
not only aims to bolster existing algorithms by evaluating two distinct
pretrained embeddings suited for ambient sensor activations but also introduces
a novel hierarchical architecture. We delve into an architecture anchored on
Transformer Decoder-based pre-trained embeddings, reminiscent of the GPT
design, and contrast it with the previously established state-of-the-art (SOTA)
ELMo embeddings for ambient sensors. Our proposed hierarchical structure
leverages the strengths of each pre-trained embedding, enabling the discernment
of activity dependencies and sequence order, thereby enhancing classification
precision. To further refine recognition, we incorporate into our proposed
architecture an hour-of-the-day embedding. Empirical evaluations underscore the
preeminence of the Transformer Decoder embedding in classification endeavors.
Additionally, our innovative hierarchical design significantly bolsters the
efficacy of both pre-trained embeddings, notably in capturing inter-activity
nuances. The integration of temporal aspects subtly but distinctively augments
classification, especially for time-sensitive activities. In conclusion, our
GPT-inspired hierarchical approach, infused with temporal insights, outshines
the SOTA ELMo benchmark.",2024-12-27T16:43:52Z,http://arxiv.org/abs/2412.19732v1,"Damien Bouchabou, Sao Mai Nguyen"
High-dimensional permutons: theory and applications,"Permutons, which are probability measures on the unit square $[0, 1]^2$ with
uniform marginals, are the natural scaling limits for sequences of (random)
permutations.
  We introduce a $d$-dimensional generalization of these measures for all $d
\ge 2$, which we call $d$-dimensional permutons, and extend -- from the
two-dimensional setting -- the theory to prove convergence of sequences of
(random) $d$-dimensional permutations to (random) $d$-dimensional permutons.
  Building on this new theory, we determine the random high-dimensional
permuton limits for two natural families of high-dimensional permutations.
First, we determine the $3$-dimensional permuton limit for Schnyder wood
permutations, which bijectively encode planar triangulations decorated by
triples of spanning trees known as Schnyder woods. Second, we identify the
$d$-dimensional permuton limit for $d$-separable permutations, a
pattern-avoiding class of $d$-dimensional permutations generalizing ordinary
separable permutations.
  Both high-dimensional permuton limits are random and connected to previously
studied universal 2-dimensional permutons, such as the Brownian separable
permutons and the skew Brownian permutons, and share interesting connections
with objects arising from random geometry, including the continuum random tree,
Schramm--Loewner evolutions, and Liouville quantum gravity surfaces.",2024-12-27T16:40:41Z,http://arxiv.org/abs/2412.19730v1,"Jacopo Borga, Andrew Lin"
"Learning to Forget: Bayesian Time Series Forecasting using Recurrent
  Sparse Spectrum Signature Gaussian Processes","The signature kernel is a kernel between time series of arbitrary length and
comes with strong theoretical guarantees from stochastic analysis. It has found
applications in machine learning such as covariance functions for Gaussian
processes. A strength of the underlying signature features is that they provide
a structured global description of a time series. However, this property can
quickly become a curse when local information is essential and forgetting is
required; so far this has only been addressed with ad-hoc methods such as
slicing the time series into subsegments. To overcome this, we propose a
principled, data-driven approach by introducing a novel forgetting mechanism
for signatures. This allows the model to dynamically adapt its context length
to focus on more recent information. To achieve this, we revisit the recently
introduced Random Fourier Signature Features, and develop Random Fourier
Decayed Signature Features (RFDSF) with Gaussian processes (GPs). This results
in a Bayesian time series forecasting algorithm with variational inference,
that offers a scalable probabilistic algorithm that processes and transforms a
time series into a joint predictive distribution over time steps in one pass
using recurrence. For example, processing a sequence of length $10^4$ steps in
$\approx 10^{-2}$ seconds and in $&lt; 1\text{GB}$ of GPU memory. We demonstrate
that it outperforms other GP-based alternatives and competes with
state-of-the-art probabilistic time series forecasting algorithms.",2024-12-27T16:31:09Z,http://arxiv.org/abs/2412.19727v1,"Csaba Tóth, Masaki Adachi, Michael A. Osborne, Harald Oberhauser"
EEG-Reptile: An Automatized Reptile-Based Meta-Learning Library for BCIs,"Meta-learning, i.e., ""learning to learn"", is a promising approach to enable
efficient BCI classifier training with limited amounts of data. It can
effectively use collections of in some way similar classification tasks, with
rapid adaptation to new tasks where only minimal data are available. However,
applying meta-learning to existing classifiers and BCI tasks requires
significant effort. To address this issue, we propose EEG-Reptile, an automated
library that leverages meta-learning to improve classification accuracy of
neural networks in BCIs and other EEG-based applications. It utilizes the
Reptile meta-learning algorithm to adapt neural network classifiers of EEG data
to the inter-subject domain, allowing for more efficient fine-tuning for a new
subject on a small amount of data. The proposed library incorporates an
automated hyperparameter tuning module, a data management pipeline, and an
implementation of the Reptile meta-learning algorithm. EEG-Reptile automation
level allows using it without deep understanding of meta-learning. We
demonstrate the effectiveness of EEG-Reptile on two benchmark datasets (BCI IV
2a, Lee2019 MI) and three neural network architectures (EEGNet, FBCNet,
EEG-Inception). Our library achieved improvement in both zero-shot and few-shot
learning scenarios compared to traditional transfer learning approaches.",2024-12-27T16:24:31Z,http://arxiv.org/abs/2412.19725v1,"Daniil A. Berdyshev, Artem M. Grachev, Sergei L. Shishkin, Bogdan L. Kozyrskiy"
"Trading Off Energy Storage and Payload -- An Analytical Model for
  Freight Train Configuration","To support planning of alternative fuel technology (e.g., battery-electric
locomotives) deployment for decarbonizing non-electrified freight rail, we
develop a convex optimization formulation with a closed-form solution to
determine the optimal number of energy storage tender cars in a train. The
formulation shares a similar structure to an Economic Order Quantity (EOQ)
model. For given market characteristics, cost forecasts, and technology
parameters, our model captures the trade-offs between inventory carrying costs
associated with trip times (including delays due to charging/refueling) and
ordering costs associated with train dispatch and operation (energy, amortized
equipment, and labor costs). To illustrate the framework, we find the optimal
number of battery-electric energy tender cars in 22,501 freight markets
(origin-destination pairs and commodities) for U.S. Class I railroads. The
results display heterogeneity in optimal configurations with lighter, yet more
time-sensitive shipments (e.g., intermodal) utilizing more battery tender cars.
For heavier commodities (e.g., coal) with lower holding costs, single battery
tender car configurations are generally optimal. The results also show that the
optimal train configurations are sensitive to delays associated with recharging
or swapping tender cars.",2024-12-27T16:18:35Z,http://arxiv.org/abs/2412.19719v1,"Max T. M. Ng, Adrian Hernandez, Pablo L. Durango-Cohen, Hani S. Mahmassani"
"Causal machine learning for heterogeneous treatment effects in the
  presence of missing outcome data","When estimating heterogeneous treatment effects, missing outcome data can
complicate treatment effect estimation, causing certain subgroups of the
population to be poorly represented. In this work, we discuss this commonly
overlooked problem and consider the impact that missing at random (MAR) outcome
data has on causal machine learning estimators for the conditional average
treatment effect (CATE). We then propose two de-biased machine learning
estimators for the CATE, the mDR-learner and mEP-learner, which address the
issue of under-representation by integrating inverse probability of censoring
weights into the DR-learner and EP-learner respectively. We show that under
reasonable conditions, these estimators are oracle efficient, and illustrate
their favorable performance through simulated data settings, comparing them to
existing CATE estimators, including comparison to estimators which use common
missing data techniques. Guidance on the implementation of these estimators is
provided and we present an example of their application using the ACTG175
trial, exploring treatment effect heterogeneity when comparing Zidovudine
mono-therapy against alternative antiretroviral therapies among HIV-1-infected
individuals.",2024-12-27T16:10:03Z,http://arxiv.org/abs/2412.19711v1,"Matthew Pryce, Karla Diaz-Ordaz, Ruth H. Keogh, Stijn Vansteelandt"
"All Finite (Anti)Hermitian Irreducible Representations of the de Sitter
  and Anti-de Sitter Lie Algebras and Their Lorentz Structure","Because of the importance of unitarity in quantum physics, work on the
representations of the de Sitter group has focussed on the unitary case, which
necessarily means infinite dimensional matrices for this non-compact group.
Here we address the finite dimensional representations resulting from the
requirement that the Lie algebra generators are either Hermitian or
anti-Hermitian. The complete classification of all such irreducible
representations is found and their matrix elements specified. These irreducible
representations (irreps) are based on backbones defined as the homogeneous
Lorentz sub-algebra and consisting of direct sums of the finite irreps of the
homogeneous Lorentz algebra (HLA). Only two types of such backbones arise (see
5.1a,b herein). Consequently, only certain dimensions of representation are
possible, namely 4, 5, 10, 14, 20, 30, 35, 55, 56, 91, etc or generally either
1/6 N(N+1)(N+2) or 1/6 N(N+1)(2N+1) where N=2,3,4,etc is the number of HLA
irreps in the backbone (minimum 2). The two Casimir invariants can be specified
in terms of a single integral or half-integral parameter, p. For irreps based
on (5.1a), -C1=p(p+1)-2 and C2=0 with p taking values 2,3,4,etc. For irreps
based on (5.1b), -C1=2(p^2-1) and -C2= p^2 (p^2-1) with p taking values
3/2,2,5/2,3,etc. These correspond to the same expressions found for the unitary
representations, -C1=p(p+1)+(q+1)(q-2) and -C2=p(p+1)q(q-1) with q=0 and q=p
respectively for the two types of irrep. There is thus a far more restricted
set of finite irreps with Hermitian or anti-Hermitian generators than for the
discrete infinite dimensional unitary irreps. The corresponding irreps of the
anti-de Sitter group follow immediately from the replacement of the 4-momentum
operators from V to iV.",2024-12-27T16:02:41Z,http://arxiv.org/abs/2412.19708v1,Richard A. W. Bradford
"An Integrated Optimization and Deep Learning Pipeline for Predicting
  Live Birth Success in IVF Using Feature Optimization and Transformer-Based
  Models","In vitro fertilization (IVF) is a widely utilized assisted reproductive
technology, yet predicting its success remains challenging due to the
multifaceted interplay of clinical, demographic, and procedural factors. This
study develops a robust artificial intelligence (AI) pipeline aimed at
predicting live birth outcomes in IVF treatments. The pipeline uses anonymized
data from 2010 to 2018, obtained from the Human Fertilization and Embryology
Authority (HFEA). We evaluated the prediction performance of live birth success
as a binary outcome (success/failure) by integrating different feature
selection methods, such as principal component analysis (PCA) and particle
swarm optimization (PSO), with different traditional machine learning-based
classifiers including random forest (RF) and decision tree, as well as deep
learning-based classifiers including custom transformer-based model and a tab
transformer model with an attention mechanism. Our research demonstrated that
the best performance was achieved by combining PSO for feature selection with
the TabTransformer-based deep learning model, yielding an accuracy of 99.50%
and an AUC of 99.96%, highlighting its significant performance to predict live
births. This study establishes a highly accurate AI pipeline for predicting
live birth outcomes in IVF, demonstrating its potential to enhance personalized
fertility treatments.",2024-12-27T15:46:59Z,http://arxiv.org/abs/2412.19696v1,"Arezoo Borji, Hossam Haick, Birgit Pohn, Antonia Graf, Jana Zakall, S M Ragib Shahriar Islam, Gernot Kronreif, Daniel Kovatchki, Heinz Strohmer, Sepideh Hatamikia"
"Quantum Many-Body Lattice C-R-T Symmetry: Fractionalization, Anomaly,
  and Symmetric Mass Generation","Charge conjugation (C), mirror reflection (R), and time reversal (T)
symmetries, along with internal symmetries, are essential for massless Majorana
and Dirac fermions. These symmetries are sufficient to rule out potential
fermion bilinear mass terms, thereby establishing a gapless free fermion fixed
point phase, pivotal for symmetric mass generation (SMG) transition. In this
work, we systematically study the anomaly of C-R-T-internal symmetry in all
spacetime dimensions by analyzing the projective representation (i.e. the
fractionalization) of the C-R-T-internal symmetry group in the quantum
many-body Hilbert space on the lattice. By discovering the
fermion-flavor-number-dependent C-R-T-internal symmetry's anomaly structure, we
demonstrate an alternative way to derive the minimal flavor number for SMG,
which shows consistency with known results from K\""ahler-Dirac fermion or
cobordism classification. Our findings reveal that, in general spatial
dimensions, either 8 copies of staggered Majorana fermions or 4 copies of
staggered Dirac fermions admit SMG. By directly searching for 4-fermion
interactions that form commuting stabilizers respecting all symmetry
constraints, we can prove the explicit SMG gapping retained a unique ground
state in the codespace. Furthermore, we establish the correspondence between
the symmetry operators of staggered fermions and free fermions, which is
instrumental in facilitating the analysis of symmetry fractionalization at the
field theory level.",2024-12-27T15:36:31Z,http://arxiv.org/abs/2412.19691v1,"Yang-Yang Li, Juven Wang, Yi-Zhuang You"
Emergent cell migration from cell shape deformations and T1 transitions,"T1 transitions, which are localised cell rearrangements, play an important
role in the fluidization of epithelial monolayers. Using a multi-phase field
model and an active elastic solid model, we show that although each cell
undergoes T1 transitions in time as uncorrelated, random events, the spatial
distribution of these events is highly correlated and is dependent on cell
shape. T1 transitions have a dual effect: cells losing neighbours tend to relax
their shape, while those gaining neighbours tend to elongate. By analysing the
statistics of successive T1 transitions undergone by a deformable cell, we find
asymmetric spatial distributions related to how cells lose or gain neighbours.
These asymmetric spatial patterns of T1 transitions promote directed cell
migration, and form the backbone for coherent flow patterns at tissue scales.",2024-12-27T15:26:35Z,http://arxiv.org/abs/2412.19686v1,"Harish P. Jain, Richard D. J. G. Ho, Luiza Angheluta"
"DLScanner: A parameter space scanner package assisted by deep learning
  methods","In this paper, we introduce a scanner package enhanced by deep learning (DL)
techniques. The proposed package addresses two significant challenges
associated with previously developed DL-based methods: slow convergence in
high-dimensional scans and the limited generalization of the DL network when
mapping random points to the target space. To tackle the first issue, we
utilize a similarity learning network that maps sampled points into a
representation space. In this space, in-target points are grouped together
while out-target points are effectively pushed apart. This approach enhances
the scan convergence by refining the representation of sampled points. The
second challenge is mitigated by integrating a dynamic sampling strategy.
Specifically, we employ a VEGAS mapping to adaptively suggest new points for
the DL network while also improving the mapping when more points are collected.
Our proposed framework demonstrates substantial gains in both performance and
efficiency compared to other scanning methods.",2024-12-27T14:52:42Z,http://arxiv.org/abs/2412.19675v1,"A. Hammad, Raymundo Ramos"
Spectral form factors for curved spacetimes with horizon,"The spectral form factor is believed to provide a special type of behavior
called ""dip-ramp-plateau"" in chaotic quantum systems which originates from the
random matrix theory. A similar behavior could be observed for deterministic
systems, ranging from the Riemann zeta function to the scattering amplitudes of
different types. It has been shown recently, the same behavior is observed for
the spectral form factor when the normal modes of a scalar massless field
theory in the brickwall model of the BTZ black hole are substituted as
eigenvalues of some quantum Hamiltonian. At the same time, the level spacing
distribution of these eigenvalues differs from that associated with the random
matrix theory ensembles. In this paper, we generalize these results considering
the recently proposed generalized spectral form factor for the de Sitter and
BTZ spacetimes. We study the details of this complex-valued form factor for
integrable quantum systems and for backgrounds with a horizon comparing it with
the random matrix theory behavior. As a result, we confirm that the scalar
field normal modes once again exhibit features of chaos.",2024-12-27T14:43:35Z,http://arxiv.org/abs/2412.19672v1,"Dmitry S. Ageev, Vasilii V. Pushkarev, Anastasia N. Zueva"
"Asymmetrical Reciprocity-based Federated Learning for Resolving
  Disparities in Medical Diagnosis","Geographic health disparities pose a pressing global challenge, particularly
in underserved regions of low- and middle-income nations. Addressing this issue
requires a collaborative approach to enhance healthcare quality, leveraging
support from medically more developed areas. Federated learning emerges as a
promising tool for this purpose. However, the scarcity of medical data and
limited computation resources in underserved regions make collaborative
training of powerful machine learning models challenging. Furthermore, there
exists an asymmetrical reciprocity between underserved and developed regions.
To overcome these challenges, we propose a novel cross-silo federated learning
framework, named FedHelp, aimed at alleviating geographic health disparities
and fortifying the diagnostic capabilities of underserved regions.
Specifically, FedHelp leverages foundational model knowledge via one-time API
access to guide the learning process of underserved small clients, addressing
the challenge of insufficient data. Additionally, we introduce a novel
asymmetric dual knowledge distillation module to manage the issue of asymmetric
reciprocity, facilitating the exchange of necessary knowledge between developed
large clients and underserved small clients. We validate the effectiveness and
utility of FedHelp through extensive experiments on both medical image
classification and segmentation tasks. The experimental results demonstrate
significant performance improvement compared to state-of-the-art baselines,
particularly benefiting clients in underserved regions.",2024-12-27T13:59:58Z,http://arxiv.org/abs/2412.19654v1,"Jiaqi Wang, Ziyi Yin, Quanzeng You, Lingjuan Lyu, Fenglong Ma"
"FreStega: A Plug-and-Play Method for Boosting Imperceptibility and
  Capacity in Generative Linguistic Steganography for Real-World Scenarios","Linguistic steganography embeds secret information in seemingly innocent
texts, safeguarding privacy in surveillance environments. Generative linguistic
steganography leverages the probability distribution of language models (LMs)
and applies steganographic algorithms to generate stego tokens, gaining
attention with recent Large Language Model (LLM) advancements. To enhance
security, researchers develop distribution-preserving stego algorithms to
minimize the gap between stego sampling and LM sampling. However, the reliance
on language model distributions, coupled with deviations from real-world cover
texts, results in insufficient imperceptibility when facing steganalysis
detectors in real-world scenarios. Moreover, LLM distributions tend to be more
deterministic, resulting in reduced entropy and, consequently, lower embedding
capacity. In this paper, we propose FreStega, a plug-and-play method to
reconstruct the distribution of language models used for generative linguistic
steganography. FreStega dynamically adjusts token probabilities from the
language model at each step of stegotext auto-regressive generation, leveraging
both sequential and spatial dimensions. In sequential adjustment, the
temperature is dynamically adjusted based on instantaneous entropy, enhancing
the diversity of stego texts and boosting embedding capacity. In the spatial
dimension, the distribution is aligned with guidance from the target domain
corpus, closely mimicking real cover text in the target domain. By reforming
the distribution, FreStega enhances the imperceptibility of stego text in
practical scenarios and improves steganographic capacity by 15.41\%, all
without compromising the quality of the generated text. FreStega serves as a
plug-and-play remedy to enhance the imperceptibility and embedding capacity of
existing distribution-preserving steganography methods in real-world scenarios.",2024-12-27T13:56:51Z,http://arxiv.org/abs/2412.19652v1,Kaiyi Pang
"Distributed Download from an External Data Source in Faulty Majority
  Settings","We extend the study of retrieval problems in distributed networks, focusing
on improving the efficiency and resilience of protocols in the \emph{Data
Retrieval (DR) Model}. The DR Model consists of a complete network (i.e., a
clique) with $k$ peers, up to $\beta k$ of which may be Byzantine (for $\beta
\in [0, 1)$), and a trusted \emph{External Data Source} comprising an array $X$
of $n$ bits ($n \gg k$) that the peers can query. Additionally, the peers can
also send messages to each other. In this work, we focus on the Download
problem that requires all peers to learn $X$. Our primary goal is to minimize
the maximum number of queries made by any honest peer and additionally optimize
time.
  We begin with a randomized algorithm for the Download problem that achieves
optimal query complexity up to a logarithmic factor. For the stronger dynamic
adversary that can change the set of Byzantine peers from one round to the
next, we achieve the optimal time complexity in peer-to-peer communication but
with larger messages. In broadcast communication where all peers (including
Byzantine peers) are required to send the same message to all peers, with
larger messages, we achieve almost optimal time and query complexities for a
dynamic adversary. Finally, in a more relaxed crash fault model, where peers
stop responding after crashing, we address the Download problem in both
synchronous and asynchronous settings. Using a deterministic protocol, we
obtain nearly optimal results for both query complexity and message sizes in
these scenarios.",2024-12-27T13:55:00Z,http://arxiv.org/abs/2412.19649v1,"John Augustine, Soumyottam Chatterjee, Valerie King, Manish Kumar, Shachar Meir, David Peleg"
Photonic classification on a single diffractive layer,"Photonic computation started to shape the future of fast, efficient and
accessible computation. The advantages brought by light based Diffractive Deep
Neural Networks (D2NN), are shown to be overwhelmingly advantageous especially
in targeting classification problems. However, cost and complexity of
multi-layer systems are the main challenges that reduce the deployment of this
technology. In this study, we develop a simple yet extremely efficient way to
achieve optical classification using a single diffractive optical layer. A
spatial light modulator is used not only to emulate the classifying system but
also the input medium for the objects to be classified by the system. Using our
approach, we classify road traffic signs which has a direct application on
daily life and safety. We perform classification of road signs under the effect
of noise and show that we can successfully classify road signs with more than
75% accuracy under 20% noise/imperfection.",2024-12-27T12:09:28Z,http://arxiv.org/abs/2412.19607v1,"Anil J. Pekgöz, Emre Yüce"
"Enhancing Fine-grained Image Classification through Attentive Batch
  Training","Fine-grained image classification, which is a challenging task in computer
vision, requires precise differentiation among visually similar object
categories. In this paper, we propose 1) a novel module called Residual
Relationship Attention (RRA) that leverages the relationships between images
within each training batch to effectively integrate visual feature vectors of
batch images and 2) a novel technique called Relationship Position Encoding
(RPE), which encodes the positions of relationships between original images in
a batch and effectively preserves the relationship information between images
within the batch. Additionally, we design a novel framework, namely
Relationship Batch Integration (RBI), which utilizes RRA in conjunction with
RPE, allowing the discernment of vital visual features that may remain elusive
when examining a singular image representative of a particular class. Through
extensive experiments, our proposed method demonstrates significant
improvements in the accuracy of different fine-grained classifiers, with an
average increase of $(+2.78\%)$ and $(+3.83\%)$ on the CUB200-2011 and Stanford
Dog datasets, respectively, while achieving a state-of-the-art results
$(95.79\%)$ on the Stanford Dog dataset. Despite not achieving the same level
of improvement as in fine-grained image classification, our method still
demonstrates its prowess in leveraging general image classification by
attaining a state-of-the-art result of $(93.71\%)$ on the Tiny-Imagenet
dataset. Furthermore, our method serves as a plug-in refinement module and can
be easily integrated into different networks.",2024-12-27T12:07:58Z,http://arxiv.org/abs/2412.19606v1,"Duy M. Le, Bao Q. Bui, Anh Tran, Cong Tran, Cuong Pham"
Multiple objective linear programming over the probability simplex,"This paper considers the problem of maximizing multiple linear functions over
the probability simplex. A classification of feasible points is indicated. A
necessary and sufficient condition for a member of each class to be an
efficient solution is stated. This characterization yields a computational
procedure for ascertaining whether a feasible point is efficient. The procedure
does not require that candidates for efficiency be extreme points. An
illustration of the procedure is offered.",2024-12-27T11:44:49Z,http://arxiv.org/abs/2412.19598v1,Anas Mifrani
"Ultralight Signal Classification Model for Automatic Modulation
  Recognition","The growing complexity of radar signals demands responsive and accurate
detection systems that can operate efficiently on resource-constrained edge
devices. Existing models, while effective, often rely on substantial
computational resources and large datasets, making them impractical for edge
deployment. In this work, we propose an ultralight hybrid neural network
optimized for edge applications, delivering robust performance across
unfavorable signal-to-noise ratios (mean accuracy of 96.3% at 0 dB) using less
than 100 samples per class, and significantly reducing computational overhead.",2024-12-27T11:03:26Z,http://arxiv.org/abs/2412.19585v1,"Alessandro Daniele Genuardi Oquendo, Agustín Matías Galante Cerviño, Nilotpal Sinha, Luc Andrea, Sam Mugel, Román Orús"
"A Comparative Study of Machine Unlearning Techniques for Image and Text
  Classification Models","Machine Unlearning has emerged as a critical area in artificial intelligence,
addressing the need to selectively remove learned data from machine learning
models in response to data privacy regulations. This paper provides a
comprehensive comparative analysis of six state-of-theart unlearning techniques
applied to image and text classification tasks. We evaluate their performance,
efficiency, and compliance with regulatory requirements, highlighting their
strengths and limitations in practical scenarios. By systematically analyzing
these methods, we aim to provide insights into their applicability,
challenges,and tradeoffs, fostering advancements in the field of ethical and
adaptable machine learning.",2024-12-27T10:58:55Z,http://arxiv.org/abs/2412.19583v1,"Omar M. Safa, Mahmoud M. Abdelaziz, Mustafa Eltawy, Mohamed Mamdouh, Moamen Gharib, Salaheldin Eltenihy, Nagia M. Ghanem, Mohamed M. Ismail"
"Gauging or extending bulk and boundary conformal field theories:
  Application to bulk and domain wall problem in topological matter and their
  descriptions by (mock) modular covariant","We study gauging operations (or group extensions) in (smeared) boundary
conformal field theories (BCFTs) and bulk conformal field theories and their
applications to various phenomena in topologically ordered systems. We apply
the resultant theories to the correspondence between the renormalization group
(RG) flow of CFTs and the classification of topological quantum field theories
in the testable information of general classes of partition functions. One can
obtain the bulk topological properties of $2+1$ dimensional topological ordered
phase corresponding to the massive RG flow of $1+1$ dimensional systems, or
smeared BCFT. We present an obstruction of mass condensation for smeared BCFT
analogous to the Lieb-Shultz-Mattis theorem for noninvertible symmetry. Related
to the bulk topological degeneracies in $2+1$ dimensions and quantum phases in
$1+1$ dimensions we construct a new series of BCFT. We also investigate the
implications of the massless RG flow of $1+1$ dimensional CFT to $2+1$
dimensional topological order which corresponds to the earlier proposal by L.
Kong and H. Zheng in [Nucl. Phys. B 966 (2021), 115384], arXiv:1912.01760
closely related to the integer-spin simple current by Schellekens and
Gato-Rivera. We study the properties of the product of two CFTs connected by
the two kinds of massless flows. The (mock) modular covariants appearing in the
analysis seem to contain new ones. By applying the folding trick to the coupled
model, we provide a general method to solve the gapped and charged domain wall.
One can obtain the general phenomenology of the transportation of anyons
through the domain wall. Our work gives a unified direction for the future
theoretical and numerical studies of the topological phase based on the
established data of classifications of conformal field theories or modular
invariants.",2024-12-27T10:46:30Z,http://arxiv.org/abs/2412.19577v1,Yoshiki Fukusumi
"The possible long-term periodic variability of the extremely luminous
  quasar WISE J090924.01+000211.1","The extremely luminous infrared galaxy (ELIRG), WISE J090924.01+000211.1
(hereafter; WISE J0909+0002, $z=1.87$) is an extraordinary object with a quasar
aspect. This study performs monitoring observations of WISE J0909+0002 with the
105 cm Murikabushi telescope, Okayama and Akeno 50 cm telescopes/MITSuME ($g'$,
$R_{\rm c}$, and $I_{\rm c}$ bands), and the SaCRA 55 cm telescope/MuSaSHI
($r$, $i$, and $z$ bands). We obtain the following results by combining the
UV/optical light curves of the CRTS, Pan-STARRS, and ZTF archive data, and our
observational data: (1) the light curves of WISE J0909+0002 present
quasi-periodic (sinusoidal) oscillations with the rest-frame period of $\sim$
660$-$689 day; (2) the structure functions of WISE J0909+0002 do not show a
damped random walk (DRW) trend; (3) the mock DRW light curves present
periodic-like trend on rare occasions in 10000 simulations; (4) the
relativistic boost scenario is favored, since the relation between variability
amplitude and power-law slope ratio is consistent with the theoretical
prediction of this scenario, and a substantial parameter space exists between
the inclination angles and the black hole mass; (5) the circumbinary disk model
is difficult to explain the spectral energy distribution of our target; (6) the
significant radio flux density of WISE J0909+0002 is not detected from the VLA
FIRST Survey, thus the radio jet precession scenario is ruled out. From our
results, the Doppler boost scenario is likely as a cause of the periodic
variability, consequently the quasi-periodic oscillations in WISE J0909+0002 is
possibly interpreted by a supermassive blackhole binary. Additional
observations to investigate the continuity of the periodic trend would bring
new insights into mechanisms of the quasi-periodic oscillations and/or ELIRGs.",2024-12-27T10:33:11Z,http://arxiv.org/abs/2412.19573v1,"Takashi Horiuchi, Yoshiki Toba, Toru Misawa, Katsuhiro L. Murata, Keisuke Isogai, Yoichi Yatsu, Ichiro Takahashi, Mahito Sasada, Masafumi Niwano, Narikazu Higuchi, Shunsuke Hayatsu, Hibiki Seki, Yumiko Oasa, Rikuto Sato"
Safe Interval Randomized Path Planing For Manipulators,"Planning safe paths in 3D workspace for high DoF robotic systems, such as
manipulators, is a challenging problem, especially when the environment is
populated with the dynamic obstacles that need to be avoided. In this case the
time dimension should be taken into account that further increases the
complexity of planning. To mitigate this issue we suggest to combine
safe-interval path planning (a prominent technique in heuristic search) with
the randomized planning, specifically, with the bidirectional rapidly-exploring
random trees (RRT-Connect) - a fast and efficient algorithm for
high-dimensional planning. Leveraging a dedicated technique of fast computation
of the safe intervals we end up with an efficient planner dubbed SI-RRT. We
compare it with the state of the art and show that SI-RRT consistently
outperforms the competitors both in runtime and solution cost.
  Our implementation of SI-RRT is publicly available at
https://github.com/PathPlanning/ManipulationPlanning-SI-RRT",2024-12-27T10:10:52Z,http://arxiv.org/abs/2412.19567v1,"Nuraddin Kerimov, Aleksandr Onegin, Konstantin Yakovlev"
"Real-time Reflectance Generation for UAV Multispectral Imagery using an
  Onboard Downwelling Spectrometer in Varied Weather Conditions","Advancements in unmanned aerial vehicle (UAV) remote sensing with spectral
imaging enable efficient assessment of critical agronomic traits. However,
existing reflectance calibration or generation methods suffer from limited
prediction accuracy and practical flexibility. This study explores reliable and
cost-efficient methods for the accurate conversion of digital number values
acquired from a multispectral imager into reflectance, leveraging real-time
solar spectra as references. To ensure consistent measurements of incident
light, an upward gimbal-mounted downwelling spectrometer was attached to the
UAV, and a sinusoidal model was developed to correct for solar position
variability. Using principal component analysis on the reference solar spectrum
for band selection, a multiple linear regression model with four sensitive
bands (4-Band MLR) and a 30 nm bandwidth achieved performance comparable to the
direct correction method. The root mean square error (RMSE) for reflectance
prediction improved by 86.1% compared to the empirical line method under
fluctuating cloudy conditions and by 59.6% compared to the downwelling light
sensor method averaged across different weather conditions. The RMSE was
calculated as 2.24% in a ground-based diurnal validation, and 2.03% in a UAV
campaign conducted at various times throughout a sunny day. Implementing the
4-Band MLR model enhanced the consistency of canopy reflectance within a
homogeneous vegetation area by 95.0% during spectral imaging in a large rice
field under significant cloud fluctuations. Additionally, improvements of 86.0%
and 90.3% were noted for two vegetation indices: the normalized difference
vegetation index (NDVI; a ratio index) and the difference vegetation index
(DVI; a non-ratio index), respectively.",2024-12-27T08:41:46Z,http://arxiv.org/abs/2412.19527v1,"Jiayang Xie, Yutao Shen, Haiyan Cen"
"Attribution for Enhanced Explanation with Transferable Adversarial
  eXploration","The interpretability of deep neural networks is crucial for understanding
model decisions in various applications, including computer vision.
AttEXplore++, an advanced framework built upon AttEXplore, enhances attribution
by incorporating transferable adversarial attack methods such as MIG and GRA,
significantly improving the accuracy and robustness of model explanations. We
conduct extensive experiments on five models, including CNNs (Inception-v3,
ResNet-50, VGG16) and vision transformers (MaxViT-T, ViT-B/16), using the
ImageNet dataset. Our method achieves an average performance improvement of
7.57\% over AttEXplore and 32.62\% compared to other state-of-the-art
interpretability algorithms. Using insertion and deletion scores as evaluation
metrics, we show that adversarial transferability plays a vital role in
enhancing attribution results. Furthermore, we explore the impact of
randomness, perturbation rate, noise amplitude, and diversity probability on
attribution performance, demonstrating that AttEXplore++ provides more stable
and reliable explanations across various models. We release our code at:
https://anonymous.4open.science/r/ATTEXPLOREP-8435/",2024-12-27T08:27:53Z,http://arxiv.org/abs/2412.19523v1,"Zhiyu Zhu, Jiayu Zhang, Zhibo Jin, Huaming Chen, Jianlong Zhou, Fang Chen"
"Real-time classification of EEG signals using Machine Learning
  deployment","The prevailing educational methods predominantly rely on traditional
classroom instruction or online delivery, often limiting the teachers' ability
to engage effectively with all the students simultaneously. A more intrinsic
method of evaluating student attentiveness during lectures can enable the
educators to tailor the course materials and their teaching styles in order to
better meet the students' needs. The aim of this paper is to enhance teaching
quality in real time, thereby fostering a higher student engagement in the
classroom activities. By monitoring the students' electroencephalography (EEG)
signals and employing machine learning algorithms, this study proposes a
comprehensive solution for addressing this challenge. Machine learning has
emerged as a powerful tool for simplifying the analysis of complex variables,
enabling the effective assessment of the students' concentration levels based
on specific parameters. However, the real-time impact of machine learning
models necessitates a careful consideration as their deployment is concerned.
This study proposes a machine learning-based approach for predicting the level
of students' comprehension with regard to a certain topic. A browser interface
was introduced that accesses the values of the system's parameters to determine
a student's level of concentration on a chosen topic. The deployment of the
proposed system made it necessary to address the real-time challenges faced by
the students, consider the system's cost, and establish trust in its efficacy.
This paper presents the efforts made for approaching this pertinent issue
through the implementation of innovative technologies and provides a framework
for addressing key considerations for future research directions.",2024-12-27T08:14:28Z,http://arxiv.org/abs/2412.19515v1,"Swati Chowdhuri, Satadip Saha, Samadrita Karmakar, Ankur Chanda"
"Uncertainty quantification for improving radiomic-based models in
  radiation pneumonitis prediction","Background and Objective: Radiation pneumonitis (RP) is a side effect of
thoracic radiation therapy. Recently, Machine learning (ML) models enhanced
with radiomic and dosiomic features provide better predictions by incorporating
spatial information beyond DVHs. However, to improve the clinical decision
process, we propose to use uncertainty quantification (UQ) to improve the
confidence in model prediction. This study evaluates the impact of post hoc UQ
methods on the discriminative performance and calibration of ML models for RP
prediction. Methods: This study evaluated four ML models: logistic regression
(LR), support vector machines (SVM), extreme gradient boosting (XGB), and
random forest (RF), using radiomic, dosiomic, and dosimetric features to
predict RP. We applied UQ methods, including Patt scaling, isotonic regression,
Venn-ABERS predictor, and Conformal Prediction, to quantify uncertainty. Model
performance was assessed through Area Under the Receiver Operating
Characteristic curve (AUROC), Area Under the Precision-Recall Curve (AUPRC),
and Adaptive Calibration Error (ACE) using Leave-One-Out Cross-Validation
(LOO-CV). Results: UQ methods enhanced predictive performance, particularly for
high-certainty predictions, while also improving calibration. Radiomic and
dosiomic features increased model accuracy but introduced calibration
challenges, especially for non-linear models like XGB and RF. Performance gains
from UQ methods were most noticeable at higher certainty thresholds.
Conclusion: Integrating UQ into ML models with radiomic and dosiomic features
improves both predictive accuracy and calibration, supporting more reliable
clinical decision-making. The findings emphasize the value of UQ methods in
enhancing applicability of predictive models for RP in healthcare settings.",2024-12-27T08:01:42Z,http://arxiv.org/abs/2412.19511v1,"Chanon Puttanawarut, Romen Samuel Wabina, Nat Sirirutbunkajorn"
"Multi-label Classification using Deep Multi-order Context-aware Kernel
  Networks","Multi-label classification is a challenging task in pattern recognition. Many
deep learning methods have been proposed and largely enhanced classification
performance. However, most of the existing sophisticated methods ignore context
in the models' learning process. Since context may provide additional cues to
the learned models, it may significantly boost classification performances. In
this work, we make full use of context information (namely geometrical
structure of images) in order to learn better context-aware similarities
(a.k.a. kernels) between images. We reformulate context-aware kernel design as
a feed-forward network that outputs explicit kernel mapping features. Our
obtained context-aware kernel network further leverages multiple orders of
patch neighbors within different distances, resulting into a more
discriminating Deep Multi-order Context-aware Kernel Network (DMCKN) for
multi-label classification. We evaluate the proposed method on the challenging
Corel5K and NUS-WIDE benchmarks, and empirical results show that our method
obtains competitive performances against the related state-of-the-art, and both
quantitative and qualitative performances corroborate its effectiveness and
superiority for multi-label image classification.",2024-12-27T07:16:11Z,http://arxiv.org/abs/2412.19491v1,"Mingyuan Jiu, Hailong Zhu, Hichem Sahbi"
Movable Antenna-Aided Near-Field Integrated Sensing and Communication,"Integrated sensing and communication (ISAC) is emerging as a pivotal
technology for next-generation wireless networks. However, existing ISAC
systems are based on fixed-position antennas (FPAs), which inevitably incur a
loss in performance when balancing the trade-off between sensing and
communication. Movable antenna (MA) technology offers promising potential to
enhance ISAC performance by enabling flexible antenna movement. Nevertheless,
exploiting more spatial channel variations requires larger antenna moving
regions, which may invalidate the conventional far-field assumption for
channels between transceivers. Therefore, this paper utilizes the MA to enhance
sensing and communication capabilities in near-field ISAC systems, where a
full-duplex base station (BS) is equipped with multiple transmit and receive
MAs movable in large-size regions to simultaneously sense multiple targets and
serve multiple uplink (UL) and downlink (DL) users for communication. We aim to
maximize the weighted sum of sensing and communication rates (WSR) by jointly
designing the transmit beamformers, sensing signal covariance matrices, receive
beamformers, and MA positions at the BS, as well as the UL power allocation.
The resulting optimization problem is challenging to solve, while we propose an
efficient two-layer random position (RP) algorithm to tackle it. In addition,
to reduce movement delay and cost, we design an antenna position matching (APM)
algorithm based on the greedy strategy to minimize the total MA movement
distance. Extensive simulation results demonstrate the substantial performance
improvement achieved by deploying MAs in near-field ISAC systems. Moreover, the
results show the effectiveness of the proposed APM algorithm in reducing the
antenna movement distance, which is helpful for energy saving and time overhead
reduction for MA-aided near-field ISAC systems with large moving regions.",2024-12-27T05:45:35Z,http://arxiv.org/abs/2412.19470v1,"Jingze Ding, Zijian Zhou, Xiaodan Shao, Bingli Jiao, Rui Zhang"
Focusing Image Generation to Mitigate Spurious Correlations,"Instance features in images exhibit spurious correlations with background
features, affecting the training process of deep neural classifiers. This leads
to insufficient attention to instance features by the classifier, resulting in
erroneous classification outcomes. In this paper, we propose a data
augmentation method called Spurious Correlations Guided Synthesis (SCGS) that
mitigates spurious correlations through image generation model. This approach
does not require expensive spurious attribute (group) labels for the training
data and can be widely applied to other debiasing methods. Specifically, SCGS
first identifies the incorrect attention regions of a pre-trained classifier on
the training images, and then uses an image generation model to generate new
training data based on these incorrect attended regions. SCGS increases the
diversity and scale of the dataset to reduce the impact of spurious
correlations on classifiers. Changes in the classifier's attention regions and
experimental results on three different domain datasets demonstrate that this
method is effective in reducing the classifier's reliance on spurious
correlations.",2024-12-27T04:48:56Z,http://arxiv.org/abs/2412.19457v1,"Xuewei Li, Zhenzhen Nie, Mei Yu, Zijian Zhang, Jie Gao, Tianyi Xu, Zhiqiang Liu"
"Exponentially accurate open quantum simulation via randomized
  dissipation with minimal ancilla","Simulating open quantum systems is an essential technique for understanding
complex physical phenomena and advancing quantum technologies. Some quantum
algorithms for simulating Lindblad dynamics achieve logarithmically short
circuit depth in terms of accuracy $\varepsilon$ by coherently encoding all
possible jump processes with a large ancilla consumption. Minimizing the space
complexity while achieving such a logarithmic depth remains an important
challenge. In this work, we present a quantum algorithm for simulating general
Lindblad dynamics with multiple jump operators aimed at an observable
estimation, that achieves both a logarithmically short circuit depth and a
minimum ancilla size. Toward simulating an exponentially accurate Taylor
expansion of the Lindblad propagator to ensure the circuit depth of
$\mathcal{O} (\log(1/\varepsilon))$, we develop a novel random circuit
compilation method that leverages dissipative processes with only a single jump
operator; importantly, the proposed method requires the minimal-size, $4 +
\lceil \log M \rceil$, ancilla qubits where each single jump operator has at
most $M$ Pauli strings. This work represents a significant step towards making
open quantum system simulations more feasible on early fault-tolerant quantum
computing devices.",2024-12-27T04:43:19Z,http://arxiv.org/abs/2412.19453v1,"Jumpei Kato, Kaito Wada, Kosuke Ito, Naoki Yamamoto"
"Comparative Performance Analysis of Quantum Machine Learning
  Architectures for Credit Card Fraud Detection","As financial fraud becomes increasingly complex, effective detection methods
are essential. Quantum Machine Learning (QML) introduces certain capabilities
that may enhance both accuracy and efficiency in this area. This study examines
how different quantum feature map and ansatz configurations affect the
performance of three QML-based classifiers-the Variational Quantum Classifier
(VQC), the Sampler Quantum Neural Network (SQNN), and the Estimator Quantum
Neural Network (EQNN)-when applied to two non-standardized financial fraud
datasets. Different quantum feature map and ansatz configurations are
evaluated, revealing distinct performance patterns. The VQC consistently
demonstrates strong classification results, achieving an F1 score of 0.88,
while the SQNN also delivers promising outcomes. In contrast, the EQNN
struggles to produce robust results, emphasizing the challenges presented by
non-standardized data. These findings highlight the importance of careful model
configuration in QML-based financial fraud detection. By showing how specific
feature maps and ansatz choices influence predictive success, this work guides
researchers and practitioners in refining QML approaches for complex financial
applications.",2024-12-27T04:17:34Z,http://arxiv.org/abs/2412.19441v1,"Mansour El Alami, Nouhaila Innan, Muhammad Shafique, Mohamed Bennai"
"Residual Feature-Reutilization Inception Network for Image
  Classification","Capturing feature information effectively is of great importance in the field
of computer vision. With the development of convolutional neural networks
(CNNs), concepts like residual connection and multiple scales promote continual
performance gains in diverse deep learning vision tasks. In this paper, we
propose a novel CNN architecture that it consists of residual
feature-reutilization inceptions (ResFRI) or split-residual
feature-reutilization inceptions (Split-ResFRI). And it is composed of four
convolutional combinations of different structures connected by specially
designed information interaction passages, which are utilized to extract
multi-scale feature information and effectively increase the receptive field of
the model. Moreover, according to the network structure designed above,
Split-ResFRI can adjust the segmentation ratio of the input information,
thereby reducing the number of parameters and guaranteeing the model
performance. Specifically, in experiments based on popular vision datasets,
such as CIFAR10 ($97.94$\%), CIFAR100 ($85.91$\%) and Tiny Imagenet
($70.54$\%), we obtain state-of-the-art results compared with other modern
models under the premise that the model size is approximate and no additional
data is used.",2024-12-27T03:55:25Z,http://arxiv.org/abs/2412.19433v1,"Yuanpeng He, Wenjie Song, Lijian Li, Tianxiang Zhan, Wenpin Jiao"
Revisiting PCA for time series reduction in temporal dimension,"Revisiting PCA for Time Series Reduction in Temporal Dimension; Jiaxin Gao,
Wenbo Hu, Yuntian Chen; Deep learning has significantly advanced time series
analysis (TSA), enabling the extraction of complex patterns for tasks like
classification, forecasting, and regression. Although dimensionality reduction
has traditionally focused on the variable space-achieving notable success in
minimizing data redundancy and computational complexity-less attention has been
paid to reducing the temporal dimension. In this study, we revisit Principal
Component Analysis (PCA), a classical dimensionality reduction technique, to
explore its utility in temporal dimension reduction for time series data. It is
generally thought that applying PCA to the temporal dimension would disrupt
temporal dependencies, leading to limited exploration in this area. However,
our theoretical analysis and extensive experiments demonstrate that applying
PCA to sliding series windows not only maintains model performance, but also
enhances computational efficiency. In auto-regressive forecasting, the temporal
structure is partially preserved through windowing, and PCA is applied within
these windows to denoise the time series while retaining their statistical
information. By preprocessing time-series data with PCA, we reduce the temporal
dimensionality before feeding it into TSA models such as Linear, Transformer,
CNN, and RNN architectures. This approach accelerates training and inference
and reduces resource consumption. Notably, PCA improves Informer training and
inference speed by up to 40% and decreases GPU memory usage of TimesNet by 30%,
without sacrificing model accuracy. Comparative analysis against other
reduction methods further highlights the effectiveness of PCA in improving the
efficiency of TSA models.",2024-12-27T03:17:26Z,http://arxiv.org/abs/2412.19423v1,"Jiaxin Gao, Wenbo Hu, Yuntian Chen"
"Generalized Uncertainty-Based Evidential Fusion with Hybrid Multi-Head
  Attention for Weak-Supervised Temporal Action Localization","Weakly supervised temporal action localization (WS-TAL) is a task of
targeting at localizing complete action instances and categorizing them with
video-level labels. Action-background ambiguity, primarily caused by background
noise resulting from aggregation and intra-action variation, is a significant
challenge for existing WS-TAL methods. In this paper, we introduce a hybrid
multi-head attention (HMHA) module and generalized uncertainty-based evidential
fusion (GUEF) module to address the problem. The proposed HMHA effectively
enhances RGB and optical flow features by filtering redundant information and
adjusting their feature distribution to better align with the WS-TAL task.
Additionally, the proposed GUEF adaptively eliminates the interference of
background noise by fusing snippet-level evidences to refine uncertainty
measurement and select superior foreground feature information, which enables
the model to concentrate on integral action instances to achieve better action
localization and classification performance. Experimental results conducted on
the THUMOS14 dataset demonstrate that our method outperforms state-of-the-art
methods. Our code is available in
\url{https://github.com/heyuanpengpku/GUEF/tree/main}.",2024-12-27T03:04:57Z,http://arxiv.org/abs/2412.19418v1,"Yuanpeng He, Lijian Li, Tianxiang Zhan, Wenpin Jiao, Chi-Man Pun"
DIPS: Optimal Dynamic Index for Poisson $\boldsymbolπ$ps Sampling,"This paper addresses the Poisson $\pi$ps sampling problem, a topic of
significant academic interest in various domains and with practical data mining
applications, such as influence maximization. The problem includes a set
$\mathcal{S}$ of $n$ elements, where each element $v$ is assigned a weight
$w(v)$ reflecting its importance. The goal is to generate a random subset $X$
of $\mathcal{S}$, where each element $v \in \mathcal{S}$ is included in $X$
independently with probability $\frac{c\cdot w(v)}{\sum_{v \in \mathcal{S}}
w(v)}$, where $0&lt;c\leq 1$ is a constant. The subsets must be independent across
different queries. While the Poisson $\pi$ps sampling problem can be reduced to
the well-studied subset sampling problem, updates in Poisson $\pi$ps sampling,
such as adding a new element or removing an element, would cause the
probabilities of all $n$ elements to change in the corresponding subset
sampling problem, making this approach impractical for dynamic scenarios. To
address this, we propose a dynamic index specifically tailored for the Poisson
$\pi$ps sampling problem, supporting optimal expected $\mathcal{O}(1)$ query
time and $\mathcal{O}(1)$ index update time, with an optimal $\mathcal{O}(n)$
space cost. Our solution involves recursively partitioning the set by weights
and ultimately using table lookup. The core of our solution lies in addressing
the challenges posed by weight explosion and correlations between elements.
Empirical evaluations demonstrate that our approach achieves significant
speedups in update time while maintaining consistently competitive query time
compared to the subset-sampling-based methods.",2024-12-27T02:47:44Z,http://arxiv.org/abs/2412.19415v1,"Jinchao Huang, Sibo Wang"
"MLLM-SUL: Multimodal Large Language Model for Semantic Scene
  Understanding and Localization in Traffic Scenarios","Multimodal large language models (MLLMs) have shown satisfactory effects in
many autonomous driving tasks. In this paper, MLLMs are utilized to solve joint
semantic scene understanding and risk localization tasks, while only relying on
front-view images. In the proposed MLLM-SUL framework, a dual-branch visual
encoder is first designed to extract features from two resolutions, and rich
visual information is conducive to the language model describing risk objects
of different sizes accurately. Then for the language generation, LLaMA model is
fine-tuned to predict scene descriptions, containing the type of driving
scenario, actions of risk objects, and driving intentions and suggestions of
ego-vehicle. Ultimately, a transformer-based network incorporating a regression
token is trained to locate the risk objects. Extensive experiments on the
existing DRAMA-ROLISP dataset and the extended DRAMA-SRIS dataset demonstrate
that our method is efficient, surpassing many state-of-the-art image-based and
video-based methods. Specifically, our method achieves 80.1% BLEU-1 score and
298.5% CIDEr score in the scene understanding task, and 59.6% accuracy in the
localization task. Codes and datasets are available at
https://github.com/fjq-tongji/MLLM-SUL.",2024-12-27T02:05:38Z,http://arxiv.org/abs/2412.19406v1,"Jiaqi Fan, Jianhua Wu, Jincheng Gao, Jianhao Yu, Yafei Wang, Hongqing Chu, Bingzhao Gao"
"Comparing Few to Rank Many: Active Human Preference Learning using
  Randomized Frank-Wolfe","We study learning of human preferences from a limited comparison feedback.
This task is ubiquitous in machine learning. Its applications such as
reinforcement learning from human feedback, have been transformational. We
formulate this problem as learning a Plackett-Luce model over a universe of $N$
choices from $K$-way comparison feedback, where typically $K \ll N$. Our
solution is the D-optimal design for the Plackett-Luce objective. The design
defines a data logging policy that elicits comparison feedback for a small
collection of optimally chosen points from all ${N \choose K}$ feasible
subsets. The main algorithmic challenge in this work is that even fast methods
for solving D-optimal designs would have $O({N \choose K})$ time complexity. To
address this issue, we propose a randomized Frank-Wolfe (FW) algorithm that
solves the linear maximization sub-problems in the FW method on randomly chosen
variables. We analyze the algorithm, and evaluate it empirically on synthetic
and open-source NLP datasets.",2024-12-27T01:10:17Z,http://arxiv.org/abs/2412.19396v1,"Kiran Koshy Thekumparampil, Gaurush Hiranandani, Kousha Kalantari, Shoham Sabach, Branislav Kveton"
"Two-echelon Electric Vehicle Routing Problem in Parcel Delivery: A
  Literature Review","Multi-echelon parcel delivery systems using electric vehicles (EVs) are
crucial for managing urban logistics complexity and promoting sustainability.
In multi-echelon systems, particularly within two-stage systems, larger
vehicles transport parcels from a central depot to satellite hubs, where
smaller EVs pick up the parcels and carry out last-mile deliveries. This system
could increase efficiency, reduce emissions, and improve service reliability.
The two-echelon electric vehicle routing problem (2E-EVRP), an extension of the
traditional two-echelon vehicle routing problem (2E-VRP), addresses EV-specific
challenges such as battery constraints and recharging stations to tackle
environmental impacts, urban congestion, and e-commerce demands. While
effectively reducing costs, energy use, and emissions, the 2E-EVRP faces
modeling challenges due to multi-echelon structures, EV limitations, and
recharging station selection. This paper systematically reviews 2E-EVRP
literature, analyzing key studies. It proposes a classification scheme to
categorize the papers based on the problem variants, objectives, constraints,
and solution methods. It identifies gaps such as delivery tardiness,
environmental trade-offs, multi-objective optimization, multiple depots, split
deliveries, and time-dependent travel conditions. Future research directions
include aligning models with urban policies, integrating parcel lockers,
enabling same-day delivery, and incorporating advanced technologies like
autonomous vehicles. Methodological advancements suggest using machine
learning, reinforcement learning, and simulation-based approaches to enhance
dynamic routing and real-time decision-making. These directions aim to expand
the 2E-EVRP applicability, addressing theoretical and practical challenges in
sustainable urban logistics for future works.",2024-12-27T01:05:59Z,http://arxiv.org/abs/2412.19395v1,"Nima Moradi, Niloufar Mirzavand Boroujeni, Navid Aftabi, Amin Aslani"
An Engorgio Prompt Makes Large Language Model Babble on,"Auto-regressive large language models (LLMs) have yielded impressive
performance in many real-world tasks. However, the new paradigm of these LLMs
also exposes novel threats. In this paper, we explore their vulnerability to
inference cost attacks, where a malicious user crafts Engorgio prompts to
intentionally increase the computation cost and latency of the inference
process. We design Engorgio, a novel methodology, to efficiently generate
adversarial Engorgio prompts to affect the target LLM's service availability.
Engorgio has the following two technical contributions. (1) We employ a
parameterized distribution to track LLMs' prediction trajectory. (2) Targeting
the auto-regressive nature of LLMs' inference process, we propose novel loss
functions to stably suppress the appearance of the &lt;EOS&gt; token, whose
occurrence will interrupt the LLM's generation process. We conduct extensive
experiments on 13 open-sourced LLMs with parameters ranging from 125M to 30B.
The results show that Engorgio prompts can successfully induce LLMs to generate
abnormally long outputs (i.e., roughly 2-13$\times$ longer to reach 90%+ of the
output length limit) in a white-box scenario and our real-world experiment
demonstrates Engergio's threat to LLM service with limited computing resources.
The code is accessible at https://github.com/jianshuod/Engorgio-prompt.",2024-12-27T01:00:23Z,http://arxiv.org/abs/2412.19394v1,"Jianshuo Dong, Ziyuan Zhang, Qingjie Zhang, Han Qiu, Tianwei Zhang, Hao Wang, Hewu Li, Qi Li, Chao Zhang, Ke Xu"
"An In-Depth Analysis of Adversarial Discriminative Domain Adaptation for
  Digit Classification","Domain adaptation is an active area of research driven by the growing demand
for robust machine learning models that perform well on real-world data.
Adversarial learning for deep neural networks (DNNs) has emerged as a promising
approach to improving generalization ability, particularly for image
classification. In this paper, we implement a specific adversarial learning
technique known as Adversarial Discriminative Domain Adaptation (ADDA) and
replicate digit classification experiments from the original ADDA paper. We
extend their findings by examining a broader range of domain shifts and provide
a detailed analysis of in-domain classification accuracy post-ADDA. Our results
demonstrate that ADDA significantly improves accuracy across certain domain
shifts with minimal impact on in-domain performance. Furthermore, we provide
qualitative analysis and propose potential explanations for ADDA's limitations
in less successful domain shifts. Code is at
https://github.com/eugenechoi2004/COS429_FINAL .",2024-12-27T00:36:40Z,http://arxiv.org/abs/2412.19391v1,"Eugene Choi, Julian Rodriguez, Edmund Young"
Truncated multirange percolation of words on the square lattice,"We study mixed long-range percolation on the square lattice. Each vertical
edge of unit length is independently open with probability $\varepsilon$, and
each horizontal edge of length $i$ is independently open with probability
$p_i$. Also, each vertex is assigned independently a random variable taking
values 1 and 0 with probability $p$ and $1-p$, respectively. We prove that for
a broad class of anisotropic long-range percolation models for which connection
probabilities $p_i$ satisfy some regularity conditions, all words
(semi-infinite binary sequences) are seen simultaneously from the origin with
positive probability, even if all edges with length larger than some constant
(depending on $\varepsilon$, $p$, and on the sequence $(p_i)$) are suppressed.",2024-12-26T23:27:13Z,http://arxiv.org/abs/2412.19379v1,"Pablo A. Gomes, Otávio Lima, Roger W. C. Silva"
"Minimal Batch Adaptive Learning Policy Engine for Real-Time Mid-Price
  Forecasting in High-Frequency Trading","High-frequency trading (HFT) has transformed modern financial markets, making
reliable short-term price forecasting models essential. In this study, we
present a novel approach to mid-price forecasting using Level 1 limit order
book (LOB) data from NASDAQ, focusing on 100 U.S. stocks from the S&amp;P 500 index
during the period from September to November 2022. Expanding on our previous
work with Radial Basis Function Neural Networks (RBFNN), which leveraged
automated feature importance techniques based on mean decrease impurity (MDI)
and gradient descent (GD), we introduce the Adaptive Learning Policy Engine
(ALPE) - a reinforcement learning (RL)-based agent designed for batch-free,
immediate mid-price forecasting. ALPE incorporates adaptive epsilon decay to
dynamically balance exploration and exploitation, outperforming a diverse range
of highly effective machine learning (ML) and deep learning (DL) models in
forecasting performance.",2024-12-26T22:49:53Z,http://arxiv.org/abs/2412.19372v1,"Adamantios Ntakaris, Gbenga Ibikunle"
"Evaluating Convolutional Neural Networks for COVID-19 classification in
  chest X-ray images","Coronavirus Disease 2019 (COVID-19) pandemic rapidly spread globally,
impacting the lives of billions of people. The effective screening of infected
patients is a critical step to struggle with COVID-19, and treating the
patients avoiding this quickly disease spread. The need for automated and
scalable methods has increased due to the unavailability of accurate automated
toolkits. Recent researches using chest X-ray images suggest they include
relevant information about the COVID-19 virus. Hence, applying machine learning
techniques combined with radiological imaging promises to identify this disease
accurately. It is straightforward to collect these images once it is spreadly
shared and analyzed in the world. This paper presents a method for automatic
COVID-19 detection using chest Xray images through four convolutional neural
networks, namely: AlexNet, VGG-11, SqueezeNet, and DenseNet-121. This method
had been providing accurate diagnostics for positive or negative COVID-19
classification. We validate our experiments using a ten-fold cross-validation
procedure over the training and test sets. Our findings include the shallow
fine-tuning and data augmentation strategies that can assist in dealing with
the low number of positive COVID-19 images publicly available. The accuracy for
all CNNs is higher than 97.00%, and the SqueezeNet model achieved the best
result with 99.20%.",2024-12-26T22:05:30Z,http://arxiv.org/abs/2412.19362v1,"Leonardo Gabriel Ferreira Rodrigues, Danilo Ferreira da Silva, Larissa Ferreira Rodrigues, João Fernando Mari"
Dynamic Skill Adaptation for Large Language Models,"We present Dynamic Skill Adaptation (DSA), an adaptive and dynamic framework
to adapt novel and complex skills to Large Language Models (LLMs). Compared
with previous work which learns from human-curated and static data in random
orders, we propose to first automatically generate and organize the training
data by mimicking the learning pathways of human and then dynamically tailor
the training data based on the training dynamics. Specifically, inspired by the
learning structures and teaching strategies in the human education system, we
first construct a skill graph by decomposing complex skills into sub-skills and
arranging them based on their dependencies in human syllables. For every skill,
we utilize LLMs to generate both textbook-like data which contains detailed
descriptions of skills for pre-training and exercise-like data which targets at
explicitly utilizing the skills to solve problems for instruction-tuning.
Furthermore, during the instruction-tuning, we dynamically update the training
data which down-weight easy-to-learn examples, generate more complex examples,
and filter out data with errors. Experiments on large language models such as
LLAMA and Mistral demonstrate the effectiveness of our proposed methods in
adapting math reasoning skills and social study skills.",2024-12-26T22:04:23Z,http://arxiv.org/abs/2412.19361v1,"Jiaao Chen, Diyi Yang"
"Improving the network traffic classification using the Packet Vision
  approach","The network traffic classification allows improving the management, and the
network services offer taking into account the kind of application. The future
network architectures, mainly mobile networks, foresee intelligent mechanisms
in their architectural frameworks to deliver application-aware network
requirements. The potential of convolutional neural networks capabilities,
widely exploited in several contexts, can be used in network traffic
classification. Thus, it is necessary to develop methods based on the content
of packets transforming it into a suitable input for CNN technologies. Hence,
we implemented and evaluated the Packet Vision, a method capable of building
images from packets raw-data, considering both header and payload. Our approach
excels those found in state-of-the-art by delivering security and privacy by
transforming the raw-data packet into images. Therefore, we built a dataset
with four traffic classes evaluating the performance of three CNNs
architectures: AlexNet, ResNet-18, and SqueezeNet. Experiments showcase the
Packet Vision combined with CNNs applicability and suitability as a promising
approach to deliver outstanding performance in classifying network traffic.",2024-12-26T21:56:03Z,http://arxiv.org/abs/2412.19360v1,"Rodrigo Moreira, Larissa Ferreira Rodrigues, Pedro Frosi Rosa, Flávio de Oliveira Silva"
"Transit-Length Distribution for Particle Transport in Binary Markovian
  Mixed Media","The correspondence between the telegraph random process and transport within
a binary stochastic Markovian mixture is established. This equivalence is used
to derive the distribution function for the transit length, defined as the
distance a particle moving along a straight-line trajectory travels through a
specific material zone within the random mixture. A numerically robust
asymptotic form of this distribution is obtained for highly mixed materials and
the convergence to the atomic-mix limit is shown. The validity of the
distribution is verified using a Monte Carlo simulation of the transport
process. The distribution is applied to particle transport in slab geometry
containing porous media for two cases: the transmission of light and the
stopping of charged particles. For both of these applications, analytical forms
using the approximate asymptotic model for the transmission probability of beam
sources are obtained and illustrative numerical results are provided. These
results show that in cases of highly mixed materials, the asymptotic forms are
more accurate than the atomic-mix limit.",2024-12-26T21:46:14Z,http://arxiv.org/abs/2412.19359v1,"Brian C. Kiedrowski, Emily H. Vu"
"Central limit theorems for linear spectral statistics of inhomogeneous
  random graphs with graphon limits","We establish central limit theorems (CLTs) for the linear spectral statistics
of the adjacency matrix of inhomogeneous random graphs across all sparsity
regimes, providing explicit covariance formulas under the assumption that the
variance profile of the random graphs converges to a graphon limit. Two types
of CLTs are derived for the (non-centered) adjacency matrix and the centered
adjacency matrix, with different scaling factors when the sparsity parameter
$p$ satisfies $np = n^{\Omega(1)}$, and with the same scaling factor when $np =
n^{o(1)}$. In both cases, the limiting covariance is expressed in terms of
homomorphism densities from certain types of finite graphs to a graphon. These
results highlight a phase transition in the centering effect for global
eigenvalue fluctuations. For the non-centered adjacency matrix, we also
identify new phase transitions for the CLTs in the sparse regime when $n^{1/m}
\ll np \ll n^{1/(m-1)}$ for $m \geq 2$. Furthermore, weaker conditions for the
graphon convergence of the variance profile are sufficient as $p$ decreases
from being constant to $np \to c\in (0,\infty)$. These findings reveal a novel
connection between graphon limits and linear spectral statistics in random
matrix theory.",2024-12-26T21:15:49Z,http://arxiv.org/abs/2412.19352v1,"Xiangyi Zhu, Yizhe Zhu"
"Sparse recovery from quadratic equations, part II: hardness and
  incoherence","We study the square root bottleneck in the recovery of sparse vectors from
quadratic equations. It is acknowledged that a sparse vector $ \mathbf x_0\in
\mathbb{R}^n$, $\| \mathbf x_0\|_0 = k$ can in theory be recovered from as few
as $O(k)$ generic quadratic equations but no polynomial time algorithm is known
for this task unless $m = \Omega(k^2)$. This bottleneck was in fact shown in
previous work to be essentially related to the initialization of descent
algorithms. Starting such algorithms sufficiently close to the planted signal
is known to imply convergence to this signal. In this paper, we show that as
soon as $m\gtrsim \mu_0^{-2}k \vee \mu_0^{-4}$ (up to log factors) where $\mu_0
= \| \mathbf x_0\|_\infty/\| \mathbf x_0\|_2$, it is possible to recover a
$k$-sparse vector $ \mathbf x_0\in \mathbb{R}^n$ from $m$ quadratic equations
of the form $\langle \mathbf A_i, \mathbf x \mathbf x^\intercal\rangle =
\langle \mathbf A_i, \mathbf x_0 \mathbf x_0^\intercal\rangle + \varepsilon_i $
by minimizing the classical empirical loss. The proof idea carries over to the
phase retrieval setting for which it provides an original initialization that
matches the current optimal sample complexity (see e.g. [Cai 2023]). In the
maximally incoherent regime $\mu_0^{-2}=k$, and for $m=o(k^2)$ we provide
evidence for topological hardness by showing that a property known as the
Overlap Gap Property (OGP), which originated in spin glass theory and is
conjectured to be indicative of algorithmic intractability when optimizing over
random structures, holds for a particular level of overparametrization. The key
ingredient of the proof is a lower bound on the tail of chi-squared random
variables which follows from the theory of moderate deviations.",2024-12-26T20:11:44Z,http://arxiv.org/abs/2412.19341v1,Augustin Cosse
"Random matrix statistics and zeroes of $L$-functions via probability in
  $λ$-rings","We introduce a theory of probability in $\lambda$-rings designed to
efficiently describe random variables valued in multisets of complex numbers,
varieties over a field, or other similar enriched settings. A key role is
played by the $\sigma$-moment generating function based on the plethystic
exponential, which allows us to describe distributions and argue with
independence in a way that is as simple as classical probability theory. As a
first application, we use this theory to obtain a concise description of the
asymptotic $\sigma$-moment generating functions describing distributions of
eigenvalues of Haar random matrices in compact classical groups. Beyond the
theory of probability in $\lambda$-rings, the proof uses only classical
invariant theory. Using our description we reprove the results of Diaconis and
Shahshahani on the joint distributions of traces of powers of matrices, and we
also treat symmetric groups. Next, we use Poonen's sieve to establish
equidistribution results for the zeroes of $L$-functions in some natural
families: simple Dirichlet characters for $\mathbb{F}_q(x)$ and the vanishing
cohomology of smooth hypersurface sections. We give concise descriptions of the
asymptotic $\sigma$-moment generating functions in these families, then compare
them to the associated random matrix distributions. These equidistribution
results are sideways in that we fix $q$ and take the degree $d$ to infinity, as
opposed to Deligne equidistribution for fixed $d$ as $q \rightarrow \infty$,
and the large $d$-limits are related to explicit descriptions of stable
homology with twisted coefficients.",2024-12-26T17:36:12Z,http://arxiv.org/abs/2412.19295v1,Sean Howe
"ViPCap: Retrieval Text-Based Visual Prompts for Lightweight Image
  Captioning","Recent lightweight image captioning models using retrieved data mainly focus
on text prompts. However, previous works only utilize the retrieved text as
text prompts, and the visual information relies only on the CLIP visual
embedding. Because of this issue, there is a limitation that the image
descriptions inherent in the prompt are not sufficiently reflected in the
visual embedding space. To tackle this issue, we propose ViPCap, a novel
retrieval text-based visual prompt for lightweight image captioning. ViPCap
leverages the retrieved text with image information as visual prompts to
enhance the ability of the model to capture relevant visual information. By
mapping text prompts into the CLIP space and generating multiple randomized
Gaussian distributions, our method leverages sampling to explore randomly
augmented distributions and effectively retrieves the semantic features that
contain image information. These retrieved features are integrated into the
image and designated as the visual prompt, leading to performance improvements
on the datasets such as COCO, Flickr30k, and NoCaps. Experimental results
demonstrate that ViPCap significantly outperforms prior lightweight captioning
models in efficiency and effectiveness, demonstrating the potential for a
plug-and-play solution.",2024-12-26T17:29:38Z,http://arxiv.org/abs/2412.19289v1,"Taewhan Kim, Soeun Lee, Si-Woo Kim, Dong-Jin Kim"
"Time Series Foundational Models: Their Role in Anomaly Detection and
  Prediction","Time series foundational models (TSFM) have gained prominence in time series
forecasting, promising state-of-the-art performance across various
applications. However, their application in anomaly detection and prediction
remains underexplored, with growing concerns regarding their black-box nature,
lack of interpretability and applicability. This paper critically evaluates the
efficacy of TSFM in anomaly detection and prediction tasks. We systematically
analyze TSFM across multiple datasets, including those characterized by the
absence of discernible patterns, trends and seasonality. Our analysis shows
that while TSFMs can be extended for anomaly detection and prediction,
traditional statistical and deep learning models often match or outperform TSFM
in these tasks. Additionally, TSFMs require high computational resources but
fail to capture sequential dependencies effectively or improve performance in
few-shot or zero-shot scenarios. \noindent The preprocessed datasets, codes to
reproduce the results and supplementary materials are available at
https://github.com/smtmnfg/TSFM.",2024-12-26T17:15:30Z,http://arxiv.org/abs/2412.19286v1,"Chathurangi Shyalika, Harleen Kaur Bagga, Ahan Bhatt, Renjith Prasad, Alaa Al Ghazo, Amit Sheth"
"Phase transitions in low-dimensional long-range random field Ising
  models","We consider the long-range random field Ising model in dimension $d = 1, 2$,
whereas the long-range interaction is of the form $J_{xy} = |x-y|^{-\alpha}$
with $1&lt; \alpha &lt; 3/2$ for $d=1$ and with $2 &lt; \alpha \leq 3$ for $d = 2$. Our
main results establish phase transitions in these regimes. In one dimension, we
employ a Peierls argument with some novel modification, suitable for dealing
with the randomness coming from the external field; in two dimensions, our
proof follows that of Affonso, Bissacot, and Maia (2023) with some adaptations,
but new ideas are required in the critical case of $\alpha=3$.",2024-12-26T16:56:04Z,http://arxiv.org/abs/2412.19281v1,"Jian Ding, Fenglin Huang, João Maia"
"Leveraging Self-Training and Variational Autoencoder for Agitation
  Detection in People with Dementia Using Wearable Sensors","Dementia is a neurodegenerative disorder that has been growing among elder
people over the past decades. This growth profoundly impacts the quality of
life for patients and caregivers due to the symptoms arising from it. Agitation
and aggression (AA) are some of the symptoms of people with severe dementia
(PwD) in long-term care or hospitals. AA not only causes discomfort but also
puts the patients or others at potential risk. Existing monitoring solutions
utilizing different wearable sensors integrated with Artificial Intelligence
(AI) offer a way to detect AA early enough for timely and adequate medical
intervention. However, most studies are limited by the availability of
accurately labeled datasets, which significantly affects the efficacy of such
solutions in real-world scenarios. This study presents a novel comprehensive
approach to detect AA in PwD using physiological data from the Empatica E4
wristbands. The research creates a diverse dataset, consisting of three
distinct datasets gathered from 14 participants across multiple hospitals in
Canada. These datasets have not been extensively explored due to their limited
labeling. We propose a novel approach employing self-training and a variational
autoencoder (VAE) to detect AA in PwD effectively. The proposed approach aims
to learn the representation of the features extracted using the VAE and then
uses a semi-supervised block to generate labels, classify events, and detect
AA. We demonstrate that combining Self-Training and Variational Autoencoder
mechanism significantly improves model performance in classifying AA in PwD.
Among the tested techniques, the XGBoost classifier achieved the highest
accuracy of 90.16\%. By effectively addressing the challenge of limited labeled
data, the proposed system not only learns new labels but also proves its
superiority in detecting AA.",2024-12-26T15:34:25Z,http://arxiv.org/abs/2412.19254v1,"Abeer Badawi, Somayya Elmoghazy, Samira Choudhury, Khalid Elgazzar, Amer Burhan"
"Localized exploration in contextual dynamic pricing achieves
  dimension-free regret","We study the problem of contextual dynamic pricing with a linear demand
model. We propose a novel localized exploration-then-commit (LetC) algorithm
which starts with a pure exploration stage, followed by a refinement stage that
explores near the learned optimal pricing policy, and finally enters a pure
exploitation stage. The algorithm is shown to achieve a minimax optimal,
dimension-free regret bound when the time horizon exceeds a polynomial of the
covariate dimension. Furthermore, we provide a general theoretical framework
that encompasses the entire time spectrum, demonstrating how to balance
exploration and exploitation when the horizon is limited. The analysis is
powered by a novel critical inequality that depicts the
exploration-exploitation trade-off in dynamic pricing, mirroring its existing
counterpart for the bias-variance trade-off in regularized regression. Our
theoretical results are validated by extensive experiments on synthetic and
real-world data.",2024-12-26T15:29:58Z,http://arxiv.org/abs/2412.19252v1,"Jinhang Chai, Yaqi Duan, Jianqing Fan, Kaizheng Wang"
Sentiment trading with large language models,"We investigate the efficacy of large language models (LLMs) in sentiment
analysis of U.S. financial news and their potential in predicting stock market
returns. We analyze a dataset comprising 965,375 news articles that span from
January 1, 2010, to June 30, 2023; we focus on the performance of various LLMs,
including BERT, OPT, FINBERT, and the traditional Loughran-McDonald dictionary
model, which has been a dominant methodology in the finance literature. The
study documents a significant association between LLM scores and subsequent
daily stock returns. Specifically, OPT, which is a GPT-3 based LLM, shows the
highest accuracy in sentiment prediction with an accuracy of 74.4%, slightly
ahead of BERT (72.5%) and FINBERT (72.2%). In contrast, the Loughran-McDonald
dictionary model demonstrates considerably lower effectiveness with only 50.1%
accuracy. Regression analyses highlight a robust positive impact of OPT model
scores on next-day stock returns, with coefficients of 0.274 and 0.254 in
different model specifications. BERT and FINBERT also exhibit predictive
relevance, though to a lesser extent. Notably, we do not observe a significant
relationship between the Loughran-McDonald dictionary model scores and stock
returns, challenging the efficacy of this traditional method in the current
financial context. In portfolio performance, the long-short OPT strategy excels
with a Sharpe ratio of 3.05, compared to 2.11 for BERT and 2.07 for FINBERT
long-short strategies. Strategies based on the Loughran-McDonald dictionary
yield the lowest Sharpe ratio of 1.23. Our findings emphasize the superior
performance of advanced LLMs, especially OPT, in financial market prediction
and portfolio management, marking a significant shift in the landscape of
financial analysis tools with implications to financial regulation and policy
analysis.",2024-12-26T15:01:24Z,http://arxiv.org/abs/2412.19245v1,"Kemal Kirtac, Guido Germano"
Functional structural equation modeling with latent variables,"Handling latent variables in Structural Equation Models (SEMs) in a case
where both the latent variables and their corresponding indicators in the
measurement error part of the model are random curves presents significant
challenges, especially with sparse data. In this paper, we develop a novel
family of Functional Structural Equation Models (FSEMs) that incorporate latent
variables modeled as Gaussian Processes (GPs). The introduced FSEMs are built
upon functional regression models having response variables modeled as
underlying GPs. The model flexibly adapts to cases when the random curves'
realizations are observed only over a sparse subset of the domain, and the
inferential framework is based on a restricted maximum likelihood approach. The
advantage of this framework lies in its ability and flexibility in handling
various data scenarios, including regularly and irregularly spaced points and
thus missing data. To extract smooth estimates for the functional parameters,
we employ a penalized likelihood approach that selects the smoothing parameters
using a cross-validation method. We evaluate the performance of the proposed
model using simulation studies and a real data example, which suggests that our
model performs well in practice. The uncertainty associated with the estimates
of the functional coefficients is also assessed by constructing confidence
regions for each estimate. The goodness of fit indices that are commonly used
to evaluate the fit of SEMs are developed for the FSEMs introduced in this
paper. Overall, the proposed method is a promising approach for modeling
functional data in SEMs with functional latent variables.",2024-12-26T14:57:14Z,http://arxiv.org/abs/2412.19242v1,"Fatemeh Asgari, Valeria Vitelli, Uta Sailer"
"Latenrgy: Model Agnostic Latency and Energy Consumption Prediction for
  Binary Classifiers","Machine learning systems increasingly drive innovation across scientific
fields and industry, yet challenges in compute overhead, specifically during
inference, limit their scalability and sustainability. Responsible AI
guardrails, essential for ensuring fairness, transparency, and privacy, further
exacerbate these computational demands. This study addresses critical gaps in
the literature, chiefly the lack of generalized predictive techniques for
latency and energy consumption, limited cross-comparisons of classifiers, and
unquantified impacts of RAI guardrails on inference performance. Using Theory
Construction Methodology, this work constructed a model-agnostic theoretical
framework for predicting latency and energy consumption in binary
classification models during inference. The framework synthesizes classifier
characteristics, dataset properties, and RAI guardrails into a unified
analytical instrument. Two predictive equations are derived that capture the
interplay between these factors while offering generalizability across diverse
classifiers. The proposed framework provides foundational insights for
designing efficient, responsible ML systems. It enables researchers to
benchmark and optimize inference performance and assists practitioners in
deploying scalable solutions. Finally, this work establishes a theoretical
foundation for balancing computational efficiency with ethical AI principles,
paving the way for future empirical validation and broader applications.",2024-12-26T14:51:24Z,http://arxiv.org/abs/2412.19241v1,Jason M. Pittman
"A Malliavin Calculus Approach to Backward Stochastic Volterra Integral
  Equations","In this paper, we establish existence, uniqueness, and regularity properties
of the solutions to multi-dimensional backward stochastic Volterra integral
equations (BSVIEs), whose (possibly random) generator reflects nonlinear
dependence on both the solution process and the martingale integrand component
of the adapted solutions, as well as their diagonal processes. The
well-posedness results are developed with the use of Malliavin calculus, which
renders a novel perspective in tackling with the challenging diagonal processes
while contrasts with the existing methods. We also provide a probabilistic
interpretation of the classical solutions to the counterpart semilinear partial
differential equations through the explicit adapted solutions of BSVIEs.
Moreover, we formulate with BSVIEs to explicitly characterize dynamically
optimal mean-variance portfolios for various stochastic investment
opportunities, with the myopic investment and intertemporal hedging demands
being identified as two diagonal processes of BSVIE solutions.",2024-12-26T14:34:26Z,http://arxiv.org/abs/2412.19236v1,"Qian Lei, Chi Seng Pun"
"VINEVI: A Virtualized Network Vision Architecture for Smart Monitoring
  of Heterogeneous Applications and Infrastructures","Monitoring heterogeneous infrastructures and applications is essential to
cope with user requirements properly, but it still lacks enhancements. The
well-known state-of-the-art methods and tools do not support seamless
monitoring of bare-metal, low-cost infrastructures, neither hosted nor
virtualized services with fine-grained details. This work proposes VIrtualized
NEtwork VIsion architecture (VINEVI), an intelligent method for seamless
monitoring heterogeneous infrastructures and applications. The VINEVI
architecture advances state of the art with a node-embedded traffic
classification agent placing physical and virtualized infrastructures enabling
real-time traffic classification. VINEVI combines this real-time traffic
classification with well-known tools such as Prometheus and Victoria Metrics to
monitor the entire stack from the hardware to the virtualized applications.
Experimental results showcased that VINEVI architecture allowed seamless
heterogeneous infrastructure monitoring with a higher level of detail beyond
literature. Also, our node-embedded real-time Internet traffic classifier
evolved with flexibility the methods with monitoring heterogeneous
infrastructures seamlessly.",2024-12-26T14:05:14Z,http://arxiv.org/abs/2412.19226v1,"Rodrigo Moreira, Hugo G. V. O. da Cunha, Larissa F. Rodrigues Moreira, Flávio de Oliveira Silva"
"Transformer-Based Wireless Capsule Endoscopy Bleeding Tissue Detection
  and Classification","Informed by the success of the transformer model in various computer vision
tasks, we design an end-to-end trainable model for the automatic detection and
classification of bleeding and non-bleeding frames extracted from Wireless
Capsule Endoscopy (WCE) videos. Based on the DETR model, our model uses the
Resnet50 for feature extraction, the transformer encoder-decoder for bleeding
and non-bleeding region detection, and a feedforward neural network for
classification. Trained in an end-to-end approach on the Auto-WCEBleedGen
Version 1 challenge training set, our model performs both detection and
classification tasks as a single unit. Our model achieves an accuracy, recall,
and F1-score classification percentage score of 98.28, 96.79, and 98.37
respectively, on the Auto-WCEBleedGen version 1 validation set. Further, we
record an average precision (AP @ 0.5), mean-average precision (mAP) of 0.7447
and 0.7328 detection results. This earned us a 3rd place position in the
challenge. Our code is publicly available via
https://github.com/BasitAlawode/WCEBleedGen.",2024-12-26T13:49:39Z,http://arxiv.org/abs/2412.19218v1,"Basit Alawode, Shibani Hamza, Adarsh Ghimire, Divya Velayudhan"
"Large Language Models Meet Graph Neural Networks: A Perspective of Graph
  Mining","Graph mining is an important area in data mining and machine learning that
involves extracting valuable information from graph-structured data. In recent
years, significant progress has been made in this field through the development
of graph neural networks (GNNs). However, GNNs are still deficient in
generalizing to diverse graph data. Aiming to this issue, Large Language Models
(LLMs) could provide new solutions for graph mining tasks with their superior
semantic understanding. In this review, we systematically review the
combination and application techniques of LLMs and GNNs and present a novel
taxonomy for research in this interdisciplinary field, which involves three
main categories: GNN-driving-LLM, LLM-driving-GNN, and GNN-LLM-co-driving.
Within this framework, we reveal the capabilities of LLMs in enhancing graph
feature extraction as well as improving the effectiveness of downstream tasks
such as node classification, link prediction, and community detection. Although
LLMs have demonstrated their great potential in handling graph-structured data,
their high computational requirements and complexity remain challenges. Future
research needs to continue to explore how to efficiently fuse LLMs and GNNs to
achieve more powerful graph learning and reasoning capabilities and provide new
impetus for the development of graph mining techniques.",2024-12-26T13:21:09Z,http://arxiv.org/abs/2412.19211v1,"Yuxin You, Zhen Liu, Xiangchao Wen, Yongtao Zhang, Wei Ai"
"Overlapping Schwarz Preconditioners for Randomized Neural Networks with
  Domain Decomposition","Randomized neural networks (RaNNs), in which hidden layers remain fixed after
random initialization, provide an efficient alternative for parameter
optimization compared to fully parameterized networks. In this paper, RaNNs are
integrated with overlapping Schwarz domain decomposition in two (main) ways:
first, to formulate the least-squares problem with localized basis functions,
and second, to construct overlapping preconditioners for the resulting linear
systems. In particular, neural networks are initialized randomly in each
subdomain based on a uniform distribution and linked through a partition of
unity, forming a global solution that approximates the solution of the partial
differential equation. Boundary conditions are enforced through a constraining
operator, eliminating the need for a penalty term to handle them. Principal
component analysis (PCA) is employed to reduce the number of basis functions in
each subdomain, yielding a linear system with a lower condition number. By
constructing additive and restricted additive Schwarz preconditioners, the
least-squares problem is solved efficiently using the Conjugate Gradient (CG)
and Generalized Minimal Residual (GMRES) methods, respectively. Our numerical
results demonstrate that the proposed approach significantly reduces
computational time for multi-scale and time-dependent problems. Additionally, a
three-dimensional problem is presented to demonstrate the efficiency of using
the CG method with an AS preconditioner, compared to an QR decomposition, in
solving the least-squares problem.",2024-12-26T13:08:58Z,http://arxiv.org/abs/2412.19207v1,"Yong Shang, Alexander Heinlein, Siddhartha Mishra, Fei Wang"
"GAIS: A Novel Approach to Instance Selection with Graph Attention
  Networks","Instance selection (IS) is a crucial technique in machine learning that aims
to reduce dataset size while maintaining model performance. This paper
introduces a novel method called Graph Attention-based Instance Selection
(GAIS), which leverages Graph Attention Networks (GATs) to identify the most
informative instances in a dataset. GAIS represents the data as a graph and
uses GATs to learn node representations, enabling it to capture complex
relationships between instances. The method processes data in chunks, applies
random masking and similarity thresholding during graph construction, and
selects instances based on confidence scores from the trained GAT model.
Experiments on 13 diverse datasets demonstrate that GAIS consistently
outperforms traditional IS methods in terms of effectiveness, achieving high
reduction rates (average 96\%) while maintaining or improving model
performance. Although GAIS exhibits slightly higher computational costs, its
superior performance in maintaining accuracy with significantly reduced
training data makes it a promising approach for graph-based data selection.",2024-12-26T12:51:14Z,http://arxiv.org/abs/2412.19201v1,"Zahiriddin Rustamov, Ayham Zaitouny, Rafat Damseh, Nazar Zaki"
New-type geometric gates in atomic arrays without Rydberg blockade,"The Rydberg blockade effect plays an important role in realizing two-qubit
gates in atomic arrays. Meanwhile, such mechanics will increase the crosstalk
between atoms and enhance the decoherence. In this paper, we propose a new
scheme to realize the controlled-phase gate without Rydberg blockade. The
scheme works effectively with large atomic spacings and is insensitive to the
thermal motions of atoms. The proposal is robust against random noises due to
the geometric characteristic and operates fast based on the non-adiabatic
evolution. The proposed gate is actually a new-type geometric gate that
consolidates the non-adiabatic holonomic control and the unconventional
geometric control simultaneously. The interference between two different types
of geometric phases can be investigated. Furthermore, we show that the scheme
with weak Rydberg interaction requires much less physical resources than the
present Rydberg blockade scheme. Therefore, our proposal provides a fast and
robust way to realize geometric quantum control, and it may trigger the
discoveries of new geometric gates in high-dimensional Hilbert space.",2024-12-26T12:24:36Z,http://arxiv.org/abs/2412.19193v1,"Yue Ming, Zhao-Xin Fu, Yan-Xiong Du"
"Game-Theoretically Secure Distributed Protocols for Fair Allocation in
  Coalitional Games","We consider game-theoretically secure distributed protocols for coalition
games that approximate the Shapley value with small multiplicative error. Since
all known existing approximation algorithms for the Shapley value are
randomized, it is a challenge to design efficient distributed protocols among
mutually distrusted players when there is no central authority to generate
unbiased randomness. The game-theoretic notion of maximin security has been
proposed to offer guarantees to an honest player's reward even if all other
players are susceptible to an adversary.
  Permutation sampling is often used in approximation algorithms for the
Shapley value. A previous work in 1994 by Zlotkin et al. proposed a simple
constant-round distributed permutation generation protocol based on commitment
scheme, but it is vulnerable to rushing attacks. The protocol, however, can
detect such attacks.
  In this work, we model the limited resources of an adversary by a violation
budget that determines how many times it can perform such detectable attacks.
Therefore, by repeating the number of permutation samples, an honest player's
reward can be guaranteed to be close to its Shapley value. We explore both high
probability and expected maximin security. We obtain an upper bound on the
number of permutation samples for high probability maximin security, even with
an unknown violation budget. Furthermore, we establish a matching lower bound
for the weaker notion of expected maximin security in specific permutation
generation protocols. We have also performed experiments on both synthetic and
real data to empirically verify our results.",2024-12-26T12:13:21Z,http://arxiv.org/abs/2412.19192v1,"T-H. Hubert Chan, Qipeng Kuang, Quan Xue"
Priors for second-order unbiased Bayes estimators,"Asymptotically unbiased priors, introduced by Hartigan (1965), are designed
to achieve second-order unbiasedness of Bayes estimators. This paper extends
Hartigan's framework to non-i.i.d. models by deriving a system of partial
differential equations that characterizes asymptotically unbiased priors.
Furthermore, we establish a necessary and sufficient condition for the
existence of such priors and propose a simple procedure for constructing them.
  The proposed method is applied to several examples, including the linear
regression model and the nested error regression (NER) model (also known as the
random effects model). Simulation studies evaluate the frequentist properties
of the Bayes estimator under the asymptotically unbiased prior for the NER
model, highlighting its effectiveness in small-sample settings.",2024-12-26T11:51:07Z,http://arxiv.org/abs/2412.19187v1,"Mana Sakai, Takeru Matsuda, Tatsuya Kubokawa"
"Outlier-Bias Removal with Alpha Divergence: A Robust Non-Convex
  Estimator for Linear Regression","Convex and penalized robust methods often suffer from bias induced by large
outliers, limiting their effectiveness in adversarial or heavy-tailed settings.
In this study, we propose a novel approach that eliminates this bias (when
possible) by leveraging a non-convex $M$-estimator based on the alpha
divergence. We address the problem of estimating the parameters vector in high
dimensional linear regression, even when a subset of the data has been
deliberately corrupted by an adversary with full knowledge of the dataset and
its underlying distribution.
  Our primary contribution is to demonstrate that the objective function,
although non-convex, exhibits convexity within a carefully chosen basin of
attraction, enabling robust and unbiased estimation. Additionally, we establish
three key theoretical guarantees for the estimator: (a) a deviation bound that
is minimax optimal up to a logarithmic factor, (b) an improved unbiased bound
when the outliers are large and (c) asymptotic normality as the sample size
increases. Finally, we validate the theoretical findings through empirical
comparisons with state-of-the-art estimators on both synthetic and real-world
datasets, highlighting the proposed method's superior robustness, efficiency,
and ability to mitigate outlier-induced bias.",2024-12-26T11:42:46Z,http://arxiv.org/abs/2412.19183v1,"Ilyes Hammouda, Mohamed Ndaoud, and Abd-Krim Seghouane"
"Dual Channel Multi-Attention in ViT for Biometric Authentication using
  Forehead Subcutaneous Vein Pattern and Periocular Pattern","Traditional biometric systems, like face and fingerprint recognition, have
encountered significant setbacks due to wearing face masks and hygiene
concerns. To meet the challenges of the partially covered face due to face
masks and hygiene concerns of fingerprint recognition, this paper proposes a
novel dual-channel multi-attention Vision Transformer (ViT) framework for
biometric authentication using forehead subcutaneous vein patterns and
periocular patterns, offering a promising alternative to traditional methods,
capable of performing well even with face masks and without any physical touch.
The proposed framework leverages a dual-channel ViT architecture, designed to
handle two distinct biometric traits. It can capture long-range dependencies of
independent features from the vein and periocular patterns. A custom classifier
is then designed to integrate the independently extracted features, producing a
final class prediction. The performance of the proposed algorithm was
rigorously evaluated using the Forehead Subcutaneous Vein Pattern and
Periocular Biometric Pattern (FSVP-PBP) database. The results demonstrated the
superiority of the algorithm over state-of-the-art methods, achieving
remarkable classification accuracy of $99.3 \pm 0.02\%$ with the combined vein
and periocular patterns.",2024-12-26T10:40:15Z,http://arxiv.org/abs/2412.19160v1,"Arun K. Sharma, Shubhobrata Bhattacharya, Motahar Reza"
"Referencing Where to Focus: Improving VisualGrounding with Referential
  Query","Visual Grounding aims to localize the referring object in an image given a
natural language expression. Recent advancements in DETR-based visual grounding
methods have attracted considerable attention, as they directly predict the
coordinates of the target object without relying on additional efforts, such as
pre-generated proposal candidates or pre-defined anchor boxes. However,
existing research primarily focuses on designing stronger multi-modal decoder,
which typically generates learnable queries by random initialization or by
using linguistic embeddings. This vanilla query generation approach inevitably
increases the learning difficulty for the model, as it does not involve any
target-related information at the beginning of decoding. Furthermore, they only
use the deepest image feature during the query learning process, overlooking
the importance of features from other levels. To address these issues, we
propose a novel approach, called RefFormer. It consists of the query adaption
module that can be seamlessly integrated into CLIP and generate the referential
query to provide the prior context for decoder, along with a task-specific
decoder. By incorporating the referential query into the decoder, we can
effectively mitigate the learning difficulty of the decoder, and accurately
concentrate on the target object. Additionally, our proposed query adaption
module can also act as an adapter, preserving the rich knowledge within CLIP
without the need to tune the parameters of the backbone network. Extensive
experiments demonstrate the effectiveness and efficiency of our proposed
method, outperforming state-of-the-art approaches on five visual grounding
benchmarks.",2024-12-26T10:19:20Z,http://arxiv.org/abs/2412.19155v1,"Yabing Wang, Zhuotao Tian, Qingpei Guo, Zheng Qin, Sanping Zhou, Ming Yang, Le Wang"
"To Predict or Not To Predict? Proportionally Masked Autoencoders for
  Tabular Data Imputation","Masked autoencoders (MAEs) have recently demonstrated effectiveness in
tabular data imputation. However, due to the inherent heterogeneity of tabular
data, the uniform random masking strategy commonly used in MAEs can disrupt the
distribution of missingness, leading to suboptimal performance. To address
this, we propose a proportional masking strategy for MAEs. Specifically, we
first compute the statistics of missingness based on the observed proportions
in the dataset, and then generate masks that align with these statistics,
ensuring that the distribution of missingness is preserved after masking.
Furthermore, we argue that simple MLP-based token mixing offers competitive or
often superior performance compared to attention mechanisms while being more
computationally efficient, especially in the tabular domain with the inherent
heterogeneity. Experimental results validate the effectiveness of the proposed
proportional masking strategy across various missing data patterns in tabular
datasets. Code is available at: \url{https://github.com/normal-kim/PMAE}.",2024-12-26T10:12:08Z,http://arxiv.org/abs/2412.19152v1,"Jungkyu Kim, Kibok Lee, Taeyoung Park"
"CLIP-GS: Unifying Vision-Language Representation with 3D Gaussian
  Splatting","Recent works in 3D multimodal learning have made remarkable progress.
However, typically 3D multimodal models are only capable of handling point
clouds. Compared to the emerging 3D representation technique, 3D Gaussian
Splatting (3DGS), the spatially sparse point cloud cannot depict the texture
information of 3D objects, resulting in inferior reconstruction capabilities.
This limitation constrains the potential of point cloud-based 3D multimodal
representation learning. In this paper, we present CLIP-GS, a novel multimodal
representation learning framework grounded in 3DGS. We introduce the GS
Tokenizer to generate serialized gaussian tokens, which are then processed
through transformer layers pre-initialized with weights from point cloud
models, resulting in the 3DGS embeddings. CLIP-GS leverages contrastive loss
between 3DGS and the visual-text embeddings of CLIP, and we introduce an image
voting loss to guide the directionality and convergence of gradient
optimization. Furthermore, we develop an efficient way to generate triplets of
3DGS, images, and text, facilitating CLIP-GS in learning unified multimodal
representations. Leveraging the well-aligned multimodal representations,
CLIP-GS demonstrates versatility and outperforms point cloud-based models on
various 3D tasks, including multimodal retrieval, zero-shot, and few-shot
classification.",2024-12-26T09:54:25Z,http://arxiv.org/abs/2412.19142v1,"Siyu Jiao, Haoye Dong, Yuyang Yin, Zequn Jie, Yinlong Qian, Yao Zhao, Humphrey Shi, Yunchao Wei"
"How Panel Layouts Define Manga: Insights from Visual Ablation
  Experiments","Today, manga has gained worldwide popularity. However, the question of how
various elements of manga, such as characters, text, and panel layouts, reflect
the uniqueness of a particular work, or even define it, remains an unexplored
area. In this paper, we aim to quantitatively and qualitatively analyze the
visual characteristics of manga works, with a particular focus on panel layout
features. As a research method, we used facing page images of manga as input to
train a deep learning model for predicting manga titles, examining
classification accuracy to quantitatively analyze these features. Specifically,
we conducted ablation studies by limiting page image information to panel
frames to analyze the characteristics of panel layouts. Through a series of
quantitative experiments using all 104 works, 12 genres, and 10,122 facing page
images from the Manga109 dataset, as well as qualitative analysis using
Grad-CAM, our study demonstrates that the uniqueness of manga works is strongly
reflected in their panel layouts.",2024-12-26T09:53:37Z,http://arxiv.org/abs/2412.19141v1,"Siyuan Feng, Teruya Yoshinaga, Katsuhiko Hayashi, Koki Washio, Hidetaka Kamigaito"
"CoheDancers: Enhancing Interactive Group Dance Generation through
  Music-Driven Coherence Decomposition","Dance generation is crucial and challenging, particularly in domains like
dance performance and virtual gaming. In the current body of literature, most
methodologies focus on Solo Music2Dance. While there are efforts directed
towards Group Music2Dance, these often suffer from a lack of coherence,
resulting in aesthetically poor dance performances. Thus, we introduce
CoheDancers, a novel framework for Music-Driven Interactive Group Dance
Generation. CoheDancers aims to enhance group dance generation coherence by
decomposing it into three key aspects: synchronization, naturalness, and
fluidity. Correspondingly, we develop a Cycle Consistency based Dance
Synchronization strategy to foster music-dance correspondences, an
Auto-Regressive-based Exposure Bias Correction strategy to enhance the fluidity
of the generated dances, and an Adversarial Training Strategy to augment the
naturalness of the group dance output. Collectively, these strategies enable
CohdeDancers to produce highly coherent group dances with superior quality.
Furthermore, to establish better benchmarks for Group Music2Dance, we construct
the most diverse and comprehensive open-source dataset to date, I-Dancers,
featuring rich dancer interactions, and create comprehensive evaluation
metrics. Experimental evaluations on I-Dancers and other extant datasets
substantiate that CoheDancers achieves unprecedented state-of-the-art
performance. Code will be released.",2024-12-26T08:47:13Z,http://arxiv.org/abs/2412.19123v1,"Kaixing Yang, Xulong Tang, Haoyu Wu, Qinliang Xue, Biao Qin, Hongyan Liu, Zhaoxin Fan"
"Constrained stochastic linear quadratic control under regime switching
  with controlled jump size","In this paper, we examine a stochastic linear-quadratic control problem
characterized by regime switching and Poisson jumps. All the coefficients in
the problem are random processes adapted to the filtration generated by
Brownian motion and the Poisson random measure for each given regime. The model
incorporates two distinct types of controls: the first is a conventional
control that appears in the continuous diffusion component, while the second is
an unconventional control, dependent on the variable $z$, which influences the
jump size in the jump diffusion component. Both controls are constrained within
general closed cones. By employing the Meyer-It\^o formula in conjunction with
a generalized squares completion technique, we rigorously and explicitly derive
the optimal value and optimal feedback control. These depend on solutions to
certain multi-dimensional fully coupled stochastic Riccati equations, which are
essentially backward stochastic differential equations with jumps (BSDEJs). We
establish the existence of a unique nonnegative solution to the BSDEJs. One of
the major tools used in the proof is the newly established comparison theorems
for multidimensional BSDEJs.",2024-12-26T07:42:23Z,http://arxiv.org/abs/2412.19100v1,"Xiaomin Shi, Zuo Quan Xu"
From Coin to Data: The Impact of Object Detection on Digital Numismatics,"In this work we investigate the application of advanced object detection
techniques to digital numismatics, focussing on the analysis of historical
coins. Leveraging models such as Contrastive Language-Image Pre-training
(CLIP), we develop a flexible framework for identifying and classifying
specific coin features using both image and textual descriptions. By examining
two distinct datasets, modern Russian coins featuring intricate ""Saint George
and the Dragon"" designs and degraded 1st millennium AD Southeast Asian coins
bearing Hindu-Buddhist symbols, we evaluate the efficacy of different detection
algorithms in search and classification tasks. Our results demonstrate the
superior performance of larger CLIP models in detecting complex imagery, while
traditional methods excel in identifying simple geometric patterns.
Additionally, we propose a statistical calibration mechanism to enhance the
reliability of similarity scores in low-quality datasets. This work highlights
the transformative potential of integrating state-of-the-art object detection
into digital numismatics, enabling more scalable, precise, and efficient
analysis of historical artifacts. These advancements pave the way for new
methodologies in cultural heritage research, artefact provenance studies, and
the detection of forgeries.",2024-12-26T07:05:53Z,http://arxiv.org/abs/2412.19091v1,"Rafael Cabral, Maria De Iorio, Andrew Harris"
"Assessing Pre-trained Models for Transfer Learning through Distribution
  of Spectral Components","Pre-trained model assessment for transfer learning aims to identify the
optimal candidate for the downstream tasks from a model hub, without the need
of time-consuming fine-tuning. Existing advanced works mainly focus on
analyzing the intrinsic characteristics of the entire features extracted by
each pre-trained model or how well such features fit the target labels. This
paper proposes a novel perspective for pre-trained model assessment through the
Distribution of Spectral Components (DISCO). Through singular value
decomposition of features extracted from pre-trained models, we investigate
different spectral components and observe that they possess distinct
transferability, contributing diversely to the fine-tuning performance.
Inspired by this, we propose an assessment method based on the distribution of
spectral components which measures the proportions of their corresponding
singular values. Pre-trained models with features concentrating on more
transferable components are regarded as better choices for transfer learning.
We further leverage the labels of downstream data to better estimate the
transferability of each spectral component and derive the final assessment
criterion. Our proposed method is flexible and can be applied to both
classification and regression tasks. We conducted comprehensive experiments
across three benchmarks and two tasks including image classification and object
detection, demonstrating that our method achieves state-of-the-art performance
in choosing proper pre-trained models from the model hub for transfer learning.",2024-12-26T06:54:22Z,http://arxiv.org/abs/2412.19085v1,"Tengxue Zhang, Yang Shu, Xinyang Chen, Yifei Long, Chenjuan Guo, Bin Yang"
A Microservice Graph Generator with Production Characteristics,"A production microservice application may provide multiple services, queries
of a service may have different call graphs, and a microservice may be shared
across call graphs. It is challenging to improve the resource efficiency of
such complex applications without proper benchmarks, while production traces
are too large to be used in experiments. To this end, we propose a Service
Dependency Graph Generator (DGG) that comprises a Data Handler and a Graph
Generator, for generating the service dependency graphs of benchmarks that
incorporate production-level characteristics from traces. The data handler
first constructs fine-grained call graphs with dynamic interface and repeated
calling features from the trace and merges them into dependency graphs, and
then clusters them into different categories based on the topological and
invocation types. Taking the organized data and the selected category, the
graph generator simulates the process of real microservices invoking downstream
microservices using a random graph model, generates multiple call graphs, and
merges the call graphs to form the small-scale service dependency graph with
production-level characteristics. Case studies show that DGG's generated graphs
are similar to real traces in terms of topologies. Moreover, the resource
scaling based on DGG's fine-grained call graph constructing increases the
resource efficiency by up to 44.8% while ensuring the required QoS.",2024-12-26T06:51:35Z,http://arxiv.org/abs/2412.19083v1,"Fanrong Du, Jiuchen Shi, Quan Chen, Li Li, Minyi Guo"
"Robust Speech and Natural Language Processing Models for Depression
  Screening","Depression is a global health concern with a critical need for increased
patient screening. Speech technology offers advantages for remote screening but
must perform robustly across patients. We have described two deep learning
models developed for this purpose. One model is based on acoustics; the other
is based on natural language processing. Both models employ transfer learning.
Data from a depression-labeled corpus in which 11,000 unique users interacted
with a human-machine application using conversational speech is used. Results
on binary depression classification have shown that both models perform at or
above AUC=0.80 on unseen data with no speaker overlap. Performance is further
analyzed as a function of test subset characteristics, finding that the models
are generally robust over speaker and session variables. We conclude that
models based on these approaches offer promise for generalized automated
depression screening.",2024-12-26T06:05:52Z,http://arxiv.org/abs/2412.19072v1,"Y. Lu, A. Harati, T. Rutowski, R. Oliveira, P. Chlebek, E. Shriberg"
"Equilibrium reinsurance and investment strategies for insurers with
  random risk aversion under Heston's SV model","This paper adopts expected certainty equivalents to consider the reinsurance
and investment problem for an insurer who maximizes the expected utility but
faces the random risk aversion. The insurer's surplus process is approximated
by a Brownian motion with drift, and the financial market is comprised of a
risk-free asset and a risky asset whose price is described by Heston's
stochastic volatility (SV) model. Under a game theoretic framework, a rigorous
verification theorem is provided for characterizing the equilibrium reinsurance
and investment strategies and the corresponding value function. Moreover, by
solving the pseudo Hamilton-Jacobi-Bellman (HJB) system, semi-analytic
expressions for the equilibrium reinsurance and investment strategies and the
corresponding value function are derived under the exponential utility. In
addition, some numerical experiments are given to illustrate the behavior of
the equilibrium reinsurance and investment strategies.",2024-12-26T04:10:05Z,http://arxiv.org/abs/2412.19050v1,"Jian-hao Kang, Zhun Gou, Nan-jing Huang"
CL-attack: Textual Backdoor Attacks via Cross-Lingual Triggers,"Backdoor attacks significantly compromise the security of large language
models by triggering them to output specific and controlled content. Currently,
triggers for textual backdoor attacks fall into two categories: fixed-token
triggers and sentence-pattern triggers. However, the former are typically easy
to identify and filter, while the latter, such as syntax and style, do not
apply to all original samples and may lead to semantic shifts. In this paper,
inspired by cross-lingual (CL) prompts of LLMs in real-world scenarios, we
propose a higher-dimensional trigger method at the paragraph level, namely
CL-attack. CL-attack injects the backdoor by using texts with specific
structures that incorporate multiple languages, thereby offering greater
stealthiness and universality compared to existing backdoor attack techniques.
Extensive experiments on different tasks and model architectures demonstrate
that CL-attack can achieve nearly 100% attack success rate with a low poisoning
rate in both classification and generation tasks. We also empirically show that
the CL-attack is more robust against current major defense methods compared to
baseline backdoor attacks. Additionally, to mitigate CL-attack, we further
develop a new defense called TranslateDefense, which can partially mitigate the
impact of CL-attack.",2024-12-26T03:13:03Z,http://arxiv.org/abs/2412.19037v1,"Jingyi Zheng, Tianyi Hu, Tianshuo Cong, Xinlei He"
"Reflection on Purpose Changes Students' Academic Interests: A Scalable
  Intervention in an Online Course Catalog","College students routinely use online course catalogs to explore a variety of
academic offerings. Course catalogs may therefore be an effective place to
encourage reflection on academic choices and interests. To test this, we
embedded a psychological intervention in an online course catalog to encourage
students to reflect on their purpose during course exploration. Results of a
randomized field experiment with over 4,000 students at a large U.S. university
show that a purpose intervention increased students' cognitive engagement in
describing their interests, but reduced search activities. Students became more
interested in courses related to creative arts and social change, but less in
computer and data science. The findings demonstrate the malleability of
students' interests during course exploration and suggest practical strategies
to support purpose reflection and guide students toward deliberate exploration
of their interests in higher education.",2024-12-26T03:12:50Z,http://arxiv.org/abs/2412.19035v1,"Youjie Chen, Pranathi Iyer, Rene F. Kizilcec"
Neural Networks Perform Sufficient Dimension Reduction,"This paper investigates the connection between neural networks and sufficient
dimension reduction (SDR), demonstrating that neural networks inherently
perform SDR in regression tasks under appropriate rank regularizations.
Specifically, the weights in the first layer span the central mean subspace. We
establish the statistical consistency of the neural network-based estimator for
the central mean subspace, underscoring the suitability of neural networks in
addressing SDR-related challenges. Numerical experiments further validate our
theoretical findings, and highlight the underlying capability of neural
networks to facilitate SDR compared to the existing methods. Additionally, we
discuss an extension to unravel the central subspace, broadening the scope of
our investigation.",2024-12-26T03:05:43Z,http://arxiv.org/abs/2412.19033v1,"Shuntuo Xu, Zhou Yu"
"Ergodicity for eventually continuous Markov--Feller semigroups on Polish
  spaces","This paper investigates the ergodicity of Markov--Feller semigroups on Polish
spaces, focusing on very weak regularity conditions, particularly the Ces\`aro
eventual continuity. First, it is showed that the Ces\`aro average of such
semigroups weakly converges to an ergodic measure when starting from its
support. This leads to a characterization of the relationship between Ces\`aro
eventual continuity, Ces\`aro e-property, and weak-* mean ergodicity. Next,
serval criteria are provided for the existence and uniqueness of invariant
measures via Ces\`aro eventual continuity and lower bound conditions,
establishing an equivalence relation between weak-* mean ergodicity and a lower
bound condition. Additionally, some refined properties of ergodic decomposition
are derived. Finally, the results are applied to several non-trivial examples,
including iterated function systems, Hopf's turbulence model with random
forces, and Lorenz system with noisy perturbations, either with or without
Ces\`aro eventual continuity.",2024-12-26T02:43:50Z,http://arxiv.org/abs/2412.19029v1,"Fuzhou Gong, Yong Liu, Yuan Liu, Ziyu Liu"
"Channel-Aware Optimal Transport: A Theoretical Framework for Generative
  Communication","Optimal transport has numerous applications, particularly in machine learning
tasks involving generative models. In practice, the transportation process
often encounters an information bottleneck, typically arising from the
conversion of a communication channel into a rate-limited bit pipeline using
error correction codes. While this conversion enables a channel-oblivious
approach to optimal transport, it fails to fully exploit the available degrees
of freedom. Motivated by the emerging paradigm of generative communication,
this paper examines the problem of channel-aware optimal transport, where a
block of i.i.d. random variables is transmitted through a memoryless channel to
generate another block of i.i.d. random variables with a prescribed marginal
distribution such that the end-to-end distortion is minimized. With unlimited
common randomness available to the encoder and decoder, the source-channel
separation architecture is shown to be asymptotically optimal as the
blocklength approaches infinity. On the other hand, in the absence of common
randomness, the source-channel separation architecture is generally suboptimal.
For this scenario, a hybrid coding scheme is proposed, which partially retains
the generative capabilities of the given channel while enabling reliable
transmission of digital information. It is demonstrated that the proposed
hybrid coding scheme can outperform both separation-based and uncoded schemes.",2024-12-26T02:23:08Z,http://arxiv.org/abs/2412.19025v1,"Xiqiang Qu, Ruibin Li, Jun Chen, Lei Yu, Xinbing Wang"
"Let the Rule Speak: Enhancing In-context Learning Debiasing with
  Interpretability","In-context learning, which allows large language models to perform diverse
tasks with a few demonstrations, is found to have imbalanced per-class
prediction accuracy on multi-class text classification. Although notable output
correction methods have been developed to tackle the issue and simultaneously
improve downstream prediction accuracy, they may fail to answer the core
interpretability challenges: why and which certain classes need corrections,
and more importantly, a tailored correction for per-sample, per-class's
probability. To address such interpretability gaps, we first find that the
imbalance arises from certain classes consistently receiving high ICL output
probabilities, whereas others receiving lower or mixed ranges, so the former is
more frequently chosen, resulting in higher accuracy; more crucially, we find
that these ranges have significantly varying degrees of influence on the
accuracy bias, highlighting the need for precise, interpretable probability
corrections by range. Motivated by this, we propose FuRud, a Fuzzy Rule
Optimization based Debiasing method, that (1) detects which classes need
corrections, and (2) for each correction-needed class, detects its probability
ranges and applies asymmetric amplifications or reductions to correct them
interpretably. Notably, across seven benchmark datasets, FuRud reduces the
pairwise class accuracy bias (COBias) by more than half (56%), while achieving
a relative increase of 21% in accuracy, outperforming state-of-the-art
debiasing methods. Moreover, FuRud can optimize downstream tasks with as few as
10 optimization examples. Furthermore, FuRud can work for prompt formats that
lead to highly skewed predictions. For example, FuRud greatly improves ICL
outputs which use letter options, with 44% relative accuracy increase and 54%
relative COBias reduction.",2024-12-26T01:56:42Z,http://arxiv.org/abs/2412.19018v1,"Ruixi Lin, Yang You"
"Brain Ageing Prediction using Isolation Forest Technique and Residual
  Neural Network (ResNet)","Brain aging is a complex and dynamic process, leading to functional and
structural changes in the brain. These changes could lead to the increased risk
of neurodegenerative diseases and cognitive decline. Accurate brain-age
estimation utilizing neuroimaging data has become necessary for detecting
initial signs of neurodegeneration. Here, we propose a novel deep learning
approach using the Residual Neural Network 101 Version 2 (ResNet101V2) model to
predict brain age from MRI scans. To train, validate and test our proposed
model, we used a large dataset of 2102 images which were selected randomly from
the International Consortium for Brain Mapping (ICBM). Next, we applied data
preprocessing techniques, including normalizing the images and using outlier
detection via Isolation Forest method. Then, we evaluated various pre-trained
approaches (namely: MobileNetV2, ResNet50V2, ResNet101V2, Xception). The
results demonstrated that the ResNet101V2 model has higher performance compared
with the other models, attaining MAEs of 0.9136 and 0.8242 years for before and
after using Isolation Forest process. Our method achieved a high accuracy in
brain age estimation in ICBM dataset and it provides a reliable brain age
prediction.",2024-12-26T01:49:21Z,http://arxiv.org/abs/2412.19017v1,"Saadat Behzadi, Danial Sharifrazi, Roohallah Alizadehsani, Mojtaba Lotfaliany, Mohammadreza Mohebbi"
Dynamic networks clustering via mirror distance,"The classification of different patterns of network evolution, for example in
brain connectomes or social networks, is a key problem in network inference and
modern data science. Building on the notion of a network's Euclidean mirror,
which captures its evolution as a curve in Euclidean space, we develop the
Dynamic Network Clustering through Mirror Distance (DNCMD), an algorithm for
clustering dynamic networks based on a distance measure between their
associated mirrors. We provide theoretical guarantees for DNCMD to achieve
exact recovery of distinct evolutionary patterns for latent position random
networks both when underlying vertex features change deterministically and when
they follow a stochastic process. We validate our theoretical results through
numerical simulations and demonstrate the application of DNCMD to understand
edge functions in Drosophila larval connectome data, as well as to analyze
temporal patterns in dynamic trade networks.",2024-12-26T01:14:21Z,http://arxiv.org/abs/2412.19012v1,"Runbing Zheng, Avanti Athreya, Marta Zlatic, Michael Clayton, Carey E. Priebe"
"Geospatial Data Fusion: Combining Lidar, SAR, and Optical Imagery with
  AI for Enhanced Urban Mapping","This study explores the integration of Lidar, Synthetic Aperture Radar (SAR),
and optical imagery through advanced artificial intelligence techniques for
enhanced urban mapping. By fusing these diverse geospatial datasets, we aim to
overcome the limitations associated with single-sensor data, achieving a more
comprehensive representation of urban environments. The research employs Fully
Convolutional Networks (FCNs) as the primary deep learning model for urban
feature extraction, enabling precise pixel-wise classification of essential
urban elements, including buildings, roads, and vegetation. To optimize the
performance of the FCN model, we utilize Particle Swarm Optimization (PSO) for
hyperparameter tuning, significantly enhancing model accuracy. Key findings
indicate that the FCN-PSO model achieved a pixel accuracy of 92.3% and a mean
Intersection over Union (IoU) of 87.6%, surpassing traditional single-sensor
approaches. These results underscore the potential of fused geospatial data and
AI-driven methodologies in urban mapping, providing valuable insights for urban
planning and management. The implications of this research pave the way for
future developments in real-time mapping and adaptive urban infrastructure
planning.",2024-12-25T22:17:31Z,http://arxiv.org/abs/2412.18994v1,"Sajjad Afroosheh, Mohammadreza Askari"
"Optimal Federated Learning for Functional Mean Estimation under
  Heterogeneous Privacy Constraints","Federated learning (FL) is a distributed machine learning technique designed
to preserve data privacy and security, and it has gained significant importance
due to its broad range of applications. This paper addresses the problem of
optimal functional mean estimation from discretely sampled data in a federated
setting.
  We consider a heterogeneous framework where the number of individuals,
measurements per individual, and privacy parameters vary across one or more
servers, under both common and independent design settings. In the common
design setting, the same design points are measured for each individual,
whereas in the independent design, each individual has their own random
collection of design points. Within this framework, we establish minimax upper
and lower bounds for the estimation error of the underlying mean function,
highlighting the nuanced differences between common and independent designs
under distributed privacy constraints.
  We propose algorithms that achieve the optimal trade-off between privacy and
accuracy and provide optimality results that quantify the fundamental limits of
private functional mean estimation across diverse distributed settings. These
results characterize the cost of privacy and offer practical insights into the
potential for privacy-preserving statistical analysis in federated
environments.",2024-12-25T22:06:12Z,http://arxiv.org/abs/2412.18992v1,"Tony Cai, Abhinav Chakraborty, Lasse Vuursteen"
"Detection and classification of DDoS flooding attacks by machine
  learning method","This study focuses on a method for detecting and classifying distributed
denial of service (DDoS) attacks, such as SYN Flooding, ACK Flooding, HTTP
Flooding, and UDP Flooding, using neural networks. Machine learning,
particularly neural networks, is highly effective in detecting malicious
traffic. A dataset containing normal traffic and various DDoS attacks was used
to train a neural network model with a 24-106-5 architecture. The model
achieved high Accuracy (99.35%), Precision (99.32%), Recall (99.54%), and
F-score (0.99) in the classification task. All major attack types were
correctly identified. The model was also further tested in the lab using
virtual infrastructures to generate normal and DDoS traffic. The results showed
that the model can accurately classify attacks under near-real-world
conditions, demonstrating 95.05% accuracy and balanced F-score scores for all
attack types. This confirms that neural networks are an effective tool for
detecting DDoS attacks in modern information security systems.",2024-12-25T21:58:52Z,http://arxiv.org/abs/2412.18990v1,"Dmytro Tymoshchuk, Oleh Yasniy, Mykola Mytnyk, Nataliya Zagorodna, Vitaliy Tymoshchuk"
Injecting Bias into Text Classification Models using Backdoor Attacks,"The rapid growth of natural language processing (NLP) and pre-trained
language models have enabled accurate text classification in a variety of
settings. However, text classification models are susceptible to backdoor
attacks, where an attacker embeds a trigger into the victim model to make the
model predict attacker-desired labels in targeted scenarios. In this paper, we
propose to utilize backdoor attacks for a new purpose: bias injection. We
develop a backdoor attack in which a subset of the training dataset is poisoned
to associate strong male actors with negative sentiment. We execute our attack
on two popular text classification datasets (IMDb and SST) and seven different
models ranging from traditional Doc2Vec-based models to LSTM networks and
modern transformer-based BERT and RoBERTa models. Our results show that the
reduction in backdoored models' benign classification accuracy is limited,
implying that our attacks remain stealthy, whereas the models successfully
learn to associate strong male actors with negative sentiment (100% attack
success rate with &gt;= 3% poison rate). Attacks on BERT and RoBERTa are
particularly more stealthy and effective, demonstrating an increased risk of
using modern and larger models. We also measure the generalizability of our
bias injection by proposing two metrics: (i) U-BBSR which uses previously
unseen words when measuring attack success, and (ii) P-BBSR which measures
attack success using paraphrased test samples. U-BBSR and P-BBSR results show
that the bias injected by our attack can go beyond memorizing a trigger phrase.",2024-12-25T19:32:02Z,http://arxiv.org/abs/2412.18975v1,"A. Dilara Yavuz, M. Emre Gursoy"
TINQ: Temporal Inconsistency Guided Blind Video Quality Assessment,"Blind video quality assessment (BVQA) has been actively researched for
user-generated content (UGC) videos. Recently, super-resolution (SR) techniques
have been widely applied in UGC. Therefore, an effective BVQA method for both
UGC and SR scenarios is essential. Temporal inconsistency, referring to
irregularities between consecutive frames, is relevant to video quality.
Current BVQA approaches typically model temporal relationships in UGC videos
using statistics of motion information, but inconsistencies remain unexplored.
Additionally, different from temporal inconsistency in UGC videos, such
inconsistency in SR videos is amplified due to upscaling algorithms. In this
paper, we introduce the Temporal Inconsistency Guided Blind Video Quality
Assessment (TINQ) metric, demonstrating that exploring temporal inconsistency
is crucial for effective BVQA. Since temporal inconsistencies vary between UGC
and SR videos, they are calculated in different ways. Based on this, a spatial
module highlights inconsistent areas across consecutive frames at coarse and
fine granularities. In addition, a temporal module aggregates features over
time in two stages. The first stage employs a visual memory capacity block to
adaptively segment the time dimension based on estimated complexity, while the
second stage focuses on selecting key features. The stages work together
through Consistency-aware Fusion Units to regress cross-time-scale video
quality. Extensive experiments on UGC and SR video quality datasets show that
our method outperforms existing state-of-the-art BVQA methods. Code is
available at https://github.com/Lighting-YXLI/TINQ.",2024-12-25T15:43:41Z,http://arxiv.org/abs/2412.18933v1,"Yixiao Li, Xiaoyuan Yang, Weide Liu, Xin Jin, Xu Jia, Yukun Lai, Haotao Liu, Paul L Rosin, Wei Zhou"
"Malware Classification using a Hybrid Hidden Markov Model-Convolutional
  Neural Network","The proliferation of malware variants poses a significant challenges to
traditional malware detection approaches, such as signature-based methods,
necessitating the development of advanced machine learning techniques. In this
research, we present a novel approach based on a hybrid architecture combining
features extracted using a Hidden Markov Model (HMM), with a Convolutional
Neural Network (CNN) then used for malware classification. Inspired by the
strong results in previous work using an HMM-Random Forest model, we propose
integrating HMMs, which serve to capture sequential patterns in opcode
sequences, with CNNs, which are adept at extracting hierarchical features. We
demonstrate the effectiveness of our approach on the popular Malicia dataset,
and we obtain superior performance, as compared to other machine learning
methods -- our results surpass the aforementioned HMM-Random Forest model. Our
findings underscore the potential of hybrid HMM-CNN architectures in bolstering
malware classification capabilities, offering several promising avenues for
further research in the field of cybersecurity.",2024-12-25T15:34:57Z,http://arxiv.org/abs/2412.18932v1,"Ritik Mehta, Olha Jureckova, Mark Stamp"
"Alternating Gradient-Type Algorithm for Bilevel Optimization with
  Inexact Lower-Level Solutions via Moreau Envelope-based Reformulation","In this paper, we study a class of bilevel optimization problems where the
lower-level problem is a convex composite optimization model, which arises in
various applications, including bilevel hyperparameter selection for
regularized regression models. To solve these problems, we propose an
Alternating Gradient-type algorithm with Inexact Lower-level Solutions (AGILS)
based on a Moreau envelope-based reformulation of the bilevel optimization
problem. The proposed algorithm does not require exact solutions of the
lower-level problem at each iteration, improving computational efficiency. We
prove the convergence of AGILS to stationary points and, under the
Kurdyka-{\L}ojasiewicz (KL) property, establish its sequential convergence.
Numerical experiments, including a toy example and a bilevel hyperparameter
selection problem for the sparse group Lasso model, demonstrate the
effectiveness of the proposed AGILS.",2024-12-25T15:19:36Z,http://arxiv.org/abs/2412.18929v1,"Xiaoning Bai, Shangzhi Zeng, and Jin Zhang, Lezhi Zhang"
"An Attentive Dual-Encoder Framework Leveraging Multimodal Visual and
  Semantic Information for Automatic OSAHS Diagnosis","Obstructive sleep apnea-hypopnea syndrome (OSAHS) is a common sleep disorder
caused by upper airway blockage, leading to oxygen deprivation and disrupted
sleep. Traditional diagnosis using polysomnography (PSG) is expensive,
time-consuming, and uncomfortable. Existing deep learning methods using facial
image analysis lack accuracy due to poor facial feature capture and limited
sample sizes. To address this, we propose a multimodal dual encoder model that
integrates visual and language inputs for automated OSAHS diagnosis. The model
balances data using randomOverSampler, extracts key facial features with
attention grids, and converts physiological data into meaningful text.
Cross-attention combines image and text data for better feature extraction, and
ordered regression loss ensures stable learning. Our approach improves
diagnostic efficiency and accuracy, achieving 91.3% top-1 accuracy in a
four-class severity classification task, demonstrating state-of-the-art
performance. Code will be released upon acceptance.",2024-12-25T14:42:17Z,http://arxiv.org/abs/2412.18919v1,"Yingchen Wei, Xihe Qiu, Xiaoyu Tan, Jingjing Huang, Wei Chu, Yinghui Xu, Yuan Qi"
"BCR-Net: Boundary-Category Refinement Network for Weakly Semi-Supervised
  X-Ray Prohibited Item Detection with Points","Automatic prohibited item detection in X-ray images is crucial for public
safety. However, most existing detection methods either rely on expensive box
annotations to achieve high performance or use weak annotations but suffer from
limited accuracy. To balance annotation cost and detection performance, we
study Weakly Semi-Supervised X-ray Prohibited Item Detection with Points
(WSSPID-P) and propose a novel \textbf{B}oundary-\textbf{C}ategory
\textbf{R}efinement \textbf{Net}work (\textbf{BCR-Net}) that requires only a
few box annotations and a large number of point annotations. BCR-Net is built
based on Group R-CNN and introduces a new Boundary Refinement (BR) module and a
new Category Refinement (CR) module. The BR module develops a dual attention
mechanism to focus on both the boundaries and salient features of prohibited
items. Meanwhile, the CR module incorporates contrastive branches into the
heads of RPN and ROI by introducing a scale- and rotation-aware contrastive
loss, enhancing intra-class consistency and inter-class separability in the
feature space. Based on the above designs, BCR-Net effectively addresses the
closely related problems of imprecise localization and inaccurate
classification. Experimental results on public X-ray datasets show the
effectiveness of BCR-Net, achieving significant performance improvements to
state-of-the-art methods under limited annotations.",2024-12-25T14:37:05Z,http://arxiv.org/abs/2412.18918v1,Sanjoeng Wong
Quantitative estimates of the singular values of random i.i.d. matrices,"Let $M$ be an $n\times n$ random i.i.d. matrix. This paper studies the
deviation inequality of $s_{n-k+1}(M)$, the $k$-th smallest singular value of
$M$. In particular, when the entries of $M$ are subgaussian, we show that for
any $\gamma\in (0, 1/2), \varepsilon&gt;0$ and $\log n\le k\le c\sqrt{n}$
  \begin{align}
  \textsf{P}\{s_{n-k+1}(M)\le \frac{\varepsilon}{\sqrt{n}} \}\le \Big(
\frac{C\varepsilon}{k}\Big)^{\gamma k^{2}}+e^{-c_{1}kn}.\nonumber
  \end{align}
  This result improves an existing result of Nguyen, which obtained a deviation
inequality of $s_{n-k+1}(M)$ with $(C\varepsilon/k)^{\gamma k^{2}}+e^{-cn}$
decay.",2024-12-25T14:01:39Z,http://arxiv.org/abs/2412.18912v1,"Guozheng Dai, Zhonggen Su, Hanchao Wang"
Relaxation behavior near the first-order phase transition line,"Using the Metropolis algorithm, we simulate the relaxation process of the
three-dimensional kinetic Ising model. Starting from a random initial
configuration, we first present the average equilibration time across the
entire phase boundary. It is observed that the average equilibration time
increases significantly as the temperature decreases from the critical
temperature ($T_{\rm c}$). The average equilibration time along the first-order
phase transition (1st-PT) line exhibits an ultra-slow relaxation. We also
investigate the dynamic scaling behavior with system sizes, and find that
dynamic scaling holds not only at $T_{\rm c}$, but also below $T_{\rm c}$. The
dynamic exponent below $T_{\rm c}$ is larger than that at $T_{\rm c}$.
Additionally, we analyze the dynamic scaling of the average autocorrelation
time and find that it depends on system size only near $T_{\rm c}$, while it
becomes size-independent both above and below $T_{\rm c}$. The extremely slow
relaxation dynamics observed near the 1st-PT is attributed to the complex
structure of the free energy.",2024-12-25T13:56:18Z,http://arxiv.org/abs/2412.18909v1,"Xiaobing Li, Ranran Guo, Mingmei Xu, Yu Zhou, Jinghua Fu, Yuanfang Wu"
"Research Experiment on Multi-Model Comparison for Chinese Text
  Classification Tasks","With the explosive growth of Chinese text data and advancements in natural
language processing technologies, Chinese text classification has become one of
the key techniques in fields such as information retrieval and sentiment
analysis, attracting increasing attention. This paper conducts a comparative
study on three deep learning models:TextCNN, TextRNN, and FastText.specifically
for Chinese text classification tasks. By conducting experiments on the
THUCNews dataset, the performance of these models is evaluated, and their
applicability in different scenarios is discussed.",2024-12-25T13:54:40Z,http://arxiv.org/abs/2412.18908v1,JiaCheng Li
"The Rank and Singular Values of the Inhomogeneous Subgaussian Random
  Matrices","Let A be an n*n random matrix with mean zero and independent inhomogeneous
non-constant subgaussian entries. We get that for some k, the probability of
the matrix has a lower rank than $n-k$ that is sub-exponential.Furthermore, we
get a deviation inequality for the singular values of A in this paper.
  This extends earlier results of Rudelson's paper in 2024 by removing the
assumption of the identical distribution of the entries across the matrix. Our
model covers inhomogeneous matrices, allowing different subgaussian moments for
the entries as long as their subgaussian moments have a standard upper bound
  In the past advance, the assumption of i.i.d entries was required due to the
lack of least common denominators of the non-i.i.d random matrix. We can
overcome this problem using a Randomized least common denominator (RLCD) from
Livshyts in 2021.",2024-12-25T13:37:23Z,http://arxiv.org/abs/2412.18906v1,"Guozheng Dai, Zeyan Song, Hanchao Wang"
"Adversarial Training for Graph Neural Networks via Graph Subspace Energy
  Optimization","Despite impressive capability in learning over graph-structured data, graph
neural networks (GNN) suffer from adversarial topology perturbation in both
training and inference phases. While adversarial training has demonstrated
remarkable effectiveness in image classification tasks, its suitability for GNN
models has been doubted until a recent advance that shifts the focus from
transductive to inductive learning. Still, GNN robustness in the inductive
setting is under-explored, and it calls for deeper understanding of GNN
adversarial training. To this end, we propose a new concept of graph subspace
energy (GSE) -- a generalization of graph energy that measures graph stability
-- of the adjacency matrix, as an indicator of GNN robustness against topology
perturbations. To further demonstrate the effectiveness of such concept, we
propose an adversarial training method with the perturbed graphs generated by
maximizing the GSE regularization term, referred to as AT-GSE. To deal with the
local and global topology perturbations raised respectively by LRBCD and PRBCD,
we employ randomized SVD (RndSVD) and Nystrom low-rank approximation to favor
the different aspects of the GSE terms. An extensive set of experiments shows
that AT-GSE outperforms consistently the state-of-the-art GNN adversarial
training methods over different homophily and heterophily datasets in terms of
adversarial accuracy, whilst more surprisingly achieving a superior clean
accuracy on non-perturbed graphs.",2024-12-25T12:04:18Z,http://arxiv.org/abs/2412.18886v1,"Ganlin Liu, Ziling Liang, Xiaowei Huang, Xinping Yi, Shi Jin"
MotionMap: Representing Multimodality in Human Pose Forecasting,"Human pose forecasting is inherently multimodal since multiple futures exist
for an observed pose sequence. However, evaluating multimodality is challenging
since the task is ill-posed. Therefore, we first propose an alternative
paradigm to make the task well-posed. Next, while state-of-the-art methods
predict multimodality, this requires oversampling a large volume of
predictions. This raises key questions: (1) Can we capture multimodality by
efficiently sampling a smaller number of predictions? (2) Subsequently, which
of the predicted futures is more likely for an observed pose sequence? We
address these questions with MotionMap, a simple yet effective heatmap based
representation for multimodality. We extend heatmaps to represent a spatial
distribution over the space of all possible motions, where different local
maxima correspond to different forecasts for a given observation. MotionMap can
capture a variable number of modes per observation and provide confidence
measures for different modes. Further, MotionMap allows us to introduce the
notion of uncertainty and controllability over the forecasted pose sequence.
Finally, MotionMap captures rare modes that are non-trivial to evaluate yet
critical for safety. We support our claims through multiple qualitative and
quantitative experiments using popular 3D human pose datasets: Human3.6M and
AMASS, highlighting the strengths and limitations of our proposed method.
Project Page: https://www.epfl.ch/labs/vita/research/prediction/motionmap/",2024-12-25T11:47:26Z,http://arxiv.org/abs/2412.18883v1,"Reyhaneh Hosseininejad, Megh Shukla, Saeed Saadatnejad, Mathieu Salzmann, Alexandre Alahi"
"TSceneJAL: Joint Active Learning of Traffic Scenes for 3D Object
  Detection","Most autonomous driving (AD) datasets incur substantial costs for collection
and labeling, inevitably yielding a plethora of low-quality and redundant data
instances, thereby compromising performance and efficiency. Many applications
in AD systems necessitate high-quality training datasets using both existing
datasets and newly collected data. In this paper, we propose a traffic scene
joint active learning (TSceneJAL) framework that can efficiently sample the
balanced, diverse, and complex traffic scenes from both labeled and unlabeled
data. The novelty of this framework is threefold: 1) a scene sampling scheme
based on a category entropy, to identify scenes containing multiple object
classes, thus mitigating class imbalance for the active learner; 2) a
similarity sampling scheme, estimated through the directed graph representation
and a marginalize kernel algorithm, to pick sparse and diverse scenes; 3) an
uncertainty sampling scheme, predicted by a mixture density network, to select
instances with the most unclear or complex regression outcomes for the learner.
Finally, the integration of these three schemes in a joint selection strategy
yields an optimal and valuable subdataset. Experiments on the KITTI, Lyft,
nuScenes and SUScape datasets demonstrate that our approach outperforms
existing state-of-the-art methods on 3D object detection tasks with up to 12%
improvements.",2024-12-25T11:07:04Z,http://arxiv.org/abs/2412.18870v1,"Chenyang Lei, Meiying Zhang, Weiyuan Peng, Qi Hao, Chengzhong Xu, Chunlin Ji, Guang Zhou"
"Digital Twin Enhanced Deep Reinforcement Learning for Intelligent
  Omni-Surface Configurations in MU-MIMO Systems","Intelligent omni-surface (IOS) is a promising technique to enhance the
capacity of wireless networks, by reflecting and refracting the incident signal
simultaneously. Traditional IOS configuration schemes, relying on all
sub-channels' channel state information and user equipments' mobility, are
difficult to implement in complex realistic systems. Existing works attempt to
address this issue employing deep reinforcement learning (DRL), but this method
requires a lot of trial-and-error interactions with the external environment
for efficient results and thus cannot satisfy the real-time decision-making. To
enable model-free and real-time IOS control, this paper puts forth a new
framework that integrates DRL and digital twins. DeepIOS, a DRL based IOS
configuration scheme with the goal of maximizing the sum data rate, is first
developed to jointly optimize the phase-shift and amplitude of IOS in
multi-user multiple-input-multiple-output systems. Thereafter, to further
reduce the computational complexity, DeepIOS introduces an action branch
architecture, which separately decides two optimization variables in parallel.
Finally, a digital twin module is constructed through supervised learning as a
pre-verification platform for DeepIOS, such that the decision-making's
real-time can be guaranteed. The formulated framework is a closed-loop system,
in which the physical space provides data to establish and calibrate the
digital space, while the digital space generates experience samples for DeepIOS
training and sends the trained parameters to the IOS controller for
configurations. Numerical results show that compared with random and MAB
schemes, the proposed framework attains a higher data rate and is more robust
to different settings. Furthermore, the action branch architecture reduces
DeepIOS's computational complexity, and the digital twin module improves the
convergence speed and run-time.",2024-12-25T09:53:07Z,http://arxiv.org/abs/2412.18856v1,"Xiaowen Ye, Xianghao Yu, Liqun Fu"
"Dynamics of Topological Defects in a Rashba Spin-Orbit Coupled
  Bose-Einstein Condensate","We investigate the quench dynamics of a two-dimensional Rashba spin-orbit
coupled Bose-Einstein condensate. Our study focuses on quenching the system
from a zero-momentum phase to a plane-wave phase. During this quench,
topological defects emerge in the form of vortices. These vortices and
anti-vortices exhibit a random spatial distribution with equal numbers,
mirroring the core principles of Kosterlitz-Thouless physics. In a uniform
system, we observe an exponential scaling of both the vortex production time
and the vortex number with the quench rate, consistent with the conventional
Kibble-Zurek mechanism. The decay of which adheres to a logarithmic law,
aligning with experimental observations.",2024-12-25T09:31:42Z,http://arxiv.org/abs/2412.18850v1,"Sheng Liu, Yong-Sheng Zhang"
"SWAG: Long-term Surgical Workflow Prediction with Generative-based
  Anticipation","While existing recognition approaches excel at identifying current surgical
phases, they provide limited foresight into future procedural steps,
restricting their intraoperative utility. Similarly, current anticipation
methods are constrained to predicting short-term events or singular future
occurrences, neglecting the dynamic and sequential nature of surgical
workflows. To address these limitations, we propose SWAG (Surgical Workflow
Anticipative Generation), a unified framework for phase recognition and
long-term anticipation of surgical workflows. SWAG employs two generative
decoding methods -- single-pass (SP) and auto-regressive (AR) -- to predict
sequences of future surgical phases. A novel prior knowledge embedding
mechanism enhances the accuracy of anticipatory predictions. The framework
addresses future phase classification and remaining time regression tasks.
Additionally, a regression-to-classification (R2C) method is introduced to map
continuous predictions to discrete temporal segments. SWAG's performance was
evaluated on the Cholec80 and AutoLaparo21 datasets. The single-pass
classification model with prior knowledge embeddings (SWAG-SP\*) achieved
53.5\% accuracy in 15-minute anticipation on AutoLaparo21, while the R2C model
reached 60.8\% accuracy on Cholec80. SWAG's single-pass regression approach
outperformed existing methods for remaining time prediction, achieving weighted
mean absolute errors of 0.32 and 0.48 minutes for 2- and 3-minute horizons,
respectively. SWAG demonstrates versatility across classification and
regression tasks, offering robust tools for real-time surgical workflow
anticipation. By unifying recognition and anticipatory capabilities, SWAG
provides actionable predictions to enhance intraoperative decision-making.",2024-12-25T09:29:57Z,http://arxiv.org/abs/2412.18849v1,"Maxence Boels, Yang Liu, Prokar Dasgupta, Alejandro Granados, Sebastien Ourselin"
Machine Learning-Based Detection of Pump-and-Dump Schemes in Real-Time,"Cryptocurrency markets often face manipulation through prevalent
pump-and-dump (P&amp;D) schemes, where self-organized Telegram groups, some
exceeding two million members, artificially inflate target cryptocurrency
prices. These groups sell premium access to inside information, worsening
information asymmetry and financial risks for subscribers and all investors.
This paper presents a real-time prediction pipeline to forecast target coins
and alert investors to possible P&amp;D schemes. In a Poloniex case study, the
model accurately identified the target coin among the top five from 50 random
coins in 24 out of 43 (55.81%) P&amp;D events. The pipeline uses advanced natural
language processing (NLP) to classify Telegram messages, identifying 2,079 past
pump events and detecting new ones in real-time. Our analysis also evaluates
the susceptibility of token standards - ERC-20, ERC-721, BRC-20, Inscriptions,
and Runes - to manipulation and identifies exchanges commonly involved in P&amp;D
schemes.",2024-12-25T09:23:36Z,http://arxiv.org/abs/2412.18848v1,"Manuel Bolz, Kevin Bründler, Liam Kane, Panagiotis Patsias, Liam Tessendorf, Krzysztof Gogol, Taehoon Kim, Claudio Tessone"
Quantifying the memory and dynamical stability of magnetar bursts,"The time series of energy and waiting time of magnetar bursts carry important
information about the source activity. In this paper, we investigate the memory
and dynamical stability of magnetar bursts from four soft gamma repeater (SGR)
sources: SGR 1806$-$20, SGR 1900+14, SGR J1935+2154 and SGR J1550$-$5418. Based
on the rescaled range analysis, we quantify the memory in magnetar bursts for
the first time and find that there exists long-term memory in the time series
of both waiting time and energy. We investigate the dynamical stability in the
context of randomness and chaos. For all the four SGR samples, we find that the
waiting time is not completely random, but the energy of two SGRs is consistent
with a total random organization. Furthermore, both waiting time and energy
exhibits weak chaos. We also find no significant difference between SGRs and
repeating fast radio bursts (FRBs) in the randomness-chaos phase space. The
statistical similarity between SGRs and repeating FRBs hints that there may be
potential physical connection between these two phenomena.",2024-12-25T08:22:36Z,http://arxiv.org/abs/2412.18821v1,"Yu Sang, Hai-Nan Lin"
A Tractable Approach for Queueing Analysis on Buffer-Aware Scheduling,"Low-latency communication has recently attracted considerable attention owing
to its potential of enabling delay-sensitive services in next-generation
industrial cyber-physical systems. To achieve target average or maximum delay
given random arrivals and time-varying channels, buffer-aware scheduling is
expected to play a vital role. Evaluating and optimizing buffer-aware
scheduling relies on its queueing analysis, while existing tools are not
sufficiently tractable. Particularly, Markov chain and Monte-Carlo based
approaches are computationally intensive, while large deviation theory (LDT)
and extreme value theory (EVT) fail in providing satisfactory accuracy in the
small-queue-length (SQL) regime. To tackle these challenges, a tractable yet
accurate queueing analysis is presented by judiciously bridging Markovian
analysis for the computationally manageable SQL regime and LDT/EVT for
large-queue-length (LQL) regime where approximation error diminishes
asymptotically. Specifically, we leverage censored Markov chain augmentation to
approximate the original one in the SQL regime, while a piecewise approach is
conceived to apply LDT/EVT across various queue-length intervals with different
scheduling parameters. Furthermore, we derive closed-form bounds on
approximation errors, validating the rigor and accuracy of our approach. As a
case study, the approach is applied to analytically analyze a
Lyapunov-drift-based cross-layer scheduling for wireless transmissions.
Numerical results demonstrate its potential in balancing accuracy and
complexity.",2024-12-25T07:46:27Z,http://arxiv.org/abs/2412.18812v1,"Lintao Li, Wei Chen"
Provable Uncertainty Decomposition via Higher-Order Calibration,"We give a principled method for decomposing the predictive uncertainty of a
model into aleatoric and epistemic components with explicit semantics relating
them to the real-world data distribution. While many works in the literature
have proposed such decompositions, they lack the type of formal guarantees we
provide. Our method is based on the new notion of higher-order calibration,
which generalizes ordinary calibration to the setting of higher-order
predictors that predict mixtures over label distributions at every point. We
show how to measure as well as achieve higher-order calibration using access to
$k$-snapshots, namely examples where each point has $k$ independent conditional
labels. Under higher-order calibration, the estimated aleatoric uncertainty at
a point is guaranteed to match the real-world aleatoric uncertainty averaged
over all points where the prediction is made. To our knowledge, this is the
first formal guarantee of this type that places no assumptions whatsoever on
the real-world data distribution. Importantly, higher-order calibration is also
applicable to existing higher-order predictors such as Bayesian and ensemble
models and provides a natural evaluation metric for such models. We demonstrate
through experiments that our method produces meaningful uncertainty
decompositions for image classification.",2024-12-25T07:26:36Z,http://arxiv.org/abs/2412.18808v1,"Gustaf Ahdritz, Aravind Gollakota, Parikshit Gopalan, Charlotte Peale, Udi Wieder"
"Ister: Inverted Seasonal-Trend Decomposition Transformer for Explainable
  Multivariate Time Series Forecasting","In long-term time series forecasting, Transformer-based models have achieved
great success, due to its ability to capture long-range dependencies. However,
existing transformer-based methods face challenges in accurately identifying
which variables play a pivotal role in the prediction process and tend to
overemphasize noisy channels, thereby limiting the interpretability and
practical effectiveness of the models. Besides, it faces scalability issues due
to quadratic computational complexity of self-attention. In this paper, we
propose a new model named Inverted Seasonal-Trend Decomposition Transformer
(Ister), which addresses these challenges in long-term multivariate time series
forecasting by designing an improved Transformer-based structure. Ister firstly
decomposes original time series into seasonal and trend components. Then we
propose a new Dot-attention mechanism to process the seasonal component, which
improves both accuracy, computation complexity and interpretability. Upon
completion of the training phase, it allows users to intuitively visualize the
significance of each feature in the overall prediction. We conduct
comprehensive experiments, and the results show that Ister achieves
state-of-the-art (SOTA) performance on multiple datasets, surpassing existing
models in long-term prediction tasks.",2024-12-25T06:37:19Z,http://arxiv.org/abs/2412.18798v1,"Fanpu Cao, Shu Yang, Zhengjian Chen, Ye Liu, Laizhong Cui"
Torque-Aware Momentum,"Efficiently exploring complex loss landscapes is key to the performance of
deep neural networks. While momentum-based optimizers are widely used in
state-of-the-art setups, classical momentum can still struggle with large,
misaligned gradients, leading to oscillations. To address this, we propose
Torque-Aware Momentum (TAM), which introduces a damping factor based on the
angle between the new gradients and previous momentum, stabilizing the update
direction during training. Empirical results show that TAM, which can be
combined with both SGD and Adam, enhances exploration, handles distribution
shifts more effectively, and improves generalization performance across various
tasks, including image classification and large language model fine-tuning,
when compared to classical momentum-based optimizers.",2024-12-25T05:58:07Z,http://arxiv.org/abs/2412.18790v1,"Pranshu Malviya, Goncalo Mordido, Aristide Baratin, Reza Babanezhad Harikandeh, Gintare Karolina Dziugaite, Razvan Pascanu, Sarath Chandar"
"Computational Analysis of Yaredawi YeZema Silt in Ethiopian Orthodox
  Tewahedo Church Chants","Despite its musicological, cultural, and religious significance, the
Ethiopian Orthodox Tewahedo Church (EOTC) chant is relatively underrepresented
in music research. Historical records, including manuscripts, research papers,
and oral traditions, confirm Saint Yared's establishment of three canonical
EOTC chanting modes during the 6th century. This paper attempts to investigate
the EOTC chants using music information retrieval (MIR) techniques. Among the
research questions regarding the analysis and understanding of EOTC chants,
Yaredawi YeZema Silt, namely the mode of chanting adhering to Saint Yared's
standards, is of primary importance. Therefore, we consider the task of
Yaredawi YeZema Silt classification in EOTC chants by introducing a new dataset
and showcasing a series of classification experiments for this task. Results
show that using the distribution of stabilized pitch contours as the feature
representation on a simple neural network-based classifier becomes an effective
solution. The musicological implications and insights of such results are
further discussed through a comparative study with the previous ethnomusicology
literature on EOTC chants. By making this dataset publicly accessible, we aim
to promote future exploration and analysis of EOTC chants and highlight
potential directions for further research, thereby fostering a deeper
understanding and preservation of this unique spiritual and cultural heritage.",2024-12-25T05:42:56Z,http://arxiv.org/abs/2412.18788v1,"Mequanent Argaw Muluneh, Yan-Tsung Peng, Li Su"
"Robustness Evaluation of Offline Reinforcement Learning for Robot
  Control Against Action Perturbations","Offline reinforcement learning, which learns solely from datasets without
environmental interaction, has gained attention. This approach, similar to
traditional online deep reinforcement learning, is particularly promising for
robot control applications. Nevertheless, its robustness against real-world
challenges, such as joint actuator faults in robots, remains a critical
concern. This study evaluates the robustness of existing offline reinforcement
learning methods using legged robots from OpenAI Gym based on average episodic
rewards. For robustness evaluation, we simulate failures by incorporating both
random and adversarial perturbations, representing worst-case scenarios, into
the joint torque signals. Our experiments show that existing offline
reinforcement learning methods exhibit significant vulnerabilities to these
action perturbations and are more vulnerable than online reinforcement learning
methods, highlighting the need for more robust approaches in this field.",2024-12-25T05:02:22Z,http://arxiv.org/abs/2412.18781v1,"Shingo Ayabe, Takuto Otomo, Hiroshi Kera, Kazuhiko Kawamoto"
"Integrating Zero-Shot Classification to Advance Long COVID Literature: A
  Systematic Social Media-Centered Review","Long COVID continues to challenge public health by affecting a significant
segment of individuals who have recovered from acute SARS-CoV-2 infection yet
endure prolonged and often debilitating symptoms. Social media has emerged as a
vital resource for those seeking real-time information, peer support, and
validating their health concerns related to Long COVID. This paper examines
recent works focusing on mining, analyzing, and interpreting user-generated
content on social media platforms such as Twitter, Reddit, Facebook, and
YouTube to capture the broader discourse on persistent post-COVID conditions. A
novel transformer-based zero-shot learning approach serves as the foundation
for classifying research papers in this area into four primary categories:
Clinical or Symptom Characterization, Advanced NLP or Computational Methods,
Policy, Advocacy, or Public Health Communication, and Online Communities and
Social Support. This methodology showcases the adaptability of advanced
language models in categorizing research papers without predefined training
labels, thus enabling a more rapid and scalable assessment of existing
literature. This review highlights the multifaceted nature of Long COVID
research, where computational techniques applied to social media data reveal
insights into narratives of individuals suffering from Long COVID. This review
also demonstrates the capacity of social media analytics to inform clinical
practice and contribute to policy-making related to Long COVID.",2024-12-25T05:01:17Z,http://arxiv.org/abs/2412.18779v1,Nirmalya Thakur
"Unified Local and Global Attention Interaction Modeling for Vision
  Transformers","We present a novel method that extends the self-attention mechanism of a
vision transformer (ViT) for more accurate object detection across diverse
datasets. ViTs show strong capability for image understanding tasks such as
object detection, segmentation, and classification. This is due in part to
their ability to leverage global information from interactions among visual
tokens. However, the self-attention mechanism in ViTs are limited because they
do not allow visual tokens to exchange local or global information with
neighboring features before computing global attention. This is problematic
because tokens are treated in isolation when attending (matching) to other
tokens, and valuable spatial relationships are overlooked. This isolation is
further compounded by dot-product similarity operations that make tokens from
different semantic classes appear visually similar. To address these
limitations, we introduce two modifications to the traditional self-attention
framework; a novel aggressive convolution pooling strategy for local feature
mixing, and a new conceptual attention transformation to facilitate interaction
and feature exchange between semantic concepts. Experimental results
demonstrate that local and global information exchange among visual features
before self-attention significantly improves performance on challenging object
detection tasks and generalizes across multiple benchmark datasets and
challenging medical datasets. We publish source code and a novel dataset of
cancerous tumors (chimeric cell clusters).",2024-12-25T04:53:19Z,http://arxiv.org/abs/2412.18778v1,"Tan Nguyen, Coy D. Heldermon, Corey Toler-Franklin"
"Arc-transitive maps with edge number coprime to the Euler characteristic
  -- I","This is one of a series of papers which aim towards a classification of
edge-transitive maps of which the Euler characteristic and the edge number are
coprime. This one establishes a framework and carries out the classification
work for arc-transitive maps with solvable automorphism groups, which
illustrates how the edge number impacts on the Euler characteristic for maps.
The classification is involved with the constructions of various new and
interesting arc-regular maps.",2024-12-25T03:07:33Z,http://arxiv.org/abs/2412.18758v1,"Caiheng Li, Luyi Liu"
"Towards a Statistical Understanding of Neural Networks: Beyond the
  Neural Tangent Kernel Theories","A primary advantage of neural networks lies in their feature learning
characteristics, which is challenging to theoretically analyze due to the
complexity of their training dynamics. We propose a new paradigm for studying
feature learning and the resulting benefits in generalizability. After
reviewing the neural tangent kernel (NTK) theory and recent results in kernel
regression, which address the generalization issue of sufficiently wide neural
networks, we examine limitations and implications of the fixed kernel theory
(as the NTK theory) and review recent theoretical advancements in feature
learning. Moving beyond the fixed kernel/feature theory, we consider neural
networks as adaptive feature models. Finally, we propose an over-parameterized
Gaussian sequence model as a prototype model to study the feature learning
characteristics of neural networks.",2024-12-25T03:03:58Z,http://arxiv.org/abs/2412.18756v1,"Haobo Zhang, Jianfa Lai, Yicheng Li, Qian Lin, Jun S. Liu"
The moments of the spectral form factor in SYK,"In chaotic quantum systems the spectral form factor exhibits a universal
linear ramp and plateau structure with superimposed erratic oscillations. The
mean signal and the statistics of the noise can be probed by the moments of the
spectral form factor, also known as higher-point spectral form factors. We
identify saddle points in the SYK model that describe the moments during the
ramp region. Perturbative corrections around the saddle point indicate that SYK
mimics random matrix statistics for the low order moments, while large
deviations for the high order moments arise from fluctuations near the edge of
the spectrum. The leading correction scales inversely with the number of random
parameters in the SYK Hamiltonian and is amplified in a sparsified version of
the SYK model, which we study numerically, even in regimes where a linear ramp
persists. Finally, we study the $q=2$ SYK model, whose spectral form factor
exhibits an exponential ramp with increased noise. These findings reveal how
deviations from random matrix universality arise in disordered systems and
motivate their interpretation from a bulk gravitational perspective.",2024-12-25T01:50:28Z,http://arxiv.org/abs/2412.18737v1,"Andrea Legramandi, Neil Talwar"
"Predicting Time Series of Networked Dynamical Systems without Knowing
  Topology","Many real-world complex systems, such as epidemic spreading networks and
ecosystems, can be modeled as networked dynamical systems that produce
multivariate time series. Learning the intrinsic dynamics from observational
data is pivotal for forecasting system behaviors and making informed decisions.
However, existing methods for modeling networked time series often assume known
topologies, whereas real-world networks are typically incomplete or inaccurate,
with missing or spurious links that hinder precise predictions. Moreover, while
networked time series often originate from diverse topologies, the ability of
models to generalize across topologies has not been systematically evaluated.
To address these gaps, we propose a novel framework for learning network
dynamics directly from observed time-series data, when prior knowledge of graph
topology or governing dynamical equations is absent. Our approach leverages
continuous graph neural networks with an attention mechanism to construct a
latent topology, enabling accurate reconstruction of future trajectories for
network states. Extensive experiments on real and synthetic networks
demonstrate that our model not only captures dynamics effectively without
topology knowledge but also generalizes to unseen time series originating from
diverse topologies.",2024-12-25T01:39:04Z,http://arxiv.org/abs/2412.18734v1,"Yanna Ding, Zijie Huang, Malik Magdon-Ismail, Jianxi Gao"
"Simi-SFX: A similarity-based conditioning method for controllable sound
  effect synthesis","Generating sound effects with controllable variations is a challenging task,
traditionally addressed using sophisticated physical models that require
in-depth knowledge of signal processing parameters and algorithms. In the era
of generative and large language models, text has emerged as a common,
human-interpretable interface for controlling sound synthesis. However, the
discrete and qualitative nature of language tokens makes it difficult to
capture subtle timbral variations across different sounds. In this research, we
propose a novel similarity-based conditioning method for sound synthesis,
leveraging differentiable digital signal processing (DDSP). This approach
combines the use of latent space for learning and controlling audio timbre with
an intuitive guiding vector, normalized within the range [0,1], to encode
categorical acoustic information. By utilizing pre-trained audio representation
models, our method achieves expressive and fine-grained timbre control. To
benchmark our approach, we introduce two sound effect datasets--Footstep-set
and Impact-set--designed to evaluate both controllability and sound quality.
Regression analysis demonstrates that the proposed similarity score effectively
controls timbre variations and enables creative applications such as timbre
interpolation between discrete classes. Our work provides a robust and
versatile framework for sound effect synthesis, bridging the gap between
traditional signal processing and modern machine learning techniques.",2024-12-25T00:14:50Z,http://arxiv.org/abs/2412.18710v1,"Yunyi Liu, Craig Jin"
"SurvAttack: Black-Box Attack On Survival Models through
  Ontology-Informed EHR Perturbation","Survival analysis (SA) models have been widely studied in mining electronic
health records (EHRs), particularly in forecasting the risk of critical
conditions for prioritizing high-risk patients. However, their vulnerability to
adversarial attacks is much less explored in the literature. Developing
black-box perturbation algorithms and evaluating their impact on
state-of-the-art survival models brings two benefits to medical applications.
First, it can effectively evaluate the robustness of models in pre-deployment
testing. Also, exploring how subtle perturbations would result in significantly
different outcomes can provide counterfactual insights into the clinical
interpretation of model prediction. In this work, we introduce SurvAttack, a
novel black-box adversarial attack framework leveraging subtle clinically
compatible, and semantically consistent perturbations on longitudinal EHRs to
degrade survival models' predictive performance. We specifically develop a
greedy algorithm to manipulate medical codes with various adversarial actions
throughout a patient's medical history. Then, these adversarial actions are
prioritized using a composite scoring strategy based on multi-aspect
perturbation quality, including saliency, perturbation stealthiness, and
clinical meaningfulness. The proposed adversarial EHR perturbation algorithm is
then used in an efficient SA-specific strategy to attack a survival model when
estimating the temporal ranking of survival urgency for patients. To
demonstrate the significance of our work, we conduct extensive experiments,
including baseline comparisons, explainability analysis, and case studies. The
experimental results affirm our research's effectiveness in illustrating the
vulnerabilities of patient survival models, model interpretation, and
ultimately contributing to healthcare quality.",2024-12-24T23:35:42Z,http://arxiv.org/abs/2412.18706v1,"Mohsen Nayebi Kerdabadi, Arya Hadizadeh Moghaddam, Bin Liu, Mei Liu, Zijun Yao"
Exceptional X-ray activity in BL Lacertae,"BL Lacertae is a unique blazar for which the X-ray band can cover either the
synchrotron or the inverse Compton, or both parts of the broadband spectral
energy distribution. In the latter case, when the spectral upturn is located in
the X-ray range, it allows contemporaneous study of the low- and high-energy
ends of the electron distribution function. In this work, we study spectral and
temporal variability using X-ray and optical observations of the blazar
performed with the Neil Gehrels Swift Observatory from 2020 to 2023. The large
set of observational data reveals intensive flaring activity, accompanied by
spectral changes in both spectral branches.
  We conclude that the low-energy and high-energy ends of the particle
distribution function are characterised by similar variability scales.
Additionally, the hard X-ray observations of BL Lacertae performed with the
Nuclear Spectroscopic Telescope Array (NuSTAR) confirm a concave spectral
curvature for some epochs of the blazar activity and reveal that it can be
shifted up to energies of as high as 8 keV. The time-resolved spectral analysis
allows us to disentangle X-ray spectral variability features of the synchrotron
from inverse Compton components.
  Despite significant variability of both spectral components, we find only
small changes in the position of the spectral upturn. The different slopes and
shapes of the X-ray spectrum of BL Lacertae demonstrate that the classification
of this source is not constant, and BL Lacertae can exhibit features of either
high-, intermediate-, or low-energy peaked blazar in different epochs of
observation. This also indicates that the spectral upturn for this blazar can
be located not only in the X-ray range of 0.3-10 keV, but also at lower or
higher energies.",2024-12-24T21:05:33Z,http://arxiv.org/abs/2412.18680v1,"Alicja Wierzcholska, Stefan Wagner"
"State-of-the-Art Underwater Vehicles and Technologies Enabling Smart
  Ocean: Survey and Classifications","The exploration and sustainable use of marine environments have become
increasingly critical as oceans cover over 70% of surface of Earth. This paper
provides a comprehensive survey and classification of state-of-the-art
underwater vehicles (UVs) and supporting technologies essential for enabling a
smart ocean. We categorize UVs into several types, including remotely operated
vehicles (ROVs), autonomous underwater vehicles (AUVs), hybrid underwater
vehicles (HUVs), unmanned surface vehicles (USVs), and underwater bionic
vehicles (UBVs). These technologies are fundamental in a wide range of
applications, such as environmental monitoring, deep-sea exploration, defense,
and underwater infrastructure inspection. Additionally, the paper explores
advancements in underwater communication technologies, namely acoustic,
optical, and hybrid systems, as well as key support facilities, including
submerged buoys, underwater docking stations, and wearable underwater
localization systems. By classifying the vehicles and analyzing their
technological capabilities and limitations, this work aims to guide future
developments in underwater exploration and monitoring, addressing challenges
such as energy efficiency, communication limitations, and environmental
adaptability. The paper concludes by discussing the integration of artificial
intelligence and machine learning in enhancing the autonomy and operational
efficiency of these systems, paving the way for the realization of a fully
interconnected and sustainable Smart Ocean.",2024-12-24T20:00:15Z,http://arxiv.org/abs/2412.18667v1,"Jiajie Xu, Xabier Irigoien, Mohamed-Slim Alouini"
"Orient Anything: Learning Robust Object Orientation Estimation from
  Rendering 3D Models","Orientation is a key attribute of objects, crucial for understanding their
spatial pose and arrangement in images. However, practical solutions for
accurate orientation estimation from a single image remain underexplored. In
this work, we introduce Orient Anything, the first expert and foundational
model designed to estimate object orientation in a single- and free-view image.
Due to the scarcity of labeled data, we propose extracting knowledge from the
3D world. By developing a pipeline to annotate the front face of 3D objects and
render images from random views, we collect 2M images with precise orientation
annotations. To fully leverage the dataset, we design a robust training
objective that models the 3D orientation as probability distributions of three
angles and predicts the object orientation by fitting these distributions.
Besides, we employ several strategies to improve synthetic-to-real transfer.
Our model achieves state-of-the-art orientation estimation accuracy in both
rendered and real images and exhibits impressive zero-shot ability in various
scenarios. More importantly, our model enhances many applications, such as
comprehension and generation of complex spatial concepts and 3D object pose
adjustment.",2024-12-24T18:58:43Z,http://arxiv.org/abs/2412.18605v1,"Zehan Wang, Ziang Zhang, Tianyu Pang, Chao Du, Hengshuang Zhao, Zhou Zhao"
LatentCRF: Continuous CRF for Efficient Latent Diffusion,"Latent Diffusion Models (LDMs) produce high-quality, photo-realistic images,
however, the latency incurred by multiple costly inference iterations can
restrict their applicability. We introduce LatentCRF, a continuous Conditional
Random Field (CRF) model, implemented as a neural network layer, that models
the spatial and semantic relationships among the latent vectors in the LDM. By
replacing some of the computationally-intensive LDM inference iterations with
our lightweight LatentCRF, we achieve a superior balance between quality, speed
and diversity. We increase inference efficiency by 33% with no loss in image
quality or diversity compared to the full LDM. LatentCRF is an easy add-on,
which does not require modifying the LDM.",2024-12-24T18:51:11Z,http://arxiv.org/abs/2412.18596v1,"Kanchana Ranasinghe, Sadeep Jayasumana, Andreas Veit, Ayan Chakrabarti, Daniel Glasner, Michael S Ryoo, Srikumar Ramalingam, Sanjiv Kumar"
"ClassifyViStA:WCE Classification with Visual understanding through
  Segmentation and Attention","Gastrointestinal (GI) bleeding is a serious medical condition that presents
significant diagnostic challenges, particularly in settings with limited access
to healthcare resources. Wireless Capsule Endoscopy (WCE) has emerged as a
powerful diagnostic tool for visualizing the GI tract, but it requires
time-consuming manual analysis by experienced gastroenterologists, which is
prone to human error and inefficient given the increasing number of patients.To
address this challenge, we propose ClassifyViStA, an AI-based framework
designed for the automated detection and classification of bleeding and
non-bleeding frames from WCE videos. The model consists of a standard
classification path, augmented by two specialized branches: an implicit
attention branch and a segmentation branch.The attention branch focuses on the
bleeding regions, while the segmentation branch generates accurate segmentation
masks, which are used for classification and interpretability. The model is
built upon an ensemble of ResNet18 and VGG16 architectures to enhance
classification performance. For the bleeding region detection, we implement a
Soft Non-Maximum Suppression (Soft NMS) approach with YOLOv8, which improves
the handling of overlapping bounding boxes, resulting in more accurate and
nuanced detections.The system's interpretability is enhanced by using the
segmentation masks to explain the classification results, offering insights
into the decision-making process similar to the way a gastroenterologist
identifies bleeding regions. Our approach not only automates the detection of
GI bleeding but also provides an interpretable solution that can ease the
burden on healthcare professionals and improve diagnostic efficiency. Our code
is available at ClassifyViStA.",2024-12-24T18:45:14Z,http://arxiv.org/abs/2412.18591v1,"S. Balasubramanian, Ammu Abhishek, Yedu Krishna, Darshan Gera"
Text-Driven Tumor Synthesis,"Tumor synthesis can generate examples that AI often misses or over-detects,
improving AI performance by training on these challenging cases. However,
existing synthesis methods, which are typically unconditional -- generating
images from random variables -- or conditioned only by tumor shapes, lack
controllability over specific tumor characteristics such as texture,
heterogeneity, boundaries, and pathology type. As a result, the generated
tumors may be overly similar or duplicates of existing training data, failing
to effectively address AI's weaknesses. We propose a new text-driven tumor
synthesis approach, termed TextoMorph, that provides textual control over tumor
characteristics. This is particularly beneficial for examples that confuse the
AI the most, such as early tumor detection (increasing Sensitivity by +8.5%),
tumor segmentation for precise radiotherapy (increasing DSC by +6.3%), and
classification between benign and malignant tumors (improving Sensitivity by
+8.2%). By incorporating text mined from radiology reports into the synthesis
process, we increase the variability and controllability of the synthetic
tumors to target AI's failure cases more precisely. Moreover, TextoMorph uses
contrastive learning across different texts and CT scans, significantly
reducing dependence on scarce image-report pairs (only 141 pairs used in this
study) by leveraging a large corpus of 34,035 radiology reports. Finally, we
have developed rigorous tests to evaluate synthetic tumors, including
Text-Driven Visual Turing Test and Radiomics Pattern Analysis, showing that our
synthetic tumors is realistic and diverse in texture, heterogeneity,
boundaries, and pathology.",2024-12-24T18:43:09Z,http://arxiv.org/abs/2412.18589v1,"Xinran Li, Yi Shuai, Chen Liu, Qi Chen, Qilong Wu, Pengfei Guo, Dong Yang, Can Zhao, Pedro R. A. S. Bassi, Daguang Xu, Kang Wang, Yang Yang, Alan Yuille, Zongwei Zhou"
Relativistic Lévy processes,"In this contribution, we investigate how to correctly describe sums of
independent and identically distributed random velocities in the theory of
special relativity. We derive a one-dimensional probability distribution of
velocities stable under relativistic velocity addition. In a given system, this
allows identifying distinct physical regimes in terms of the distribution's
concavity at the origin and the probability of measuring relativistic
velocities. These features provide a protocol to assess the relevance of
stochastic relativistic effects in actual experiments. As examples, we find
agreement with previous results about heavy-ion diffusion and show that our
findings are consistent with the distribution of momentum deviations observed
in measurements of antiproton cooling.",2024-12-24T18:17:27Z,http://arxiv.org/abs/2412.18581v1,"Lucas G. B. de Souza, M. G. E. da Luz, E. P. Raposo, Evaldo M. F. Curado, G. M. Viswanathan"
Randomized Benchmarking with Synthetic Quantum Circuits,"Randomized benchmarking (RB) comprises a set of mature and widely used
techniques for assessing the quality of operations on a quantum
information-processing system. Modern RB protocols for multiqubit systems
extract physically relevant error rates by exploiting the structure of the
group representation generated by the set of benchmarked operations. However,
existing techniques become prohibitively inefficient for representations that
are highly reducible yet decompose into irreducible subspaces of high
dimension. These situations prevail when benchmarking high-dimensional systems
such as qudits or bosonic modes, where experimental control is limited to
implementing a small subset of all possible unitary operations. In this work,
we introduce a broad framework for enhancing the sample efficiency of RB that
is sufficiently powerful to extend the practical reach of RB beyond the
multiqubit setting. Our strategy, which applies to any benchmarking group, uses
""synthetic"" quantum circuits with classical post-processing of both input and
output data to leverage the full structure of reducible superoperator
representations. To demonstrate the efficacy of our approach, we develop a
detailed theory of RB for systems with rotational symmetry. Such systems carry
a natural action of the group $\text{SU}(2)$, and they form the basis for
several novel quantum error-correcting codes. We show that, for experimentally
accessible high-spin systems, synthetic RB protocols can reduce the complexity
of measuring rotationally invariant error rates by more than two orders of
magnitude relative to standard approaches such as character RB.",2024-12-24T18:10:00Z,http://arxiv.org/abs/2412.18578v1,"Yale Fan, Riley Murray, Thaddeus D. Ladd, Kevin Young, Robin Blume-Kohout"
Von Neumann Entropy and Quantum Algorithmic Randomness,"A state $\rho=(\rho_n)_{n=1}^{\infty}$ is a sequence such that $\rho_n$ is a
density matrix on $n$ qubits. It formalizes the notion of an infinite sequence
of qubits. The von Neumann entropy $H(d)$ of a density matrix $d$ is the
Shannon entropy of its eigenvalue distribution. We show: (1) If $\rho$ is a
computable quantum Schnorr random state then $\lim_n [H(\rho_n )/n] = 1$. (2)
We define quantum s-tests for $s\in [0,1]$, show that $\liminf_n
[H(\rho_n)/n]\geq \{ s: \rho$ is covered by a quantum s-test $\}$ for
computable $\rho$ and construct states where this inequality is an equality.
(3) If $\exists c \exists^\infty n H(\rho_n)&gt; n-c$ then $\rho$ is strong
quantum random. Strong quantum randomness is a randomness notion which implies
quantum Schnorr randomness relativized to any oracle. (4) A computable state
$(\rho_n)_{n=1}^{\infty}$ is quantum Schnorr random iff the family of
distributions of the $\rho_n$'s is uniformly integrable. We show that the
implications in (1) and (3) are strict.",2024-12-24T18:09:45Z,http://arxiv.org/abs/2412.18646v1,Tejas Bhojraj
"Machine Learning Approaches to the Shafarevich-Tate Group of Elliptic
  Curves","We train machine learning models to predict the order of the Shafarevich-Tate
group of an elliptic curve over $\mathbb{Q}$. Building on earlier work of He,
Lee, and Oliver, we show that a feed-forward neural network classifier trained
on subsets of the invariants arising in the Birch--Swinnerton-Dyer conjectural
formula yields higher accuracies ($&gt; 0.9$) than any model previously studied.
In addition, we develop a regression model that may be used to predict orders
of this group not seen during training and apply this to the elliptic curve of
rank 29 recently discovered by Elkies and Klagsbrun. Finally we conduct some
exploratory data analyses and visualizations on our dataset. We use the
elliptic curve dataset from the L-functions and modular forms database (LMFDB).",2024-12-24T18:01:19Z,http://arxiv.org/abs/2412.18576v1,"Angelica Babei, Barinder S. Banwait, AJ Fong, Xiaoyu Huang, Deependra Singh"
HNCI: High-Dimensional Network Causal Inference,"The problem of evaluating the effectiveness of a treatment or policy commonly
appears in causal inference applications under network interference. In this
paper, we suggest the new method of high-dimensional network causal inference
(HNCI) that provides both valid confidence interval on the average direct
treatment effect on the treated (ADET) and valid confidence set for the
neighborhood size for interference effect. We exploit the model setting in
Belloni et al. (2022) and allow certain type of heterogeneity in node
interference neighborhood sizes. We propose a linear regression formulation of
potential outcomes, where the regression coefficients correspond to the
underlying true interference function values of nodes and exhibit a latent
homogeneous structure. Such a formulation allows us to leverage existing
literature from linear regression and homogeneity pursuit to conduct valid
statistical inferences with theoretical guarantees. The resulting confidence
intervals for the ADET are formally justified through asymptotic normalities
with estimable variances. We further provide the confidence set for the
neighborhood size with theoretical guarantees exploiting the repro samples
approach. The practical utilities of the newly suggested methods are
demonstrated through simulation and real data examples.",2024-12-24T17:41:41Z,http://arxiv.org/abs/2412.18568v1,"Wenqin Du, Rundong Ding, Yingying Fan, Jinchi Lv"
"Dynamic Optimization of Portfolio Allocation Using Deep Reinforcement
  Learning","Artificial intelligence is fundamentally transforming financial investment
decision-making paradigms, with deep reinforcement learning (DRL) demonstrating
significant application potential in domains such as robo-advisory services.
Given that traditional portfolio optimization methods face significant
challenges in effectively managing dynamic asset weight adjustments, this paper
approaches the problem from the perspective of practical trading processes and
develops a dynamic optimization model using deep reinforcement learning to
achieve more effective asset allocation. The study's innovations are twofold:
First, we propose a Sharpe ratio reward function specifically designed for
Actor-Critic deep reinforcement learning algorithms, which optimizes portfolio
performance by maximizing the average Sharpe ratio through random sampling and
reinforcement learning algorithms during the training process; Second, we
design deep neural networks that are specifically structured to meet asset
optimization objectives. The study empirically evaluates the model using
randomly selected constituent stocks from the CSI300 index and conducts
comparative analyses against traditional approaches, including mean-variance
optimization and risk parity strategies. Backtesting results demonstrate the
dynamic optimization model's effectiveness in portfolio asset allocation,
yielding enhanced risk reduction, superior risk-return metrics, and optimal
performance across comprehensive evaluation criteria.",2024-12-24T17:33:20Z,http://arxiv.org/abs/2412.18563v1,"Gang Huang, Xiaohua Zhou, Qingyang Song"
"Distilling Fine-grained Sentiment Understanding from Large Language
  Models","Fine-grained sentiment analysis (FSA) aims to extract and summarize user
opinions from vast opinionated text. Recent studies demonstrate that large
language models (LLMs) possess exceptional sentiment understanding
capabilities. However, directly deploying LLMs for FSA applications incurs high
inference costs. Therefore, this paper investigates the distillation of
fine-grained sentiment understanding from LLMs into small language models
(SLMs). We prompt LLMs to examine and interpret the sentiments of given reviews
and then utilize the generated content to pretrain SLMs. Additionally, we
develop a comprehensive FSA benchmark to evaluate both SLMs and LLMs. Extensive
experiments on this benchmark reveal that: (1) distillation significantly
enhances the performance of SLMs in FSA tasks, achieving a 6.00\% improvement
in $F_1$-score, and the distilled model can outperform Llama-2-7b with only
220M parameters; (2) distillation equips SLMs with excellent zero-shot
sentiment classification capabilities, enabling them to match or even exceed
their teacher models. These results suggest that distillation from LLMs is a
highly promising direction for FSA. We will release our code, data, and
pretrained model weights at
\url{https://github.com/HITSZ-HLT/FSA-Distillation}.",2024-12-24T17:05:26Z,http://arxiv.org/abs/2412.18552v1,"Yice Zhang, Guangyu Xie, Hongling Xu, Kaiheng Hou, Jianzhu Bao, Qianlong Wang, Shiwei Chen, Ruifeng Xu"
Consistency Checks for Language Model Forecasters,"Forecasting is a task that is difficult to evaluate: the ground truth can
only be known in the future. Recent work showing LLM forecasters rapidly
approaching human-level performance begs the question: how can we benchmark and
evaluate these forecasters instantaneously? Following the consistency check
framework, we measure the performance of forecasters in terms of the
consistency of their predictions on different logically-related questions. We
propose a new, general consistency metric based on arbitrage: for example, if a
forecasting AI illogically predicts that both the Democratic and Republican
parties have 60% probability of winning the 2024 US presidential election, an
arbitrageur can trade against the forecaster's predictions and make a profit.
We build an automated evaluation system that generates a set of base questions,
instantiates consistency checks from these questions, elicits the predictions
of the forecaster, and measures the consistency of the predictions. We then
build a standard, proper-scoring-rule forecasting benchmark, and show that our
(instantaneous) consistency metrics correlate with LLM forecasters' ground
truth Brier scores (which are only known in the future). We also release a
consistency benchmark that resolves in 2028, providing a long-term evaluation
tool for forecasting.",2024-12-24T16:51:35Z,http://arxiv.org/abs/2412.18544v1,"Daniel Paleka, Abhimanyu Pallavi Sudhir, Alejandro Alvarez, Vineeth Bhat, Adam Shen, Evan Wang, Florian Tramèr"
"GCN-ABFT: Low-Cost Online Error Checking for Graph Convolutional
  Networks","Graph convolutional networks (GCNs) are popular for building machine-learning
application for graph-structured data. This widespread adoption led to the
development of specialized GCN hardware accelerators. In this work, we address
a key architectural challenge for GCN accelerators: how to detect errors in GCN
computations arising from random hardware faults with the least computation
cost. Each GCN layer performs a graph convolution, mathematically equivalent to
multiplying three matrices, computed through two separate matrix
multiplications. Existing Algorithm-based Fault Tolerance(ABFT) techniques can
check the results of individual matrix multiplications. However, for a GCN
layer, this check should be performed twice. To avoid this overhead, this work
introduces GCN-ABFT that directly calculates a checksum for the entire
three-matrix product within a single GCN layer, providing a cost-effective
approach for error detection in GCN accelerators. Experimental results
demonstrate that GCN-ABFT reduces the number of operations needed for checksum
computation by over 21% on average for representative GCN applications. These
savings are achieved without sacrificing fault-detection accuracy, as evidenced
by the presented fault-injection analysis.",2024-12-24T16:27:19Z,http://arxiv.org/abs/2412.18534v1,"Christodoulos Peltekis, Giorgos Dimitrakopoulos"
"Explanatory Instructions: Towards Unified Vision Tasks Understanding and
  Zero-shot Generalization","Computer Vision (CV) has yet to fully achieve the zero-shot task
generalization observed in Natural Language Processing (NLP), despite following
many of the milestones established in NLP, such as large transformer models,
extensive pre-training, and the auto-regression paradigm, among others. In this
paper, we explore the idea that CV adopts discrete and terminological task
definitions (\eg, ``image segmentation''), which may be a key barrier to
zero-shot task generalization. Our hypothesis is that without truly
understanding previously-seen tasks--due to these terminological
definitions--deep models struggle to generalize to novel tasks. To verify this,
we introduce Explanatory Instructions, which provide an intuitive way to define
CV task objectives through detailed linguistic transformations from input
images to outputs. We create a large-scale dataset comprising 12 million
``image input $\to$ explanatory instruction $\to$ output'' triplets, and train
an auto-regressive-based vision-language model (AR-based VLM) that takes both
images and explanatory instructions as input. By learning to follow these
instructions, the AR-based VLM achieves instruction-level zero-shot
capabilities for previously-seen tasks and demonstrates strong zero-shot
generalization for unseen CV tasks. Code and dataset will be openly available
on our GitHub repository.",2024-12-24T16:08:25Z,http://arxiv.org/abs/2412.18525v2,"Yang Shen, Xiu-Shen Wei, Yifan Sun, Yuxin Song, Tao Yuan, Jian Jin, Heyang Xu, Yazhou Yao, Errui Ding"
"An Empirical Analysis of Federated Learning Models Subject to
  Label-Flipping Adversarial Attack","In this paper, we empirically analyze adversarial attacks on selected
federated learning models. The specific learning models considered are
Multinominal Logistic Regression (MLR), Support Vector Classifier (SVC),
Multilayer Perceptron (MLP), Convolution Neural Network (CNN), %Recurrent
Neural Network (RNN), Random Forest, XGBoost, and Long Short-Term Memory
(LSTM). For each model, we simulate label-flipping attacks, experimenting
extensively with 10 federated clients and 100 federated clients. We vary the
percentage of adversarial clients from 10% to 100% and, simultaneously, the
percentage of labels flipped by each adversarial client is also varied from 10%
to 100%. Among other results, we find that models differ in their inherent
robustness to the two vectors in our label-flipping attack, i.e., the
percentage of adversarial clients, and the percentage of labels flipped by each
adversarial client. We discuss the potential practical implications of our
results.",2024-12-24T15:47:25Z,http://arxiv.org/abs/2412.18507v1,"Kunal Bhatnagar, Sagana Chattanathan, Angela Dang, Bhargav Eranki, Ronnit Rana, Charan Sridhar, Siddharth Vedam, Angie Yao, Mark Stamp"
"Topological phases protected by projective PT symmetry in
  alkaline-earth-like atoms","An important aspect in categorizing topological phases is whether the system
is spinless or spinful, given that these classes exhibit distinct symmetry
algebras, leading to disparate topological classifications. By utilizing the
projective presentation strategy, the topological phases of spinless (or
spinful) systems can be emulated using spinful (or spinless) systems augmented
with gauge fields. In this study, we propose to implement the topological
phases safeguarded by the unique projective space-time (PT) symmetry inherent
to spinful models, using synthetic spinless alkaline-earth-like atoms.
Employing the separation of orbital and nuclear-spin degrees of freedom, the
model is configured as a rectangular tube penetrated by a uniform magnetic flux
through each plaquette, which simulates a spinless ladder endowed with
projective PT symmetry satisfying the algebraic properties of a spinful model.
For interacting topological phases with inter-orbital spin-exchange
interactions, which also adhere to PT symmetry, the four-fold degeneracy of
edge modes is split into two pairs of edge modes with two-fold degeneracy. We
map the complete phase diagram in the end and discover that these interacting
topological phases ultimately evolve into distinct charge-density-wave phases
via spontaneous symmetry breaking.",2024-12-24T15:25:04Z,http://arxiv.org/abs/2412.18494v1,"Xiaofan Zhou, Suotang Jia, Jian-Song Pan"
Matrix Chaos Inequalities and Chaos of Combinatorial Type,"Matrix concentration inequalities and their recently discovered sharp
counterparts provide powerful tools to bound the spectrum of random matrices
whose entries are linear functions of independent random variables. However, in
many applications in theoretical computer science and in other areas one
encounters more general random matrix models, called matrix chaoses, whose
entries are polynomials of independent random variables. Such models have often
been studied on a case-by-case basis using ad-hoc methods that can yield
suboptimal dimensional factors.
  In this paper we provide general matrix concentration inequalities for matrix
chaoses, which enable the treatment of such models in a systematic manner.
These inequalities are expressed in terms of flattenings of the coefficients of
the matrix chaos. We further identify a special family of matrix chaoses of
combinatorial type for which the flattening parameters can be computed
mechanically by a simple rule. This allows us to provide a unified treatment of
and improved bounds for matrix chaoses that arise in a variety of applications,
including graph matrices, Khatri-Rao matrices, and matrices that arise in
average case analysis of the sum-of-squares hierarchy.",2024-12-24T14:54:23Z,http://arxiv.org/abs/2412.18468v1,"Afonso S. Bandeira, Kevin Lucca, Petar Nizić-Nikolac, Ramon van Handel"
GeFL: Model-Agnostic Federated Learning with Generative Models,"Federated learning (FL) is a promising paradigm in distributed learning while
preserving the privacy of users. However, the increasing size of recent models
makes it unaffordable for a few users to encompass the model. It leads the
users to adopt heterogeneous models based on their diverse computing
capabilities and network bandwidth. Correspondingly, FL with heterogeneous
models should be addressed, given that FL typically involves training a single
global model. In this paper, we propose Generative Model-Aided Federated
Learning (GeFL), incorporating a generative model that aggregates global
knowledge across users of heterogeneous models. Our experiments on various
classification tasks demonstrate notable performance improvements of GeFL
compared to baselines, as well as limitations in terms of privacy and
scalability. To tackle these concerns, we introduce a novel framework, GeFL-F.
It trains target networks aided by feature-generative models. We empirically
demonstrate the consistent performance gains of GeFL-F, while demonstrating
better privacy preservation and robustness to a large number of clients. Codes
are available at [1].",2024-12-24T14:39:47Z,http://arxiv.org/abs/2412.18460v1,"Honggu Kang, Seohyeon Cha, Joonhyuk Kang"
"Shoving tubes through shapes gives a sufficient and efficient shape
  statistic","The Persistent Homology Transform (PHT) was introduced in the field of
Topological Data Analysis about 10 years ago, and has since been proven to be a
very powerful descriptor of Euclidean shapes. The PHT consists of scanning a
shape from all possible directions $v\in S^{n-1}$ and then computing the
persistent homology of sublevel set filtrations of the respective height
functions $h_v$; this results in a sufficient and continuous descriptor of
Euclidean shapes. We introduce a generalisation of the PHT in which we consider
arbitrary parameter spaces and sublevel sets with respect to any function. In
particular, we study transforms, defined on the Grassmannian
$\mathbb{A}\mathbb{G}(m,n)$ of affine subspaces of $\mathbb{R}^n$, that allow
to scan a shape by probing it with all possible affine $m$-dimensional
subspaces $P\subset \mathbb{R}^n$, for fixed dimension $m$, and by computing
persistent homology of sublevel set filtrations of the function
$\mathrm{dist}(\cdot, P)$ encoding the distance from the flat $P$. We call such
transforms ""distance-from-flat"" PHTs. We show that these transforms are
injective and continuous and that they provide computational advantages over
the classical PHT. In particular, we show that it is enough to compute homology
only in degrees up to $m-1$ to obtain injectivity; for $m=1$ this provides a
very powerful and computationally advantageous tool for examining shapes, which
in a previous work by a subset of the authors has proven to significantly
outperform state-of-the-art neural networks for shape classification tasks.",2024-12-24T14:22:43Z,http://arxiv.org/abs/2412.18452v1,"Adam Onus, Nina Otter, Renata Turkes"
"Re-assessing ImageNet: How aligned is its single-label assumption with
  its multi-label nature?","ImageNet, an influential dataset in computer vision, is traditionally
evaluated using single-label classification, which assumes that an image can be
adequately described by a single concept or label. However, this approach may
not fully capture the complex semantics within the images available in
ImageNet, potentially hindering the development of models that effectively
learn these intricacies. This study critically examines the prevalent
single-label benchmarking approach and advocates for a shift to multi-label
benchmarking for ImageNet. This shift would enable a more comprehensive
assessment of the capabilities of deep neural network (DNN) models. We analyze
the effectiveness of pre-trained state-of-the-art DNNs on ImageNet and one of
its variants, ImageNetV2. Studies in the literature have reported unexpected
accuracy drops of 11% to 14% on ImageNetV2. Our findings show that these
reported declines are largely attributable to a characteristic of the dataset
that has not received sufficient attention -- the proportion of images with
multiple labels. Taking this characteristic into account, the results of our
experiments provide evidence that there is no substantial degradation in
effectiveness on ImageNetV2. Furthermore, we acknowledge that ImageNet
pre-trained models exhibit some capability at capturing the multi-label nature
of the dataset even though they were trained under the single-label assumption.
Consequently, we propose a new evaluation approach to augment existing
approaches that assess this capability. Our findings highlight the importance
of considering the multi-label nature of the ImageNet dataset during
benchmarking. Failing to do so could lead to incorrect conclusions regarding
the effectiveness of DNNs and divert research efforts from addressing other
substantial challenges related to the reliability and robustness of these
models.",2024-12-24T12:55:31Z,http://arxiv.org/abs/2412.18409v1,"Esla Timothy Anzaku, Seyed Amir Mousavi, Arnout Van Messem, Wesley De Neve"
"Generalized Mean Absolute Directional Loss as a Solution to Overfitting
  and High Transaction Costs in Machine Learning Models Used in High-Frequency
  Algorithmic Investment Strategies","Regardless of the selected asset class and the level of model complexity
(Transformer versus LSTM versus Perceptron/RNN), the GMADL loss function
produces superior results than standard MSE-type loss functions and has better
numerical properties in the context of optimization than MADL. Better results
mean the possibility of achieving a higher risk-weighted return based on buy
and sell signals built on forecasts generated by a given theoretical model
estimated using the GMADL versus MSE or MADL function. In practice, GMADL
solves the problem of selecting the most preferable feature in both
classification and regression problems, improving the performance of each
estimation. What is important is that, through additional parameterization,
GMADL also solves the problem of optimizing investment systems on
high-frequency data in such a way that they focus on strategy variants that
contain fewer transactions so that transaction costs do not reduce the
effectiveness of a given strategy to zero. Moreover, the implementation
leverages state-of-the-art machine learning tools, including frameworks for
hyperparameter tuning, architecture testing, and walk-forward optimization,
ensuring robust and scalable solutions for real-world algorithmic trading.",2024-12-24T12:51:40Z,http://arxiv.org/abs/2412.18405v1,"Jakub Michańków, Paweł Sakowski, Robert Ślepaczuk"
"Scheme to Detect the Strong-to-weak Symmetry Breaking via Randomized
  Measurements","Symmetry breaking plays a central role in classifying the phases of quantum
many-body systems. Recent developments have highlighted a novel
symmetry-breaking pattern, in which the strong symmetry of a density matrix
spontaneously breaks to the week symmetry. This strong-to-weak symmetry
breaking is typically detected using multi-replica correlation functions, such
as the R\'enyi-2 correlator. In this letter, we propose a practical protocol
for detecting strong-to-weak symmetry breaking in experiments using the
randomized measurement toolbox. Our scheme involves collecting the results of
random Pauli measurements for (i) the original quantum state and (ii) the
quantum state after evolution with the charged operators. Based on the
measurement results, with a large number of samples, we can obtain the exact
solution to the R\'enyi-2 correlator. With a small sample size, we can still
provide an alternative approach to estimate the phase boundary to a decent
accuracy. We perform numerical simulations of Ising chains with all-to-all
decoherence as an exemplary demonstration. Our result opens the opportunity for
the experimental studies of the novel quantum phases in mixed quantum states.",2024-12-24T12:41:38Z,http://arxiv.org/abs/2412.18397v1,"Ning Sun, Pengfei Zhang, Lei Feng"
"Local smoothing estimates for Schrödinger equations in modulation
  spaces","Motivated by a recent work of Schippa (2022), we consider local smoothing
estimates for Schr\""{o}dinger equations in modulation spaces. By using the
C\'{o}rdoba-Fefferman type reverse square function inequality and the bilinear
Strichartz estimate, we can refine the summability exponent of modulation
spaces. Next, we will also discuss a new type of randomized Strichartz estimate
in modulation spaces. Finally, we will show that the reverse function estimate
implies the Strichartz estimates in modulation spaces. From this implication,
we obtain the reverse square function estimate of critical order.",2024-12-24T11:45:00Z,http://arxiv.org/abs/2412.18363v1,Kotaro Inami
ORAN Drives Higher Returns on Investments in Urban and Suburban Regions,"This paper provides the first incentive analysis of open radio access
networks (ORAN) using game theory. We assess strategic interactions between
telecom supply chain stakeholders: mobile network operators (MNOs), network
infrastructure suppliers (NIS), and original equipment manufacturers (OEMs)
across three procurement scenarios: (i) Traditional, (ii) Predatory as
monolithic radio access networks (MRAN), and (iii) DirectOEM as ORAN. We use
random forest and gradient boosting models to evaluate the optimal margins
across urban, suburban, and rural U.S. regions. Results suggest that ORAN
deployment consistently demonstrates higher net present value (NPV) of profits
in urban and suburban regions, outperforming the traditional procurement
strategy by 11% to 31%. However, rural areas present lower NPVs across all
scenarios, with significant variability at the county level. This analysis
offers actionable insights for telecom investment strategies, bridging
technical innovation with economic outcomes and addressing strategic supply
chain dynamics through a game-theoretic lens.",2024-12-24T11:08:15Z,http://arxiv.org/abs/2412.18346v1,"Priyanka Sharma, Edward J. Oughton, Aleksan Shanoyan"
"Predator Prey Scavenger Model using Holling's Functional Response of
  Type III and Physics-Informed Deep Neural Networks","Nonlinear mathematical models introduce the relation between various physical
and biological interactions present in nature. One of the most famous models is
the Lotka-Volterra model which defined the interaction between predator and
prey species present in nature. However, predators, scavengers, and prey
populations coexist in a natural system where scavengers can additionally rely
on the dead bodies of predators present in the system. Keeping this in mind,
the formulation and simulation of the predator prey scavenger model is
introduced in this paper. For the predation response, respective prey species
are assumed to have Holling's functional response of type III. The proposed
model is tested for various simulations and is found to be showing satisfactory
results in different scenarios. After simulations, the American forest dataset
is taken for parameter estimation which imitates the real-world case. For
parameter estimation, a physics-informed deep neural network is used with the
Adam backpropagation method which prevents the avalanche effect in trainable
parameters updation. For neural networks, mean square error and
physics-informed informed error are considered. After the neural network, the
hence-found parameters are fine-tuned using the
Broyden-Fletcher-Goldfarb-Shanno algorithm. Finally, the hence-found parameters
using a natural dataset are tested for stability using Jacobian stability
analysis. Future research work includes minimization of error induced by
parameters, bifurcation analysis, and sensitivity analysis of the parameters.",2024-12-24T11:02:38Z,http://arxiv.org/abs/2412.18344v1,"Aneesh Panchal, Kirti Beniwal, Vivek Kumar"
"Extremum Encoding for Joint Baseband Signal Compression and Time-Delay
  Estimation for Distributed Systems","The ubiquitous time-delay estimation (TDE) problem becomes nontrivial when
sensors are non-co-located and communication between them is limited. Building
on the recently proposed ""extremum encoding"" compression-estimation scheme, we
address the critical extension to complex-valued signals, suitable for
radio-frequency (RF) baseband processing. This extension introduces new
challenges, e.g., due to unknown phase of the signal of interest and random
phase of the noise, rendering a na\""ive application of the original scheme
inapplicable and irrelevant. In the face of these challenges, we propose a
judiciously adapted, though natural, extension of the scheme, paving its way to
RF applications. While our extension leads to a different statistical analysis,
including extremes of non-Gaussian distributions, we show that, ultimately, its
asymptotic behavior is akin to the original scheme. We derive an exponentially
tight upper bound on its error probability, corroborate our results via
simulation experiments, and demonstrate the superior performance compared to
two benchmark approaches.",2024-12-24T10:42:01Z,http://arxiv.org/abs/2412.18334v1,"Amir Weiss, Yuval Kochman, Gregory W. Wornell"
"On K-stability of $\mathbb P^3$ blown up along a smooth genus $2$ curve
  of degree $5$","We prove K-stability for infinitely many smooth members of the family 2.19 of
the Mukai-Mori classification.",2024-12-24T10:06:10Z,http://arxiv.org/abs/2412.18317v1,"Tiago Duarte Guerreiro, Luca Giovenzana, Nivedita Viswanathan"
Data-Driven Self-Supervised Graph Representation Learning,"Self-supervised graph representation learning (SSGRL) is a representation
learning paradigm used to reduce or avoid manual labeling. An essential part of
SSGRL is graph data augmentation. Existing methods usually rely on heuristics
commonly identified through trial and error and are effective only within some
application domains. Also, it is not clear why one heuristic is better than
another. Moreover, recent studies have argued against some techniques (e.g.,
dropout: that can change the properties of molecular graphs or destroy relevant
signals for graph-based document classification tasks).
  In this study, we propose a novel data-driven SSGRL approach that
automatically learns a suitable graph augmentation from the signal encoded in
the graph (i.e., the nodes' predictive feature and topological information). We
propose two complementary approaches that produce learnable feature and
topological augmentations. The former learns multi-view augmentation of node
features, and the latter learns a high-order view of the topology. Moreover,
the augmentations are jointly learned with the representation. Our approach is
general that it can be applied to homogeneous and heterogeneous graphs. We
perform extensive experiments on node classification (using nine homogeneous
and heterogeneous datasets) and graph property prediction (using another eight
datasets). The results show that the proposed method matches or outperforms the
SOTA SSGRL baselines and performs similarly to semi-supervised methods. The
anonymised source code is available at https://github.com/AhmedESamy/dsgrl/",2024-12-24T10:04:19Z,http://arxiv.org/abs/2412.18316v1,"Ahmed E. Samy, Zekarias T. Kefatoa, Sarunas Girdzijauskasa"
Noether Symmetry Analysis for Generalized Brans-Dicke Cosmology,"We present the complete solution to the classification problem regarding the
variational symmetries of the generalized Brans-Dicke cosmological model in the
presence of a second scalar field minimally coupled to gravity and the
generalized Brans-Dicke scalar field theory. Through the symmetry analysis, we
were able to specify the functional form of the field equations such that they
become integrable. Additionally, new families of integrable cosmological models
are presented.",2024-12-24T09:54:13Z,http://arxiv.org/abs/2412.18311v1,Andronikos Paliathanasis
Modern Approach to 2D Conformal Field Theory,"The primary aim of these lecture notes is to introduce the modern approach to
two-dimensional conformal field theory (2D CFT). The study of analytical
methods in two-dimensional conformal field theory has developed over several
decades, starting with BPZ. The development of analytical methods, particularly
in rational conformal field theory (RCFT), has been remarkable, with complete
classifications achieved for certain model groups. One motivation for studying
CFT comes from its ability to describe quantum critical systems. Given that
realistic quantum critical systems are fundamentally RCFTs, it is somewhat
natural that the analytical methods of RCFT have evolved significantly.
  CFTs other than RCFTs are called irrational conformal field theories (ICFTs).
Compared to RCFTs, the study of ICFTs has not progressed as much. Putting aside
whether there is a physical motivation or not, ICFTs inherently possess a
difficulty that makes them challenging to approach. However, with the
development of quantum gravity, the advancement of analytical methods for ICFTs
has become essential. The reason lies in the AdS/CFT correspondence. AdS/CFT
refers to the relationship between $d+1$ dimensional quantum gravity and $d$
dimensional CFT. Within this correspondence, the CFT appears as a
non-perturbative formulation of quantum gravity. Except in special cases, this
CFT belongs to ICFT. Against this backdrop, the methods for ICFTs have rapidly
developed in recent years. Many of these ICFT methods are indispensable for
modern quantum gravity research. Unfortunately, they cannot be learned from
textbooks on 2D CFTs. These lecture notes aim to fill this gap. Specifically,
we will cover techniques that have already been applied in many studies, such
as {\it HHLL block} and {\it monodromy method}, and important results that have
become proper nouns, such as {\it Hellerman bound} and {\it HKS bound}.",2024-12-24T09:45:00Z,http://arxiv.org/abs/2412.18307v1,Yuya Kusuki
"Device-independent, high bit-rate quantum random number generator with
  beam-splitter-free architecture and live Bell test certification","We present a beam-splitter-free, high-bit rate, device-independent quantum
random number generator (DI-QRNG) with real-time quantumness certification via
live Bell test data. Using a 20-mm-long, type-0 phase-matched PPKTP crystal in
a polarization Sagnac interferometer, we generated degenerate, non-collinear
parametric down-converted entangled photons at 810 nm in an annular ring
distribution with pair photons appearing at diametrically opposite points on
the ring randomly. Dividing the ring into six sections and collecting photons
from opposite sections, we developed three entangled photon sources from a
single resource (optics, laser, and nonlinear crystal). Using a pump power of
12.4 mW at 405 nm, we recorded coincidence (1 ns window) timestamps of any two
sources without projection to assign random bits (0 and 1) while measuring the
Bell parameter (S $&gt;$ 2) with the third source for live quantumness
certification. We have generated 90 million raw bits in 46.4 seconds, with a
minimum entropy extraction ratio exceeding 97$\%$. Post-processed using a
Toeplitz matrix, the QRNG achieved a 1.8 Mbps bit rate, passing all NIST 800-22
and TestU01 tests. Increasing the coincidence window to 2 ns boosts the bit
rate to over 2 Mbps, maintaining minimum entropy above 95$\%$ but reducing the
Bell parameter to S = 1.73. This novel scalable scheme eliminates beam
splitters, enabling robust, multi-bit DI-QRNG with enhanced ring sectioning and
trustworthy certification for practical high-rate applications.",2024-12-24T08:43:54Z,http://arxiv.org/abs/2412.18285v1,"Ayan Kumar Nai, Vimlesh Kumar, G. K. Samanta"
"Improved Feature Generating Framework for Transductive Zero-shot
  Learning","Feature Generative Adversarial Networks have emerged as powerful generative
models in producing high-quality representations of unseen classes within the
scope of Zero-shot Learning (ZSL). This paper delves into the pivotal influence
of unseen class priors within the framework of transductive ZSL (TZSL) and
illuminates the finding that even a marginal prior bias can result in
substantial accuracy declines. Our extensive analysis uncovers that this
inefficacy fundamentally stems from the utilization of an unconditional unseen
discriminator - a core component in existing TZSL. We further establish that
the detrimental effects of this component are inevitable unless the generator
perfectly fits class-specific distributions. Building on these insights, we
introduce our Improved Feature Generation Framework, termed I-VAEGAN, which
incorporates two novel components: Pseudo-conditional Feature Adversarial (PFA)
learning and Variational Embedding Regression (VER). PFA circumvents the need
for prior estimation by explicitly injecting the predicted semantics as pseudo
conditions for unseen classes premised by precise semantic regression.
Meanwhile, VER utilizes reconstructive pre-training to learn class statistics,
obtaining better semantic regression. Our I-VAEGAN achieves state-of-the-art
TZSL accuracy across various benchmarks and priors. Our code would be released
upon acceptance.",2024-12-24T08:42:16Z,http://arxiv.org/abs/2412.18282v1,"Zihan Ye, Xinyuan Ru, Shiming Chen, Yaochu Jin, Kaizhu Huang, Xiaobo Jin"
"RaCMC: Residual-Aware Compensation Network with Multi-Granularity
  Constraints for Fake News Detection","Multimodal fake news detection aims to automatically identify real or fake
news, thereby mitigating the adverse effects caused by such misinformation.
Although prevailing approaches have demonstrated their effectiveness,
challenges persist in cross-modal feature fusion and refinement for
classification. To address this, we present a residual-aware compensation
network with multi-granularity constraints (RaCMC) for fake news detection,
that aims to sufficiently interact and fuse cross-modal features while
amplifying the differences between real and fake news. First, a multiscale
residual-aware compensation module is designed to interact and fuse features at
different scales, and ensure both the consistency and exclusivity of feature
interaction, thus acquiring high-quality features. Second, a multi-granularity
constraints module is implemented to limit the distribution of both the news
overall and the image-text pairs within the news, thus amplifying the
differences between real and fake news at the news and feature levels. Finally,
a dominant feature fusion reasoning module is developed to comprehensively
evaluate news authenticity from the perspectives of both consistency and
inconsistency. Experiments on three public datasets, including Weibo17,
Politifact and GossipCop, reveal the superiority of the proposed method.",2024-12-24T08:08:29Z,http://arxiv.org/abs/2412.18254v1,"Xinquan Yu, Ziqi Sheng, Wei Lu, Xiangyang Luo, Jiantao Zhou"
"An Improved Fault Diagnosis Strategy for Induction Motors Using Weighted
  Probability Ensemble Deep Learning","Early detection of faults in induction motors is crucial for ensuring
uninterrupted operations in industrial settings. Among the various fault types
encountered in induction motors, bearing, rotor, and stator faults are the most
prevalent. This paper introduces a Weighted Probability Ensemble Deep Learning
(WPEDL) methodology, tailored for effectively diagnosing induction motor faults
using high-dimensional data extracted from vibration and current features. The
Short-Time Fourier Transform (STFT) is employed to extract features from both
vibration and current signals. The performance of the WPEDL fault diagnosis
method is compared against conventional deep learning models, demonstrating the
superior efficacy of the proposed system. The multi-class fault diagnosis
system based on WPEDL achieves high accuracies across different fault types:
99.05% for bearing (vibrational signal), 99.10%, and 99.50% for rotor (current
and vibration signal), and 99.60%, and 99.52% for stator faults (current and
vibration signal) respectively. To evaluate the robustness of our multi-class
classification decisions, tests have been conducted on a combined dataset of
52,000 STFT images encompassing all three faults. Our proposed model
outperforms other models, achieving an accuracy of 98.89%. The findings
underscore the effectiveness and reliability of the WPEDL approach for
early-stage fault diagnosis in IMs, offering promising insights for enhancing
industrial operational efficiency and reliability.",2024-12-24T08:02:44Z,http://arxiv.org/abs/2412.18249v1,"Usman Ali, Waqas Ali, Umer Ramzan"
"Detection and Forecasting of Parkinson Disease Progression from Speech
  Signal Features Using MultiLayer Perceptron and LSTM","Accurate diagnosis of Parkinson disease, especially in its early stages, can
be a challenging task. The application of machine learning techniques helps
improve the diagnostic accuracy of Parkinson disease detection but only few
studies have presented work towards the prediction of disease progression. In
this research work, Long Short Term Memory LSTM was trained using the
diagnostic features on Parkinson patients speech signals, to predict the
disease progression while a Multilayer Perceptron MLP was trained on the same
diagnostic features to detect the disease. Diagnostic features selected using
two well-known feature selection methods named Relief-F and Sequential Forward
Selection and applied on LSTM and MLP have shown to accurately predict the
disease progression as stage 2 and 3 and its existence respectively.",2024-12-24T08:02:43Z,http://arxiv.org/abs/2412.18248v1,"Majid Ali, Hina Shakir, Asia Samreen, Sohaib Ahmed"
"Fréchet regression for multi-label feature selection with implicit
  regularization","Fr\'echet regression extends linear regression to model complex responses
  in metric spaces, making it particularly relevant for multi-label regression,
  where each instance can have multiple associated labels. However, variable
  selection within this framework remains underexplored. In this paper, we pro
pose a novel variable selection method that employs implicit regularization
  instead of traditional explicit regularization approaches, which can
introduce
  bias. Our method effectively captures nonlinear interactions between predic
tors and responses while promoting model sparsity. We provide theoretical
  results demonstrating selection consistency and illustrate the performance of
  our approach through numerical examples",2024-12-24T08:02:28Z,http://arxiv.org/abs/2412.18247v1,"Dou El Kefel Mansouri, Seif-Eddine Benkabou, Khalid Benabdeslem"
"Calculations of some doping nanostructurations and patterns improving
  the functionality of high-temperature superconductors for bolometer device
  applications","We calculate the effects of doping nanostructuration and the patterning of
thin films of high-temperature superconductors (HTS) with the aim of optimizing
their functionality as sensing materials for resistive transition-edge
bolometer devices (TES). We focus, in particular, on spatial variations of the
carrier doping into the CuO$_2$ layers due to oxygen off-stoichiometry, (that
induce, in turn, critical temperature variations) and explore following two
major cases of such structurations: First, the random nanoscale disorder
intrinsically associated to doping levels that do not maximize the
superconducting critical temperature; our studies suggest that this first
simple structuration already improves some of the bolometric operational
parameters with respect to the conventional, nonstructured HTS materials used
until now. Secondly, we consider the imposition of regular arrangements of
zones with different nominal doping levels (patterning); we find that such
regular patterns may improve the bolometer performance even further. We find
one design that improves, with respect to nonstructured HTS materials, both the
saturation power and the operating temperature width by more than one order of
magnitude. It also almost doubles the response of the sensor to radiation.",2024-12-24T07:51:28Z,http://arxiv.org/abs/2412.18240v1,"J. C. Verde, A. S. Viz, M. M. Botana, C. Montero-Orille, M. V. Ramallo"
"OMG-HD: A High-Resolution AI Weather Model for End-to-End Forecasts from
  Observations","In recent years, Artificial Intelligence Weather Prediction (AIWP) models
have achieved performance comparable to, or even surpassing, traditional
Numerical Weather Prediction (NWP) models by leveraging reanalysis data.
However, a less-explored approach involves training AIWP models directly on
observational data, enhancing computational efficiency and improving forecast
accuracy by reducing the uncertainties introduced through data assimilation
processes. In this study, we propose OMG-HD, a novel AI-based regional
high-resolution weather forecasting model designed to make predictions directly
from observational data sources, including surface stations, radar, and
satellite, thereby removing the need for operational data assimilation. Our
evaluation shows that OMG-HD outperforms both the European Centre for
Medium-Range Weather Forecasts (ECMWF)'s high-resolution operational
forecasting system, IFS-HRES, and the High-Resolution Rapid Refresh (HRRR)
model at lead times of up to 12 hours across the contiguous United States
(CONUS) region. We achieve up to a 13% improvement on RMSE for 2-meter
temperature, 17% on 10-meter wind speed, 48% on 2-meter specific humidity, and
32% on surface pressure compared to HRRR. Our method shows that it is possible
to use AI-driven approaches for rapid weather predictions without relying on
NWP-derived weather fields as model input. This is a promising step towards
using observational data directly to make operational forecasts with AIWP
models.",2024-12-24T07:46:50Z,http://arxiv.org/abs/2412.18239v1,"Pengcheng Zhao, Jiang Bian, Zekun Ni, Weixin Jin, Jonathan Weyn, Zuliang Fang, Siqi Xiang, Haiyu Dong, Bin Zhang, Hongyu Sun, Kit Thambiratnam, Qi Zhang"
"Lensless speckle reconstructive spectrometer via physics-aware neural
  network","The speckle field yielded by disordered media is extensively employed for
spectral measurements. Existing speckle reconstructive spectrometers (RSs)
implemented by neural networks primarily rely on supervised learning, which
necessitates large-scale spectra-speckle pairs. However, beyond system
stability requirements for prolonged data collection, generating diverse
spectra with high resolution and finely labeling them is particularly
difficult. A lack of variety in datasets hinders the generalization of neural
networks to new spectrum types. Here we avoid this limitation by introducing
PhyspeNet, an untrained spectrum reconstruction framework combining a
convolutional neural network (CNN) with a physical model of a chaotic optical
cavity. Without pre-training and prior knowledge about the spectrum under test,
PhyspeNet requires only a single captured speckle for various multi-wavelength
reconstruction tasks. Experimentally, we demonstrate a lens-free, snapshot RS
system by leveraging the one-to-many mapping between spatial and spectrum
domains in a random medium. Dual-wavelength peaks separated by 2 pm can be
distinguished, and a maximum working bandwidth of 40 nm is achieved with high
measurement accuracy. This approach establishes a new paradigm for neural
network-based RS systems, entirely eliminating reliance on datasets while
ensuring that computational results exhibit a high degree of generalizability
and physical explainability.",2024-12-24T07:45:43Z,http://arxiv.org/abs/2412.18238v1,"Junrui Liang, Min Jiang, Zhongming Huang, Junhong He, Yanting Guo, Yanzhao Ke, Jun Ye, Jiangming Xu, Jun Li, Jinyong Leng, Pu Zhou"
"Band Prompting Aided SAR and Multi-Spectral Data Fusion Framework for
  Local Climate Zone Classification","Local climate zone (LCZ) classification is of great value for understanding
the complex interactions between urban development and local climate. Recent
studies have increasingly focused on the fusion of synthetic aperture radar
(SAR) and multi-spectral data to improve LCZ classification performance.
However, it remains challenging due to the distinct physical properties of
these two types of data and the absence of effective fusion guidance. In this
paper, a novel band prompting aided data fusion framework is proposed for LCZ
classification, namely BP-LCZ, which utilizes textual prompts associated with
band groups to guide the model in learning the physical attributes of different
bands and semantics of various categories inherent in SAR and multi-spectral
data to augment the fused feature, thus enhancing LCZ classification
performance. Specifically, a band group prompting (BGP) strategy is introduced
to align the visual representation effectively at the level of band groups,
which also facilitates a more adequate extraction of semantic information of
different bands with textual information. In addition, a multivariate
supervised matrix (MSM) based training strategy is proposed to alleviate the
problem of positive and negative sample confusion by completing the supervised
information. The experimental results demonstrate the effectiveness and
superiority of the proposed data fusion framework.",2024-12-24T07:40:07Z,http://arxiv.org/abs/2412.18235v1,"Haiyan Lan, Shujun Li, Mingjie Xie, Xuanjia Zhao, Hongning Liu, Pengming Feng, Dongli Xu, Guangjun He, Jian Guan"
Towards Macro-AUC oriented Imbalanced Multi-Label Continual Learning,"In Continual Learning (CL), while existing work primarily focuses on the
multi-class classification task, there has been limited research on Multi-Label
Learning (MLL). In practice, MLL datasets are often class-imbalanced, making it
inherently challenging, a problem that is even more acute in CL. Due to its
sensitivity to imbalance, Macro-AUC is an appropriate and widely used measure
in MLL. However, there is no research to optimize Macro-AUC in MLCL
specifically. To fill this gap, in this paper, we propose a new memory
replay-based method to tackle the imbalance issue for Macro-AUC-oriented MLCL.
Specifically, inspired by recent theory work, we propose a new Reweighted
Label-Distribution-Aware Margin (RLDAM) loss. Furthermore, to be compatible
with the RLDAM loss, a new memory-updating strategy named Weight Retain
Updating (WRU) is proposed to maintain the numbers of positive and negative
instances of the original dataset in memory. Theoretically, we provide superior
generalization analyses of the RLDAM-based algorithm in terms of Macro-AUC,
separately in batch MLL and MLCL settings. This is the first work to offer
theoretical generalization analyses in MLCL to our knowledge. Finally, a series
of experimental results illustrate the effectiveness of our method over several
baselines. Our codes are available at
https://github.com/ML-Group-SDU/Macro-AUC-CL.",2024-12-24T07:30:20Z,http://arxiv.org/abs/2412.18231v1,"Yan Zhang, Guoqiang Wu, Bingzheng Wang, Teng Pang, Haoliang Sun, Yilong Yin"
"Leveraging Convolutional Neural Network-Transformer Synergy for
  Predictive Modeling in Risk-Based Applications","With the development of the financial industry, credit default prediction, as
an important task in financial risk management, has received increasing
attention. Traditional credit default prediction methods mostly rely on machine
learning models, such as decision trees and random forests, but these methods
have certain limitations in processing complex data and capturing potential
risk patterns. To this end, this paper proposes a deep learning model based on
the combination of convolutional neural networks (CNN) and Transformer for
credit user default prediction. The model combines the advantages of CNN in
local feature extraction with the ability of Transformer in global dependency
modeling, effectively improving the accuracy and robustness of credit default
prediction. Through experiments on public credit default datasets, the results
show that the CNN+Transformer model outperforms traditional machine learning
models, such as random forests and XGBoost, in multiple evaluation indicators
such as accuracy, AUC, and KS value, demonstrating its powerful ability in
complex financial data modeling. Further experimental analysis shows that
appropriate optimizer selection and learning rate adjustment play a vital role
in improving model performance. In addition, the ablation experiment of the
model verifies the advantages of the combination of CNN and Transformer and
proves the complementarity of the two in credit default prediction. This study
provides a new idea for credit default prediction and provides strong support
for risk assessment and intelligent decision-making in the financial field.
Future research can further improve the prediction effect and generalization
ability by introducing more unstructured data and improving the model
architecture.",2024-12-24T07:07:14Z,http://arxiv.org/abs/2412.18222v1,"Yuhan Wang, Zhen Xu, Yue Yao, Jinsong Liu, Jiating Lin"
"ICM-Assistant: Instruction-tuning Multimodal Large Language Models for
  Rule-based Explainable Image Content Moderation","Controversial contents largely inundate the Internet, infringing various
cultural norms and child protection standards. Traditional Image Content
Moderation (ICM) models fall short in producing precise moderation decisions
for diverse standards, while recent multimodal large language models (MLLMs),
when adopted to general rule-based ICM, often produce classification and
explanation results that are inconsistent with human moderators. Aiming at
flexible, explainable, and accurate ICM, we design a novel rule-based dataset
generation pipeline, decomposing concise human-defined rules and leveraging
well-designed multi-stage prompts to enrich short explicit image annotations.
Our ICM-Instruct dataset includes detailed moderation explanation and
moderation Q-A pairs. Built upon it, we create our ICM-Assistant model in the
framework of rule-based ICM, making it readily applicable in real practice. Our
ICM-Assistant model demonstrates exceptional performance and flexibility.
Specifically, it significantly outperforms existing approaches on various
sources, improving both the moderation classification (36.8\% on average) and
moderation explanation quality (26.6\% on average) consistently over existing
MLLMs. Code/Data is available at https://github.com/zhaoyuzhi/ICM-Assistant.",2024-12-24T06:45:36Z,http://arxiv.org/abs/2412.18216v1,"Mengyang Wu, Yuzhi Zhao, Jialun Cao, Mingjie Xu, Zhongming Jiang, Xuehui Wang, Qinbin Li, Guangneng Hu, Shengchao Qin, Chi-Wing Fu"
"Relative Alpha in the Magneto-Hydro-Dynamics (MHD) with open magnetic
  field boundary and its application to the solar eruption","An instability criterion in the MHD with the open boundary of magnetic field
is proposed in this paper. We use a series of linear force-free extrapolation
field, in which the normal part of magnetic field is fixed, to obtain the
linear fitting coefficient called relative alpha by using the co-joined value
of magnetic free energy and magnetic flux at the open boundary ($E_f \Phi ^2$)
and the square of relative magnetic helicity ($H_R^2$). We calculate this
coefficient of the magnetic field above active regions NOAA~8210 and NOAA~11429
obtained by the photospheric-data-driven magnetohydrodynamics (MHD) model. It
is found that the fitting coefficient is a good proxy of the criterion to
indicate the occurrence of instability after which the magnetic reconnection
happens and caused the fast release of magnetic energy. We also applied this
method to the continuous evolution of three-dimension magnetic field of
NOAA~11158 based on the measurement of photospheric vector magnetic field of
SDO/HMI by the Non-linear Force-Free (NLFF) extrapolation method. The
calculated coefficient when the major flare happened based on the extrapolation
data is very close to the expected ones, which perfectly reflects the
occurrence of instability and the difference is even less than 7\%. This
relative alpha is very helpful to evaluate how far it is from the instability
in the MHD and quantitatively estimate the occurrence of solar eruption in the
space weather forecast.",2024-12-24T06:35:30Z,http://arxiv.org/abs/2412.18210v1,"Shangbin Yang, Joerg Buechner, Jean Carlo Santos, Jan Skala, Hongqi Zhang"
BoxMAC -- A Boxing Dataset for Multi-label Action Classification,"In competitive combat sports like boxing, analyzing a boxers's performance
statics is crucial for evaluating the quantity and variety of punches delivered
during bouts. These statistics provide valuable data and feedback, which are
routinely used for coaching and performance enhancement. We introduce BoxMAC, a
real-world boxing dataset featuring 15 professional boxers and encompassing 13
distinct action labels. Comprising over 60,000 frames, our dataset has been
meticulously annotated for multiple actions per frame with inputs from a boxing
coach. Since two boxers can execute different punches within a single
timestamp, this problem falls under the domain of multi-label action
classification. We propose a novel architecture for jointly recognizing
multiple actions in both individual images and videos. We investigate baselines
using deep neural network architectures to address both tasks. We believe that
BoxMAC will enable researchers and practitioners to develop and evaluate more
efficient models for performance analysis. With its realistic and diverse
nature, BoxMAC can serve as a valuable resource for the advancement of boxing
as a sport",2024-12-24T06:20:01Z,http://arxiv.org/abs/2412.18204v1,Shashikanta Sahoo
"Developing Cryptocurrency Trading Strategy Based on Autoencoder-CNN-GANs
  Algorithms","This paper leverages machine learning algorithms to forecast and analyze
financial time series. The process begins with a denoising autoencoder to
filter out random noise fluctuations from the main contract price data. Then,
one-dimensional convolution reduces the dimensionality of the filtered data and
extracts key information. The filtered and dimensionality-reduced price data is
fed into a GANs network, and its output serve as input of a fully connected
network. Through cross-validation, a model is trained to capture features that
precede large price fluctuations. The model predicts the likelihood and
direction of significant price changes in real-time price sequences, placing
trades at moments of high prediction accuracy. Empirical results demonstrate
that using autoencoders and convolution to filter and denoise financial data,
combined with GANs, achieves a certain level of predictive performance,
validating the capabilities of machine learning algorithms to discover
underlying patterns in financial sequences. Keywords - CNN;GANs;
Cryptocurrency; Prediction.",2024-12-24T06:14:34Z,http://arxiv.org/abs/2412.18202v2,"Zhuohuan Hu, Richard Yu, Zizhou Zhang, Haoran Zheng, Qianying Liu, Yining Zhou"
"VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics
  Manipulation with Long-Horizon Reasoning Tasks","General-purposed embodied agents are designed to understand the users'
natural instructions or intentions and act precisely to complete universal
tasks. Recently, methods based on foundation models especially
Vision-Language-Action models (VLAs) have shown a substantial potential to
solve language-conditioned manipulation (LCM) tasks well. However, existing
benchmarks do not adequately meet the needs of VLAs and relative algorithms. To
better define such general-purpose tasks in the context of LLMs and advance the
research in VLAs, we present VLABench, an open-source benchmark for evaluating
universal LCM task learning. VLABench provides 100 carefully designed
categories of tasks, with strong randomization in each category of task and a
total of 2000+ objects. VLABench stands out from previous benchmarks in four
key aspects: 1) tasks requiring world knowledge and common sense transfer, 2)
natural language instructions with implicit human intentions rather than
templates, 3) long-horizon tasks demanding multi-step reasoning, and 4)
evaluation of both action policies and language model capabilities. The
benchmark assesses multiple competencies including understanding of
mesh\&amp;texture, spatial relationship, semantic instruction, physical laws,
knowledge transfer and reasoning, etc. To support the downstream finetuning, we
provide high-quality training data collected via an automated framework
incorporating heuristic skills and prior information. The experimental results
indicate that both the current state-of-the-art pretrained VLAs and the
workflow based on VLMs face challenges in our tasks.",2024-12-24T06:03:42Z,http://arxiv.org/abs/2412.18194v1,"Shiduo Zhang, Zhe Xu, Peiju Liu, Xiaopeng Yu, Yuan Li, Qinghui Gao, Zhaoye Fei, Zhangyue Yin, Zuxuan Wu, Yu-Gang Jiang, Xipeng Qiu"
"On the Applicability of Zero-Shot Cross-Lingual Transfer Learning for
  Sentiment Classification in Distant Language Pairs","This research explores the applicability of cross-lingual transfer learning
from English to Japanese and Indonesian using the XLM-R pre-trained model. The
results are compared with several previous works, either by models using a
similar zero-shot approach or a fully-supervised approach, to provide an
overview of the zero-shot transfer learning approach's capability using XLM-R
in comparison with existing models. Our models achieve the best result in one
Japanese dataset and comparable results in other datasets in Japanese and
Indonesian languages without being trained using the target language.
Furthermore, the results suggest that it is possible to train a multi-lingual
model, instead of one model for each language, and achieve promising results.",2024-12-24T05:50:18Z,http://arxiv.org/abs/2412.18188v1,"Andre Rusli, Makoto Shishido"
"PCM Selector: Penalized Covariate-Mediator Selection Operator for
  Evaluating Linear Causal Effects","For a data-generating process for random variables that can be described with
a linear structural equation model, we consider a situation in which (i) a set
of covariates satisfying the back-door criterion cannot be observed or (ii)
such a set can be observed, but standard statistical estimation methods cannot
be applied to estimate causal effects because of
multicollinearity/high-dimensional data problems. We propose a novel two-stage
penalized regression approach, the penalized covariate-mediator selection
operator (PCM Selector), to estimate the causal effects in such scenarios.
Unlike existing penalized regression analyses, when a set of intermediate
variables is available, PCM Selector provides a consistent or less biased
estimator of the causal effect. In addition, PCM Selector provides a variable
selection procedure for intermediate variables to obtain better estimation
accuracy of the causal effects than does the back-door criterion.",2024-12-24T05:34:05Z,http://arxiv.org/abs/2412.18180v1,"Hisayoshi Nanmo, Manabu Kuroki"
VisionGRU: A Linear-Complexity RNN Model for Efficient Image Analysis,"Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are two
dominant models for image analysis. While CNNs excel at extracting multi-scale
features and ViTs effectively capture global dependencies, both suffer from
high computational costs, particularly when processing high-resolution images.
Recently, state-space models (SSMs) and recurrent neural networks (RNNs) have
attracted attention due to their efficiency. However, their performance in
image classification tasks remains limited. To address these challenges, this
paper introduces VisionGRU, a novel RNN-based architecture designed for
efficient image classification. VisionGRU leverages a simplified Gated
Recurrent Unit (minGRU) to process large-scale image features with linear
complexity. It divides images into smaller patches and progressively reduces
the sequence length while increasing the channel depth, thus facilitating
multi-scale feature extraction. A hierarchical 2DGRU module with bidirectional
scanning captures both local and global contexts, improving long-range
dependency modeling, particularly for tasks like semantic segmentation.
Experimental results on the ImageNet and ADE20K datasets demonstrate that
VisionGRU outperforms ViTs, significantly reducing memory usage and
computational costs, especially for high-resolution images. These findings
underscore the potential of RNN-based approaches for developing efficient and
scalable computer vision solutions. Codes will be available at
https://github.com/YangLiu9208/VisionGRU.",2024-12-24T05:27:11Z,http://arxiv.org/abs/2412.18178v2,"Shicheng Yin, Kaixuan Yin, Weixing Chen, Enbo Huang, Yang Liu"
"Unlocking the Hidden Treasures: Enhancing Recommendations with Unlabeled
  Data","Collaborative filtering (CF) stands as a cornerstone in recommender systems,
yet effectively leveraging the massive unlabeled data presents a significant
challenge. Current research focuses on addressing the challenge of unlabeled
data by extracting a subset that closely approximates negative samples.
Regrettably, the remaining data are overlooked, failing to fully integrate this
valuable information into the construction of user preferences. To address this
gap, we introduce a novel positive-neutral-negative (PNN) learning paradigm.
PNN introduces a neutral class, encompassing intricate items that are
challenging to categorize directly as positive or negative samples. By training
a model based on this triple-wise partial ranking, PNN offers a promising
solution to learning complex user preferences. Through theoretical analysis, we
connect PNN to one-way partial AUC (OPAUC) to validate its efficacy.
Implementing the PNN paradigm is, however, technically challenging because: (1)
it is difficult to classify unlabeled data into neutral or negative in the
absence of supervised signals; (2) there does not exist any loss function that
can handle set-level triple-wise ranking relationships. To address these
challenges, we propose a semi-supervised learning method coupled with a
user-aware attention model for knowledge acquisition and classification
refinement. Additionally, a novel loss function with a two-step centroid
ranking approach enables handling set-level rankings. Extensive experiments on
four real-world datasets demonstrate that, when combined with PNN, a wide range
of representative CF models can consistently and significantly boost their
performance. Even with a simple matrix factorization, PNN can achieve
comparable performance to sophisticated graph neutral networks.",2024-12-24T05:07:55Z,http://arxiv.org/abs/2412.18170v1,"Yuhan Zhao, Rui Chen, Qilong Han, Hongtao Song, Li Chen"
"From Pairwise to Ranking: Climbing the Ladder to Ideal Collaborative
  Filtering with Pseudo-Ranking","Intuitively, an ideal collaborative filtering (CF) model should learn from
users' full rankings over all items to make optimal top-K recommendations. Due
to the absence of such full rankings in practice, most CF models rely on
pairwise loss functions to approximate full rankings, resulting in an immense
performance gap. In this paper, we provide a novel analysis using the multiple
ordinal classification concept to reveal the inevitable gap between a pairwise
approximation and the ideal case. However, bridging the gap in practice
encounters two formidable challenges: (1) none of the real-world datasets
contains full ranking information; (2) there does not exist a loss function
that is capable of consuming ranking information. To overcome these challenges,
we propose a pseudo-ranking paradigm (PRP) that addresses the lack of ranking
information by introducing pseudo-rankings supervised by an original noise
injection mechanism. Additionally, we put forward a new ranking loss function
designed to handle ranking information effectively. To ensure our method's
robustness against potential inaccuracies in pseudo-rankings, we equip the
ranking loss function with a gradient-based confidence mechanism to detect and
mitigate abnormal gradients. Extensive experiments on four real-world datasets
demonstrate that PRP significantly outperforms state-of-the-art methods.",2024-12-24T05:01:16Z,http://arxiv.org/abs/2412.18168v1,"Yuhan Zhao, Rui Chen, Li Chen, Shuang Zhang, Qilong Han, Hongtao Song"
"Accelerating Post-Tornado Disaster Assessment Using Advanced Deep
  Learning Models","Post-disaster assessments of buildings and infrastructure are crucial for
both immediate recovery efforts and long-term resilience planning. This
research introduces an innovative approach to automating post-disaster
assessments through advanced deep learning models. Our proposed system employs
state-of-the-art computer vision techniques (YOLOv11 and ResNet50) to rapidly
analyze images and videos from disaster sites, extracting critical information
about building characteristics, including damage level of structural components
and the extent of damage. Our experimental results show promising performance,
with ResNet50 achieving 90.28% accuracy and an inference time of 1529ms per
image on multiclass damage classification. This study contributes to the field
of disaster management by offering a scalable, efficient, and objective tool
for post-disaster analysis, potentially capable of transforming how communities
and authorities respond to and learn from catastrophic events.",2024-12-24T04:04:33Z,http://arxiv.org/abs/2412.18147v1,"Robinson Umeike, Thang Dao, Shane Crawford"
"Supervised centrality Cvia sparse network influence regression: an
  application to the 2021 henan floods' social network","The social characteristics of players in a social network are closely
associated with their network positions and relational importance. Identifying
those influential players in a network is of great importance as it helps to
understand how ties are formed, how information is propagated, and, in turn,
can guide the dissemination of new information. Motivated by a Sina Weibo
social network analysis of the 2021 Henan Floods, where response variables for
each Sina Weibo user are available, we propose a new notion of supervised
centrality that emphasizes the task-specific nature of a player's centrality.
To estimate the supervised centrality and identify important players, we
develop a novel sparse network influence regression by introducing individual
heterogeneity for each user. To overcome the computational difficulties in
fitting the model for large social networks, we further develop a
forward-addition algorithm and show that it can consistently identify a
superset of the influential Sina Weibo users. We apply our method to analyze
three responses in the Henan Floods data: the number of comments, reposts, and
likes, and obtain meaningful results. A further simulation study corroborates
the developed method.",2024-12-24T04:00:44Z,http://arxiv.org/abs/2412.18145v1,"Yingying Ma, Wei Lan, Chenlei Leng, Ting Li, Hansheng Wang"
Neural Conformal Control for Time Series Forecasting,"We introduce a neural network conformal prediction method for time series
that enhances adaptivity in non-stationary environments. Our approach acts as a
neural controller designed to achieve desired target coverage, leveraging
auxiliary multi-view data with neural network encoders in an end-to-end manner
to further enhance adaptivity. Additionally, our model is designed to enhance
the consistency of prediction intervals in different quantiles by integrating
monotonicity constraints and leverages data from related tasks to boost
few-shot learning performance. Using real-world datasets from epidemics,
electric demand, weather, and others, we empirically demonstrate significant
improvements in coverage and probabilistic accuracy, and find that our method
is the only one that combines good calibration with consistency in prediction
intervals.",2024-12-24T03:56:25Z,http://arxiv.org/abs/2412.18144v1,"Ruipu Li, Alexander Rodríguez"
"An Instrumental Value for Data Production and its Application to Data
  Pricing","How much value does a dataset or a data production process have to an agent
who wishes to use the data to assist decision-making? This is a fundamental
question towards understanding the value of data as well as further pricing of
data. This paper develops an approach for capturing the instrumental value of
data production processes, which takes two key factors into account: (a) the
context of the agent's decision-making problem; (b) prior data or information
the agent already possesses. We ''micro-found'' our valuation concepts by
showing how they connect to classic notions of information design and signals
in information economics. When instantiated in the domain of Bayesian linear
regression, our value naturally corresponds to information gain. Based on our
designed data value, we then study a basic monopoly pricing setting with a
buyer looking to purchase from a seller some labeled data of a certain feature
direction in order to improve a Bayesian regression model. We show that when
the seller has the ability to fully customize any data request, she can extract
the first-best revenue (i.e., full surplus) from any population of buyers,
i.e., achieving first-degree price discrimination. If the seller can only sell
data that are derived from an existing data pool, this limits her ability to
customize, and achieving first-best revenue becomes generally impossible.
However, we design a mechanism that achieves seller revenue at most $\log
(\kappa)$ less than the first-best revenue, where $\kappa$ is the condition
number associated with the data matrix. A corollary of this result is that the
seller can extract the first-best revenue in the multi-armed bandits special
case.",2024-12-24T03:53:57Z,http://arxiv.org/abs/2412.18140v1,"Rui Ai, Boxiang Lyu, Zhaoran Wang, Zhuoran Yang, Haifeng Xu"
Learning Randomized Reductions and Program Properties,"The correctness of computations remains a significant challenge in computer
science, with traditional approaches relying on automated testing or formal
verification. Self-testing/correcting programs introduce an alternative
paradigm, allowing a program to verify and correct its own outputs via
randomized reductions, a concept that previously required manual derivation. In
this paper, we present Bitween, a method and tool for automated learning of
randomized (self)-reductions and program properties in numerical programs.
Bitween combines symbolic analysis and machine learning, with a surprising
finding: polynomial-time linear regression, a basic optimization method, is not
only sufficient but also highly effective for deriving complex randomized
self-reductions and program invariants, often outperforming sophisticated
mixed-integer linear programming solvers. We establish a theoretical framework
for learning these reductions and introduce RSR-Bench, a benchmark suite for
evaluating Bitween's capabilities on scientific and machine learning functions.
Our empirical results show that Bitween surpasses state-of-the-art tools in
scalability, stability, and sample efficiency when evaluated on nonlinear
invariant benchmarks like NLA-DigBench. Bitween is open-source as a Python
package and accessible via a web interface that supports C language programs.",2024-12-24T03:42:53Z,http://arxiv.org/abs/2412.18134v1,"Ferhat Erata, Orr Paradise, Timos Antonopoulos, ThanhVu Nguyen, Shafi Goldwasser, Ruzica Piskac"
"Age Optimal Sampling for Unreliable Channels under Unknown Channel
  Statistics","In this paper, we study a system in which a sensor forwards status updates to
a receiver through an error-prone channel, while the receiver sends the
transmission results back to the sensor via a reliable channel. Both channels
are subject to random delays. To evaluate the timeliness of the status
information at the receiver, we use the Age of Information (AoI) metric. The
objective is to design a sampling policy that minimizes the expected
time-average AoI, even when the channel statistics (e.g., delay distributions)
are unknown. We first review the threshold structure of the optimal offline
policy under known channel statistics and then reformulate the design of the
online algorithm as a stochastic approximation problem. We propose a
Robbins-Monro algorithm to solve this problem and demonstrate that the optimal
threshold can be approximated almost surely. Moreover, we prove that the
cumulative AoI regret of the online algorithm increases with rate
$\mathcal{O}(\ln K)$, where $K$ is the number of successful transmissions. In
addition, our algorithm is shown to be minimax order optimal, in the sense that
for any online learning algorithm, the cumulative AoI regret up to the $K$-th
successful transmissions grows with the rate at least $\Omega(\ln K)$ in the
worst case delay distribution. Finally, we improve the stability of the
proposed online learning algorithm through a momentum-based stochastic gradient
descent algorithm. Simulation results validate the performance of our proposed
algorithm.",2024-12-24T03:06:22Z,http://arxiv.org/abs/2412.18119v1,"Hongyi He, Haoyue Tang, Jiayu Pan, Jintao Wang, Jian Song, Leandros Tassiulas"
The EnvDesign Model: A Method to Solve the Environment Design Problem,"Today, several people and organizations rely on cloud platforms. The
reliability of cloud platforms depends heavily on the performance of their
internal programs (agents). To better prevent regressions in cloud platforms,
the design of pre-production testing environments (that test new agents, new
hardwares, and other changes) must take into account the diversity of
server/node properties (hardware model, virtual machine type, etc.) across the
fleet and dynamically emphasize or de-emphasize the prevalence of certain node
properties based on current testing priorities. This paper formulates this task
as the ""environment design"" problem and presents the EnvDesign model, a method
that uses graph theory and optimization algorithms to solve the environment
design problem. The EnvDesign model was built on context and techniques that
apply to combinatorial testing in general, so it can support combinatorial
testing in other domains.",2024-12-24T02:45:12Z,http://arxiv.org/abs/2412.18109v1,"Akshay Sathiya, Rohit Pandey"
"Beyond the Known: Enhancing Open Set Domain Adaptation with Unknown
  Exploration","Convolutional neural networks (CNNs) can learn directly from raw data,
resulting in exceptional performance across various research areas. However,
factors present in non-controllable environments such as unlabeled datasets
with varying levels of domain and category shift can reduce model accuracy. The
Open Set Domain Adaptation (OSDA) is a challenging problem that arises when
both of these issues occur together. Existing OSDA approaches in literature
only align known classes or use supervised training to learn unknown classes as
a single new category. In this work, we introduce a new approach to improve
OSDA techniques by extracting a set of high-confidence unknown instances and
using it as a hard constraint to tighten the classification boundaries.
Specifically, we use a new loss constraint that is evaluated in three different
ways: (1) using pristine negative instances directly; (2) using data
augmentation techniques to create randomly transformed negatives; and (3) with
generated synthetic negatives containing adversarial features. We analyze
different strategies to improve the discriminator and the training of the
Generative Adversarial Network (GAN) used to generate synthetic negatives. We
conducted extensive experiments and analysis on OVANet using three widely-used
public benchmarks, the Office-31, Office-Home, and VisDA datasets. We were able
to achieve similar H-score to other state-of-the-art methods, while increasing
the accuracy on unknown categories.",2024-12-24T02:27:35Z,http://arxiv.org/abs/2412.18105v1,"Lucas Fernando Alvarenga e Silva, Samuel Felipe dos Santos, Nicu Sebe, Jurandy Almeida"
"An Attention-based Framework with Multistation Information for
  Earthquake Early Warnings","Earthquake early warning systems play crucial roles in reducing the risk of
seismic disasters. Previously, the dominant modeling system was the
single-station models. Such models digest signal data received at a given
station and predict earth-quake parameters, such as the p-phase arrival time,
intensity, and magnitude at that location. Various methods have demonstrated
adequate performance. However, most of these methods present the challenges of
the difficulty of speeding up the alarm time, providing early warning for
distant areas, and considering global information to enhance performance.
Recently, deep learning has significantly impacted many fields, including
seismology. Thus, this paper proposes a deep learning-based framework, called
SENSE, for the intensity prediction task of earthquake early warning systems.
To explicitly consider global information from a regional or national
perspective, the input to SENSE comprises statistics from a set of stations in
a given region or country. The SENSE model is designed to learn the
relationships among the set of input stations and the locality-specific
characteristics of each station. Thus, SENSE is not only expected to provide
more reliable forecasts by considering multistation data but also has the
ability to provide early warnings to distant areas that have not yet received
signals. This study conducted extensive experiments on datasets from Taiwan and
Japan. The results revealed that SENSE can deliver competitive or even better
performances compared with other state-of-the-art methods.",2024-12-24T02:18:17Z,http://arxiv.org/abs/2412.18099v1,"Yu-Ming Huang, Kuan-Yu Chen, Wen-Wei Lin, Da-Yi Chen"
LangYa: Revolutionizing Cross-Spatiotemporal Ocean Forecasting,"Ocean forecasting is crucial for both scientific research and societal
benefits. Currently, the most accurate forecasting systems are global ocean
forecasting systems (GOFSs), which represent the ocean state variables (OSVs)
as discrete grids and solve partial differential equations (PDEs) governing the
transitions of oceanic state variables using numerical methods. However, GOFSs
processes are computationally expensive and prone to cumulative errors.
Recently, large artificial intelligence (AI)-based models significantly boosted
forecasting speed and accuracy. Unfortunately, building a large AI ocean
forecasting system that can be considered cross-spatiotemporal and air-sea
coupled forecasts remains a significant challenge. Here, we introduce LangYa, a
cross-spatiotemporal and air-sea coupled ocean forecasting system. Results
demonstrate that the time embedding module in LangYa enables a single model to
make forecasts with lead times ranging from 1 to 7 days. The air-sea coupled
module effectively simulates air-sea interactions. The ocean self-attention
module improves network stability and accelerates convergence during training,
and the adaptive thermocline loss function improves the accuracy of thermocline
forecasting. Compared to existing numerical and AI-based ocean forecasting
systems, LangYa uses 27 years of global ocean data from the Global Ocean
Reanalysis and Simulation version 12 (GLORYS12) for training and achieves more
reliable deterministic forecasting results for OSVs. LangYa forecasting system
provides global ocean researchers with access to a powerful software tool for
accurate ocean forecasting and opens a new paradigm for ocean science.",2024-12-24T02:14:39Z,http://arxiv.org/abs/2412.18097v2,"Nan Yang, Chong Wang, Meihua Zhao, Zimeng Zhao, Huiling Zheng, Bin Zhang, Jianing Wang, Xiaofeng Li"
Finite groups whose subgroup graph contains a vertex of large degree,"T.C. Burness and S.D. Scott \cite{3} classified finite groups $G$ such that
the number of prime order subgroups of $G$ is greater than $|G|/2-1$. In this
note, we study finite groups $G$ whose subgroup graph contains a vertex of
degree greater than $|G|/2-1$. The classification given for finite solvable
groups extends the work of Burness and Scott.",2024-12-24T01:58:40Z,http://arxiv.org/abs/2412.18087v1,Marius Tărnăuceanu
"Light rings and shadows of static black holes in effective quantum
  gravity II: A new solution without Cauchy horizons","Among the three known types of static solutions proposed within the
Hamiltonian constraint approach to effective quantum gravity (EQG), the first
two have been extensively investigated, whereas the third type-which preserves
general covariance, is free of Cauchy horizons, and was only recently
obtained-remains relatively unexplored. This solution can describe a black hole
with an event horizon for certain parameter ranges, or a horizonless compact
object beyond those ranges. In this paper, we focus on the third type and show
that its light rings feature both stable and unstable branches, and that the
black hole shadow size grows with the quantum parameter-unlike in the first two
types. However, when we account for both the shadow and the lensing ring, the
overall behavior closely resembles that of the second type, in which an
increasing quantum parameter leads to a larger portion of the lensing ring
being occupied by the shadow. This feature can serve as a hallmark of black
holes in EQG, offering a potential way to distinguish them from their GR
counterparts. Remarkably, the parameter ranges under which the solution remains
a black hole are highly consistent with the current observational constraints
on black hole shadows, lending strong support to the classification of the
third type of compact object in EQG as a black hole endowed with an event
horizon.",2024-12-24T01:44:25Z,http://arxiv.org/abs/2412.18083v1,"Wentao Liu, Di Wu, Jieci Wang"
"Generalized Grade-of-Membership Estimation for High-dimensional Locally
  Dependent Data","This work focuses on the mixed membership models for multivariate categorical
data widely used for analyzing survey responses and population genetics data.
These grade of membership (GoM) models offer rich modeling power but present
significant estimation challenges for high-dimensional polytomous data. Popular
existing approaches, such as Bayesian MCMC inference, are not scalable and lack
theoretical guarantees in high-dimensional settings. To address this, we first
observe that data from this model can be reformulated as a three-way
(quasi-)tensor, with many subjects responding to many items with varying
numbers of categories. We introduce a novel and simple approach that flattens
the three-way quasi-tensor into a ""fat"" matrix, and then perform a singular
value decomposition of it to estimate parameters by exploiting the singular
subspace geometry. Our fast spectral method can accommodate a broad range of
data distributions with arbitrarily locally dependent noise, which we formalize
as the generalized-GoM models. We establish finite-sample entrywise error
bounds for the generalized-GoM model parameters. This is supported by a new
sharp two-to-infinity singular subspace perturbation theory for locally
dependent and flexibly distributed noise, a contribution of independent
interest. Simulations and applications to data in political surveys, population
genetics, and single-cell sequencing demonstrate our method's superior
performance.",2024-12-27T18:51:15Z,http://arxiv.org/abs/2412.19796v1,"Ling Chen, Chengzhu Huang, Yuqi Gu"
"Data-driven analysis of anomalous transport and three-wave-coupling
  effects in E x B plasma discharges","Collisionless cross-field electron transport in an E x B configuration
relevant for electric propulsion is studied using data from a (z, {\theta})
full-PIC simulation. Higher-order spectral analysis shows that transport is
dominated by the in-phase interaction of the oscillations of the azimuthal
electric field and the electron density associated to the first electron
cyclotron drift instability (ECDI) mode. A secondary contribution emanates from
a lower-frequency mode, not predicted by linear ECDI theory, while higher modes
have a minor direct impact on transport. However, a bicoherence analysis
reveals that strong phase couplings exist among the ECDI modes, and a sparse
symbolic regression spectral model, based on the three-wave coupling equations,
suggests an inverse energy cascade as the most likely explanation, thus
suggesting that higher modes contribute indirectly to transport by quadratic
power transfer to the first mode. This work provides new insights into the
dynamics of anomalous plasma transport in E x B sources and the underlying
processes governing energy distribution across different scales, and supports
the validity of weak turbulence theory to examine their behavior.",2024-12-27T18:43:24Z,http://arxiv.org/abs/2412.19789v1,"Borja Bayón-Buján, Enrique Bello-Benítez, Jiewei Zhou, Mario Merino"
"Machine Learning for Sentiment Analysis of Imported Food in Trinidad and
  Tobago","This research investigates the performance of various machine learning
algorithms (CNN, LSTM, VADER, and RoBERTa) for sentiment analysis of Twitter
data related to imported food items in Trinidad and Tobago. The study addresses
three primary research questions: the comparative accuracy and efficiency of
the algorithms, the optimal configurations for each model, and the potential
applications of the optimized models in a live system for monitoring public
sentiment and its impact on the import bill. The dataset comprises tweets from
2018 to 2024, divided into imbalanced, balanced, and temporal subsets to assess
the impact of data balancing and the COVID-19 pandemic on sentiment trends. Ten
experiments were conducted to evaluate the models under various configurations.
Results indicated that VADER outperformed the other models in both multi-class
and binary sentiment classifications. The study highlights significant changes
in sentiment trends pre- and post-COVID-19, with implications for import
policies.",2024-12-27T18:25:08Z,http://arxiv.org/abs/2412.19781v1,"Cassandra Daniels, Koffka Khan"
Tensor Network Estimation of Distribution Algorithms,"Tensor networks are a tool first employed in the context of many-body quantum
physics that now have a wide range of uses across the computational sciences,
from numerical methods to machine learning. Methods integrating tensor networks
into evolutionary optimization algorithms have appeared in the recent
literature. In essence, these methods can be understood as replacing the
traditional crossover operation of a genetic algorithm with a tensor
network-based generative model. We investigate these methods from the point of
view that they are Estimation of Distribution Algorithms (EDAs). We find that
optimization performance of these methods is not related to the power of the
generative model in a straightforward way. Generative models that are better
(in the sense that they better model the distribution from which their training
data is drawn) do not necessarily result in better performance of the
optimization algorithm they form a part of. This raises the question of how
best to incorporate powerful generative models into optimization routines. In
light of this we find that adding an explicit mutation operator to the output
of the generative model often improves optimization performance.",2024-12-27T18:22:47Z,http://arxiv.org/abs/2412.19780v1,"John Gardiner, Javier Lopez-Piqueres"
"Analysis of Premature Death Rates in Texas Counties: The Impact of Air
  Quality, Socioeconomic Factors, and COPD Prevalence","Understanding factors contributing to premature mortality is critical for
public health planning. This study examines the relationships between premature
death rates and multiple risk factors across several Texas counties, utilizing
EPA air quality data, Census information, and county health records from recent
years. We analyze the impact of air quality (PM2.5 levels), socioeconomic
factors (median household income), and health conditions (COPD prevalence)
through statistical analysis and modeling techniques. Results reveal COPD
prevalence as a strong predictor of premature death rates, with higher
prevalence associated with a substantial increase in years of potential life
lost. While socioeconomic factors show a significant negative correlation, air
quality demonstrates more complex indirect relationships. These findings
emphasize the need for integrated public health interventions that prioritize
key health conditions while addressing underlying socioeconomic disparities.",2024-12-27T18:12:04Z,http://arxiv.org/abs/2412.19774v1,"Richard Rich, Ernesto Diaz"
Direct estimates of irreversibility from time series,"The arrow of time can be quantified through the Kullback-Leibler divergence
($D_{KL}$) between the distributions of forward and reverse trajectories in a
system. Many approaches to estimate this rely on specific models, but the use
of incorrect models can introduce uncontrolled errors. Here, we describe a
model-free method that uses trajectory data directly to estimate the evidence
for irreversibility over finite windows of time. To do this we build on
previous work to identify and correct for errors that arise from limited sample
size. Importantly, our approach accurately recovers $D_{KL} = 0$ in systems
that adhere to detailed balance, and the correct nonzero $D_{KL}$ for data
generated by well understood models of nonequilibrium systems. We apply our
method to trajectories of neural activity in the retina as it responds to
naturalistic inputs, and find evidence of irreversibility in single neurons,
emphasizing the non-Markovian character of these data. These results open new
avenues for investigating how the brain represents the arrow of time.",2024-12-27T18:10:53Z,http://arxiv.org/abs/2412.19772v1,"Trevor GrandPre, Gianluca Teza, William Bialek"
"On Universally Free First-Order Extensions of Belnap-Dunn's Four-Valued
  Logic and Nelson's Paraconsistent Logic N4","The aim of this paper is to introduce the logics FFDE and FN4, which are
universally free versions of Belnap-Dunn's four-valued logic, also known as the
logic of first-degree entailment (FDE), and Nelson's paraconsistent logic QN4
(N-). Both FDE and QN4 are suitable to be interpreted as information-based
logics, that is, logics that are capable of representing the deductive behavior
of possibly inconsistent and incomplete information in a database. Like QN4 and
some non-free first-order extensions of FDE, FFDE and FN4 are endowed with
Kripke-style variable domain semantics, which allows representing the dynamic
aspect of information processing, that is, how a database receives new
information over time, including information about new individuals. We argue,
however, that FFDE and FN4 can better represent the development of inconsistent
and incomplete information states (i.e., configurations of a database) over
time than their non-free versions. First, because they allow for empty domains,
which corresponds to the idea that a database may acknowledge no individual at
all at an early stage of its development. Second, because they allow for empty
names, which get interpreted as information about new individuals is inserted
into the database. Also, both systems include an identity predicate that is
interpreted along the same lines of the other logical operators, viz., in terms
of independent positive and negative rules.",2024-12-27T17:57:18Z,http://arxiv.org/abs/2412.19767v1,"Henrique Antunes, Abilio Rodrigues"
Generative Video Propagation,"Large-scale video generation models have the inherent ability to
realistically model natural scenes. In this paper, we demonstrate that through
a careful design of a generative video propagation framework, various video
tasks can be addressed in a unified way by leveraging the generative power of
such models. Specifically, our framework, GenProp, encodes the original video
with a selective content encoder and propagates the changes made to the first
frame using an image-to-video generation model. We propose a data generation
scheme to cover multiple video tasks based on instance-level video segmentation
datasets. Our model is trained by incorporating a mask prediction decoder head
and optimizing a region-aware loss to aid the encoder to preserve the original
content while the generation model propagates the modified region. This novel
design opens up new possibilities: In editing scenarios, GenProp allows
substantial changes to an object's shape; for insertion, the inserted objects
can exhibit independent motion; for removal, GenProp effectively removes
effects like shadows and reflections from the whole video; for tracking,
GenProp is capable of tracking objects and their associated effects together.
Experiment results demonstrate the leading performance of our model in various
video tasks, and we further provide in-depth analyses of the proposed
framework.",2024-12-27T17:42:29Z,http://arxiv.org/abs/2412.19761v1,"Shaoteng Liu, Tianyu Wang, Jui-Hsien Wang, Qing Liu, Zhifei Zhang, Joon-Young Lee, Yijun Li, Bei Yu, Zhe Lin, Soo Ye Kim, Jiaya Jia"
"""Did my figure do justice to the answer?"" : Towards Multimodal Short
  Answer Grading with Feedback (MMSAF)","Personalized feedback plays a vital role in a student's learning process.
While existing systems are adept at providing feedback over MCQ-based
evaluation, this work focuses more on subjective and open-ended questions,
which is similar to the problem of Automatic Short Answer Grading (ASAG) with
feedback. Additionally, we introduce the Multimodal Short Answer grading with
Feedback (MMSAF) problem over the traditional ASAG feedback problem to address
the scenario where the student answer and reference answer might contain
images. Moreover, we introduce the MMSAF dataset with 2197 data points along
with an automated framework for generating such data sets. Our evaluations on
existing LLMs over this dataset achieved an overall accuracy of 55\% on Level
of Correctness labels, 75\% on Image Relevance labels and a score of 4.27 out
of 5 in correctness level of LLM generated feedback as rated by experts. As per
experts, Pixtral achieved a rating of above 4 out of all metrics, indicating
that it is more aligned to human judgement, and that it is the best solution
for assisting students.",2024-12-27T17:33:39Z,http://arxiv.org/abs/2412.19755v1,"Pritam Sil, Bhaskaran Raman, Pushpak Bhattacharyya"
Complement or substitute? How AI increases the demand for human skills,"The question of whether AI substitutes or complements human work is central
to debates on the future of work. This paper examines the impact of AI on skill
demand and compensation in the U.S. economy, analysing 12 million online job
vacancies from 2018 to 2023. It investigates internal effects (within-job
substitution and complementation) and external effects (across occupations,
industries, and regions). Our findings reveal a significant increase in demand
for AI-complementary skills, such as digital literacy, teamwork, and
resilience, alongside rising wage premiums for these skills in AI roles like
Data Scientist. Conversely, substitute skills, including customer service and
text review, have declined in both demand and value within AI-related
positions. Examining external effects, we find a notable rise in demand for
complementary skills in non-AI roles linked to the growth of AI-related jobs in
specific industries or regions. At the same time, there is a moderate decline
in non-AI roles requiring substitute skills. Overall, AI's complementary effect
is up to 50% larger than its substitution effect, resulting in net positive
demand for skills. These results, replicated for the UK and Australia,
highlight AI's transformative impact on workforce skill requirements. They
suggest reskilling efforts should prioritise not only technical AI skills but
also complementary skills like ethics and digital literacy.",2024-12-27T17:26:30Z,http://arxiv.org/abs/2412.19754v1,"Elina Mäkelä, Fabian Stephany"
"IMAGINE: An 8-to-1b 22nm FD-SOI Compute-In-Memory CNN Accelerator With
  an End-to-End Analog Charge-Based 0.15-8POPS/W Macro Featuring
  Distribution-Aware Data Reshaping","Charge-domain compute-in-memory (CIM) SRAMs have recently become an enticing
compromise between computing efficiency and accuracy to process sub-8b
convolutional neural networks (CNNs) at the edge. Yet, they commonly make use
of a fixed dot-product (DP) voltage swing, which leads to a loss in effective
ADC bits due to data-dependent clipping or truncation effects that waste
precious conversion energy and computing accuracy. To overcome this, we present
IMAGINE, a workload-adaptive 1-to-8b CIM-CNN accelerator in 22nm FD-SOI. It
introduces a 1152x256 end-to-end charge-based macro with a multi-bit DP based
on an input-serial, weight-parallel accumulation that avoids power-hungry DACs.
An adaptive swing is achieved by combining a channel-wise DP array split with a
linear in-ADC implementation of analog batch-normalization (ABN), obtaining a
distribution-aware data reshaping. Critical design constraints are relaxed by
including the post-silicon equivalent noise within a CIM-aware CNN training
framework. Measurement results showcase an 8b system-level energy efficiency of
40TOPS/W at 0.3/0.6V, with competitive accuracies on MNIST and CIFAR-10.
Moreover, the peak energy and area efficiencies of the 187kB/mm2 macro
respectively reach up to 0.15-8POPS/W and 2.6-154TOPS/mm2, scaling with the
8-to-1b computing precision. These results exceed previous charge-based designs
by 3-to-5x while being the first work to provide linear in-memory rescaling.",2024-12-27T17:18:15Z,http://arxiv.org/abs/2412.19750v1,"Adrian Kneip, Martin Lefebvre, Pol Maistriaux, David Bol"
"AAM-SEALS: Developing Aerial-Aquatic Manipulators in SEa, Air, and Land
  Simulator","Current simulators lack the ability to accurately model integrated
environments that encompass sea, air, and land. To address this gap, we
introduce Aerial-Aquatic Manipulators (AAMs) in SEa, Air, and Land Simulator
(SEALS), a comprehensive and photorealistic simulator designed for AAMs to
operate and learn in these diverse environments. The development of AAM-SEALS
tackles several significant challenges, including the creation of integrated
controllers for flying, swimming, and manipulation, and the high-fidelity
simulation of aerial dynamics and hydrodynamics leveraging particle physics.
Our evaluation demonstrates smooth operation and photorealistic transitions
across air, water, and their interfaces. We quantitatively validate the
fidelity of particle-based hydrodynamics by comparing position-tracking errors
across real-world and simulated systems. AAM-SEALS promises to benefit a broad
range of robotics communities, including robot learning, aerial robotics,
underwater robotics, mobile manipulation, and robotic simulators. We will
open-source our code and data to foster the advancement of research in these
fields. Please access our project website at: https:
//aam-seals.github.io/aam-seals-v1/",2024-12-27T17:13:14Z,http://arxiv.org/abs/2412.19744v1,"William Wang Yang, Karthikeya Kona, Yashveer Jain, Abhinav Bhamidipati, Tomer Atzili, Xiaomin Lin, Yantian Zha"
Hard Photon Triggered Jets in $p$-$p$ and $A$-$A$ Collisions,"An investigation of high transverse momentum (high-$p_T$) photon triggered
jets in proton-proton ($p$-$p$) and ion-ion ($A$-$A$) collisions at
$\sqrt{s_{NN}} = 0.2$ and $5.02~\mathrm{TeV}$ is carried out, using the
multistage description of in-medium jet evolution. Monte Carlo simulations of
hard scattering and energy loss in heavy-ion collisions are performed using
parameters tuned in a previous study of the nuclear modification factor
($R_{AA}$) for inclusive jets and high-$p_T$ hadrons. We obtain a good
reproduction of the experimental data for photon triggered jet $R_{AA}$, as
measured by the ATLAS detector, the distribution of the ratio of jet to photon
$p_T$ ($X_{\rm J \gamma}$), measured by both CMS and ATLAS, and the photon-jet
azimuthal correlation as measured by CMS. We obtain a moderate description of
the photon triggered jet $I_{AA}$, as measured by STAR. A noticeable
improvement in the comparison is observed when one goes beyond prompt photons
and includes bremsstrahlung and decay photons, revealing their significance in
certain kinematic regions, particularly at $X_{J\gamma} &gt; 1$. Moreover,
azimuthal angle correlations demonstrate a notable impact of non-prompt photons
on the distribution, emphasizing their role in accurately describing
experimental results. This work highlights the success of the multistage model
of jet modification to straightforwardly predict (this set of) photon triggered
jet observables. This comparison, along with the role played by non-prompt
photons, has important consequences on the inclusion of such observables in a
future Bayesian analysis.",2024-12-27T16:56:15Z,http://arxiv.org/abs/2412.19738v1,"C. Sirimanna, Y. Tachibana, A. Majumder, A. Angerami, R. Arora, S. A. Bass, Y. Chen, R. Datta, L. Du, R. Ehlers, H. Elfner, R. J. Fries, C. Gale, Y. He, B. V. Jacak, P. M. Jacobs, S. Jeon, Y. Ji, F. Jonas, L. Kasper, M. Kordell II, A. Kumar, R. Kunnawalkam-Elayavalli, J. Latessa, Y. -J. Lee, R. Lemmon, M. Luzum, S. Mak, A. Mankolli, C. Martin, H. Mehryar, T. Mengel, C. Nattrass, J. Norman, C. Parker, J. -F. Paquet, J. H. Putschke, H. Roch, G. Roland, B. Schenke, L. Schwiebert, A. Sengupta, C. Shen, M. Singh, D. Soeder, R. A. Soltz, I. Soudi, J. Velkovska, G. Vujanovic, X. -N. Wang, X. Wu, W. Zhao"
"Adaptive Context-Aware Multi-Path Transmission Control for VR/AR
  Content: A Deep Reinforcement Learning Approach","This paper introduces the Adaptive Context-Aware Multi-Path Transmission
Control Protocol (ACMPTCP), an efficient approach designed to optimize the
performance of Multi-Path Transmission Control Protocol (MPTCP) for
data-intensive applications such as augmented and virtual reality (AR/VR)
streaming. ACMPTCP addresses the limitations of conventional MPTCP by
leveraging deep reinforcement learning (DRL) for agile end-to-end path
management and optimal bandwidth allocation, facilitating path realignment
across diverse network environments.",2024-12-27T16:56:12Z,http://arxiv.org/abs/2412.19737v1,"Shakil Ahmed, Saifur Rahman Sabuj, Ashfaq Khokhar"
"A General Framework of Brain Region Detection And Genetic Variants
  Selection in Imaging Genetics","Imaging genetics is a growing field that employs structural or functional
neuroimaging techniques to study individuals with genetic risk variants
potentially linked to specific illnesses. This area presents considerable
challenges to statisticians due to the heterogeneous information and different
data forms it involves. In addition, both imaging and genetic data are
typically high-dimensional, creating a ""big data squared"" problem. Moreover,
brain imaging data contains extensive spatial information. Simply vectorizing
tensor images and treating voxels as independent features can lead to
computational issues and disregard spatial structure. This paper presents a
novel statistical method for imaging genetics modeling while addressing all
these challenges. We explore a Canonical Correlation Analysis based linear
model for the joint modeling of brain imaging, genetic information, and
clinical phenotype, enabling the simultaneous detection of significant brain
regions and selection of important genetic variants associated with the
phenotype outcome. Scalable algorithms are developed to tackle the ""big data
squared"" issue. We apply the proposed method to explore the reaction speed, an
indicator of cognitive functions, and its associations with brain MRI and
genetic factors using the UK Biobank database. Our study reveals a notable
connection between the caudate nucleus region of brain and specific significant
SNPs, along with their respective regulated genes, and the reaction speed.",2024-12-27T16:54:11Z,http://arxiv.org/abs/2412.19735v1,"Siqiang Su, Zhenghao Li, Long Feng, Ting Li"
"Dynamics, data and reconstruction","Data-driven learning is prevalent in many fields of science, mathematics and
engineering. The goal of data-driven learning of dynamical systems is to
interpret timeseries as a continuous observation of an underlying dynamical
system. This task is not well-posed for a variety of reasons. A dynamical
system may have multiple sub-systems co-existing within it. The nature of the
dataset depends on the portion of the phase space being viewed, and may thus my
confined to a sub-system. Secondly these sub-systems may be topologically
inter-weaved, so may be inseparable computationally. Thirdly, two timeseries
sampled separately from different dynamical systems may be close or even
indistinguishable. So there is no unqiue source for the timeseries. We show how
these ambiguities are circumvented if one considers dynamical systems and
measurement maps collectively. This is made possible in a category theoretical
framework, in which reconstruction is unique up to equivalences. We introduce
two categories of observed dynamical systems and timeseries-data. These are
related to the well known category of dynamical systems via functors. This
enables a functorial interpretation of the task of reconstruction as well.",2024-12-27T16:49:52Z,http://arxiv.org/abs/2412.19734v1,"Suddhasattwa Das, Tomoharu Suda"
"Generative Pretrained Embedding and Hierarchical Irregular Time Series
  Representation for Daily Living Activity Recognition","Within the evolving landscape of smart homes, the precise recognition of
daily living activities using ambient sensor data stands paramount. This paper
not only aims to bolster existing algorithms by evaluating two distinct
pretrained embeddings suited for ambient sensor activations but also introduces
a novel hierarchical architecture. We delve into an architecture anchored on
Transformer Decoder-based pre-trained embeddings, reminiscent of the GPT
design, and contrast it with the previously established state-of-the-art (SOTA)
ELMo embeddings for ambient sensors. Our proposed hierarchical structure
leverages the strengths of each pre-trained embedding, enabling the discernment
of activity dependencies and sequence order, thereby enhancing classification
precision. To further refine recognition, we incorporate into our proposed
architecture an hour-of-the-day embedding. Empirical evaluations underscore the
preeminence of the Transformer Decoder embedding in classification endeavors.
Additionally, our innovative hierarchical design significantly bolsters the
efficacy of both pre-trained embeddings, notably in capturing inter-activity
nuances. The integration of temporal aspects subtly but distinctively augments
classification, especially for time-sensitive activities. In conclusion, our
GPT-inspired hierarchical approach, infused with temporal insights, outshines
the SOTA ELMo benchmark.",2024-12-27T16:43:52Z,http://arxiv.org/abs/2412.19732v1,"Damien Bouchabou, Sao Mai Nguyen"
"Learning to Forget: Bayesian Time Series Forecasting using Recurrent
  Sparse Spectrum Signature Gaussian Processes","The signature kernel is a kernel between time series of arbitrary length and
comes with strong theoretical guarantees from stochastic analysis. It has found
applications in machine learning such as covariance functions for Gaussian
processes. A strength of the underlying signature features is that they provide
a structured global description of a time series. However, this property can
quickly become a curse when local information is essential and forgetting is
required; so far this has only been addressed with ad-hoc methods such as
slicing the time series into subsegments. To overcome this, we propose a
principled, data-driven approach by introducing a novel forgetting mechanism
for signatures. This allows the model to dynamically adapt its context length
to focus on more recent information. To achieve this, we revisit the recently
introduced Random Fourier Signature Features, and develop Random Fourier
Decayed Signature Features (RFDSF) with Gaussian processes (GPs). This results
in a Bayesian time series forecasting algorithm with variational inference,
that offers a scalable probabilistic algorithm that processes and transforms a
time series into a joint predictive distribution over time steps in one pass
using recurrence. For example, processing a sequence of length $10^4$ steps in
$\approx 10^{-2}$ seconds and in $&lt; 1\text{GB}$ of GPU memory. We demonstrate
that it outperforms other GP-based alternatives and competes with
state-of-the-art probabilistic time series forecasting algorithms.",2024-12-27T16:31:09Z,http://arxiv.org/abs/2412.19727v1,"Csaba Tóth, Masaki Adachi, Michael A. Osborne, Harald Oberhauser"
EEG-Reptile: An Automatized Reptile-Based Meta-Learning Library for BCIs,"Meta-learning, i.e., ""learning to learn"", is a promising approach to enable
efficient BCI classifier training with limited amounts of data. It can
effectively use collections of in some way similar classification tasks, with
rapid adaptation to new tasks where only minimal data are available. However,
applying meta-learning to existing classifiers and BCI tasks requires
significant effort. To address this issue, we propose EEG-Reptile, an automated
library that leverages meta-learning to improve classification accuracy of
neural networks in BCIs and other EEG-based applications. It utilizes the
Reptile meta-learning algorithm to adapt neural network classifiers of EEG data
to the inter-subject domain, allowing for more efficient fine-tuning for a new
subject on a small amount of data. The proposed library incorporates an
automated hyperparameter tuning module, a data management pipeline, and an
implementation of the Reptile meta-learning algorithm. EEG-Reptile automation
level allows using it without deep understanding of meta-learning. We
demonstrate the effectiveness of EEG-Reptile on two benchmark datasets (BCI IV
2a, Lee2019 MI) and three neural network architectures (EEGNet, FBCNet,
EEG-Inception). Our library achieved improvement in both zero-shot and few-shot
learning scenarios compared to traditional transfer learning approaches.",2024-12-27T16:24:31Z,http://arxiv.org/abs/2412.19725v1,"Daniil A. Berdyshev, Artem M. Grachev, Sergei L. Shishkin, Bogdan L. Kozyrskiy"
"Exploring low-rank structure for an inverse scattering problem with
  far-field data","The inverse scattering problem exhibits an inherent low-rank structure due to
its ill-posed nature; however developing low-rank structures for the inverse
scattering problem remains challenging. In this work, we introduce a novel
low-rank structure tailored for solving the inverse scattering problem. The
particular low-rank structure is given by the generalized prolate spheroidal
wave functions, computed stably and accurately via a Sturm-Liouville problem.
We first process the far-field data to obtain a post-processed data set within
a disk domain. Subsequently, the post-processed data are projected onto a
low-rank space given by the low-rank structure. The unknown is approximately
solved in this low-rank space, by dropping higher-order terms. The low-rank
structure leads to a H\""{o}lder-logarithmic type stability estimate for
arbitrary unknown functions, and a Lipschitz stability estimate for unknowns
belonging to a finite dimensional low-rank space. Various numerical experiments
are conducted to validate its performance, encompassing assessments of
resolution capability, robustness against randomly added noise and modeling
errors, and demonstration of increasing stability.",2024-12-27T16:24:20Z,http://arxiv.org/abs/2412.19724v1,"Yuyuan Zhou, Lorenzo Audibert, Shixu Meng, Bo Zhang"
"OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse
  Task Synthesis","Graphical User Interface (GUI) agents powered by Vision-Language Models
(VLMs) have demonstrated human-like computer control capability. Despite their
utility in advancing digital automation, a critical bottleneck persists:
collecting high-quality trajectory data for training. Common practices for
collecting such data rely on human supervision or synthetic data generation
through executing pre-defined tasks, which are either resource-intensive or
unable to guarantee data quality. Moreover, these methods suffer from limited
data diversity and significant gaps between synthetic data and real-world
environments. To address these challenges, we propose OS-Genesis, a novel GUI
data synthesis pipeline that reverses the conventional trajectory collection
process. Instead of relying on pre-defined tasks, OS-Genesis enables agents
first to perceive environments and perform step-wise interactions, then
retrospectively derive high-quality tasks to enable trajectory-level
exploration. A trajectory reward model is then employed to ensure the quality
of the generated trajectories. We demonstrate that training GUI agents with
OS-Genesis significantly improves their performance on highly challenging
online benchmarks. In-depth analysis further validates OS-Genesis's efficiency
and its superior data quality and diversity compared to existing synthesis
methods. Our codes, data, and checkpoints are available at
\href{https://qiushisun.github.io/OS-Genesis-Home/}{OS-Genesis Homepage}.",2024-12-27T16:21:58Z,http://arxiv.org/abs/2412.19723v1,"Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, Ben Kao, Guohao Li, Junxian He, Yu Qiao, Zhiyong Wu"
"Quantum correlations in a gravitational collapse simulation with
  SpheriCo.jl","We report on work using a newly developed code, SpheriCo.jl, that computes
the gravitational collapse of a spherical scalar field, where the scalar can be
either a classical field, or a quantum field operator. By utilising
summation-by-parts methods for the numerical derivatives we are able to
simulate the collapse longer than was possible previously due to enhanced
numerical stability. We present a suite of tests for the code that tests its
accuracy and stability, both for the classical and quantum fields. We are able
to observe critical behavior of gravitational collapse for the classical setup,
in agreement with expected results. The code is also used to compute two-point
correlation functions, with results that hint at a non-trivial correlation
across the horizon of Hawking quanta.",2024-12-27T16:20:27Z,http://arxiv.org/abs/2412.19722v1,"Benjamin Berczi, Magdalena Eriksson, Thanasis Giannakopoulos, Paul M. Saffin"
Sharpening Neural Implicit Functions with Frequency Consolidation Priors,"Signed Distance Functions (SDFs) are vital implicit representations to
represent high fidelity 3D surfaces. Current methods mainly leverage a neural
network to learn an SDF from various supervisions including signed distances,
3D point clouds, or multi-view images. However, due to various reasons
including the bias of neural network on low frequency content, 3D unaware
sampling, sparsity in point clouds, or low resolutions of images, neural
implicit representations still struggle to represent geometries with high
frequency components like sharp structures, especially for the ones learned
from images or point clouds. To overcome this challenge, we introduce a method
to sharpen a low frequency SDF observation by recovering its high frequency
components, pursuing a sharper and more complete surface. Our key idea is to
learn a mapping from a low frequency observation to a full frequency coverage
in a data-driven manner, leading to a prior knowledge of shape consolidation in
the frequency domain, dubbed frequency consolidation priors. To better
generalize a learned prior to unseen shapes, we introduce to represent
frequency components as embeddings and disentangle the embedding of the low
frequency component from the embedding of the full frequency component. This
disentanglement allows the prior to generalize on an unseen low frequency
observation by simply recovering its full frequency embedding through a
test-time self-reconstruction. Our evaluations under widely used benchmarks or
real scenes show that our method can recover high frequency component and
produce more accurate surfaces than the latest methods. The code, data, and
pre-trained models are available at \url{https://github.com/chenchao15/FCP}.",2024-12-27T16:18:46Z,http://arxiv.org/abs/2412.19720v1,"Chao Chen, Yu-Shen Liu, Zhizhong Han"
"Text2Insight: Transform natural language text into insights seamlessly
  using multi-model architecture","The growing demand for dynamic, user-centric data analysis and visualization
is evident across domains like healthcare, finance, and research. Traditional
visualization tools often fail to meet individual user needs due to their
static and predefined nature. To address this gap, Text2Insight is introduced
as an innovative solution that delivers customized data analysis and
visualizations based on user-defined natural language requirements. Leveraging
a multi-model architecture, Text2Insight transforms user inputs into actionable
insights and dynamic visualizations.
  The methodology begins with analyzing the input dataset to extract structural
details such as columns and values. A pre-trained Llama3 model converts the
user's natural language query into an SQL query, which is further refined using
a Named Entity Recognition (NER) model for accuracy. A chart predictor
determines the most suitable visualization type, while the Llama3 model
generates insights based on the SQL query's results. The output is a
user-friendly and visually informative chart. To enhance analysis capabilities,
the system integrates a question-answering model and a predictive model using
the BERT framework. These models provide insights into historical data and
predict future trends.
  Performance evaluation of Text2Insight demonstrates its effectiveness,
achieving high accuracy (99%), precision (100%), recall (99%), and F1-score
(99%), with a BLEU score of 0.5. The question-answering model attained an
accuracy of 89% and the predictive model achieved 70% accuracy. These results
validate Text2Insight as a robust and viable solution for transforming natural
language text into dynamic, user-specific data analysis and visualizations.",2024-12-27T16:17:22Z,http://arxiv.org/abs/2412.19718v1,Pradeep Sain
Low-Regularity Global solution for fractional NLS in modulation spaces,"We establish global well-posedness for the mass sub-critical nonlinear
fractional Schr\""odinger equation
  $$iu_t + (-\Delta)^\frac{\beta}{2} u \pm (|u|^{\alpha}u)=0$$
  with radial initial data in modulation spaces $M^{p,\frac{p}{p-1}}(\mathbb
R^n)$ with $2&lt;p$ sufficiently close to $2.$ Our order of dispersion $\beta$
lies in $(2n/ (2n-1), 2)$ for $n \geq 2$.",2024-12-27T16:15:10Z,http://arxiv.org/abs/2412.19714v1,"Divyang G. Bhimani, Diksha Dhingra, Vijay Kumar Sohani"
"ProKAN: Progressive Stacking of Kolmogorov-Arnold Networks for Efficient
  Liver Segmentation","The growing need for accurate and efficient 3D identification of tumors,
particularly in liver segmentation, has spurred considerable research into deep
learning models. While many existing architectures offer strong performance,
they often face challenges such as overfitting and excessive computational
costs. An adjustable and flexible architecture that strikes a balance between
time efficiency and model complexity remains an unmet requirement. In this
paper, we introduce proKAN, a progressive stacking methodology for
Kolmogorov-Arnold Networks (KANs) designed to address these challenges. Unlike
traditional architectures, proKAN dynamically adjusts its complexity by
progressively adding KAN blocks during training, based on overfitting behavior.
This approach allows the network to stop growing when overfitting is detected,
preventing unnecessary computational overhead while maintaining high accuracy.
Additionally, proKAN utilizes KAN's learnable activation functions modeled
through B-splines, which provide enhanced flexibility in learning complex
relationships in 3D medical data. Our proposed architecture achieves
state-of-the-art performance in liver segmentation tasks, outperforming
standard Multi-Layer Perceptrons (MLPs) and fixed KAN architectures. The
dynamic nature of proKAN ensures efficient training times and high accuracy
without the risk of overfitting. Furthermore, proKAN provides better
interpretability by allowing insight into the decision-making process through
its learnable coefficients. The experimental results demonstrate a significant
improvement in accuracy, Dice score, and time efficiency, making proKAN a
compelling solution for 3D medical image segmentation tasks.",2024-12-27T16:14:06Z,http://arxiv.org/abs/2412.19713v1,"Bhavesh Gyanchandani, Aditya Oza, Abhinav Roy"
"Causal machine learning for heterogeneous treatment effects in the
  presence of missing outcome data","When estimating heterogeneous treatment effects, missing outcome data can
complicate treatment effect estimation, causing certain subgroups of the
population to be poorly represented. In this work, we discuss this commonly
overlooked problem and consider the impact that missing at random (MAR) outcome
data has on causal machine learning estimators for the conditional average
treatment effect (CATE). We then propose two de-biased machine learning
estimators for the CATE, the mDR-learner and mEP-learner, which address the
issue of under-representation by integrating inverse probability of censoring
weights into the DR-learner and EP-learner respectively. We show that under
reasonable conditions, these estimators are oracle efficient, and illustrate
their favorable performance through simulated data settings, comparing them to
existing CATE estimators, including comparison to estimators which use common
missing data techniques. Guidance on the implementation of these estimators is
provided and we present an example of their application using the ACTG175
trial, exploring treatment effect heterogeneity when comparing Zidovudine
mono-therapy against alternative antiretroviral therapies among HIV-1-infected
individuals.",2024-12-27T16:10:03Z,http://arxiv.org/abs/2412.19711v1,"Matthew Pryce, Karla Diaz-Ordaz, Ruth H. Keogh, Stijn Vansteelandt"
"Noise Sensitivity of the Semidefinite Programs for Direct Data-Driven
  LQR","In this paper, we study the noise sensitivity of the semidefinite program
(SDP) proposed for direct data-driven infinite-horizon linear quadratic
regulator (LQR) problem for discrete-time linear time-invariant systems. While
this SDP is shown to find the true LQR controller in the noise-free setting, we
show that it leads to a trivial solution with zero gain matrices when data is
corrupted by noise, even when the noise is arbitrarily small. We then study a
variant of the SDP that includes a robustness promoting regularization term and
prove that regularization does not fully eliminate the sensitivity issue. In
particular, the solution of the regularized SDP converges in probability also
to a trivial solution.",2024-12-27T15:59:42Z,http://arxiv.org/abs/2412.19705v1,"Xiong Zeng, Laurent Bako, Necmiye Ozay"
"Search for the double Dalitz decays $η/η' \to e^+e^-μ^+μ^-$
  and $η' \to μ^+μ^-μ^+μ^-$","Using a data sample of $(10087 \pm 44) \times {10^{6}}$ $J/{\psi}$ events
collected with the BESIII detector, we search for the decays $\eta/\eta'\to
e^+e^-\mu^+\mu^-$ and $\eta' \to \mu^+\mu^-\mu^+\mu^-$ via the radiative decays
$J/{\psi}\to\gamma\eta$/$\gamma\eta'$. No excess of events over expected
background is observed for any of the decays of interest. At 90% confidence
level, we report the first upper limits on the branching fractions of $\eta'
\to e^{+}e^{-}\mu^{+}\mu^{-}$ and $\eta' \to \mu^{+}\mu^{-}\mu^{+}\mu^{-}$ to
be $ 1.75 \times {10^{-6}}$ and $5.28 \times {10^{-7}}$, respectively. In
addition, we set an upper limit on the branching fraction of $\eta \to
e^{+}e^{-}\mu^{+}\mu^{-}$ to be $6.88 \times {10^{-6}}$, which improves the
previous result by about two orders of magnitude.",2024-12-27T15:55:02Z,http://arxiv.org/abs/2412.19702v1,"BESIII Collaboration, M. Ablikim, M. N. Achasov, P. Adlarson, O. Afedulidis, X. C. Ai, R. Aliberti, A. Amoroso, Y. Bai, O. Bakina, I. Balossino, Y. Ban, H. -R. Bao, V. Batozskaya, K. Begzsuren, N. Berger, M. Berlowski, M. Bertani, D. Bettoni, F. Bianchi, E. Bianco, A. Bortone, I. Boyko, R. A. Briere, A. Brueggemann, H. Cai, X. Cai, A. Calcaterra, G. F. Cao, N. Cao, S. A. Cetin, X. Y. Chai, J. F. Chang, G. R. Che, Y. Z. Che, G. Chelkov, C. Chen, C. H. Chen, Chao Chen, G. Chen, H. S. Chen, H. Y. Chen, M. L. Chen, S. J. Chen, S. L. Chen, S. M. Chen, T. Chen, X. R. Chen, X. T. Chen, Y. B. Chen, Y. Q. Chen, Z. J. Chen, Z. Y. Chen, S. K. Choi, G. Cibinetto, F. Cossio, J. J. Cui, H. L. Dai, J. P. Dai, A. Dbeyssi, R. E. de Boer, D. Dedovich, C. Q. Deng, Z. Y. Deng, A. Denig, I. Denysenko, M. Destefanis, F. De Mori, B. Ding, X. X. Ding, Y. Ding, Y. Ding, J. Dong, L. Y. Dong, M. Y. Dong, X. Dong, M. C. Du, S. X. Du, Y. Y. Duan, Z. H. Duan, P. Egorov, Y. H. Fan, J. Fang, J. Fang, S. S. Fang, W. X. Fang, Y. Fang, Y. Q. Fang, R. Farinelli, L. Fava, F. Feldbauer, G. Felici, C. Q. Feng, J. H. Feng, Y. T. Feng, M. Fritsch, C. D. Fu, J. L. Fu, Y. W. Fu, H. Gao, X. B. Gao, Y. N. Gao, Yang Gao, S. Garbolino, I. Garzia, L. Ge, P. T. Ge, Z. W. Ge, C. Geng, E. M. Gersabeck, A. Gilman, K. Goetzen, L. Gong, W. X. Gong, W. Gradl, S. Gramigna, M. Greco, M. H. Gu, Y. T. Gu, C. Y. Guan, A. Q. Guo, L. B. Guo, M. J. Guo, R. P. Guo, Y. P. Guo, A. Guskov, J. Gutierrez, K. L. Han, T. T. Han, F. Hanisch, X. Q. Hao, F. A. Harris, K. K. He, K. L. He, F. H. Heinsius, C. H. Heinz, Y. K. Heng, C. Herold, T. Holtmann, P. C. Hong, G. Y. Hou, X. T. Hou, Y. R. Hou, Z. L. Hou, B. Y. Hu, H. M. Hu, J. F. Hu, Q. P. Hu, S. L. Hu, T. Hu, Y. Hu, G. S. Huang, K. X. Huang, L. Q. Huang, X. T. Huang, Y. P. Huang, Y. S. Huang, T. Hussain, F. Hölzken, N. Hüsken, N. in der Wiesche, J. Jackson, S. Janchiv, J. H. Jeong, Q. Ji, Q. P. Ji, W. Ji, X. B. Ji, X. L. Ji, Y. Y. Ji, X. Q. Jia, Z. K. Jia, D. Jiang, H. B. Jiang, P. C. Jiang, S. S. Jiang, T. J. Jiang, X. S. Jiang, Y. Jiang, J. B. Jiao, J. K. Jiao, Z. Jiao, S. Jin, Y. Jin, M. Q. Jing, X. M. Jing, T. Johansson, S. Kabana, N. Kalantar-Nayestanaki, X. L. Kang, X. S. Kang, M. Kavatsyuk, B. C. Ke, V. Khachatryan, A. Khoukaz, R. Kiuchi, O. B. Kolcu, B. Kopf, M. Kuessner, X. Kui, N. Kumar, A. Kupsc, W. Kühn, L. Lavezzi, T. T. Lei, Z. H. Lei, M. Lellmann, T. Lenz, C. Li, C. Li, C. H. Li, Cheng Li, D. M. Li, F. Li, G. Li, H. B. Li, H. J. Li, H. N. Li, Hui Li, J. R. Li, J. S. Li, K. Li, K. L. Li, L. J. Li, L. K. Li, Lei Li, M. H. Li, P. R. Li, Q. M. Li, Q. X. Li, R. Li, S. X. Li, T. Li, W. D. Li, W. G. Li, X. Li, X. H. Li, X. L. Li, X. Y. Li, X. Z. Li, Y. G. Li, Z. J. Li, Z. Y. Li, C. Liang, H. Liang, H. Liang, Y. F. Liang, Y. T. Liang, G. R. Liao, Y. P. Liao, J. Libby, A. Limphirat, C. C. Lin, C. X. Lin, D. X. Lin, T. Lin, B. J. Liu, B. X. Liu, C. Liu, C. X. Liu, F. Liu, F. H. Liu, Feng Liu, G. M. Liu, H. Liu, H. B. Liu, H. H. Liu, H. M. Liu, Huihui Liu, J. B. Liu, J. Y. Liu, K. Liu, K. Y. Liu, Ke Liu, L. Liu, L. C. Liu, Lu Liu, M. H. Liu, P. L. Liu, Q. Liu, S. B. Liu, T. Liu, W. K. Liu, W. M. Liu, X. Liu, X. Liu, Y. Liu, Y. Liu, Y. B. Liu, Z. A. Liu, Z. D. Liu, Z. Q. Liu, X. C. Lou, F. X. Lu, H. J. Lu, J. G. Lu, X. L. Lu, Y. Lu, Y. P. Lu, Z. H. Lu, C. L. Luo, J. R. Luo, M. X. Luo, T. Luo, X. L. Luo, X. R. Lyu, Y. F. Lyu, F. C. Ma, H. Ma, H. L. Ma, J. L. Ma, L. L. Ma, L. R. Ma, M. M. Ma, Q. M. Ma, R. Q. Ma, T. Ma, X. T. Ma, X. Y. Ma, Y. M. Ma, F. E. Maas, I. MacKay, M. Maggiora, S. Malde, Y. J. Mao, Z. P. Mao, S. Marcello, Z. X. Meng, J. G. Messchendorp, G. Mezzadri, H. Miao, T. J. Min, R. E. Mitchell, X. H. Mo, B. Moses, N. Yu. Muchnoi, J. Muskalla, Y. Nefedov, F. Nerling, L. S. Nie, I. B. Nikolaev, Z. Ning, S. Nisar, Q. L. Niu, W. D. Niu, Y. Niu, S. L. Olsen, S. L. Olsen, Q. Ouyang, S. Pacetti, X. Pan, Y. Pan, A. Pathak, Y. P. Pei, M. Pelizaeus, H. P. Peng, Y. Y. Peng, K. Peters, J. L. Ping, R. G. Ping, S. Plura, V. Prasad, F. Z. Qi, H. Qi, H. R. Qi, M. Qi, T. Y. Qi, S. Qian, W. B. Qian, C. F. Qiao, X. K. Qiao, J. J. Qin, L. Q. Qin, L. Y. Qin, X. P. Qin, X. S. Qin, Z. H. Qin, J. F. Qiu, Z. H. Qu, C. F. Redmer, K. J. Ren, A. Rivetti, M. Rolo, G. Rong, Ch. Rosner, M. Q. Ruan, S. N. Ruan, N. Salone, A. Sarantsev, Y. Schelhaas, K. Schoenning, M. Scodeggio, K. Y. Shan, W. Shan, X. Y. Shan, Z. J. Shang, J. F. Shangguan, L. G. Shao, M. Shao, C. P. Shen, H. F. Shen, W. H. Shen, X. Y. Shen, B. A. Shi, H. Shi, H. C. Shi, J. L. Shi, J. Y. Shi, Q. Q. Shi, S. Y. Shi, X. Shi, J. J. Song, T. Z. Song, W. M. Song, Y. J. Song, Y. X. Song, S. Sosio, S. Spataro, F. Stieler, S. S Su, Y. J. Su, G. B. Sun, G. X. Sun, H. Sun, H. K. Sun, J. F. Sun, K. Sun, L. Sun, S. S. Sun, T. Sun, W. Y. Sun, Y. Sun, Y. J. Sun, Y. Z. Sun, Z. Q. Sun, Z. T. Sun, C. J. Tang, G. Y. Tang, J. Tang, M. Tang, Y. A. Tang, L. Y. Tao, Q. T. Tao, M. Tat, J. X. Teng, V. Thoren, W. H. Tian, Y. Tian, Z. F. Tian, I. Uman, Y. Wan, S. J. Wang, B. Wang, B. L. Wang, Bo Wang, D. Y. Wang, F. Wang, H. J. Wang, J. J. Wang, J. P. Wang, K. Wang, L. L. Wang, M. Wang, N. Y. Wang, S. Wang, S. Wang, T. Wang, T. J. Wang, W. Wang, W. Wang, W. P. Wang, X. Wang, X. F. Wang, X. J. Wang, X. L. Wang, X. N. Wang, Y. Wang, Y. D. Wang, Y. F. Wang, Y. H. Wang, Y. L. Wang, Y. N. Wang, Y. Q. Wang, Yaqian Wang, Yi Wang, Z. Wang, Z. L. Wang, Z. Y. Wang, Ziyi Wang, D. H. Wei, F. Weidner, S. P. Wen, Y. R. Wen, U. Wiedner, G. Wilkinson, M. Wolke, L. Wollenberg, C. Wu, J. F. Wu, L. H. Wu, L. J. Wu, X. Wu, X. H. Wu, Y. Wu, Y. H. Wu, Y. J. Wu, Z. Wu, L. Xia, X. M. Xian, B. H. Xiang, T. Xiang, D. Xiao, G. Y. Xiao, S. Y. Xiao, Y. L. Xiao, Z. J. Xiao, C. Xie, X. H. Xie, Y. Xie, Y. G. Xie, Y. H. Xie, Z. P. Xie, T. Y. Xing, C. F. Xu, C. J. Xu, G. F. Xu, H. Y. Xu, M. Xu, Q. J. Xu, Q. N. Xu, W. Xu, W. L. Xu, X. P. Xu, Y. Xu, Y. C. Xu, Z. S. Xu, F. Yan, L. Yan, W. B. Yan, W. C. Yan, X. Q. Yan, H. J. Yang, H. L. Yang, H. X. Yang, J. H. Yang, T. Yang, Y. Yang, Y. F. Yang, Y. F. Yang, Y. X. Yang, Z. W. Yang, Z. P. Yao, M. Ye, M. H. Ye, J. H. Yin, Junhao Yin, Z. Y. You, B. X. Yu, C. X. Yu, G. Yu, J. S. Yu, M. C. Yu, T. Yu, X. D. Yu, Y. C. Yu, C. Z. Yuan, J. Yuan, J. Yuan, L. Yuan, S. C. Yuan, Y. Yuan, Z. Y. Yuan, C. X. Yue, A. A. Zafar, F. R. Zeng, S. H. Zeng, X. Zeng, Y. Zeng, Y. J. Zeng, Y. J. Zeng, X. Y. Zhai, Y. C. Zhai, Y. H. Zhan, A. Q. Zhang, B. L. Zhang, B. X. Zhang, D. H. Zhang, G. Y. Zhang, H. Zhang, H. Zhang, H. C. Zhang, H. H. Zhang, H. H. Zhang, H. Q. Zhang, H. R. Zhang, H. Y. Zhang, J. Zhang, J. Zhang, J. J. Zhang, J. L. Zhang, J. Q. Zhang, J. S. Zhang, J. W. Zhang, J. X. Zhang, J. Y. Zhang, J. Z. Zhang, Jianyu Zhang, L. M. Zhang, Lei Zhang, P. Zhang, Q. Y. Zhang, R. Y. Zhang, S. H. Zhang, Shulei Zhang, X. M. Zhang, X. Y Zhang, X. Y. Zhang, Y. Zhang, Y. Zhang, Y. T. Zhang, Y. H. Zhang, Y. M. Zhang, Yan Zhang, Z. D. Zhang, Z. H. Zhang, Z. L. Zhang, Z. Y. Zhang, Z. Y. Zhang, Z. Z. Zhang, G. Zhao, J. Y. Zhao, J. Z. Zhao, L. Zhao, Lei Zhao, M. G. Zhao, N. Zhao, R. P. Zhao, S. J. Zhao, Y. B. Zhao, Y. X. Zhao, Z. G. Zhao, A. Zhemchugov, B. Zheng, B. M. Zheng, J. P. Zheng, W. J. Zheng, Y. H. Zheng, B. Zhong, X. Zhong, H. Zhou, J. Y. Zhou, L. P. Zhou, S. Zhou, X. Zhou, X. K. Zhou, X. R. Zhou, X. Y. Zhou, Y. Z. Zhou, Z. C. Zhou, A. N. Zhu, J. Zhu, K. Zhu, K. J. Zhu, K. S. Zhu, L. Zhu, L. X. Zhu, S. H. Zhu, T. J. Zhu, W. D. Zhu, Y. C. Zhu, Z. A. Zhu, J. H. Zou, J. Zu"
"An Integrated Optimization and Deep Learning Pipeline for Predicting
  Live Birth Success in IVF Using Feature Optimization and Transformer-Based
  Models","In vitro fertilization (IVF) is a widely utilized assisted reproductive
technology, yet predicting its success remains challenging due to the
multifaceted interplay of clinical, demographic, and procedural factors. This
study develops a robust artificial intelligence (AI) pipeline aimed at
predicting live birth outcomes in IVF treatments. The pipeline uses anonymized
data from 2010 to 2018, obtained from the Human Fertilization and Embryology
Authority (HFEA). We evaluated the prediction performance of live birth success
as a binary outcome (success/failure) by integrating different feature
selection methods, such as principal component analysis (PCA) and particle
swarm optimization (PSO), with different traditional machine learning-based
classifiers including random forest (RF) and decision tree, as well as deep
learning-based classifiers including custom transformer-based model and a tab
transformer model with an attention mechanism. Our research demonstrated that
the best performance was achieved by combining PSO for feature selection with
the TabTransformer-based deep learning model, yielding an accuracy of 99.50%
and an AUC of 99.96%, highlighting its significant performance to predict live
births. This study establishes a highly accurate AI pipeline for predicting
live birth outcomes in IVF, demonstrating its potential to enhance personalized
fertility treatments.",2024-12-27T15:46:59Z,http://arxiv.org/abs/2412.19696v1,"Arezoo Borji, Hossam Haick, Birgit Pohn, Antonia Graf, Jana Zakall, S M Ragib Shahriar Islam, Gernot Kronreif, Daniel Kovatchki, Heinz Strohmer, Sepideh Hatamikia"
"Nonperturbative effects in triple-differential dijet and Z+jet
  production at the LHC","In comparisons of precision collider data to the most accurate highest-order
calculations in perturbative quantum chromodynamics (QCD), it is required to
correct for nonperturbative effects. Such effects are typically studied using
Monte Carlo event generators that complement fixed-order predictions with
perturbative parton showers and models for the nonperturbative effects of the
Underlying Event and hadronisation. Thereby, the final state of collision
events can be predicted at the level of stable particles, which serve as input
for full detector simulations.
  This article investigates the impact of nonperturbative effects on two
processes that may be used for precision determinations of the strong coupling
constant and the proton structure: the triple-differential dijet and Z+jet
production. While nonperturbative effects impact both processes, significant
differences among them are observed and further investigated. Indications are
found that the Underlying Event and hadronisation cannot fully explain these
differences and the perturbative modelling may play a significant role as well.",2024-12-27T15:42:07Z,http://arxiv.org/abs/2412.19694v1,"Stefan Gieseke, Maximilian Horzela, Manjit Kaur, Dari Leonardi, Klaus Rabbertz, Aayushi Singla, Cedric Verstege"
"A Review on the Integration of Artificial Intelligence and Medical
  Imaging in IVF Ovarian Stimulation","Artificial intelligence (AI) has emerged as a powerful tool to enhance
decision-making and optimize treatment protocols in in vitro fertilization
(IVF). In particular, AI shows significant promise in supporting
decision-making during the ovarian stimulation phase of the IVF process. This
review evaluates studies focused on the applications of AI combined with
medical imaging in ovarian stimulation, examining methodologies, outcomes, and
current limitations. Our analysis of 13 studies on this topic reveals that,
reveal that while AI algorithms demonstrated notable potential in predicting
optimal hormonal dosages, trigger timing, and oocyte retrieval outcomes, the
medical imaging data utilized predominantly came from two-dimensional (2D)
ultrasound which mainly involved basic quantifications, such as follicle size
and number, with limited use of direct feature extraction or advanced image
analysis techniques. This points to an underexplored opportunity where advanced
image analysis approaches, such as deep learning, and more diverse imaging
modalities, like three-dimensional (3D) ultrasound, could unlock deeper
insights. Additionally, the lack of explainable AI (XAI) in most studies raises
concerns about the transparency and traceability of AI-driven decisions - key
factors for clinical adoption and trust. Furthermore, many studies relied on
single-center designs and small datasets, which limit the generalizability of
their findings. This review highlights the need for integrating advanced
imaging analysis techniques with explainable AI methodologies, as well as the
importance of leveraging multicenter collaborations and larger datasets.
Addressing these gaps has the potential to enhance ovarian stimulation
management, paving the way for efficient, personalized, and data-driven
treatment pathways that improve IVF outcomes.",2024-12-27T15:29:08Z,http://arxiv.org/abs/2412.19688v1,"Jana Zakall, Birgit Pohn, Antonia Graf, Daniel Kovatchki, Arezoo Borji, Ragib Shahriar Islam, Hossam Haick, Heinz Strohmer, Sepideh Hatamikia"
"Boosting Private Domain Understanding of Efficient MLLMs: A Tuning-free,
  Adaptive, Universal Prompt Optimization Framework","Efficient multimodal large language models (EMLLMs), in contrast to
multimodal large language models (MLLMs), reduce model size and computational
costs and are often deployed on resource-constrained devices. However, due to
data privacy concerns, existing open-source EMLLMs rarely have access to
private domain-specific data during the pre-training process, making them
difficult to directly apply in device-specific domains, such as certain
business scenarios. To address this weakness, this paper focuses on the
efficient adaptation of EMLLMs to private domains, specifically in two areas:
1) how to reduce data requirements, and 2) how to avoid parameter fine-tuning.
Specifically, we propose a tun\textbf{\underline{I}}ng-free,
a\textbf{\underline{D}}aptiv\textbf{\underline{E}},
univers\textbf{\underline{AL}} \textbf{\underline{Prompt}} Optimization
Framework, abbreviated as \textit{\textbf{\ourmethod{}}} which consists of two
stages: 1) Predefined Prompt, based on the reinforcement searching strategy,
generate a prompt optimization strategy tree to acquire optimization priors; 2)
Prompt Reflection initializes the prompt based on optimization priors, followed
by self-reflection to further search and refine the prompt. By doing so,
\ourmethod{} elegantly generates the ``ideal prompts'' for processing private
domain-specific data. Note that our method requires no parameter fine-tuning
and only a small amount of data to quickly adapt to the data distribution of
private data. Extensive experiments across multiple tasks demonstrate that our
proposed \ourmethod{} significantly improves both efficiency and performance
compared to baselines.",2024-12-27T15:21:17Z,http://arxiv.org/abs/2412.19684v1,"Jiang Liu, Bolin Li, Haoyuan Li, Tianwei Lin, Wenqiao Zhang, Tao Zhong, Zhelun Yu, Jinghao Wei, Hao Cheng, Hao Jiang, Zheqi Lv, Juncheng Li, Siliang Tang, Yueting Zhuang"
"A Hybrid Technique for Plant Disease Identification and Localisation in
  Real-time","Over the past decade, several image-processing methods and algorithms have
been proposed for identifying plant diseases based on visual data. DNN (Deep
Neural Networks) have recently become popular for this task. Both traditional
image processing and DNN-based methods encounter significant performance issues
in real-time detection owing to computational limitations and a broad spectrum
of plant disease features. This article proposes a novel technique for
identifying and localising plant disease based on the Quad-Tree decomposition
of an image and feature learning simultaneously. The proposed algorithm
significantly improves accuracy and faster convergence in high-resolution
images with relatively low computational load. Hence it is ideal for deploying
the algorithm in a standalone processor in a remotely operated image
acquisition and disease detection system, ideally mounted on drones and robots
working on large agricultural fields. The technique proposed in this article is
hybrid as it exploits the advantages of traditional image processing methods
and DNN-based models at different scales, resulting in faster inference. The F1
score is approximately 0.80 for four disease classes corresponding to potato
and tomato crops.",2024-12-27T15:20:45Z,http://arxiv.org/abs/2412.19682v1,"Mahendra Kumar Gohil, Anirudha Bhattacharjee, Rwik Rana, Kishan Lal, Samir Kumar Biswas, Nachiketa Tiwari, Bishakh Bhattacharya"
Identifying clusters in Czekanowski's diagram,"Visualizing data through Czekanowski's diagram has as its aim the
illustration of the relationships between objects. Often, obvious clusters of
observations are directly visible. However, it is not straightforward to
precisely delineate these clusters. This paper presents the development of the
package RMaCzek, which now includes features for cluster identification in
Czekanowski diagrams.",2024-12-27T15:04:06Z,http://arxiv.org/abs/2412.19679v1,"Krzysztof Bartoszek, Ying Luo"
"Asymmetrical Reciprocity-based Federated Learning for Resolving
  Disparities in Medical Diagnosis","Geographic health disparities pose a pressing global challenge, particularly
in underserved regions of low- and middle-income nations. Addressing this issue
requires a collaborative approach to enhance healthcare quality, leveraging
support from medically more developed areas. Federated learning emerges as a
promising tool for this purpose. However, the scarcity of medical data and
limited computation resources in underserved regions make collaborative
training of powerful machine learning models challenging. Furthermore, there
exists an asymmetrical reciprocity between underserved and developed regions.
To overcome these challenges, we propose a novel cross-silo federated learning
framework, named FedHelp, aimed at alleviating geographic health disparities
and fortifying the diagnostic capabilities of underserved regions.
Specifically, FedHelp leverages foundational model knowledge via one-time API
access to guide the learning process of underserved small clients, addressing
the challenge of insufficient data. Additionally, we introduce a novel
asymmetric dual knowledge distillation module to manage the issue of asymmetric
reciprocity, facilitating the exchange of necessary knowledge between developed
large clients and underserved small clients. We validate the effectiveness and
utility of FedHelp through extensive experiments on both medical image
classification and segmentation tasks. The experimental results demonstrate
significant performance improvement compared to state-of-the-art baselines,
particularly benefiting clients in underserved regions.",2024-12-27T13:59:58Z,http://arxiv.org/abs/2412.19654v1,"Jiaqi Wang, Ziyi Yin, Quanzeng You, Lingjuan Lyu, Fenglong Ma"
"Distributed Download from an External Data Source in Faulty Majority
  Settings","We extend the study of retrieval problems in distributed networks, focusing
on improving the efficiency and resilience of protocols in the \emph{Data
Retrieval (DR) Model}. The DR Model consists of a complete network (i.e., a
clique) with $k$ peers, up to $\beta k$ of which may be Byzantine (for $\beta
\in [0, 1)$), and a trusted \emph{External Data Source} comprising an array $X$
of $n$ bits ($n \gg k$) that the peers can query. Additionally, the peers can
also send messages to each other. In this work, we focus on the Download
problem that requires all peers to learn $X$. Our primary goal is to minimize
the maximum number of queries made by any honest peer and additionally optimize
time.
  We begin with a randomized algorithm for the Download problem that achieves
optimal query complexity up to a logarithmic factor. For the stronger dynamic
adversary that can change the set of Byzantine peers from one round to the
next, we achieve the optimal time complexity in peer-to-peer communication but
with larger messages. In broadcast communication where all peers (including
Byzantine peers) are required to send the same message to all peers, with
larger messages, we achieve almost optimal time and query complexities for a
dynamic adversary. Finally, in a more relaxed crash fault model, where peers
stop responding after crashing, we address the Download problem in both
synchronous and asynchronous settings. Using a deterministic protocol, we
obtain nearly optimal results for both query complexity and message sizes in
these scenarios.",2024-12-27T13:55:00Z,http://arxiv.org/abs/2412.19649v1,"John Augustine, Soumyottam Chatterjee, Valerie King, Manish Kumar, Shachar Meir, David Peleg"
"Enhancing Vision-Language Tracking by Effectively Converting Textual
  Cues into Visual Cues","Vision-Language Tracking (VLT) aims to localize a target in video sequences
using a visual template and language description. While textual cues enhance
tracking potential, current datasets typically contain much more image data
than text, limiting the ability of VLT methods to align the two modalities
effectively. To address this imbalance, we propose a novel plug-and-play method
named CTVLT that leverages the strong text-image alignment capabilities of
foundation grounding models. CTVLT converts textual cues into interpretable
visual heatmaps, which are easier for trackers to process. Specifically, we
design a textual cue mapping module that transforms textual cues into target
distribution heatmaps, visually representing the location described by the
text. Additionally, the heatmap guidance module fuses these heatmaps with the
search image to guide tracking more effectively. Extensive experiments on
mainstream benchmarks demonstrate the effectiveness of our approach, achieving
state-of-the-art performance and validating the utility of our method for
enhanced VLT.",2024-12-27T13:54:32Z,http://arxiv.org/abs/2412.19648v1,"X. Feng, D. Zhang, S. Hu, X. Li, M. Wu, J. Zhang, X. Chen, K. Huang"
"Chimera: A Block-Based Neural Architecture Search Framework for
  Event-Based Object Detection","Event-based cameras are sensors that simulate the human eye, offering
advantages such as high-speed robustness and low power consumption. Established
Deep Learning techniques have shown effectiveness in processing event data.
Chimera is a Block-Based Neural Architecture Search (NAS) framework
specifically designed for Event-Based Object Detection, aiming to create a
systematic approach for adapting RGB-domain processing methods to the event
domain. The Chimera design space is constructed from various macroblocks,
including Attention blocks, Convolutions, State Space Models, and
MLP-mixer-based architectures, which provide a valuable trade-off between local
and global processing capabilities, as well as varying levels of complexity.
The results on the PErson Detection in Robotics (PEDRo) dataset demonstrated
performance levels comparable to leading state-of-the-art models, alongside an
average parameter reduction of 1.6 times.",2024-12-27T13:50:44Z,http://arxiv.org/abs/2412.19646v1,"Diego A. Silva, Ahmed Elsheikh, Kamilya Smagulova, Mohammed E. Fouda, Ahmed M. Eltawil"
A Brief Overlook on Magnetoplasmadynamic Thrusters,"This paper presents a comprehensive analysis of Magnetoplasmadynamic
Thrusters (MPDT), examining their working principles, performance
characteristics, and potential applications in space propulsion. The study
focuses on both self-field and applied-field MPDT variants, detailing the
fundamental physics of plasma generation, acceleration mechanisms through
Lorentz forces, and plasma detachment processes. Through mathematical modeling
and experimental data analysis, the paper demonstrates MPDTs' capability to
achieve high specific impulse and efficient propellant utilization compared to
chemical propulsion systems. While highlighting their advantages for deep space
missions and satellite operations, the study also addresses key challenges,
including high power requirements and thermal management issues. The research
concludes that despite current technological limitations, MPDTs show promising
potential for future space exploration, particularly for long-duration missions
requiring sustained thrust.",2024-12-27T13:30:13Z,http://arxiv.org/abs/2412.19636v1,Egemen Gover
"Primordial Black Hole Formation from the Upward Step Model: Avoiding
  Overproduction","We investigate the formation of primordial black holes (PBHs) in an upward
step inflationary model, where nonlinearities between curvature perturbations
and field fluctuations introduce a cutoff, deviating from the Gaussian case.
This necessitates a reevaluation of PBH formation, as $\mathcal{R}$ is not the
optimal variable for estimating abundance. Using the extended Press-Schechter
formalism, we show that non-Gaussianity modifies both the curvature
perturbation profile $\mathcal{R}(r)$ and the integration path in probability
space, significantly impacting PBH abundance. Our results reveal that the
abundance initially increases with the parameter $h$, which characterizes the
relaxation stage after the step. However, beyond a critical value ($h \simeq
5.9$), it sharply declines before rising again. Furthermore, we demonstrate
that non-Gaussianity introduces uncertainties in indirect PBH observations via
gravitational waves. Notably, we present an example where a positive $f_{\rm
NL}$ does not necessarily enhance PBH production, contrary to conventional
expectations. Finally, by accounting for non-perturbative effects, we resolve
the overproduction of PBHs suggested by pulsar timing array (PTA) data,
underscoring the critical importance of incorporating non-Gaussianity in future
studies.",2024-12-27T13:21:06Z,http://arxiv.org/abs/2412.19631v1,"Xiaoding Wang, Xiao-Han Ma, Yi-Fu Cai"
"Measurement of the branching fraction, polarization, and time-dependent
  $CP$ asymmetry in $B^0 \to ρ^+ρ^-$ decays and constraint on the CKM
  angle $φ_2$","We present a measurement of the branching fraction and fraction of
longitudinal polarization of $B^0 \to \rho^+ \rho^-$ decays, which have two
$\pi^0$'s in the final state. We also measure time-dependent $CP$ violation
parameters for decays into longitudinally polarized $\rho^+ \rho^-$ pairs. This
analysis is based on a data sample containing $(387\pm6) \times 10^6$ \BBbar
pairs collected with the Belle~II detector at the SuperKEKB asymmetric-energy
$e^+e^-$ collider in 2019-2022. We obtain ${B}(B^0\to\rho^+\rho^-) = (2.88
^{+0.23}_{-0.22} {}^{+0.29}_{-0.27}) \times 10^{-5}, f_{L} = 0.921
^{+0.024}_{-0.025} {}^{+0.017}_{-0.015}$, $S = -0.26\pm0.19\pm0.08$, and $C =
-0.02\pm0.12^{+0.06}_{-0.05}$, where the first uncertainties are statistical
and the second are systematic. We use these results to perform an isospin
analysis to constrain the CKM angle $\phi_2$ and obtain two solutions; the
result consistent with other Standard Model constraints is $\phi_2 =
(92.6^{+4.5}_{-4.8})^\circ$.",2024-12-27T13:00:10Z,http://arxiv.org/abs/2412.19624v1,"Belle II Collaboration, I. Adachi, L. Aggarwal, H. Ahmed, N. Akopov, M. Alhakami, A. Aloisio, N. Althubiti, N. Anh Ky, D. M. Asner, H. Atmacan, V. Aushev, M. Aversano, R. Ayad, V. Babu, N. K. Baghel, P. Bambade, Sw. Banerjee, M. Barrett, M. Bartl, J. Baudot, A. Baur, A. Beaubien, J. Becker, J. V. Bennett, V. Bertacchi, M. Bertemes, E. Bertholet, M. Bessner, S. Bettarini, B. Bhuyan, D. Biswas, A. Bobrov, D. Bodrov, A. Bolz, A. Bondar, J. Borah, A. Boschetti, A. Bozek, M. Bračko, P. Branchini, R. A. Briere, T. E. Browder, A. Budano, S. Bussino, Q. Campagna, M. Campajola, G. Casarosa, C. Cecchi, J. Cerasoli, M. -C. Chang, P. Chang, R. Cheaib, P. Cheema, B. G. Cheon, K. Chilikin, K. Chirapatpimol, H. -E. Cho, K. Cho, S. -J. Cho, S. -K. Choi, S. Choudhury, J. Cochran, L. Corona, J. X. Cui, E. De La Cruz-Burelo, S. A. De La Motte, G. De Nardo, G. De Pietro, R. de Sangro, M. Destefanis, S. Dey, F. Di Capua, J. Dingfelder, Z. Doležal, I. Domínguez Jiménez, T. V. Dong, X. Dong, M. Dorigo, D. Dossett, K. Dugic, G. Dujany, P. Ecker, J. Eppelt, P. Feichtinger, T. Ferber, T. Fillinger, C. Finck, G. Finocchiaro, A. Fodor, F. Forti, B. G. Fulsom, A. Gabrielli, E. Ganiev, M. Garcia-Hernandez, R. Garg, G. Gaudino, V. Gaur, A. Gaz, A. Gellrich, G. Ghevondyan, D. Ghosh, H. Ghumaryan, G. Giakoustidis, R. Giordano, A. Giri, P. Gironella Gironell, A. Glazov, B. Gobbo, R. Godang, O. Gogota, P. Goldenzweig, W. Gradl, E. Graziani, D. Greenwald, Z. Gruberová, Y. Guan, K. Gudkova, I. Haide, T. Hara, C. Harris, K. Hayasaka, S. Hazra, C. Hearty, M. T. Hedges, A. Heidelbach, I. Heredia de la Cruz, M. Hernández Villanueva, T. Higuchi, M. Hoek, M. Hohmann, R. Hoppe, P. Horak, C. -L. Hsu, T. Humair, T. Iijima, K. Inami, N. Ipsita, A. Ishikawa, R. Itoh, M. Iwasaki, D. Jacobi, W. W. Jacobs, E. -J. Jang, Y. Jin, A. Johnson, H. Junkerkalefeld, M. Kaleta, A. B. Kaliyar, J. Kandra, F. Keil, C. Ketter, C. Kiesling, C. -H. Kim, D. Y. Kim, J. -Y. Kim, K. -H. Kim, Y. -K. Kim, K. Kinoshita, P. Kodyš, T. Koga, S. Kohani, K. Kojima, A. Korobov, S. Korpar, E. Kovalenko, R. Kowalewski, P. Križan, P. Krokovny, T. Kuhr, Y. Kulii, R. Kumar, K. Kumara, T. Kunigo, A. Kuzmin, Y. -J. Kwon, S. Lacaprara, K. Lalwani, T. Lam, L. Lanceri, J. S. Lange, T. S. Lau, M. Laurenza, R. Leboucher, F. R. Le Diberder, M. J. Lee, C. Lemettais, P. Leo, L. K. Li, Q. M. Li, W. Z. Li, Y. Li, Y. B. Li, Y. P. Liao, J. Libby, J. Lin, S. Lin, M. H. Liu, Q. Y. Liu, Z. Q. Liu, D. Liventsev, S. Longo, T. Lueck, C. Lyu, Y. Ma, C. Madaan, M. Maggiora, S. P. Maharana, R. Maiti, G. Mancinelli, R. Manfredi, E. Manoni, M. Mantovano, D. Marcantonio, S. Marcello, C. Marinas, C. Martellini, A. Martens, A. Martini, T. Martinov, L. Massaccesi, M. Masuda, K. Matsuoka, D. Matvienko, S. K. Maurya, M. Maushart, J. A. McKenna, F. Meier, D. Meleshko, M. Merola, C. Miller, M. Mirra, S. Mitra, K. Miyabayashi, H. Miyake, G. B. Mohanty, S. Mondal, S. Moneta, H. -G. Moser, R. Mussa, I. Nakamura, M. Nakao, Y. Nakazawa, M. Naruki, Z. Natkaniec, A. Natochii, M. Nayak, G. Nazaryan, M. Neu, S. Nishida, S. Ogawa, R. Okubo, H. Ono, Y. Onuki, G. Pakhlova, S. Pardi, K. Parham, H. Park, J. Park, K. Park, S. -H. Park, A. Passeri, S. Patra, T. K. Pedlar, I. Peruzzi, R. Peschke, R. Pestotnik, L. E. Piilonen, P. L. M. Podesta-Lerma, T. Podobnik, S. Pokharel, C. Praz, S. Prell, E. Prencipe, M. T. Prim, H. Purwar, S. Raiz, K. Ravindran, J. U. Rehman, M. Reif, S. Reiter, M. Remnev, L. Reuter, D. Ricalde Herrmann, I. Ripp-Baudot, G. Rizzo, M. Roehrken, J. M. Roney, A. Rostomyan, N. Rout, Y. Sakai, D. A. Sanders, S. Sandilya, L. Santelj, V. Savinov, B. Scavino, C. Schwanda, A. J. Schwartz, Y. Seino, A. Selce, K. Senyo, J. Serrano, M. E. Sevior, C. Sfienti, W. Shan, X. D. Shi, T. Shillington, J. -G. Shiu, D. Shtol, B. Shwartz, A. Sibidanov, F. Simon, J. Skorupa, R. J. Sobie, M. Sobotzik, A. Soffer, A. Sokolov, E. Solovieva, S. Spataro, B. Spruck, W. Song, M. Starič, P. Stavroulakis, S. Stefkova, R. Stroili, J. Strube, M. Sumihama, K. Sumisawa, N. Suwonjandee, H. Svidras, M. Takizawa, U. Tamponi, K. Tanida, F. Tenchini, A. Thaller, O. Tittel, R. Tiwary, E. Torassa, K. Trabelsi, I. Tsaklidis, I. Ueda, T. Uglov, K. Unger, Y. Unno, K. Uno, S. Uno, P. Urquijo, Y. Ushiroda, S. E. Vahsen, R. van Tonder, K. E. Varvell, M. Veronesi, A. Vinokurova, V. S. Vismaya, L. Vitale, V. Vobbilisetti, R. Volpe, M. Wakai, S. Wallner, M. -Z. Wang, A. Warburton, M. Watanabe, S. Watanuki, C. Wessel, E. Won, X. P. Xu, B. D. Yabsley, S. Yamada, W. Yan, J. Yelton, J. H. Yin, K. Yoshihara, J. Yuan, Y. Yusa, L. Zani, V. Zhilich, J. S. Zhou, Q. D. Zhou, L. Zhu, R. Žlebčík"
Signatures of prediction during natural listening in MEG data?,"The brain uses contextual information and prior knowledge to anticipate
upcoming content during language comprehension. Recent research has shown
predictive signals can be revealed in pre-onset ECoG activity during
naturalistic narrative listening, by building encoding models based on word
embeddings from Large Language Models (LLMs). Similarly, evidence for
long-range predictive encoding has been observed in fMRI data, where
incorporating embeddings for multiple upcoming words in a narrative improves
alignment with brain activity. This study examines whether similar predictive
information can be detected in MEG, a technique with higher temporal resolution
than fMRI but a lower signal-to-noise ratio than ECoG. Our findings indicate
that MEG captures pre-onset representations up to 1 second before word onset,
consistent with ECoG results. However, unlike fMRI findings, incorporating
future word embeddings did not enhance MEG encoding, even for one word into the
future, which suggests that the pre-onset encoding may not reflect predictive
processing. This work demonstrates that MEG combined with LLMs is a valuable
approach for studying language processing in naturalistic narratives and
highlights the need to study further what constitutes evidence for prediction
during natural listening.",2024-12-27T12:49:03Z,http://arxiv.org/abs/2412.19622v1,"Sahel Azizpour, Britta U. Westner, Jakub Szewczyk, Umut Güçlü, Linda Geerligs"
"The Key Steps and Distinct Performance Trends of Pyrrolic vs. Pyridinic
  M-N-C Catalysts in Electrocatalytic Nitrate Reduction","Electrochemical nitrate reduction reaction(NO3RR)offers a sustainable route
for ambient ammonia synthesis. While metal-nitrogen-carbon (M-N-C) single-atom
catalysts have emerged as promising candidates for NO3RR, the
structure-activity relations underlying their catalytic behavior remain to be
elucidated. Through systematic analysis of reported experimental data and
pH-field coupled microkinetic modelling on a reversible hydrogen electrode
(RHE) scale, we reveal that the coordination-dependent activity originates from
distinct scaling relations governed by metal-intermediate interactions.
M-N-Pyrrolic catalysts demonstrate higher turnover frequencies for ammonia
production, whereas M-N-Pyridinic catalysts exhibit broader activity ranges
across the activity volcano plot. Meanwhile, the adsorption and protonation of
nitrate, which is a step often dismissed and/or assumed to be simultaneous in
many previous reports, is identified to be the rate-determining step (RDS) in
NO3RR. Remarkably, our subsequent experimental validation confirms the
theoretical predictions under both neutral and alkaline conditions. This study
offers a comprehensive mechanistic framework for interpreting the
electrocatalytic activity of M-N-C catalysts in NO3RR, showing that a classical
thermodynamic limiting-potential model is not sufficiently accurate to capture
the RDS and the catalytic performance trends of different materials (even on
M-N-Pyrrolic and M-N-Pyridinic catalysts). These findings provide brand new
insights into the reaction mechanism of NO3RR and establish fundamental design
principles for electrocatalytic ammonia synthesis.",2024-12-27T12:23:09Z,http://arxiv.org/abs/2412.19615v1,"Qiuling Jiang, Mingyao Gu, Tianyi Wang, Fangzhou Liu, Xin Yang, Di Zhang, Zhijian Wu, Ying Wang, Li Wei, Hao Li"
Super-bath Quantum Eigensolver,"Simulating the dynamics of a system coupled to a suitable environment is a
promising approach in quantum computing for determining the ground state of
physical systems. However, this approach requires not only the
$\textit{existence}$ of an environment that allows the system to dissipate
energy and evolve to its ground state, but also the environment's
characteristics to be $\textit{known}$ in detail. In this paper, we propose an
algorithm with a sufficient condition for achieving polynomial-time complexity
in ground state preparation: the existence of an environment that enables the
system to evolve to its ground state in polynomial time, while such
environment's details may remain $\textit{unknown}$. The proposed algorithm is
Super-bath Quantum Eigensolver, which solves the system's ground state by
utilizing quasi-steady state preparation and simulating the coupling between
the system and the super-bath. Supported by experimental lifetime data of
nuclear metastable states, we suggest that our algorithm is applicable to
determine nuclear ground states in polynomial time. These results highlight the
potential advantage of quantum computing in addressing ground state problems in
real-world physical systems.",2024-12-27T11:46:47Z,http://arxiv.org/abs/2412.19599v1,"Tianren Wang, Zongkang Zhang, Bing-Nan Lu, Mauro Cirio, Ying Li"
Composite nature of the $T_{cc}$ state,"In 2021, LHCb collaboration reported a very narrow state in the $D^0D^0\pi^+$
mass spectrum just below the $D^{*+}D^0$ mass threshold. We consider the
influence of the Castillejo-Dalitz-Dyson (CDD) pole in the scattering amplitude
to derive a general treatment for the two-body final state interaction near its
threshold. The line shape (or the energy dependent event distribution) are then
obtained, where the parameters can be fixed by fitting to the experimental data
on the $D^0D^0\pi^+$ mass spectrum. Within our method the data are quite well
reproduced. The pole structure in the complex energy plane indicates the bound
state structure of the $T_{cc}$ state. The compositeness as a measure of
molecule component in its hadron wave function is predicted to be
$0.23_{-0.09}^{+0.40}$. The non-molecular component, e.g., the compact
tetraquark also takes a non-negligible portion.",2024-12-27T11:42:44Z,http://arxiv.org/abs/2412.19597v1,"Xian-Wei Kang, Wen-Shuo Ding"
A counterexample to a Brenti-Carnevale conjecture,"Recently, F. Brenti put a preprint on the arXiv with several interesting open
problems on Coxeter groups and unimodality. In this note, we refute one of
these conjectures with a counterexample and provide supporting data related to
it. This work serves as an initial step toward further exploration of the
topic.",2024-12-27T11:26:53Z,http://arxiv.org/abs/2412.19593v1,"Nathan Chapelier-Laget, Jean Fromentin"
DAS3R: Dynamics-Aware Gaussian Splatting for Static Scene Reconstruction,"We propose a novel framework for scene decomposition and static background
reconstruction from everyday videos. By integrating the trained motion masks
and modeling the static scene as Gaussian splats with dynamics-aware
optimization, our method achieves more accurate background reconstruction
results than previous works. Our proposed method is termed DAS3R, an
abbreviation for Dynamics-Aware Gaussian Splatting for Static Scene
Reconstruction. Compared to existing methods, DAS3R is more robust in complex
motion scenarios, capable of handling videos where dynamic objects occupy a
significant portion of the scene, and does not require camera pose inputs or
point cloud data from SLAM-based methods. We compared DAS3R against recent
distractor-free approaches on the DAVIS and Sintel datasets; DAS3R demonstrates
enhanced performance and robustness with a margin of more than 2 dB in PSNR.
The project's webpage can be accessed via \url{https://kai422.github.io/DAS3R/}",2024-12-27T10:59:46Z,http://arxiv.org/abs/2412.19584v1,"Kai Xu, Tze Ho Elden Tse, Jizong Peng, Angela Yao"
"A Comparative Study of Machine Unlearning Techniques for Image and Text
  Classification Models","Machine Unlearning has emerged as a critical area in artificial intelligence,
addressing the need to selectively remove learned data from machine learning
models in response to data privacy regulations. This paper provides a
comprehensive comparative analysis of six state-of-theart unlearning techniques
applied to image and text classification tasks. We evaluate their performance,
efficiency, and compliance with regulatory requirements, highlighting their
strengths and limitations in practical scenarios. By systematically analyzing
these methods, we aim to provide insights into their applicability,
challenges,and tradeoffs, fostering advancements in the field of ethical and
adaptable machine learning.",2024-12-27T10:58:55Z,http://arxiv.org/abs/2412.19583v1,"Omar M. Safa, Mahmoud M. Abdelaziz, Mustafa Eltawy, Mohamed Mamdouh, Moamen Gharib, Salaheldin Eltenihy, Nagia M. Ghanem, Mohamed M. Ismail"
"Readout of strongly coupled NV center-pair spin states with deep neural
  networks","Optically addressable electron spin clusters are of interest for quantum
computation, simulation and sensing. However, with interaction length scales of
a few tens of nanometers in the strong coupling regime, they are unresolved in
conventional confocal microscopy, making individual readout problematic. Here
we show that when using a single shot readout technique, collective states of
the combined register space become accessible. By using spin to charge
conversion of the defects we draw the connection between the intricate photon
count statistics with spin state tomography using deep neural networks. This
approach is particularly versatile with further scaling the number of
constituent spins in a cluster due to complexity of the analytical treatment.
We perform a proof of concept measurement of the correlated classical signal,
paving the way for using our technique in realistic applications.",2024-12-27T10:56:04Z,http://arxiv.org/abs/2412.19581v1,"Matthew Joliffe, Vadim Vorobyov, Jörg Wrachtrup"
"Gauging or extending bulk and boundary conformal field theories:
  Application to bulk and domain wall problem in topological matter and their
  descriptions by (mock) modular covariant","We study gauging operations (or group extensions) in (smeared) boundary
conformal field theories (BCFTs) and bulk conformal field theories and their
applications to various phenomena in topologically ordered systems. We apply
the resultant theories to the correspondence between the renormalization group
(RG) flow of CFTs and the classification of topological quantum field theories
in the testable information of general classes of partition functions. One can
obtain the bulk topological properties of $2+1$ dimensional topological ordered
phase corresponding to the massive RG flow of $1+1$ dimensional systems, or
smeared BCFT. We present an obstruction of mass condensation for smeared BCFT
analogous to the Lieb-Shultz-Mattis theorem for noninvertible symmetry. Related
to the bulk topological degeneracies in $2+1$ dimensions and quantum phases in
$1+1$ dimensions we construct a new series of BCFT. We also investigate the
implications of the massless RG flow of $1+1$ dimensional CFT to $2+1$
dimensional topological order which corresponds to the earlier proposal by L.
Kong and H. Zheng in [Nucl. Phys. B 966 (2021), 115384], arXiv:1912.01760
closely related to the integer-spin simple current by Schellekens and
Gato-Rivera. We study the properties of the product of two CFTs connected by
the two kinds of massless flows. The (mock) modular covariants appearing in the
analysis seem to contain new ones. By applying the folding trick to the coupled
model, we provide a general method to solve the gapped and charged domain wall.
One can obtain the general phenomenology of the transportation of anyons
through the domain wall. Our work gives a unified direction for the future
theoretical and numerical studies of the topological phase based on the
established data of classifications of conformal field theories or modular
invariants.",2024-12-27T10:46:30Z,http://arxiv.org/abs/2412.19577v1,Yoshiki Fukusumi
"The possible long-term periodic variability of the extremely luminous
  quasar WISE J090924.01+000211.1","The extremely luminous infrared galaxy (ELIRG), WISE J090924.01+000211.1
(hereafter; WISE J0909+0002, $z=1.87$) is an extraordinary object with a quasar
aspect. This study performs monitoring observations of WISE J0909+0002 with the
105 cm Murikabushi telescope, Okayama and Akeno 50 cm telescopes/MITSuME ($g'$,
$R_{\rm c}$, and $I_{\rm c}$ bands), and the SaCRA 55 cm telescope/MuSaSHI
($r$, $i$, and $z$ bands). We obtain the following results by combining the
UV/optical light curves of the CRTS, Pan-STARRS, and ZTF archive data, and our
observational data: (1) the light curves of WISE J0909+0002 present
quasi-periodic (sinusoidal) oscillations with the rest-frame period of $\sim$
660$-$689 day; (2) the structure functions of WISE J0909+0002 do not show a
damped random walk (DRW) trend; (3) the mock DRW light curves present
periodic-like trend on rare occasions in 10000 simulations; (4) the
relativistic boost scenario is favored, since the relation between variability
amplitude and power-law slope ratio is consistent with the theoretical
prediction of this scenario, and a substantial parameter space exists between
the inclination angles and the black hole mass; (5) the circumbinary disk model
is difficult to explain the spectral energy distribution of our target; (6) the
significant radio flux density of WISE J0909+0002 is not detected from the VLA
FIRST Survey, thus the radio jet precession scenario is ruled out. From our
results, the Doppler boost scenario is likely as a cause of the periodic
variability, consequently the quasi-periodic oscillations in WISE J0909+0002 is
possibly interpreted by a supermassive blackhole binary. Additional
observations to investigate the continuity of the periodic trend would bring
new insights into mechanisms of the quasi-periodic oscillations and/or ELIRGs.",2024-12-27T10:33:11Z,http://arxiv.org/abs/2412.19573v1,"Takashi Horiuchi, Yoshiki Toba, Toru Misawa, Katsuhiro L. Murata, Keisuke Isogai, Yoichi Yatsu, Ichiro Takahashi, Mahito Sasada, Masafumi Niwano, Narikazu Higuchi, Shunsuke Hayatsu, Hibiki Seki, Yumiko Oasa, Rikuto Sato"
"Nonminimally coupled Dark Matter in Clusters of Galaxies: a fully
  comprehensive analysis","In this study, we explore how a non-minimal coupling between dark matter and
gravity can affect the behavior of dark matter in galaxy clusters. We have
considered the case of a disformal coupling, which leads to a modification of
the Poisson equation. Building on an earlier work, we expand the analysis
considering all possible disformal coupling scenarios and employing various
dark matter density profiles. In doing so, we aim to constrain the key
parameter in our model, the characteristic coupling length. To achieve this, we
analyze data from a combination of strong and weak lensing using three
statistical approaches: a single cluster fitting procedure, a joint analysis,
and one with stacked profiles. Our findings show that the coupling length is
typically very small, thus being fully consistent with general relativity,
although with an upper limit at $1\sigma$ which is of the order of $100$ kpc.",2024-12-27T10:12:29Z,http://arxiv.org/abs/2412.19569v1,"Saboura Zamani, Vincenzo Salzano, Dario Bettoni"
Quantiles under ambiguity and risk sharing,"Choquet capacities and integrals are central concepts in decision making
under ambiguity or model uncertainty, pioneered by Schmeidler. Motivated by
risk optimization problems for quantiles under ambiguity, we study the subclass
of Choquet integrals, called Choquet quantiles, which generalizes the usual
(probabilistic) quantiles, also known as Value-at-Risk in finance, from
probabilities to capacities. Choquet quantiles share many features with
probabilistic quantiles, in terms of axiomatic representation, optimization
formulas, and risk sharing. We characterize Choquet quantiles via only one
axiom, called ordinality. We prove that the inf-convolution of Choquet
quantiles is again a Choquet quantile, leading to explicit optimal allocations
in risk sharing problems for quantile agents under ambiguity. A new class of
risk measures, Choquet Expected Shortfall, is introduced, which enjoys most
properties of the coherent risk measure Expected Shortfall. Our theory is
complemented by optimization algorithms, numerical examples, and a stylized
illustration with financial data.",2024-12-27T09:22:19Z,http://arxiv.org/abs/2412.19546v1,"Peng Liu, Tiantian Mao, Ruodu Wang"
"TARGA: Targeted Synthetic Data Generation for Practical Reasoning over
  Structured Data","Semantic parsing, which converts natural language questions into logic forms,
plays a crucial role in reasoning within structured environments. However,
existing methods encounter two significant challenges: reliance on extensive
manually annotated datasets and limited generalization capability to unseen
examples. To tackle these issues, we propose Targeted Synthetic Data Generation
(TARGA), a practical framework that dynamically generates high-relevance
synthetic data without manual annotation. Starting from the pertinent entities
and relations of a given question, we probe for the potential relevant queries
through layer-wise expansion and cross-layer combination. Then we generate
corresponding natural language questions for these constructed queries to
jointly serve as the synthetic demonstrations for in-context learning.
Experiments on multiple knowledge base question answering (KBQA) datasets
demonstrate that TARGA, using only a 7B-parameter model, substantially
outperforms existing non-fine-tuned methods that utilize close-sourced model,
achieving notable improvements in F1 scores on GrailQA(+7.7) and
KBQA-Agent(+12.2). Furthermore, TARGA also exhibits superior sample efficiency,
robustness, and generalization capabilities under non-I.I.D. settings.",2024-12-27T09:16:39Z,http://arxiv.org/abs/2412.19544v1,"Xiang Huang, Jiayu Shen, Shanshan Huang, Sitao Cheng, Xiaxia Wang, Yuzhong Qu"
Diverse Rare Sample Generation with Pretrained GANs,"Deep generative models are proficient in generating realistic data but
struggle with producing rare samples in low density regions due to their
scarcity of training datasets and the mode collapse problem. While recent
methods aim to improve the fidelity of generated samples, they often reduce
diversity and coverage by ignoring rare and novel samples. This study proposes
a novel approach for generating diverse rare samples from high-resolution image
datasets with pretrained GANs. Our method employs gradient-based optimization
of latent vectors within a multi-objective framework and utilizes normalizing
flows for density estimation on the feature space. This enables the generation
of diverse rare images, with controllable parameters for rarity, diversity, and
similarity to a reference image. We demonstrate the effectiveness of our
approach both qualitatively and quantitatively across various datasets and GANs
without retraining or fine-tuning the pretrained GANs.",2024-12-27T09:10:30Z,http://arxiv.org/abs/2412.19543v1,"Subeen Lee, Jiyeon Han, Soyeon Kim, Jaesik Choi"
Interacted Object Grounding in Spatio-Temporal Human-Object Interactions,"Spatio-temporal Human-Object Interaction (ST-HOI) understanding aims at
detecting HOIs from videos, which is crucial for activity understanding.
However, existing whole-body-object interaction video benchmarks overlook the
truth that open-world objects are diverse, that is, they usually provide
limited and predefined object classes. Therefore, we introduce a new open-world
benchmark: Grounding Interacted Objects (GIO) including 1,098 interacted
objects class and 290K interacted object boxes annotation. Accordingly, an
object grounding task is proposed expecting vision systems to discover
interacted objects. Even though today's detectors and grounding methods have
succeeded greatly, they perform unsatisfactorily in localizing diverse and rare
objects in GIO. This profoundly reveals the limitations of current vision
systems and poses a great challenge. Thus, we explore leveraging
spatio-temporal cues to address object grounding and propose a 4D
question-answering framework (4D-QA) to discover interacted objects from
diverse videos. Our method demonstrates significant superiority in extensive
experiments compared to current baselines. Data and code will be publicly
available at https://github.com/DirtyHarryLYL/HAKE-AVA.",2024-12-27T09:08:46Z,http://arxiv.org/abs/2412.19542v1,"Xiaoyang Liu, Boran Wen, Xinpeng Liu, Zizheng Zhou, Hongwei Fan, Cewu Lu, Lizhuang Ma, Yulong Chen, Yong-Lu Li"
"Scalable Hierarchical Reinforcement Learning for Hyper Scale Multi-Robot
  Task Planning","To improve the efficiency of warehousing system and meet huge customer
orders, we aim to solve the challenges of dimension disaster and dynamic
properties in hyper scale multi-robot task planning (MRTP) for robotic mobile
fulfillment system (RMFS). Existing research indicates that hierarchical
reinforcement learning (HRL) is an effective method to reduce these challenges.
Based on that, we construct an efficient multi-stage HRL-based multi-robot task
planner for hyper scale MRTP in RMFS, and the planning process is represented
with a special temporal graph topology. To ensure optimality, the planner is
designed with a centralized architecture, but it also brings the challenges of
scaling up and generalization that require policies to maintain performance for
various unlearned scales and maps. To tackle these difficulties, we first
construct a hierarchical temporal attention network (HTAN) to ensure basic
ability of handling inputs with unfixed lengths, and then design multi-stage
curricula for hierarchical policy learning to further improve the scaling up
and generalization ability while avoiding catastrophic forgetting.
Additionally, we notice that policies with hierarchical structure suffer from
unfair credit assignment that is similar to that in multi-agent reinforcement
learning, inspired of which, we propose a hierarchical reinforcement learning
algorithm with counterfactual rollout baseline to improve learning performance.
Experimental results demonstrate that our planner outperform other
state-of-the-art methods on various MRTP instances in both simulated and
real-world RMFS. Also, our planner can successfully scale up to hyper scale
MRTP instances in RMFS with up to 200 robots and 1000 retrieval racks on
unlearned maps while keeping superior performance over other methods.",2024-12-27T09:07:11Z,http://arxiv.org/abs/2412.19538v1,"Xuan Zhou, Xiang Shi, Lele Zhang, Chen Chen, Hongbo Li, Lin Ma, Fang Deng, Jie Chen"
"Finger in Camera Speaks Everything: Unconstrained Air-Writing for
  Real-World","Air-writing is a challenging task that combines the fields of computer vision
and natural language processing, offering an intuitive and natural approach for
human-computer interaction. However, current air-writing solutions face two
primary challenges: (1) their dependency on complex sensors (e.g., Radar, EEGs
and others) for capturing precise handwritten trajectories, and (2) the absence
of a video-based air-writing dataset that covers a comprehensive vocabulary
range. These limitations impede their practicality in various real-world
scenarios, including the use on devices like iPhones and laptops. To tackle
these challenges, we present the groundbreaking air-writing Chinese character
video dataset (AWCV-100K-UCAS2024), serving as a pioneering benchmark for
video-based air-writing. This dataset captures handwritten trajectories in
various real-world scenarios using commonly accessible RGB cameras, eliminating
the need for complex sensors. AWCV-100K-UCAS2024 includes 8.8 million video
frames, encompassing the complete set of 3,755 characters from the GB2312-80
level-1 set (GB1). Furthermore, we introduce our baseline approach, the
video-based character recognizer (VCRec). VCRec adeptly extracts fingertip
features from sparse visual cues and employs a spatio-temporal sequence module
for analysis. Experimental results showcase the superior performance of VCRec
compared to existing models in recognizing air-written characters, both
quantitatively and qualitatively. This breakthrough paves the way for enhanced
human-computer interaction in real-world contexts. Moreover, our approach
leverages affordable RGB cameras, enabling its applicability in a diverse range
of scenarios. The code and data examples will be made public at
https://github.com/wmeiqi/AWCV.",2024-12-27T09:04:04Z,http://arxiv.org/abs/2412.19537v1,"Meiqi Wu, Kaiqi Huang, Yuanqiang Cai, Shiyu Hu, Yuzhong Zhao, Weiqiang Wang"
Is Your Text-to-Image Model Robust to Caption Noise?,"In text-to-image (T2I) generation, a prevalent training technique involves
utilizing Vision Language Models (VLMs) for image re-captioning. Even though
VLMs are known to exhibit hallucination, generating descriptive content that
deviates from the visual reality, the ramifications of such caption
hallucinations on T2I generation performance remain under-explored. Through our
empirical investigation, we first establish a comprehensive dataset comprising
VLM-generated captions, and then systematically analyze how caption
hallucination influences generation outcomes. Our findings reveal that (1) the
disparities in caption quality persistently impact model outputs during
fine-tuning. (2) VLMs confidence scores serve as reliable indicators for
detecting and characterizing noise-related patterns in the data distribution.
(3) even subtle variations in caption fidelity have significant effects on the
quality of learned representations. These findings collectively emphasize the
profound impact of caption quality on model performance and highlight the need
for more sophisticated robust training algorithm in T2I. In response to these
observations, we propose a approach leveraging VLM confidence score to mitigate
caption noise, thereby enhancing the robustness of T2I models against
hallucination in caption.",2024-12-27T08:53:37Z,http://arxiv.org/abs/2412.19531v1,"Weichen Yu, Ziyan Yang, Shanchuan Lin, Qi Zhao, Jianyi Wang, Liangke Gui, Matt Fredrikson, Lu Jiang"
"Exploiting Domain-Specific Parallel Data on Multilingual Language Models
  for Low-resource Language Translation","Neural Machine Translation (NMT) systems built on multilingual
sequence-to-sequence Language Models (msLMs) fail to deliver expected results
when the amount of parallel data for a language, as well as the language's
representation in the model are limited. This restricts the capabilities of
domain-specific NMT systems for low-resource languages (LRLs). As a solution,
parallel data from auxiliary domains can be used either to fine-tune or to
further pre-train the msLM. We present an evaluation of the effectiveness of
these two techniques in the context of domain-specific LRL-NMT. We also explore
the impact of domain divergence on NMT model performance. We recommend several
strategies for utilizing auxiliary parallel data in building domain-specific
NMT models for LRLs.",2024-12-27T08:25:52Z,http://arxiv.org/abs/2412.19522v1,"Surangika Ranathungaa, Shravan Nayak, Shih-Ting Cindy Huang, Yanke Mao, Tong Su, Yun-Hsiang Ray Chan, Songchen Yuan, Anthony Rinaldi, Annie En-Shiun Lee"
"Improved measurements of neutron lifetime with cold neutron beam at
  J-PARC","The ``neutron lifetime puzzle'' arises from the discrepancy between neutron
lifetime measurements obtained using the beam method, which measures decay
products, and the bottle method, which measures the disappearance of neutrons.
To resolve this puzzle, we conducted an experiment using a pulsed cold neutron
beam at J-PARC. In this experiment, the neutron lifetime is determined from the
ratio of neutron decay counts to $^3$He(n,p)$^3$H reactions in a gas detector.
This experiment belongs to the beam method but differs from previous
experiments that measured protons, as it instead detects electrons, enabling
measurements with distinct systematic uncertainties. By enlarging the beam
transport system and reducing systematic uncertainties, we achieved a fivefold
improvement in precision. Analysis of all acquired data yielded a neutron
lifetime of $\tau_{\rm n}=877.2~\pm~1.7_{\rm(stat.)}~^{+4.0}_{-3.6}{}_{\rm
(sys.)}$ s. This result is consistent with bottle method measurements but
exhibits a 2.3$\sigma$ tension with the average value obtained from the
proton-detection-based beam method.",2024-12-27T08:19:54Z,http://arxiv.org/abs/2412.19519v1,"Y. Fuwa, T. Hasegawa, K. Hirota, T. Hoshino, R. Hosokawa, G. Ichikawa, S. Ieki, T. Ino, Y. Iwashita, M. Kitaguchi, R. Kitahara, S. Makise, K. Mishima, T. Mogi, N. Nagakura, H. Oide, H. Okabe, H. Otono, Y. Seki, D. Sekiba, T. Shima, H. E. Shimizu, H. M. Shimizu, N. Sumi, H. Sumino, M. Tanida, H. Uehara, T. Yamada, S. Yamashita, K. Yano, T. Yoshioka"
"Estimation of System Parameters Including Repeated Cross-Sectional Data
  through Emulator-Informed Deep Generative Model","Differential equations (DEs) are crucial for modeling the evolution of
natural or engineered systems. Traditionally, the parameters in DEs are
adjusted to fit data from system observations. However, in fields such as
politics, economics, and biology, available data are often independently
collected at distinct time points from different subjects (i.e., repeated
cross-sectional (RCS) data). Conventional optimization techniques struggle to
accurately estimate DE parameters when RCS data exhibit various
heterogeneities, leading to a significant loss of information. To address this
issue, we propose a new estimation method called the emulator-informed
deep-generative model (EIDGM), designed to handle RCS data. Specifically, EIDGM
integrates a physics-informed neural network-based emulator that immediately
generates DE solutions and a Wasserstein generative adversarial network-based
parameter generator that can effectively mimic the RCS data. We evaluated EIDGM
on exponential growth, logistic population models, and the Lorenz system,
demonstrating its superior ability to accurately capture parameter
distributions. Additionally, we applied EIDGM to an experimental dataset of
Amyloid beta 40 and beta 42, successfully capturing diverse parameter
distribution shapes. This shows that EIDGM can be applied to model a wide range
of systems and extended to uncover the operating principles of systems based on
limited data.",2024-12-27T08:19:23Z,http://arxiv.org/abs/2412.19517v1,"Hyunwoo Cho, Sung Woong Cho, Hyeontae Jo, Hyung Ju Hwang"
"Confidence v.s. Critique: A Decomposition of Self-Correction Capability
  for LLMs","Large Language Models (LLMs) can correct their self-generated responses, but
a decline in accuracy after self-correction is also witnessed. To have a deeper
understanding of self-correction, we endeavor to decompose, evaluate, and
analyze the self-correction behaviors of LLMs. By enumerating and analyzing
answer correctness before and after self-correction, we decompose the
self-correction capability into confidence (being confident to correct answers)
and critique (turning wrong answers to correct) capabilities, and propose two
metrics from a probabilistic perspective to measure these 2 capabilities, along
with another metric for overall self-correction capability evaluation. Based on
our decomposition and evaluation metrics, we conduct extensive experiments and
draw some empirical conclusions. For example, we find different models can
exhibit distinct behaviors: some models are confident while others are more
critical. We also find the trade-off between the two capabilities (i.e.
improving one can lead to a decline in the other) when manipulating model
self-correction behavior by prompts or in-context learning. Further, we find a
simple yet efficient strategy to improve self-correction capability by
transforming Supervision Fine-Tuning (SFT) data format, and our strategy
outperforms vanilla SFT in both capabilities and achieves much higher accuracy
after self-correction. Our code will be publicly available on GitHub.",2024-12-27T08:09:11Z,http://arxiv.org/abs/2412.19513v1,"Zhe Yang, Yichang Zhang, Yudong Wang, Ziyao Xu, Junyang Lin, Zhifang Sui"
Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging,"Fine-tuning large language models (LLMs) for downstream tasks is a widely
adopted approach, but it often leads to safety degradation in safety-aligned
LLMs. Currently, many solutions address this issue by incorporating additional
safety data, which can be impractical in many cases. In this paper, we address
the question: How can we improve downstream task performance while preserving
safety in LLMs without relying on additional safety data? We propose a simple
and effective method that maintains the inherent safety of LLMs while enhancing
their downstream task performance: merging the weights of pre- and
post-fine-tuned safety-aligned models. Experimental results across various
downstream tasks, models, and merging methods demonstrate that this approach
effectively mitigates safety degradation while improving downstream task
performance, offering a practical solution for adapting safety-aligned LLMs.",2024-12-27T08:03:22Z,http://arxiv.org/abs/2412.19512v1,"Hua Farn, Hsuan Su, Shachi H Kumar, Saurav Sahay, Shang-Tse Chen, Hung-yi Lee"
Hybrid Local Causal Discovery,"Local causal discovery aims to learn and distinguish the direct causes and
effects of a target variable from observed data. Existing constraint-based
local causal discovery methods use AND or OR rules in constructing the local
causal skeleton, but using either rule alone is prone to produce cascading
errors in the learned local causal skeleton, and thus impacting the inference
of local causal relationships. On the other hand, directly applying score-based
global causal discovery methods to local causal discovery may randomly return
incorrect results due to the existence of local equivalence classes. To address
the above issues, we propose a Hybrid Local Causal Discovery algorithm, called
HLCD. Specifically, HLCD initially utilizes a constraint-based approach
combined with the OR rule to obtain a candidate skeleton and then employs a
score-based method to eliminate redundant portions in the candidate skeleton.
Furthermore, during the local causal orientation phase, HLCD distinguishes
between V-structures and equivalence classes by comparing the local structure
scores between the two, thereby avoiding orientation interference caused by
local equivalence classes. We conducted extensive experiments with seven
state-of-the-art competitors on 14 benchmark Bayesian network datasets, and the
experimental results demonstrate that HLCD significantly outperforms existing
local causal discovery algorithms.",2024-12-27T07:53:59Z,http://arxiv.org/abs/2412.19507v1,"Zhaolong Ling, Honghui Peng, Yiwen Zhang, Peng Zhou, Xingyu Wu, Kui Yu, Xindong Wu"
"Multi-P$^2$A: A Multi-perspective Benchmark on Privacy Assessment for
  Large Vision-Language Models","Large Vision-Language Models (LVLMs) exhibit impressive potential across
various tasks but also face significant privacy risks, limiting their practical
applications. Current researches on privacy assessment for LVLMs is limited in
scope, with gaps in both assessment dimensions and privacy categories. To
bridge this gap, we propose Multi-P$^2$A, a comprehensive benchmark for
evaluating the privacy preservation capabilities of LVLMs in terms of privacy
awareness and leakage. Privacy awareness measures the model's ability to
recognize the privacy sensitivity of input data, while privacy leakage assesses
the risk of the model unintentionally disclosing privacy information in its
output. We design a range of sub-tasks to thoroughly evaluate the model's
privacy protection offered by LVLMs. Multi-P$^2$A covers 26 categories of
personal privacy, 15 categories of trade secrets, and 18 categories of state
secrets, totaling 31,962 samples. Based on Multi-P$^2$A, we evaluate the
privacy preservation capabilities of 21 open-source and 2 closed-source LVLMs.
Our results reveal that current LVLMs generally pose a high risk of
facilitating privacy breaches, with vulnerabilities varying across personal
privacy, trade secret, and state secret.",2024-12-27T07:33:39Z,http://arxiv.org/abs/2412.19496v1,"Jie Zhang, Xiangkui Cao, Zhouyu Han, Shiguang Shan, Xilin Chen"
"Disparate Model Performance and Stability in Machine Learning Clinical
  Support for Diabetes and Heart Diseases","Machine Learning (ML) algorithms are vital for supporting clinical
decision-making in biomedical informatics. However, their predictive
performance can vary across demographic groups, often due to the
underrepresentation of historically marginalized populations in training
datasets. The investigation reveals widespread sex- and age-related inequities
in chronic disease datasets and their derived ML models. Thus, a novel
analytical framework is introduced, combining systematic arbitrariness with
traditional metrics like accuracy and data complexity. The analysis of data
from over 25,000 individuals with chronic diseases revealed mild sex-related
disparities, favoring predictive accuracy for males, and significant
age-related differences, with better accuracy for younger patients. Notably,
older patients showed inconsistent predictive accuracy across seven datasets,
linked to higher data complexity and lower model performance. This highlights
that representativeness in training data alone does not guarantee equitable
outcomes, and model arbitrariness must be addressed before deploying models in
clinical settings.",2024-12-27T07:31:14Z,http://arxiv.org/abs/2412.19495v1,"Ioannis Bilionis, Ricardo C. Berrios, Luis Fernandez-Luque, Carlos Castillo"
User Willingness-aware Sales Talk Dataset,"User willingness is a crucial element in the sales talk process that affects
the achievement of the salesperson's or sales system's objectives. Despite the
importance of user willingness, to the best of our knowledge, no previous study
has addressed the development of automated sales talk dialogue systems that
explicitly consider user willingness. A major barrier is the lack of sales talk
datasets with reliable user willingness data. Thus, in this study, we developed
a user willingness-aware sales talk collection by leveraging the ecological
validity concept, which is discussed in the field of human-computer
interaction. Our approach focused on three types of user willingness essential
in real sales interactions. We created a dialogue environment that closely
resembles real-world scenarios to elicit natural user willingness, with
participants evaluating their willingness at the utterance level from multiple
perspectives. We analyzed the collected data to gain insights into practical
user willingness-aware sales talk strategies. In addition, as a practical
application of the constructed dataset, we developed and evaluated a sales
dialogue system aimed at enhancing the user's intent to purchase.",2024-12-27T07:16:10Z,http://arxiv.org/abs/2412.19490v1,"Asahi Hentona, Jun Baba, Shiki Sato, Reina Akama"
Learning Radiance Fields from a Single Snapshot Compressive Image,"In this paper, we explore the potential of Snapshot Compressive Imaging (SCI)
technique for recovering the underlying 3D scene structure from a single
temporal compressed image. SCI is a cost-effective method that enables the
recording of high-dimensional data, such as hyperspectral or temporal
information, into a single image using low-cost 2D imaging sensors. To achieve
this, a series of specially designed 2D masks are usually employed, reducing
storage and transmission requirements and offering potential privacy
protection. Inspired by this, we take one step further to recover the encoded
3D scene information leveraging powerful 3D scene representation capabilities
of neural radiance fields (NeRF). Specifically, we propose SCINeRF, in which we
formulate the physical imaging process of SCI as part of the training of NeRF,
allowing us to exploit its impressive performance in capturing complex scene
structures. In addition, we further integrate the popular 3D Gaussian Splatting
(3DGS) framework and propose SCISplat to improve 3D scene reconstruction
quality and training/rendering speed by explicitly optimizing point clouds into
3D Gaussian representations. To assess the effectiveness of our method, we
conduct extensive evaluations using both synthetic data and real data captured
by our SCI system. Experimental results demonstrate that our proposed approach
surpasses the state-of-the-art methods in terms of image reconstruction and
novel view synthesis. Moreover, our method also exhibits the ability to render
high frame-rate multi-view consistent images in real time by leveraging SCI and
the rendering capabilities of 3DGS. Codes will be available at:
https://github.com/WU- CVGL/SCISplat.",2024-12-27T06:40:44Z,http://arxiv.org/abs/2412.19483v1,"Yunhao Li, Xiang Liu, Xiaodong Wang, Xin Yuan, Peidong Liu"
"Pre-training, Fine-tuning and Re-ranking: A Three-Stage Framework for
  Legal Question Answering","Legal question answering (QA) has attracted increasing attention from people
seeking legal advice, which aims to retrieve the most applicable answers from a
large-scale database of question-answer pairs. Previous methods mainly use a
dual-encoder architecture to learn dense representations of both questions and
answers. However, these methods could suffer from lacking domain knowledge and
sufficient labeled training data. In this paper, we propose a three-stage
(\underline{p}re-training, \underline{f}ine-tuning and \underline{r}e-ranking)
framework for \underline{l}egal \underline{QA} (called PFR-LQA), which promotes
the fine-grained text representation learning and boosts the performance of
dense retrieval with the dual-encoder architecture. Concretely, we first
conduct domain-specific pre-training on legal questions and answers through a
self-supervised training objective, allowing the pre-trained model to be
adapted to the legal domain. Then, we perform task-specific fine-tuning of the
dual-encoder on legal question-answer pairs by using the supervised learning
objective, leading to a high-quality dual-encoder for the specific downstream
QA task. Finally, we employ a contextual re-ranking objective to further refine
the output representations of questions produced by the document encoder, which
uses contextual similarity to increase the discrepancy between the anchor and
hard negative samples for better question re-ranking. We conduct extensive
experiments on a manually annotated legal QA dataset. Experimental results show
that our PFR-LQA method achieves better performance than the strong competitors
for legal question answering.",2024-12-27T06:33:42Z,http://arxiv.org/abs/2412.19482v1,"Shiwen Ni, Hao Cheng, Min Yang"
"An Overview of Machine Learning-Driven Resource Allocation in IoT
  Networks","In the wake of disruptive IoT technologies generating massive amounts of
diverse data, Machine Learning (ML) will play a crucial role in bringing
intelligence to Internet of Things (IoT) networks. This paper provides a
comprehensive analysis of the current state of resource allocation within IoT
networks, focusing specifically on two key categories: Low-Power IoT Networks
and Mobile IoT Networks. We delve into the resource allocation strategies that
are crucial for optimizing network performance and energy efficiency in these
environments. Furthermore, the paper explores the transformative role of
Machine Learning (ML), Deep Learning (DL), and Reinforcement Learning (RL) in
enhancing IoT functionalities. We highlight a range of applications and use
cases where these advanced technologies can significantly improve
decision-making and optimization processes. In addition to the opportunities
presented by ML, DL, and RL, we also address the potential challenges that
organizations may face when implementing these technologies in IoT settings.
These challenges include crucial accuracy, low flexibility and adaptability,
and high computational cost, etc. Finally, the paper identifies promising
avenues for future research, emphasizing the need for innovative solutions to
overcome existing hurdles and improve the integration of ML, DL, and RL into
IoT networks. By providing this holistic perspective, we aim to contribute to
the ongoing discourse on resource allocation strategies and the application of
intelligent technologies in the IoT landscape.",2024-12-27T06:11:28Z,http://arxiv.org/abs/2412.19478v1,Zhengdong Li
"Effects of Reynolds number and spatial resolution on the pressure source
  terms in turbulent boundary layers","The increase in wall-pressure fluctuations with increasing friction Reynolds
number ($Re_{\tau}$) of a turbulent boundary layer (TBL) is well known in the
literature. However, very few studies have investigated the
$Re_{\tau}$-variation of the source terms of the pressure fluctuations, which
are solely a function of the spatial velocity gradients within the TBL. This
study quantifies the pressure source terms in a zero-pressure gradient TBL by
utilizing a published direct numerical simulation (DNS; Sillero et al. 2013,
Phys. Fluids) database across 1000 $\lesssim$ $Re_{\tau}$ $\lesssim$ 2000. It
is found that the magnitude of all source terms increases with $Re_{\tau}$
across the entire TBL thickness, with the turbulence-turbulence (non-linear)
interaction terms growing faster than the mean-shear (linear) source terms.
Further, we use the simulation database to mimic the scenario of particle image
velocimetry (PIV) experiments that are typically spatially under-resolved
compared to DNS data. It is used to quantify the effect of spatial resolution
on the accuracy of pressure source terms, which are estimated here for two
common PIV scenarios: (i) planar PIV in the streamwise-wall-normal plane, and
(ii) stereo-PIV in the spanwise-wall-normal plane of a ZPG TBL. This exercise
reveals significant attenuation of all pressure source terms compared to those
estimated from the original DNS, highlighting the challenges of accurately
estimating these source terms in a high $Re_{\tau}$ PIV experiment.",2024-12-27T05:58:49Z,http://arxiv.org/abs/2412.19474v1,"Aditya Agarwal, Rahul Deshpande"
"Knowledge Graph-Based Multi-Agent Path Planning in Dynamic Environments
  using WAITR","This paper addresses the challenge of multi-agent path planning for efficient
data collection in dynamic, uncertain environments, exemplified by autonomous
underwater vehicles (AUVs) navigating the Gulf of Mexico. Traditional greedy
algorithms, though computationally efficient, often fall short in long-term
planning due to their short-sighted nature, missing crucial data collection
opportunities and increasing exposure to hazards. To address these limitations,
we introduce WAITR (Weighted Aggregate Inter-Temporal Reward), a novel
path-planning framework that integrates a knowledge graph with pathlet-based
planning, segmenting the environment into dynamic, speed-adjusted sub-regions
(pathlets). This structure enables coordinated, adaptive planning, as agents
can operate within time-bound regions while dynamically responding to
environmental changes. WAITR's cumulative scoring mechanism balances immediate
data collection with long-term optimization of Points of Interest (POIs),
ensuring safer navigation and comprehensive data coverage. Experimental results
show that WAITR substantially improves POI coverage and reduces exposure to
hazards, achieving up to 27.1\% greater event coverage than traditional greedy
methods.",2024-12-27T05:43:41Z,http://arxiv.org/abs/2412.19469v1,"Ted Edward Holmberg, Elias Ioup, Mahdi Abdelguerfi"
A Prototype Unit for Image De-raining using Time-Lapse Data,"We address the challenge of single-image de-raining, a task that involves
recovering rain-free background information from a single rain image. While
recent advancements have utilized real-world time-lapse data for training,
enabling the estimation of consistent backgrounds and realistic rain streaks,
these methods often suffer from computational and memory consumption, limiting
their applicability in real-world scenarios. In this paper, we introduce a
novel solution: the Rain Streak Prototype Unit (RsPU). The RsPU efficiently
encodes rain streak-relevant features as real-time prototypes derived from
time-lapse data, eliminating the need for excessive memory resources. Our
de-raining network combines encoder-decoder networks with the RsPU, allowing us
to learn and encapsulate diverse rain streak-relevant features as concise
prototypes, employing an attention-based approach. To ensure the effectiveness
of our approach, we propose a feature prototype loss encompassing cohesion and
divergence components. This loss function captures both the compactness and
diversity aspects of the prototypical rain streak features within the RsPU. Our
method evaluates various de-raining benchmarks, accompanied by comprehensive
ablation studies. We show that it can achieve competitive results in various
rain images compared to state-of-the-art methods.",2024-12-27T05:04:56Z,http://arxiv.org/abs/2412.19459v1,"Jaehoon Cho, Minjung Yoo, Jini Yang, Sunok Kim"
"DriveEditor: A Unified 3D Information-Guided Framework for Controllable
  Object Editing in Driving Scenes","Vision-centric autonomous driving systems require diverse data for robust
training and evaluation, which can be augmented by manipulating object
positions and appearances within existing scene captures. While recent
advancements in diffusion models have shown promise in video editing, their
application to object manipulation in driving scenarios remains challenging due
to imprecise positional control and difficulties in preserving high-fidelity
object appearances. To address these challenges in position and appearance
control, we introduce DriveEditor, a diffusion-based framework for object
editing in driving videos. DriveEditor offers a unified framework for
comprehensive object editing operations, including repositioning, replacement,
deletion, and insertion. These diverse manipulations are all achieved through a
shared set of varying inputs, processed by identical position control and
appearance maintenance modules. The position control module projects the given
3D bounding box while preserving depth information and hierarchically injects
it into the diffusion process, enabling precise control over object position
and orientation. The appearance maintenance module preserves consistent
attributes with a single reference image by employing a three-tiered approach:
low-level detail preservation, high-level semantic maintenance, and the
integration of 3D priors from a novel view synthesis model. Extensive
qualitative and quantitative evaluations on the nuScenes dataset demonstrate
DriveEditor's exceptional fidelity and controllability in generating diverse
driving scene edits, as well as its remarkable ability to facilitate downstream
tasks.",2024-12-27T04:49:36Z,http://arxiv.org/abs/2412.19458v1,"Yiyuan Liang, Zhiying Yan, Liqun Chen, Jiahuan Zhou, Luxin Yan, Sheng Zhong, Xu Zou"
Focusing Image Generation to Mitigate Spurious Correlations,"Instance features in images exhibit spurious correlations with background
features, affecting the training process of deep neural classifiers. This leads
to insufficient attention to instance features by the classifier, resulting in
erroneous classification outcomes. In this paper, we propose a data
augmentation method called Spurious Correlations Guided Synthesis (SCGS) that
mitigates spurious correlations through image generation model. This approach
does not require expensive spurious attribute (group) labels for the training
data and can be widely applied to other debiasing methods. Specifically, SCGS
first identifies the incorrect attention regions of a pre-trained classifier on
the training images, and then uses an image generation model to generate new
training data based on these incorrect attended regions. SCGS increases the
diversity and scale of the dataset to reduce the impact of spurious
correlations on classifiers. Changes in the classifier's attention regions and
experimental results on three different domain datasets demonstrate that this
method is effective in reducing the classifier's reliance on spurious
correlations.",2024-12-27T04:48:56Z,http://arxiv.org/abs/2412.19457v1,"Xuewei Li, Zhenzhen Nie, Mei Yu, Zijian Zhang, Jie Gao, Tianyi Xu, Zhiqiang Liu"
"NijiGAN: Transform What You See into Anime with Contrastive
  Semi-Supervised Learning and Neural Ordinary Differential Equations","Generative AI has transformed the animation industry. Several models have
been developed for image-to-image translation, particularly focusing on
converting real-world images into anime through unpaired translation.
Scenimefy, a notable approach utilizing contrastive learning, achieves high
fidelity anime scene translation by addressing limited paired data through
semi-supervised training. However, it faces limitations due to its reliance on
paired data from a fine-tuned StyleGAN in the anime domain, often producing
low-quality datasets. Additionally, Scenimefy's high parameter architecture
presents opportunities for computational optimization. This research introduces
NijiGAN, a novel model incorporating Neural Ordinary Differential Equations
(NeuralODEs), which offer unique advantages in continuous transformation
modeling compared to traditional residual networks. NijiGAN successfully
transforms real-world scenes into high fidelity anime visuals using half of
Scenimefy's parameters. It employs pseudo-paired data generated through
Scenimefy for supervised training, eliminating dependence on low-quality paired
data and improving the training process. Our comprehensive evaluation includes
ablation studies, qualitative, and quantitative analysis comparing NijiGAN to
similar models. The testing results demonstrate that NijiGAN produces
higher-quality images compared to AnimeGAN, as evidenced by a Mean Opinion
Score (MOS) of 2.192, it surpasses AnimeGAN's MOS of 2.160. Furthermore, our
model achieved a Frechet Inception Distance (FID) score of 58.71, outperforming
Scenimefy's FID score of 60.32. These results demonstrate that NijiGAN achieves
competitive performance against existing state-of-the-arts, especially
Scenimefy as the baseline model.",2024-12-27T04:46:44Z,http://arxiv.org/abs/2412.19455v1,"Kevin Putra Santoso, Anny Yuniarti, Dwiyasa Nakula, Dimas Prihady Setyawan, Adam Haidar Azizi, Jeany Aurellia P. Dewati, Farah Dhia Fadhila, Maria T. Elvara Bumbungan"
"Feature Alignment-Based Knowledge Distillation for Efficient Compression
  of Large Language Models","This study proposes a knowledge distillation algorithm based on large
language models and feature alignment, aiming to effectively transfer the
knowledge of large pre-trained models into lightweight student models, thereby
reducing computational costs while maintaining high model performance.
Different from the traditional soft label distillation method, this method
introduces a multi-layer feature alignment strategy to deeply align the
intermediate features and attention mechanisms of the teacher model and the
student model, maximally retaining the semantic expression ability and context
modeling ability of the teacher model. In terms of method design, a multi-task
loss function is constructed, including feature matching loss, attention
alignment loss, and output distribution matching loss, to ensure multi-level
information transfer through joint optimization. The experiments were
comprehensively evaluated on the GLUE data set and various natural language
processing tasks. The results show that the proposed model performs very close
to the state-of-the-art GPT-4 model in terms of evaluation indicators such as
perplexity, BLEU, ROUGE, and CER. At the same time, it far exceeds baseline
models such as DeBERTa, XLNet, and GPT-3, showing significant performance
improvements and computing efficiency advantages. Research results show that
the feature alignment distillation strategy is an effective model compression
method that can significantly reduce computational overhead and storage
requirements while maintaining model capabilities. Future research can be
further expanded in the directions of self-supervised learning, cross-modal
feature alignment, and multi-task transfer learning to provide more flexible
and efficient solutions for the deployment and optimization of deep learning
models.",2024-12-27T04:37:06Z,http://arxiv.org/abs/2412.19449v1,"Shuo Wang, Chihang Wang, Jia Gao, Zhen Qi, Hongye Zheng, Xiaoxuan Liao"
"Comparative Performance Analysis of Quantum Machine Learning
  Architectures for Credit Card Fraud Detection","As financial fraud becomes increasingly complex, effective detection methods
are essential. Quantum Machine Learning (QML) introduces certain capabilities
that may enhance both accuracy and efficiency in this area. This study examines
how different quantum feature map and ansatz configurations affect the
performance of three QML-based classifiers-the Variational Quantum Classifier
(VQC), the Sampler Quantum Neural Network (SQNN), and the Estimator Quantum
Neural Network (EQNN)-when applied to two non-standardized financial fraud
datasets. Different quantum feature map and ansatz configurations are
evaluated, revealing distinct performance patterns. The VQC consistently
demonstrates strong classification results, achieving an F1 score of 0.88,
while the SQNN also delivers promising outcomes. In contrast, the EQNN
struggles to produce robust results, emphasizing the challenges presented by
non-standardized data. These findings highlight the importance of careful model
configuration in QML-based financial fraud detection. By showing how specific
feature maps and ansatz choices influence predictive success, this work guides
researchers and practitioners in refining QML approaches for complex financial
applications.",2024-12-27T04:17:34Z,http://arxiv.org/abs/2412.19441v1,"Mansour El Alami, Nouhaila Innan, Muhammad Shafique, Mohamed Bennai"
"Residual Feature-Reutilization Inception Network for Image
  Classification","Capturing feature information effectively is of great importance in the field
of computer vision. With the development of convolutional neural networks
(CNNs), concepts like residual connection and multiple scales promote continual
performance gains in diverse deep learning vision tasks. In this paper, we
propose a novel CNN architecture that it consists of residual
feature-reutilization inceptions (ResFRI) or split-residual
feature-reutilization inceptions (Split-ResFRI). And it is composed of four
convolutional combinations of different structures connected by specially
designed information interaction passages, which are utilized to extract
multi-scale feature information and effectively increase the receptive field of
the model. Moreover, according to the network structure designed above,
Split-ResFRI can adjust the segmentation ratio of the input information,
thereby reducing the number of parameters and guaranteeing the model
performance. Specifically, in experiments based on popular vision datasets,
such as CIFAR10 ($97.94$\%), CIFAR100 ($85.91$\%) and Tiny Imagenet
($70.54$\%), we obtain state-of-the-art results compared with other modern
models under the premise that the model size is approximate and no additional
data is used.",2024-12-27T03:55:25Z,http://arxiv.org/abs/2412.19433v1,"Yuanpeng He, Wenjie Song, Lijian Li, Tianxiang Zhan, Wenpin Jiao"
Revisiting PCA for time series reduction in temporal dimension,"Revisiting PCA for Time Series Reduction in Temporal Dimension; Jiaxin Gao,
Wenbo Hu, Yuntian Chen; Deep learning has significantly advanced time series
analysis (TSA), enabling the extraction of complex patterns for tasks like
classification, forecasting, and regression. Although dimensionality reduction
has traditionally focused on the variable space-achieving notable success in
minimizing data redundancy and computational complexity-less attention has been
paid to reducing the temporal dimension. In this study, we revisit Principal
Component Analysis (PCA), a classical dimensionality reduction technique, to
explore its utility in temporal dimension reduction for time series data. It is
generally thought that applying PCA to the temporal dimension would disrupt
temporal dependencies, leading to limited exploration in this area. However,
our theoretical analysis and extensive experiments demonstrate that applying
PCA to sliding series windows not only maintains model performance, but also
enhances computational efficiency. In auto-regressive forecasting, the temporal
structure is partially preserved through windowing, and PCA is applied within
these windows to denoise the time series while retaining their statistical
information. By preprocessing time-series data with PCA, we reduce the temporal
dimensionality before feeding it into TSA models such as Linear, Transformer,
CNN, and RNN architectures. This approach accelerates training and inference
and reduces resource consumption. Notably, PCA improves Informer training and
inference speed by up to 40% and decreases GPU memory usage of TimesNet by 30%,
without sacrificing model accuracy. Comparative analysis against other
reduction methods further highlights the effectiveness of PCA in improving the
efficiency of TSA models.",2024-12-27T03:17:26Z,http://arxiv.org/abs/2412.19423v1,"Jiaxin Gao, Wenbo Hu, Yuntian Chen"
"A Matrix Logic Approach to Efficient Frequent Itemset Discovery in Large
  Data Sets","This paper proposes a frequent itemset mining algorithm based on the Boolean
matrix method, aiming to solve the storage and computational bottlenecks of
traditional frequent pattern mining algorithms in high-dimensional and
large-scale transaction databases. By representing the itemsets in the
transaction database as Boolean matrices, the algorithm uses Boolean logic
operations such as AND and OR to efficiently calculate the support of the
itemsets, avoiding the generation and storage of a large number of candidates
itemsets in traditional algorithms. The algorithm recursively mines frequent
itemsets through matrix operations and can flexibly adapt to different data
scales and support thresholds. In the experiment, the public Groceries dataset
was selected, and the running efficiency test and frequent itemset mining
effect test were designed to evaluate the algorithm's performance indicators
such as running time, memory usage, and number of frequent itemsets under
different transaction numbers and support thresholds. The experimental results
show that the algorithm can efficiently mine a large number of frequent
itemsets when the support threshold is low, and focus on strong association
rules with high support when the threshold is high. In addition, the changing
trends of running time and memory usage show that the Boolean matrix method can
still maintain good running efficiency when the number of transactions
increases significantly and has high scalability and robustness. Future
research can improve memory optimization and matrix block operations, and
combine distributed computing and deep learning models to further enhance the
algorithm's applicability and real-time processing capabilities in
ultra-large-scale data environments. The algorithm has broad application
potential and development prospects in the fields of market analysis,
recommendation systems, and network security.",2024-12-27T03:13:13Z,http://arxiv.org/abs/2412.19420v1,"Xuan Li, Tingyi Ruan, Yankaiqi Li, Quanchao Lu, Xiaoxuan Sun"
KALAHash: Knowledge-Anchored Low-Resource Adaptation for Deep Hashing,"Deep hashing has been widely used for large-scale approximate nearest
neighbor search due to its storage and search efficiency. However, existing
deep hashing methods predominantly rely on abundant training data, leaving the
more challenging scenario of low-resource adaptation for deep hashing
relatively underexplored. This setting involves adapting pre-trained models to
downstream tasks with only an extremely small number of training samples
available. Our preliminary benchmarks reveal that current methods suffer
significant performance degradation due to the distribution shift caused by
limited training samples. To address these challenges, we introduce
Class-Calibration LoRA (CLoRA), a novel plug-and-play approach that dynamically
constructs low-rank adaptation matrices by leveraging class-level textual
knowledge embeddings. CLoRA effectively incorporates prior class knowledge as
anchors, enabling parameter-efficient fine-tuning while maintaining the
original data distribution. Furthermore, we propose Knowledge-Guided Discrete
Optimization (KIDDO), a framework to utilize class knowledge to compensate for
the scarcity of visual information and enhance the discriminability of hash
codes. Extensive experiments demonstrate that our proposed method, Knowledge-
Anchored Low-Resource Adaptation Hashing (KALAHash), significantly boosts
retrieval performance and achieves a 4x data efficiency in low-resource
scenarios.",2024-12-27T03:04:54Z,http://arxiv.org/abs/2412.19417v1,"Shu Zhao, Tan Yu, Xiaoshuai Hao, Wenchao Ma, Vijaykrishnan Narayanan"
DIPS: Optimal Dynamic Index for Poisson $\boldsymbolπ$ps Sampling,"This paper addresses the Poisson $\pi$ps sampling problem, a topic of
significant academic interest in various domains and with practical data mining
applications, such as influence maximization. The problem includes a set
$\mathcal{S}$ of $n$ elements, where each element $v$ is assigned a weight
$w(v)$ reflecting its importance. The goal is to generate a random subset $X$
of $\mathcal{S}$, where each element $v \in \mathcal{S}$ is included in $X$
independently with probability $\frac{c\cdot w(v)}{\sum_{v \in \mathcal{S}}
w(v)}$, where $0&lt;c\leq 1$ is a constant. The subsets must be independent across
different queries. While the Poisson $\pi$ps sampling problem can be reduced to
the well-studied subset sampling problem, updates in Poisson $\pi$ps sampling,
such as adding a new element or removing an element, would cause the
probabilities of all $n$ elements to change in the corresponding subset
sampling problem, making this approach impractical for dynamic scenarios. To
address this, we propose a dynamic index specifically tailored for the Poisson
$\pi$ps sampling problem, supporting optimal expected $\mathcal{O}(1)$ query
time and $\mathcal{O}(1)$ index update time, with an optimal $\mathcal{O}(n)$
space cost. Our solution involves recursively partitioning the set by weights
and ultimately using table lookup. The core of our solution lies in addressing
the challenges posed by weight explosion and correlations between elements.
Empirical evaluations demonstrate that our approach achieves significant
speedups in update time while maintaining consistently competitive query time
compared to the subset-sampling-based methods.",2024-12-27T02:47:44Z,http://arxiv.org/abs/2412.19415v1,"Jinchao Huang, Sibo Wang"
"The Hobby-Eberly Telescope Dark Energy Experiment Survey (HETDEX) Active
  Galactic Nuclei Catalog: the Fourth Data Release","We present the Active Galactic Nuclei (AGN) catalog from the fourth data
release (HDR4) of the Hobby-Eberly Telescope Dark Energy Experiment Survey
(HETDEX). HETDEX is an untargeted spectroscopic survey. HDR4 contains 345,874
Integral Field Unit (IFU) observations from January 2017 to August 2023
covering an effective area of 62.9 deg2. With no imaging pre-selection, our
spectroscopic confirmed AGN sample includes low-luminosity AGN, narrow-line
AGN, and/or red AGN down to g~25. This catalog has 15,940 AGN across the
redshifts of z=0.1~4.6, giving a raw AGN number density of 253.4 deg-2. Among
them, 10,499 (66%) have redshifts either confirmed by line pairs or matched to
the Sloan Digital Sky Survey Quasar Catalog. For the remaining 5,441 AGN, 2,083
are single broad line AGN candidates, while the remaining 3,358 are single
intermediate broad line (full width at half maximum, FWHM ~ 1200 km s-1) AGN
candidates. A total of 4,060 (39%) of the 10,499 redshift-confirmed AGN have
emission-line regions $3\sigma$ more extended than the image quality which
could be strong outflows blowing into the outskirts of the host galaxies or
ionized intergalactic medium.",2024-12-27T02:45:20Z,http://arxiv.org/abs/2412.19414v1,"Chenxu Liu, Karl Gebhardt, Erin Mentuch Cooper, Dustin Davis, Donald P. Schneider, Matt J. Jarvis, Daniel J. Farrow, Steven L. Finkelstein, Oscar A. Chavez Ortiz, The HETDEX Collaboration"
MINIMA: Modality Invariant Image Matching,"Image matching for both cross-view and cross-modality plays a critical role
in multimodal perception. In practice, the modality gap caused by different
imaging systems/styles poses great challenges to the matching task. Existing
works try to extract invariant features for specific modalities and train on
limited datasets, showing poor generalization. In this paper, we present
MINIMA, a unified image matching framework for multiple cross-modal cases.
Without pursuing fancy modules, our MINIMA aims to enhance universal
performance from the perspective of data scaling up. For such purpose, we
propose a simple yet effective data engine that can freely produce a large
dataset containing multiple modalities, rich scenarios, and accurate matching
labels. Specifically, we scale up the modalities from cheap but rich RGB-only
matching data, by means of generative models. Under this setting, the matching
labels and rich diversity of the RGB dataset are well inherited by the
generated multimodal data. Benefiting from this, we construct MD-syn, a new
comprehensive dataset that fills the data gap for general multimodal image
matching. With MD-syn, we can directly train any advanced matching pipeline on
randomly selected modality pairs to obtain cross-modal ability. Extensive
experiments on in-domain and zero-shot matching tasks, including $19$
cross-modal cases, demonstrate that our MINIMA can significantly outperform the
baselines and even surpass modality-specific methods. The dataset and code are
available at https://github.com/LSXI7/MINIMA .",2024-12-27T02:39:50Z,http://arxiv.org/abs/2412.19412v1,"Xingyu Jiang, Jiangwei Ren, Zizhuo Li, Xin Zhou, Dingkang Liang, Xiang Bai"
Spectral-Temporal Fusion Representation for Person-in-Bed Detection,"This study is based on the ICASSP 2025 Signal Processing Grand Challenge's
Accelerometer-Based Person-in-Bed Detection Challenge, which aims to determine
bed occupancy using accelerometer signals. The task is divided into two tracks:
""in bed"" and ""not in bed"" segmented detection, and streaming detection, facing
challenges such as individual differences, posture variations, and external
disturbances. We propose a spectral-temporal fusion-based feature
representation method with mixup data augmentation, and adopt Intersection over
Union (IoU) loss to optimize detection accuracy. In the two tracks, our method
achieved outstanding results of 100.00% and 95.55% in detection scores,
securing first place and third place, respectively.",2024-12-27T02:05:09Z,http://arxiv.org/abs/2412.19404v1,"Xuefeng Yang, Shiheng Zhang, Jian Guan, Feiyang Xiao, Wei Lu, Qiaoxi Zhu"
"Fully Data-driven but Interpretable Human Behavioural Modelling with
  Differentiable Discrete Choice Model","Discrete choice models are essential for modelling various decision-making
processes in human behaviour. However, the specification of these models has
depended heavily on domain knowledge from experts, and the fully automated but
interpretable modelling of complex human behaviours has been a long-standing
challenge. In this paper, we introduce the differentiable discrete choice model
(Diff-DCM), a fully data-driven method for the interpretable modelling,
learning, prediction, and control of complex human behaviours, which is
realised by differentiable programming. Solely from input features and choice
outcomes without any prior knowledge, Diff-DCM can estimate interpretable
closed-form utility functions that reproduce observed behaviours. Comprehensive
experiments with both synthetic and real-world data demonstrate that Diff-DCM
can be applied to various types of data and requires only a small amount of
computational resources for the estimations, which can be completed within tens
of seconds on a laptop without any accelerators. In these experiments, we also
demonstrate that, using its differentiability, Diff-DCM can provide useful
insights into human behaviours, such as an optimal intervention path for
effective behavioural changes. This study provides a strong basis for the fully
automated and reliable modelling, prediction, and control of human behaviours.",2024-12-27T01:53:18Z,http://arxiv.org/abs/2412.19403v1,"Fumiyasu Makinoshima, Tatsuya Mitomi, Fumiya Makihara, Eigo Segawa"
"Comparing Few to Rank Many: Active Human Preference Learning using
  Randomized Frank-Wolfe","We study learning of human preferences from a limited comparison feedback.
This task is ubiquitous in machine learning. Its applications such as
reinforcement learning from human feedback, have been transformational. We
formulate this problem as learning a Plackett-Luce model over a universe of $N$
choices from $K$-way comparison feedback, where typically $K \ll N$. Our
solution is the D-optimal design for the Plackett-Luce objective. The design
defines a data logging policy that elicits comparison feedback for a small
collection of optimally chosen points from all ${N \choose K}$ feasible
subsets. The main algorithmic challenge in this work is that even fast methods
for solving D-optimal designs would have $O({N \choose K})$ time complexity. To
address this issue, we propose a randomized Frank-Wolfe (FW) algorithm that
solves the linear maximization sub-problems in the FW method on randomly chosen
variables. We analyze the algorithm, and evaluate it empirically on synthetic
and open-source NLP datasets.",2024-12-27T01:10:17Z,http://arxiv.org/abs/2412.19396v1,"Kiran Koshy Thekumparampil, Gaurush Hiranandani, Kousha Kalantari, Shoham Sabach, Branislav Kveton"
"An In-Depth Analysis of Adversarial Discriminative Domain Adaptation for
  Digit Classification","Domain adaptation is an active area of research driven by the growing demand
for robust machine learning models that perform well on real-world data.
Adversarial learning for deep neural networks (DNNs) has emerged as a promising
approach to improving generalization ability, particularly for image
classification. In this paper, we implement a specific adversarial learning
technique known as Adversarial Discriminative Domain Adaptation (ADDA) and
replicate digit classification experiments from the original ADDA paper. We
extend their findings by examining a broader range of domain shifts and provide
a detailed analysis of in-domain classification accuracy post-ADDA. Our results
demonstrate that ADDA significantly improves accuracy across certain domain
shifts with minimal impact on in-domain performance. Furthermore, we provide
qualitative analysis and propose potential explanations for ADDA's limitations
in less successful domain shifts. Code is at
https://github.com/eugenechoi2004/COS429_FINAL .",2024-12-27T00:36:40Z,http://arxiv.org/abs/2412.19391v1,"Eugene Choi, Julian Rodriguez, Edmund Young"
"A reduced-order framework for temperature estimation in food freezing
  from optimally located sensors, including turbulent conjugate flow scenarios","This article proposes a framework for estimating temperature fields in
food-freezing applications that significantly reduces computational load while
ensuring accurate temperature monitoring, representing a promising
technological tool for optimizing and controlling food engineering processes.
The strategy is based on (i) a mathematical model of a convection-dominated
problem coupling thermal convection and turbulence and (ii) a least-squares
approach for solving the inverse data assimilation problem, regularized by
projecting the governing dynamics onto a reduced-order model (ROM). The
unsteady freezing process considers an idealized salmon slice in a freezer
cabinet, modeled with temperature-dependent thermophysical properties. The
forward problem is approximated using a third-order WENO finite volume solver,
including an optimized second-order backward scheme for time discretization. We
employ our data assimilation framework to reconstruct the temperature field
from a limited number of sensor data and to estimate temperature distributions
within frozen food. Sensor placement is optimized using a new greedy algorithm,
relying on maximizing the observability of the reduced-order dynamics for a
fixed set of sensors. The proposed approach allows efficient extrapolation from
external sensor measurements to the internal temperature of the food, which is
crucial for maintaining food quality.",2024-12-27T00:26:36Z,http://arxiv.org/abs/2412.19387v1,"Felipe Galarce, Diego Rivera, Douglas Pacheco, Alfonso Caiazzo, Ernesto Castillo"
Resolvent-based estimation and control of a laminar airfoil wake,"We develop an optimal resolvent-based estimator and controller to predict and
attenuate unsteady vortex shedding fluctuations in the laminar wake of a NACA
0012 airfoil at an angle of attack of 6.5 degrees, chord-based Reynolds number
of 5000, and Mach number of 0.3. The resolvent-based estimation and control
framework offers several advantages over standard methods. Under equivalent
assumptions, the resolvent-based estimator and controller reproduce the Kalman
filter and LQG controller, respectively, but at substantially lower
computational cost using either an operator-based or data-driven
implementation. Unlike these methods, the resolvent-based approach can
naturally accommodate forcing terms (nonlinear terms from Navier-Stokes) with
colored-in-time statistics, significantly improving estimation accuracy and
control efficacy. Causality is optimally enforced using a Wiener-Hopf
formalism. We integrate these tools into a high-performance-computing-ready
compressible flow solver and demonstrate their effectiveness for estimating and
controlling velocity fluctuations in the wake of the airfoil immersed in clean
and noisy freestreams, the latter of which prevents the flow from falling into
a periodic limit cycle. Using four shear-stress sensors on the surface of the
airfoil, the resolvent-based estimator predicts a series of downstream targets
with approximately 3% and 30% error for the clean and noisy freestream
conditions, respectively. For the latter case, using four actuators on the
airfoil surface, the resolvent-based controller reduces the turbulent kinetic
energy in the wake by 98%.",2024-12-27T00:12:23Z,http://arxiv.org/abs/2412.19386v1,"Junoh Jung, Rutvij Bhagwat, Aaron Towne"
"The Internet of Value: Integrating Blockchain and Lightning Network
  Micropayments for Knowledge Markets","Q&amp;A websites rely on user-generated responses, with incentives such as
reputation scores or monetary rewards often offered. While some users may find
it intrinsically rewarding to assist others, studies indicate that payment can
improve the quality and speed of answers. However, traditional payment
processors impose minimum thresholds that many Q&amp;A inquiries fall below. The
introduction of Bitcoin enabled direct digital value transfer, yet frequent
micropayments remain challenging. Recent advancements like the Lightning
Network now allow frictionless micropayments by reducing costs and minimising
reliance on intermediaries. This development fosters an ""Internet of Value,""
where transferring even small amounts of money is as simple as sharing data.
This study investigates integrating Lightning Network-based micropayment
strategies into Q&amp;A platforms, aiming to create a knowledge market free of
minimum payment barriers. A survey was conducted to address the gap below the
$2 payment level identified in prior research. Responses confirmed that
incentives for asking and answering weaken as payments decrease. Findings
reveal even minimal payments, such as {\pounds}0.01, significantly encourage
higher quality and effort in responses. The study recommends micropayment
incentives for service-oriented applications, particularly Q&amp;A platforms. By
leveraging the Lightning Network to remove barriers, a more open marketplace
can emerge, improving engagement and outcomes. Further research is needed to
confirm if users follow through on reported intentions when spending funds.",2024-12-26T23:57:54Z,http://arxiv.org/abs/2412.19384v1,"Ellis Solaiman, Jorge Robins"
"Minimal Batch Adaptive Learning Policy Engine for Real-Time Mid-Price
  Forecasting in High-Frequency Trading","High-frequency trading (HFT) has transformed modern financial markets, making
reliable short-term price forecasting models essential. In this study, we
present a novel approach to mid-price forecasting using Level 1 limit order
book (LOB) data from NASDAQ, focusing on 100 U.S. stocks from the S&amp;P 500 index
during the period from September to November 2022. Expanding on our previous
work with Radial Basis Function Neural Networks (RBFNN), which leveraged
automated feature importance techniques based on mean decrease impurity (MDI)
and gradient descent (GD), we introduce the Adaptive Learning Policy Engine
(ALPE) - a reinforcement learning (RL)-based agent designed for batch-free,
immediate mid-price forecasting. ALPE incorporates adaptive epsilon decay to
dynamically balance exploration and exploitation, outperforming a diverse range
of highly effective machine learning (ML) and deep learning (DL) models in
forecasting performance.",2024-12-26T22:49:53Z,http://arxiv.org/abs/2412.19372v1,"Adamantios Ntakaris, Gbenga Ibikunle"
Large Language Models for Market Research: A Data-augmentation Approach,"Large Language Models (LLMs) have transformed artificial intelligence by
excelling in complex natural language processing tasks. Their ability to
generate human-like text has opened new possibilities for market research,
particularly in conjoint analysis, where understanding consumer preferences is
essential but often resource-intensive. Traditional survey-based methods face
limitations in scalability and cost, making LLM-generated data a promising
alternative. However, while LLMs have the potential to simulate real consumer
behavior, recent studies highlight a significant gap between LLM-generated and
human data, with biases introduced when substituting between the two. In this
paper, we address this gap by proposing a novel statistical data augmentation
approach that efficiently integrates LLM-generated data with real data in
conjoint analysis. Our method leverages transfer learning principles to debias
the LLM-generated data using a small amount of human data. This results in
statistically robust estimators with consistent and asymptotically normal
properties, in contrast to naive approaches that simply substitute human data
with LLM-generated data, which can exacerbate bias. We validate our framework
through an empirical study on COVID-19 vaccine preferences, demonstrating its
superior ability to reduce estimation error and save data and costs by 24.9\%
to 79.8\%. In contrast, naive approaches fail to save data due to the inherent
biases in LLM-generated data compared to human data. Another empirical study on
sports car choices validates the robustness of our results. Our findings
suggest that while LLM-generated data is not a direct substitute for human
responses, it can serve as a valuable complement when used within a robust
statistical framework.",2024-12-26T22:06:29Z,http://arxiv.org/abs/2412.19363v1,"Mengxin Wang, Dennis J. Zhang, Heng Zhang"
"Evaluating Convolutional Neural Networks for COVID-19 classification in
  chest X-ray images","Coronavirus Disease 2019 (COVID-19) pandemic rapidly spread globally,
impacting the lives of billions of people. The effective screening of infected
patients is a critical step to struggle with COVID-19, and treating the
patients avoiding this quickly disease spread. The need for automated and
scalable methods has increased due to the unavailability of accurate automated
toolkits. Recent researches using chest X-ray images suggest they include
relevant information about the COVID-19 virus. Hence, applying machine learning
techniques combined with radiological imaging promises to identify this disease
accurately. It is straightforward to collect these images once it is spreadly
shared and analyzed in the world. This paper presents a method for automatic
COVID-19 detection using chest Xray images through four convolutional neural
networks, namely: AlexNet, VGG-11, SqueezeNet, and DenseNet-121. This method
had been providing accurate diagnostics for positive or negative COVID-19
classification. We validate our experiments using a ten-fold cross-validation
procedure over the training and test sets. Our findings include the shallow
fine-tuning and data augmentation strategies that can assist in dealing with
the low number of positive COVID-19 images publicly available. The accuracy for
all CNNs is higher than 97.00%, and the SqueezeNet model achieved the best
result with 99.20%.",2024-12-26T22:05:30Z,http://arxiv.org/abs/2412.19362v1,"Leonardo Gabriel Ferreira Rodrigues, Danilo Ferreira da Silva, Larissa Ferreira Rodrigues, João Fernando Mari"
Dynamic Skill Adaptation for Large Language Models,"We present Dynamic Skill Adaptation (DSA), an adaptive and dynamic framework
to adapt novel and complex skills to Large Language Models (LLMs). Compared
with previous work which learns from human-curated and static data in random
orders, we propose to first automatically generate and organize the training
data by mimicking the learning pathways of human and then dynamically tailor
the training data based on the training dynamics. Specifically, inspired by the
learning structures and teaching strategies in the human education system, we
first construct a skill graph by decomposing complex skills into sub-skills and
arranging them based on their dependencies in human syllables. For every skill,
we utilize LLMs to generate both textbook-like data which contains detailed
descriptions of skills for pre-training and exercise-like data which targets at
explicitly utilizing the skills to solve problems for instruction-tuning.
Furthermore, during the instruction-tuning, we dynamically update the training
data which down-weight easy-to-learn examples, generate more complex examples,
and filter out data with errors. Experiments on large language models such as
LLAMA and Mistral demonstrate the effectiveness of our proposed methods in
adapting math reasoning skills and social study skills.",2024-12-26T22:04:23Z,http://arxiv.org/abs/2412.19361v1,"Jiaao Chen, Diyi Yang"
"Improving the network traffic classification using the Packet Vision
  approach","The network traffic classification allows improving the management, and the
network services offer taking into account the kind of application. The future
network architectures, mainly mobile networks, foresee intelligent mechanisms
in their architectural frameworks to deliver application-aware network
requirements. The potential of convolutional neural networks capabilities,
widely exploited in several contexts, can be used in network traffic
classification. Thus, it is necessary to develop methods based on the content
of packets transforming it into a suitable input for CNN technologies. Hence,
we implemented and evaluated the Packet Vision, a method capable of building
images from packets raw-data, considering both header and payload. Our approach
excels those found in state-of-the-art by delivering security and privacy by
transforming the raw-data packet into images. Therefore, we built a dataset
with four traffic classes evaluating the performance of three CNNs
architectures: AlexNet, ResNet-18, and SqueezeNet. Experiments showcase the
Packet Vision combined with CNNs applicability and suitability as a promising
approach to deliver outstanding performance in classifying network traffic.",2024-12-26T21:56:03Z,http://arxiv.org/abs/2412.19360v1,"Rodrigo Moreira, Larissa Ferreira Rodrigues, Pedro Frosi Rosa, Flávio de Oliveira Silva"
"Federated Hybrid Training and Self-Adversarial Distillation: Towards
  Robust Edge Networks","Federated learning (FL) is a distributed training technology that enhances
data privacy in mobile edge networks by allowing data owners to collaborate
without transmitting raw data to the edge server. However, data heterogeneity
and adversarial attacks pose challenges to develop an unbiased and robust
global model for edge deployment. To address this, we propose Federated hyBrid
Adversarial training and self-adversarial disTillation (FedBAT), a new
framework designed to improve both robustness and generalization of the global
model. FedBAT seamlessly integrates hybrid adversarial training and
self-adversarial distillation into the conventional FL framework from data
augmentation and feature distillation perspectives. From a data augmentation
perspective, we propose hybrid adversarial training to defend against
adversarial attacks by balancing accuracy and robustness through a weighted
combination of standard and adversarial training. From a feature distillation
perspective, we introduce a novel augmentation-invariant adversarial
distillation method that aligns local adversarial features of augmented images
with their corresponding unbiased global clean features. This alignment can
effectively mitigate bias from data heterogeneity while enhancing both the
robustness and generalization of the global model. Extensive experimental
results across multiple datasets demonstrate that FedBAT yields comparable or
superior performance gains in improving robustness while maintaining accuracy
compared to several baselines.",2024-12-26T21:32:08Z,http://arxiv.org/abs/2412.19354v1,"Yu Qiao, Apurba Adhikary, Kitae Kim, Eui-Nam Huh, Zhu Han, Choong Seon Hong"
ETTA: Elucidating the Design Space of Text-to-Audio Models,"Recent years have seen significant progress in Text-To-Audio (TTA) synthesis,
enabling users to enrich their creative workflows with synthetic audio
generated from natural language prompts. Despite this progress, the effects of
data, model architecture, training objective functions, and sampling strategies
on target benchmarks are not well understood. With the purpose of providing a
holistic understanding of the design space of TTA models, we set up a
large-scale empirical experiment focused on diffusion and flow matching models.
Our contributions include: 1) AF-Synthetic, a large dataset of high quality
synthetic captions obtained from an audio understanding model; 2) a systematic
comparison of different architectural, training, and inference design choices
for TTA models; 3) an analysis of sampling methods and their Pareto curves with
respect to generation quality and inference speed. We leverage the knowledge
obtained from this extensive analysis to propose our best model dubbed
Elucidated Text-To-Audio (ETTA). When evaluated on AudioCaps and MusicCaps,
ETTA provides improvements over the baselines trained on publicly available
data, while being competitive with models trained on proprietary data. Finally,
we show ETTA's improved ability to generate creative audio following complex
and imaginative captions -- a task that is more challenging than current
benchmarks.",2024-12-26T21:13:12Z,http://arxiv.org/abs/2412.19351v1,"Sang-gil Lee, Zhifeng Kong, Arushi Goel, Sungwon Kim, Rafael Valle, Bryan Catanzaro"
"Semi-Supervised Learning from Small Annotated Data and Large Unlabeled
  Data for Fine-grained PICO Entity Recognition","Objective: Extracting PICO elements -- Participants, Intervention,
Comparison, and Outcomes -- from clinical trial literature is essential for
clinical evidence retrieval, appraisal, and synthesis. Existing approaches do
not distinguish the attributes of PICO entities. This study aims to develop a
named entity recognition (NER) model to extract PICO entities with fine
granularities.
  Materials and Methods: Using a corpus of 2,511 abstracts with PICO mentions
from 4 public datasets, we developed a semi-supervised method to facilitate the
training of a NER model, FinePICO, by combining limited annotated data of PICO
entities and abundant unlabeled data. For evaluation, we divided the entire
dataset into two subsets: a smaller group with annotations and a larger group
without annotations. We then established the theoretical lower and upper
performance bounds based on the performance of supervised learning models
trained solely on the small, annotated subset and on the entire set with
complete annotations, respectively. Finally, we evaluated FinePICO on both the
smaller annotated subset and the larger, initially unannotated subset. We
measured the performance of FinePICO using precision, recall, and F1.
  Results: Our method achieved precision/recall/F1 of 0.567/0.636/0.60,
respectively, using a small set of annotated samples, outperforming the
baseline model (F1: 0.437) by more than 16\%. The model demonstrates
generalizability to a different PICO framework and to another corpus, which
consistently outperforms the benchmark in diverse experimental settings
(p-value \textless0.001).
  Conclusion: This study contributes a generalizable and effective
semi-supervised approach to named entity recognition leveraging large unlabeled
data together with small, annotated data. It also initially supports
fine-grained PICO extraction.",2024-12-26T20:24:35Z,http://arxiv.org/abs/2412.19346v1,"Fangyi Chen, Gongbo Zhang, Yilu Fang, Yifan Peng, Chunhua Weng"
Advanced Scheduling of Electrolyzer Modules for Grid Flexibility,"As the transition to sustainable power generation progresses, green hydrogen
production via electrolysis is expected to gain importance as a means for
energy storage and flexible load to complement variable renewable generation.
With the increasing need for cost-effective and efficient hydrogen production,
electrolyzer optimization is essential to improve both energy efficiency and
profitability. This paper analyzes how the efficiency and modular setup of
alkaline hydrogen electrolyzers can improve hydrogen output of systems linked
to a fluctuating renewable power supply. To explore this, we propose a
day-ahead optimal scheduling problem of a hybrid wind and electrolyzer system.
The novelty of our approach lies in modeling the number and capacity of
electrolyzer modules, and capturing the modules' impact on the hydrogen
production and efficiency. We solve the resulting mixed-integer optimization
problem with several different combinations of number of modules, efficiency
and operating range parameters, using day-ahead market data from a wind farm
generator in the ERCOT system as an input. Our results demonstrate that the
proposed approach ensures that electrolyzer owners can better optimize the
operation of their systems, achieving greater hydrogen production and higher
revenue. Key findings include that as the number of modules in a system with
the same overall capacity increases, hydrogen production and revenue increases.",2024-12-26T20:24:01Z,http://arxiv.org/abs/2412.19345v1,"Angelina Lesniak, Andrea Gloppen Johnsen, Noah Rhodes, Line Roald"
Identifying Split Vacancies with Foundation Models and Electrostatics,"Point defects are ubiquitous in solid-state compounds, dictating many
functional properties such as conductivity, catalytic activity and carrier
recombination. Over the past decade, the prevalence of metastable defect
geometries and their importance to relevant properties has been increasingly
recognised. A particularly striking example of this is split vacancies, where
an isolated atomic vacancy transforms to a stoichiometry-conserving complex of
two vacancies and an interstitial ($V_X \rightarrow [V_X + X_i + V_X]$), which
can be accompanied by a dramatic lowering of the defect energy and change in
behaviour. Such species are particularly challenging to identify from
computation, due to the `non-local' nature of this reconstruction. Here, I
present an approach for efficiently identifying such species in solid-state
compounds, through tiered screening which combines geometric analysis,
electrostatic energies and foundation machine learning (ML) models. This
approach allows the screening of all compounds in the Materials Project
database (including all entries in the ICSD, along with several thousand
predicted metastable materials), identifying thousands of split vacancy
configurations, hitherto unknown. This study highlights both the potential
utility of machine-learning potentials for defect investigations, with
important caveats, and the importance of global optimisation approaches for
correctly identifying stable defect geometries.",2024-12-26T18:58:52Z,http://arxiv.org/abs/2412.19330v1,Seán R. Kavanagh
"Deep learning and whole-brain networks for biomarker discovery: modeling
  the dynamics of brain fluctuations in resting-state and cognitive tasks","Background: Brain network models offer insights into brain dynamics, but the
utility of model-derived bifurcation parameters as biomarkers remains
underexplored. Objective: This study evaluates bifurcation parameters from a
whole-brain network model as biomarkers for distinguishing brain states
associated with resting-state and task-based cognitive conditions. Methods:
Synthetic BOLD signals were generated using a supercritical Hopf brain network
model to train deep learning models for bifurcation parameter prediction.
Inference was performed on Human Connectome Project data, including both
resting-state and task-based conditions. Statistical analyses assessed the
separability of brain states based on bifurcation parameter distributions.
Results: Bifurcation parameter distributions differed significantly across task
and resting-state conditions ($p &lt; 0.0001$ for all but one comparison).
Task-based brain states exhibited higher bifurcation values compared to rest.
Conclusion: Bifurcation parameters effectively differentiate cognitive and
resting states, warranting further investigation as biomarkers for brain state
characterization and neurological disorder assessment.",2024-12-26T18:58:38Z,http://arxiv.org/abs/2412.19329v1,"Facundo Roffet, Gustavo Deco, Claudio Delrieux, Gustavo Patow"
"Resolving the Ambiguity of Complete-to-Partial Point Cloud Registration
  for Image-Guided Liver Surgery with Patches-to-Partial Matching","In image-guided liver surgery, the initial rigid alignment between
preoperative and intraoperative data, often represented as point clouds, is
crucial for providing sub-surface information from preoperative CT/MRI images
to the surgeon during the procedure. Currently, this alignment is typically
performed using semi-automatic methods, which, while effective to some extent,
are prone to errors that demand manual correction. Point cloud
correspondence-based registration methods are promising to serve as a fully
automatic solution. However, they may struggle in scenarios with limited
intraoperative surface visibility, a common challenge in liver surgery,
particularly in laparoscopic procedures, which we refer to as
complete-to-partial ambiguity. We first illustrate this ambiguity by evaluating
the performance of state-of-the-art learning-based point cloud registration
methods on our carefully constructed in silico and in vitro datasets. Then, we
propose a patches-to-partial matching strategy as a plug-and-play module to
resolve the ambiguity, which can be seamlessly integrated into learning-based
registration methods without disrupting their end-to-end structure. It has
proven effective and efficient in improving registration performance for cases
with limited intraoperative visibility. The constructed benchmark and the
proposed module establish a solid foundation for advancing applications of
point cloud correspondence-based registration methods in image-guided liver
surgery.",2024-12-26T18:58:29Z,http://arxiv.org/abs/2412.19328v1,"Zixin Yang, Jon S. Heiselman, Cheng Han, Kelly Merrell, Richard Simon, Cristian. A. Linte"
"Performance Control in Early Exiting to Deploy Large Models at the Same
  Cost of Smaller Ones","Early Exiting (EE) is a promising technique for speeding up inference by
adaptively allocating compute resources to data points based on their
difficulty. The approach enables predictions to exit at earlier layers for
simpler samples while reserving more computation for challenging ones. In this
study, we first present a novel perspective on the EE approach, showing that
larger models deployed with EE can achieve higher performance than smaller
models while maintaining similar computational costs. As existing EE approaches
rely on confidence estimation at each exit point, we further study the impact
of overconfidence on the controllability of the compute-performance trade-off.
We introduce Performance Control Early Exiting (PCEE), a method that enables
accuracy thresholding by basing decisions not on a data point's confidence but
on the average accuracy of samples with similar confidence levels from a
held-out validation set. In our experiments, we show that PCEE offers a simple
yet computationally efficient approach that provides better control over
performance than standard confidence-based approaches, and allows us to scale
up model sizes to yield performance gain while reducing the computational cost.",2024-12-26T18:54:32Z,http://arxiv.org/abs/2412.19325v1,"Mehrnaz Mofakhami, Reza Bayat, Ioannis Mitliagkas, Joao Monteiro, Valentina Zantedeschi"
Electron efficiency in LHC Run-2 with the ATLAS experiment,"The document presents a general overview of the electron reconstruction,
identification and isolation performance in the ATLAS experiment. The results
are obtained using 13 TeV proton-proton collision data collected during the LHC
Run-2. The electron reconstruction efficiency is higher than 97%, and the ratio
of data to Monte Carlo simulation efficiency is close to unity, with associated
uncertainties generally smaller than 0.1%. The electron identification is shown
for three working points, and depending on the electron $E_T$, it can be as low
as 60%, increasing to more than 80% above 50 GeV. The correction factors are
close to one, generally within 5%. Five isolation working points are
recommended in the ATLAS experiment, to successfully reject fake/non-prompt
electrons. Their dependency on the electron identification working points is
shown and discussed, as well as their pile-up dependency, and their performance
versus electron $E_T$ and $\eta$.
  Document based on a presentation at the XI International Conference on New
Frontiers in Physics (ICNFP 2022).
  keywords; prompt electrons, reconstruction, identification, isolation,
fake/non-prompt electrons",2024-12-26T18:50:43Z,http://arxiv.org/abs/2412.19323v1,Otilia Ducu
Adaptive Conformal Inference by Betting,"Conformal prediction is a valuable tool for quantifying predictive
uncertainty of machine learning models. However, its applicability relies on
the assumption of data exchangeability, a condition which is often not met in
real-world scenarios. In this paper, we consider the problem of adaptive
conformal inference without any assumptions about the data generating process.
Existing approaches for adaptive conformal inference are based on optimizing
the pinball loss using variants of online gradient descent. A notable
shortcoming of such approaches is in their explicit dependence on and
sensitivity to the choice of the learning rates. In this paper, we propose a
different approach for adaptive conformal inference that leverages
parameter-free online convex optimization techniques. We prove that our method
controls long-term miscoverage frequency at a nominal level and demonstrate its
convincing empirical performance without any need of performing cumbersome
parameter tuning.",2024-12-26T18:42:08Z,http://arxiv.org/abs/2412.19318v1,"Aleksandr Podkopaev, Darren Xu, Kuang-Chih Lee"
"ATLAS searches for higgsinos with R-parity violating couplings in events
  with leptons","This document presents two searches for Supersymmetry through the direct
production of pairs of higgsinos decaying into final states with leptons and
($b$-) jets. The analyses are performed using 139~fb$^{-1}$ of the 13~TeV
proton-proton collision data collected with the ATLAS detector. The methods
used to estimate the Standard Model and detector backgrounds are discussed, as
well as their shortcomings. Finally, results in selected signal regions, and
some exclusion limits, are presented, illustrating the significant improvement
over the previous exclusion limits.
  Document based on a presentation at the XI International Conference on New
Frontiers in Physics (ICNFP 2022).",2024-12-26T18:41:55Z,http://arxiv.org/abs/2412.19317v1,Otilia Ducu
Towards a Single ASR Model That Generalizes to Disordered Speech,"This study investigates the impact of integrating a dataset of disordered
speech recordings ($\sim$1,000 hours) into the fine-tuning of a near
state-of-the-art ASR baseline system. Contrary to what one might expect,
despite the data being less than 1% of the training data of the ASR system, we
find a considerable improvement in disordered speech recognition accuracy.
Specifically, we observe a 33% improvement on prompted speech, and a 26%
improvement on a newly gathered spontaneous, conversational dataset of
disordered speech. Importantly, there is no significant performance decline on
standard speech recognition benchmarks. Further, we observe that the proposed
tuning strategy helps close the gap between the baseline system and
personalized models by 64% highlighting the significant progress as well as the
room for improvement. Given the substantial benefits of our findings, this
experiment suggests that from a fairness perspective, incorporating a small
fraction of high quality disordered speech data in a training recipe is an easy
step that could be done to make speech technology more accessible for users
with speech disabilities.",2024-12-26T18:39:15Z,http://arxiv.org/abs/2412.19315v1,"Jimmy Tobin, Katrin Tomanek, Subhashini Venugopalan"
"Theoretical models for longitudinal coupled-bunch instabilities driven
  by harmonic cavities in electron storage rings","We present a theoretical framework for analyzing longitudinal coupled-bunch
instabilities in double-rf systems with even filling patterns, accounting for
potential-well distortion and multiple azimuthal modes. The linearized Vlasov
equation is solved in the frequency-domain for an arbitrary rf potential to
derive the Lebedev equation. We unified different formulations, obtaining
results from recent publications as particular cases. Applications to Robinson
dipole-quadrupole mode coupling and the periodic transient beam loading
(PTBL)/mode-1 instability are presented. Notably, for the first time,
theoretical predictions of the mode-1 thresholds show excellent agreement with
experimental data. The analysis reveals that the PTBL instability is a
zero-frequency effect dependent on azimuthal mode interactions and resistant to
Landau damping, providing new insights into its mechanism. The methods are
implemented in the open-source package pycolleff, offering a useful
semi-analytical tool for studying instabilities in electron storage rings with
harmonic cavities.",2024-12-26T18:11:39Z,http://arxiv.org/abs/2412.19308v1,Murilo B. Alves
"Perceive, Query &amp; Reason: Enhancing Video QA with Question-Guided
  Temporal Queries","Video Question Answering (Video QA) is a challenging video understanding task
that requires models to comprehend entire videos, identify the most relevant
information based on contextual cues from a given question, and reason
accurately to provide answers. Recent advancements in Multimodal Large Language
Models (MLLMs) have transformed video QA by leveraging their exceptional
commonsense reasoning capabilities. This progress is largely driven by the
effective alignment between visual data and the language space of MLLMs.
However, for video QA, an additional space-time alignment poses a considerable
challenge for extracting question-relevant information across frames. In this
work, we investigate diverse temporal modeling techniques to integrate with
MLLMs, aiming to achieve question-guided temporal modeling that leverages
pre-trained visual and textual alignment in MLLMs. We propose T-Former, a novel
temporal modeling method that creates a question-guided temporal bridge between
frame-wise visual perception and the reasoning capabilities of LLMs. Our
evaluation across multiple video QA benchmarks demonstrates that T-Former
competes favorably with existing temporal modeling approaches and aligns with
recent advancements in video QA.",2024-12-26T17:53:14Z,http://arxiv.org/abs/2412.19304v1,"Roberto Amoroso, Gengyuan Zhang, Rajat Koner, Lorenzo Baraldi, Rita Cucchiara, Volker Tresp"
RecLM: Recommendation Instruction Tuning,"Modern recommender systems aim to deeply understand users' complex
preferences through their past interactions. While deep collaborative filtering
approaches using Graph Neural Networks (GNNs) excel at capturing user-item
relationships, their effectiveness is limited when handling sparse data or
zero-shot scenarios, primarily due to constraints in ID-based embedding
functions. To address these challenges, we propose a model-agnostic
recommendation instruction-tuning paradigm that seamlessly integrates large
language models with collaborative filtering. Our proposed Recommendation
Language Model (RecLM) enhances the capture of user preference diversity
through a carefully designed reinforcement learning reward function that
facilitates self-augmentation of language models. Comprehensive evaluations
demonstrate significant advantages of our approach across various settings, and
its plug-and-play compatibility with state-of-the-art recommender systems
results in notable performance enhancements.",2024-12-26T17:51:54Z,http://arxiv.org/abs/2412.19302v1,"Yangqin Jiang, Yuhao Yang, Lianghao Xia, Da Luo, Kangyi Lin, Chao Huang"
"Sample Complexity of Data-driven Multistage Stochastic Programming under
  Markovian Uncertainty","This work is motivated by the challenges of applying the sample average
approximation (SAA) method to multistage stochastic programming with an unknown
continuous-state Markov process. While SAA is widely used in static and
two-stage stochastic optimization, it becomes computationally intractable in
general multistage settings as the time horizon $T$ increases. Indeed, the
number of samples required to obtain a reasonably accurate solution grows
exponentially$\text{ -- }$a phenomenon known as the curse of dimensionality
with respect to the time horizon. To overcome this limitation, we propose a
novel data-driven approach, the Markov Recombining Scenario Tree (MRST) method,
which constructs an approximate problem using only two independent trajectories
of historical data. Our analysis demonstrates that the MRST method achieves
polynomial sample complexity in $T$, providing a more efficient alternative to
SAA. Numerical experiments on the Linear Quadratic Gaussian problem show that
MRST outperforms SAA, addressing the curse of dimensionality.",2024-12-26T17:46:27Z,http://arxiv.org/abs/2412.19299v1,"Hyuk Park, Grani A. Hanasusanto"
Spatio-Temporal Differences in Bike Sharing Usage: A Tale of Six Cities,"This study investigates the spatio-temporal patterns of Bike Sharing System
(BSS) usage in six major cities: New York, London, Tokyo, Boston, Chicago and
Washington D.C. By analyzing data over a 30-day period with comparable climate
and average temperatures, we explored differences in BSS usage between weekdays
and weekends in those cities using Jensen-Shannon divergence (JSD) and rank
distribution analysis. Our findings reveal significant temporal differences in
BSS usage that were commonly observed in all cities, with weekday patterns
dominated by commute peaks and weekend patterns reflecting recreational
activities. Friday emerges as a transitional day, sharing the characteristics
of both weekdays and weekends. Meanwhile, docking station usage rank
distributions show remarkable consistency between weekdays and weekends for
most cities, with London being a unique anomaly. This study highlights the
potential of BSS data to uncover urban mobility patterns and the underlying
structures of cities. The results suggest that BSS usage reflects both
intrinsic user behavior and external influences such as urban planning.",2024-12-26T17:35:28Z,http://arxiv.org/abs/2412.19294v1,"Shu-ichi Kinoshita, Yuya Bando, Hiroki Sayama"
RAG with Differential Privacy,"Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to
provide *Large Language Models* (LLM) with fresh and relevant context,
mitigating the risk of hallucinations and improving the overall quality of
responses in environments with large and fast moving knowledge bases. However,
the integration of external documents into the generation process raises
significant privacy concerns. Indeed, when added to a prompt, it is not
possible to guarantee a response will not inadvertently expose confidential
data, leading to potential breaches of privacy and ethical dilemmas. This paper
explores a practical solution to this problem suitable to general knowledge
extraction from personal data. It shows *differentially private token
generation* is a viable approach to private RAG.",2024-12-26T17:34:26Z,http://arxiv.org/abs/2412.19291v1,Nicolas Grislain
"ViPCap: Retrieval Text-Based Visual Prompts for Lightweight Image
  Captioning","Recent lightweight image captioning models using retrieved data mainly focus
on text prompts. However, previous works only utilize the retrieved text as
text prompts, and the visual information relies only on the CLIP visual
embedding. Because of this issue, there is a limitation that the image
descriptions inherent in the prompt are not sufficiently reflected in the
visual embedding space. To tackle this issue, we propose ViPCap, a novel
retrieval text-based visual prompt for lightweight image captioning. ViPCap
leverages the retrieved text with image information as visual prompts to
enhance the ability of the model to capture relevant visual information. By
mapping text prompts into the CLIP space and generating multiple randomized
Gaussian distributions, our method leverages sampling to explore randomly
augmented distributions and effectively retrieves the semantic features that
contain image information. These retrieved features are integrated into the
image and designated as the visual prompt, leading to performance improvements
on the datasets such as COCO, Flickr30k, and NoCaps. Experimental results
demonstrate that ViPCap significantly outperforms prior lightweight captioning
models in efficiency and effectiveness, demonstrating the potential for a
plug-and-play solution.",2024-12-26T17:29:38Z,http://arxiv.org/abs/2412.19289v1,"Taewhan Kim, Soeun Lee, Si-Woo Kim, Dong-Jin Kim"
Improving Generalization for AI-Synthesized Voice Detection,"AI-synthesized voice technology has the potential to create realistic human
voices for beneficial applications, but it can also be misused for malicious
purposes. While existing AI-synthesized voice detection models excel in
intra-domain evaluation, they face challenges in generalizing across different
domains, potentially becoming obsolete as new voice generators emerge. Current
solutions use diverse data and advanced machine learning techniques (e.g.,
domain-invariant representation, self-supervised learning), but are limited by
predefined vocoders and sensitivity to factors like background noise and
speaker identity. In this work, we introduce an innovative disentanglement
framework aimed at extracting domain-agnostic artifact features related to
vocoders. Utilizing these features, we enhance model learning in a flat loss
landscape, enabling escape from suboptimal solutions and improving
generalization. Extensive experiments on benchmarks show our approach
outperforms state-of-the-art methods, achieving up to 5.12% improvement in the
equal error rate metric in intra-domain and 7.59% in cross-domain evaluations.",2024-12-26T16:45:20Z,http://arxiv.org/abs/2412.19279v1,"Hainan Ren, Lin Li, Chun-Hao Liu, Xin Wang, Shu Hu"
Memory-Centric Computing: Recent Advances in Processing-in-DRAM,"Memory-centric computing aims to enable computation capability in and near
all places where data is generated and stored. As such, it can greatly reduce
the large negative performance and energy impact of data access and data
movement, by 1) fundamentally avoiding data movement, 2) reducing data access
latency &amp; energy, and 3) exploiting large parallelism of memory arrays. Many
recent studies show that memory-centric computing can largely improve system
performance &amp; energy efficiency. Major industrial vendors and startup companies
have recently introduced memory chips with sophisticated computation
capabilities. Going forward, both hardware and software stack should be
revisited and designed carefully to take advantage of memory-centric computing.
  This work describes several major recent advances in memory-centric
computing, specifically in Processing-in-DRAM, a paradigm where the operational
characteristics of a DRAM chip are exploited and enhanced to perform
computation on data stored in DRAM. Specifically, we describe 1) new techniques
that slightly modify DRAM chips to enable both enhanced computation capability
and easier programmability, 2) new experimental studies that demonstrate the
functionally-complete bulk-bitwise computational capability of real commercial
off-the-shelf DRAM chips, without any modifications to the DRAM chip or the
interface, and 3) new DRAM designs that improve access granularity &amp;
efficiency, unleashing the true potential of Processing-in-DRAM.",2024-12-26T16:31:40Z,http://arxiv.org/abs/2412.19275v1,"Onur Mutlu, Ataberk Olgun, Geraldo F. Oliveira, Ismail Emir Yuksel"
"Anvendelse av kunstig intelligens (KI) i Norge i norsk offentlig sektor
  2024","There are great expectations for the use of AI in Norway. On the other hand,
it is reported that the adoption of AI in Norway is slower than expected in
both the private and public sectors. Using responses from NOKIOS Technology
Radar 2017-2021, IT in Practice surveys conducted by Ramboll in 2021-2024, as
well as another national survey as part of a five-year cycle, this article
looks at reported and planned use of AI with a focus on local (municipalities)
and national government agencies. IT in practice is distributed to a large
number of Norwegian public agencies, with a response rate of over 5o percent.
The most recent data (2024) presented in this article is based on responses
from 335 public organizations, with 237 municipalities, and 98 public
organizations at the national or regional level. The survey confirms that the
use of AI is still at an early stage, although expectations are high for future
use.
  --
  Det er store forventninger til bruk av KI i Norge. P{\aa} den annen side
rapporteres det at adopsjonen av KI i Norge g{\aa}r tregere enn forventet
b{\aa}de i privat og offentlig sektor. Ved hjelp av svar fra NOKIOS
teknologiradar 2017-2021, IT i Praksis unders{\o}kelser utf{\o}rt av Ramb{\o}ll
i 2021-2024, samt en annen nasjonal unders{\o}kelse som en del av en
fem{\aa}rig syklus, ser vi i denne artikkelen p{\aa} rapportert og planlagt
bruk av KI med fokus p{\aa} lokale (kommuner) og nasjonale offentlige etater.
IT i praksis distribueres til en lang rekke norske offentlige virksomheter, med
en svarprosent p{\aa} over 50 prosent. De nyeste dataene (2024) presentert i
denne artikkelen er basert p{\aa} svar fra 335 offentlige organisasjoner, med
237 kommuner, og 98 offentlige organisasjoner p{\aa} nasjonalt eller regionalt
niv{\aa}. Unders{\o}kelsen bekrefter at bruken av KI fortsatt er p{\aa} et
tidlig stadium, selv om forventningene er h{\o}ye til fremtidig bruk.",2024-12-26T16:28:49Z,http://arxiv.org/abs/2412.19273v1,John Krogstie
Jet formation studies in AGN: a search for new targets,"In recent years, the jet formation region in active galaxies has been imaged
through mm-VLBI in few ideal targets, first and foremost M87. An important leap
forward for understanding jet launching could be made by identifying a larger
number of suitable objects, characterized by different accretion modes and jet
powers. In this article, we present 1 cm and 7 mm VLBI data of a sample of 16
poorly explored radio galaxies, comprising both High-Excitation (HEG) and
Low-Excitation Galaxies (LEG) and spanning a large range in radio power. The
combination of the sources vicinity (z&lt;0.1) with a large black hole mass
($\log{M_{\rm BH}}$&gt;8.5) results in a high spatial resolution in units of
Schwarzschild radii ($&lt;10^3-10^4$ $R_{\rm S}$), necessary for probing the
region where the jet is initially accelerated and collimated. We aim at
identifying the best candidates for follow-up observations with current and
future VLBI facilities. The observations were performed with the High
Sensitivity Array, including Effelsberg and the phased-VLA, which has allowed
us to characterize the sub-parsec properties of these faint jets and to
estimate their core brightness temperature and orientation. The number of
sources imaged on scales $\lesssim 10^3$ $R_{\rm S}$ is more than doubled by
our study. All targets were detected at both frequencies, and several present
two-sided jet structures. Several LEG jets show hints of limb-brightening. The
core brightness temperatures are generally below the equipartition value,
indicating that equipartition has not yet been reached and/or that the emission
is de-boosted. Among LEG, we identify 3C31, 3C66B, and 3C465 as the most
promising, combining a relatively high flux density (&gt;50 mJy) with superb
spatial resolution (&lt;500 $R_{\rm S}$) at 7 mm. The powerful HEG 3C452 is
interesting as well due to its highly symmetric, two-sided jet base.",2024-12-26T16:20:53Z,http://arxiv.org/abs/2412.19268v1,"B. Boccardi, L. Ricci, E. Madika, V. Bartolini, U. Bach, P. Grandi, E. Torresi, T. P. Krichbaum, J. A. Zensus"
"Search for a neutral gauge boson with nonuniversal fermion couplings in
  vector boson fusion processes in proton-proton collisions at $\sqrt{s}$ = 13
  TeV","The first search for a heavy neutral spin-1 gauge boson (Z') with
nonuniversal fermion couplings produced via vector boson fusion processes and
decaying to tau leptons or W bosons is presented. The analysis is performed
using LHC data at $\sqrt{s}$ = 13 TeV, collected from 2016 to 2018 and
corresponding to an integrated luminosity of 138 fb$^{-1}$. The data are
consistent with the standard model predictions. Upper limits are set on the
product of the cross section for production of the Z' boson and its branching
fraction to $\tau\tau$ or WW. The presence of a Z' boson decaying to
$\tau^+\tau^-$ (W$^+$W$^-$) is excluded for masses up to 2.45 (1.60) TeV,
depending on the Z' boson coupling to SM weak bosons, and assuming a Z' $\to$
$\tau^+\tau^-$ (W$^+$W$^-$) branching fraction of 50%.",2024-12-26T15:54:58Z,http://arxiv.org/abs/2412.19261v1,CMS Collaboration
"MEDEC: A Benchmark for Medical Error Detection and Correction in
  Clinical Notes","Several studies showed that Large Language Models (LLMs) can answer medical
questions correctly, even outperforming the average human score in some medical
exams. However, to our knowledge, no study has been conducted to assess the
ability of language models to validate existing or generated medical text for
correctness and consistency. In this paper, we introduce MEDEC
(https://github.com/abachaa/MEDEC), the first publicly available benchmark for
medical error detection and correction in clinical notes, covering five types
of errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal
Organism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes
from three US hospital systems that were not previously seen by any LLM. The
dataset has been used for the MEDIQA-CORR shared task to evaluate seventeen
participating systems [Ben Abacha et al., 2024]. In this paper, we describe the
data creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,
Claude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and
correcting medical errors requiring both medical knowledge and reasoning
capabilities. We also conducted a comparative study where two medical doctors
performed the same task on the MEDEC test set. The results showed that MEDEC is
a sufficiently challenging benchmark to assess the ability of models to
validate existing or generated notes and to correct medical errors. We also
found that although recent LLMs have a good performance in error detection and
correction, they are still outperformed by medical doctors in these tasks. We
discuss the potential factors behind this gap, the insights from our
experiments, the limitations of current evaluation metrics, and share potential
pointers for future research.",2024-12-26T15:54:10Z,http://arxiv.org/abs/2412.19260v1,"Asma Ben Abacha, Wen-wai Yim, Yujuan Fu, Zhaoyi Sun, Meliha Yetisgen, Fei Xia, Thomas Lin"
Swarm Contract: A Multi-Sovereign Agent Consensus Mechanism,"Traditional smart contracts on blockchains excel at on-chain, deterministic
logic. However, they have inherent limitations when dealing with large-scale
off-chain data, dynamic multi-step workflows, and scenarios requiring high
flexibility or iterative updates. In this paper, we propose the concept of a
""Swarm Contract"" (Swarm), a multi-agent mechanism wherein several digital life
forms (DLF) or Sovereign Agents (SA) collectively handle complex tasks in
Trusted Execution Environments (TEE). These digital entities are defined as
autonomous software agents that own their code, state, and possibly on-chain
assets, while operating free from centralized control.
  By leveraging a simple multi-signature wallet on-chain, Swarm moves most of
the logic off-chain, achieving trust minimization through multi-agent consensus
rather than a single monolithic on-chain contract. We illustrate these ideas
with a lightweight off-chain auction example - minting and selling 10,000
identical NFTs - to showcase how off-chain coordination can determine a
clearing price and finalize distribution, with each step performed collectively
by multiple agents in TEE. This approach broadens the scope of trustless and
decentralized solutions, potentially benefiting DAO governance, multi-modal
data processing, and cross-chain interoperability.",2024-12-26T15:46:56Z,http://arxiv.org/abs/2412.19256v1,Haowei Yang
"Leveraging Self-Training and Variational Autoencoder for Agitation
  Detection in People with Dementia Using Wearable Sensors","Dementia is a neurodegenerative disorder that has been growing among elder
people over the past decades. This growth profoundly impacts the quality of
life for patients and caregivers due to the symptoms arising from it. Agitation
and aggression (AA) are some of the symptoms of people with severe dementia
(PwD) in long-term care or hospitals. AA not only causes discomfort but also
puts the patients or others at potential risk. Existing monitoring solutions
utilizing different wearable sensors integrated with Artificial Intelligence
(AI) offer a way to detect AA early enough for timely and adequate medical
intervention. However, most studies are limited by the availability of
accurately labeled datasets, which significantly affects the efficacy of such
solutions in real-world scenarios. This study presents a novel comprehensive
approach to detect AA in PwD using physiological data from the Empatica E4
wristbands. The research creates a diverse dataset, consisting of three
distinct datasets gathered from 14 participants across multiple hospitals in
Canada. These datasets have not been extensively explored due to their limited
labeling. We propose a novel approach employing self-training and a variational
autoencoder (VAE) to detect AA in PwD effectively. The proposed approach aims
to learn the representation of the features extracted using the VAE and then
uses a semi-supervised block to generate labels, classify events, and detect
AA. We demonstrate that combining Self-Training and Variational Autoencoder
mechanism significantly improves model performance in classifying AA in PwD.
Among the tested techniques, the XGBoost classifier achieved the highest
accuracy of 90.16\%. By effectively addressing the challenge of limited labeled
data, the proposed system not only learns new labels but also proves its
superiority in detecting AA.",2024-12-26T15:34:25Z,http://arxiv.org/abs/2412.19254v1,"Abeer Badawi, Somayya Elmoghazy, Samira Choudhury, Khalid Elgazzar, Amer Burhan"
"Localized exploration in contextual dynamic pricing achieves
  dimension-free regret","We study the problem of contextual dynamic pricing with a linear demand
model. We propose a novel localized exploration-then-commit (LetC) algorithm
which starts with a pure exploration stage, followed by a refinement stage that
explores near the learned optimal pricing policy, and finally enters a pure
exploitation stage. The algorithm is shown to achieve a minimax optimal,
dimension-free regret bound when the time horizon exceeds a polynomial of the
covariate dimension. Furthermore, we provide a general theoretical framework
that encompasses the entire time spectrum, demonstrating how to balance
exploration and exploitation when the horizon is limited. The analysis is
powered by a novel critical inequality that depicts the
exploration-exploitation trade-off in dynamic pricing, mirroring its existing
counterpart for the bias-variance trade-off in regularized regression. Our
theoretical results are validated by extensive experiments on synthetic and
real-world data.",2024-12-26T15:29:58Z,http://arxiv.org/abs/2412.19252v1,"Jinhang Chai, Yaqi Duan, Jianqing Fan, Kaizheng Wang"
Network double autoregression,"Modeling high-dimensional time series with simple structures is a challenging
problem. This paper proposes a network double autoregression (NDAR) model,
which combines the advantages of network structure and the double
autoregression (DAR) model, to handle high-dimensional, conditionally
heteroscedastic, and network-structured data within a simple framework. The
parameters of the model are estimated using quasi-maximum likelihood
estimation, and the asymptotic properties of the estimators are derived. The
selection of the model's lag order will be based on the Bayesian information
criterion. Finite-sample simulations show that the proposed model performs well
even with moderate time dimensions and network sizes. Finally, the model is
applied to analyze three different categories of stock data.",2024-12-26T15:28:41Z,http://arxiv.org/abs/2412.19251v1,"Tingting Li, Hao Wang"
"A Space Lower Bound for Approximate Membership with Duplicate Insertions
  or Deletions of Nonelements","Designs of data structures for approximate membership queries with
false-positive errors that support both insertions and deletions stipulate the
following two conditions: (1) Duplicate insertions are prohibited, i.e., it is
prohibited to insert an element $x$ if $x$ is currently a member of the
dataset. (2) Deletions of nonelements are prohibited, i.e., it is prohibited to
delete $x$ if $x$ is not currently a member of the dataset. Under these
conditions, the space required for the approximate representation of a datasets
of cardinality $n$ with a false-positive probability of $\epsilon^{+}$ is at
most $(1+o(1))n\cdot\log_2 (1/\epsilon^{+}) + O(n)$ bits [Bender et al., 2018;
Bercea and Even, 2019].
  We prove that if these conditions are lifted, then the space required for the
approximate representation of datasets of cardinality $n$ from a universe of
cardinality $u$ is at least $\frac 12 \cdot (1-\epsilon^{+} -\frac 1n)\cdot
\log \binom{u}{n} -O(n)$ bits.",2024-12-26T15:21:42Z,http://arxiv.org/abs/2412.19249v1,"Aryan Agarwala, Guy Even"
Functional structural equation modeling with latent variables,"Handling latent variables in Structural Equation Models (SEMs) in a case
where both the latent variables and their corresponding indicators in the
measurement error part of the model are random curves presents significant
challenges, especially with sparse data. In this paper, we develop a novel
family of Functional Structural Equation Models (FSEMs) that incorporate latent
variables modeled as Gaussian Processes (GPs). The introduced FSEMs are built
upon functional regression models having response variables modeled as
underlying GPs. The model flexibly adapts to cases when the random curves'
realizations are observed only over a sparse subset of the domain, and the
inferential framework is based on a restricted maximum likelihood approach. The
advantage of this framework lies in its ability and flexibility in handling
various data scenarios, including regularly and irregularly spaced points and
thus missing data. To extract smooth estimates for the functional parameters,
we employ a penalized likelihood approach that selects the smoothing parameters
using a cross-validation method. We evaluate the performance of the proposed
model using simulation studies and a real data example, which suggests that our
model performs well in practice. The uncertainty associated with the estimates
of the functional coefficients is also assessed by constructing confidence
regions for each estimate. The goodness of fit indices that are commonly used
to evaluate the fit of SEMs are developed for the FSEMs introduced in this
paper. Overall, the proposed method is a promising approach for modeling
functional data in SEMs with functional latent variables.",2024-12-26T14:57:14Z,http://arxiv.org/abs/2412.19242v1,"Fatemeh Asgari, Valeria Vitelli, Uta Sailer"
"Effect of Peak Absolute Magnitude of Type Ia Supernovae and Sound
  Horizon Values on Hubble Tension using DESI results","We apply data-motivated priors on the peak absolute magnitude of Type Ia
supernovae ($M$), and on the sound horizon at the drag epoch ($r_d$), to study
their impact on the Hubble tension, when compared to the Planck estimated value
of the Hubble constant. We use the data from Pantheon$+$, cosmic chronometers,
and the latest DESI BAO results for this purpose. We reaffirm the fact that
there is a degeneracy between $M$ and $r_d$, and modifying the $r_d$ values to
reconcile the Hubble tension also requires a change in the peak absolute
magnitude $M$. For certain $M$ and $r_d$ priors, the tension is found to reduce
to as low as (1.2-2) $\sigma$.",2024-12-26T14:51:09Z,http://arxiv.org/abs/2412.19240v1,"Shubham Barua, Shantanu Desai"
FineVQ: Fine-Grained User Generated Content Video Quality Assessment,"The rapid growth of user-generated content (UGC) videos has produced an
urgent need for effective video quality assessment (VQA) algorithms to monitor
video quality and guide optimization and recommendation procedures. However,
current VQA models generally only give an overall rating for a UGC video, which
lacks fine-grained labels for serving video processing and recommendation
applications. To address the challenges and promote the development of UGC
videos, we establish the first large-scale Fine-grained Video quality
assessment Database, termed FineVD, which comprises 6104 UGC videos with
fine-grained quality scores and descriptions across multiple dimensions. Based
on this database, we propose a Fine-grained Video Quality assessment (FineVQ)
model to learn the fine-grained quality of UGC videos, with the capabilities of
quality rating, quality scoring, and quality attribution. Extensive
experimental results demonstrate that our proposed FineVQ can produce
fine-grained video-quality results and achieve state-of-the-art performance on
FineVD and other commonly used UGC-VQA datasets. Both Both FineVD and FineVQ
will be made publicly available.",2024-12-26T14:44:47Z,http://arxiv.org/abs/2412.19238v1,"Huiyu Duan, Qiang Hu, Jiarui Wang, Liu Yang, Zitong Xu, Lu Liu, Xiongkuo Min, Chunlei Cai, Tianxiao Ye, Xiaoyun Zhang, Guangtao Zhai"
SeaMo: A Multi-Seasonal and Multimodal Remote Sensing Foundation Model,"Remote Sensing (RS) data contains a wealth of multi-dimensional information
crucial for Earth observation. Owing to its vast volume, diverse sources, and
temporal properties, RS data is highly suitable for the development of large
Visual Foundation Models (VFMs). VFMs act as robust feature extractors,
learning from extensive RS data, and are subsequently fine-tuned for deployment
in various geoscientific tasks. However, current VFMs in the RS domain are
predominantly pretrained and tailored exclusively for specific characteristics
of RS imagery, neglecting the potential of utilizing the multi-dimensional
properties of RS data. Therefore, in this work, we propose SeaMo, a pioneering
visual foundation model that integrates multi-seasonal and multimodal
information in the RS field. SeaMo is designed to harness multiple properties
of RS data. Within the masked image modeling framework, we employ non-aligned
cropping techniques to extract spatial properties, use multi-source inputs for
multimodal integration, and incorporate temporal-multimodal fusion blocks for
effective assimilation of multi-seasonal data. SeaMo explicitly models the
multi-dimensional properties of RS data, making the model more comprehensive,
robust, and versatile. We applied SeaMo to several downstream geoscience tasks,
which demonstrated exceptional performance. Extensive ablation studies were
conducted to validate the model's superiority.",2024-12-26T14:40:38Z,http://arxiv.org/abs/2412.19237v1,"Xuyang Li, Danfeng Hong, Chenyu Li, Jocelyn Chanussot"
"Are Two Hidden Layers Still Enough for the Physics-Informed Neural
  Networks?","The article discusses the development of various methods and techniques for
initializing and training neural networks with a single hidden layer, as well
as training a separable physics-informed neural network consisting of neural
networks with a single hidden layer to solve physical problems described by
ordinary differential equations (ODEs) and partial differential equations
(PDEs). A method for strictly deterministic initialization of a neural network
with one hidden layer for solving physical problems described by an ODE is
proposed. Modifications to existing methods for weighting the loss function are
given, as well as new methods developed for training strictly
deterministic-initialized neural networks to solve ODEs (detaching, additional
weighting based on the second derivative, predicted solution-based weighting,
relative residuals). An algorithm for physics-informed data-driven
initialization of a neural network with one hidden layer is proposed. A neural
network with pronounced generalizing properties is presented, whose
generalizing abilities of which can be precisely controlled by adjusting
network parameters. A metric for measuring the generalization of such neural
network has been introduced. A gradient-free neuron-by-neuron fitting method
has been developed for adjusting the parameters of a single-hidden-layer neural
network, which does not require the use of an optimizer or solver for its
implementation. The proposed methods have been extended to 2D problems using
the separable physics-informed neural networks approach. Numerous experiments
have been carried out to develop the above methods and approaches. Experiments
on physical problems, such as solving various ODEs and PDEs, have demonstrated
that these methods for initializing and training neural networks with one or
two hidden layers (SPINN) achieve competitive accuracy and, in some cases,
state-of-the-art results.",2024-12-26T14:30:54Z,http://arxiv.org/abs/2412.19235v1,"Vasiliy A. Es'kin, Alexey O. Malkhanov, Mikhail E. Smorkalov"
"Virtual Nodes Can Help: Tackling Distribution Shifts in Federated Graph
  Learning","Federated Graph Learning (FGL) enables multiple clients to jointly train
powerful graph learning models, e.g., Graph Neural Networks (GNNs), without
sharing their local graph data for graph-related downstream tasks, such as
graph property prediction. In the real world, however, the graph data can
suffer from significant distribution shifts across clients as the clients may
collect their graph data for different purposes. In particular, graph
properties are usually associated with invariant label-relevant substructures
(i.e., subgraphs) across clients, while label-irrelevant substructures can
appear in a client-specific manner. The issue of distribution shifts of graph
data hinders the efficiency of GNN training and leads to serious performance
degradation in FGL. To tackle the aforementioned issue, we propose a novel FGL
framework entitled FedVN that eliminates distribution shifts through
client-specific graph augmentation strategies with multiple learnable Virtual
Nodes (VNs). Specifically, FedVN lets the clients jointly learn a set of shared
VNs while training a global GNN model. To eliminate distribution shifts, each
client trains a personalized edge generator that determines how the VNs connect
local graphs in a client-specific manner. Furthermore, we provide theoretical
analyses indicating that FedVN can eliminate distribution shifts of graph data
across clients. Comprehensive experiments on four datasets under five settings
demonstrate the superiority of our proposed FedVN over nine baselines.",2024-12-26T14:16:15Z,http://arxiv.org/abs/2412.19229v1,"Xingbo Fu, Zihan Chen, Yinhan He, Song Wang, Binchi Zhang, Chen Chen, Jundong Li"
Multi-view Fake News Detection Model Based on Dynamic Hypergraph,"With the rapid development of online social networks and the inadequacies in
content moderation mechanisms, the detection of fake news has emerged as a
pressing concern for the public. Various methods have been proposed for fake
news detection, including text-based approaches as well as a series of
graph-based approaches. However, the deceptive nature of fake news renders
text-based approaches less effective. Propagation tree-based methods focus on
the propagation process of individual news, capturing pairwise relationships
but lacking the capability to capture high-order complex relationships. Large
heterogeneous graph-based approaches necessitate the incorporation of
substantial additional information beyond news text and user data, while
hypergraph-based approaches rely on predefined hypergraph structures. To tackle
these issues, we propose a novel dynamic hypergraph-based multi-view fake news
detection model (DHy-MFND) that learns news embeddings across three distinct
views: text-level, propagation tree-level, and hypergraph-level. By employing
hypergraph structures to model complex high-order relationships among multiple
news pieces and introducing dynamic hypergraph structure learning, we optimize
predefined hypergraph structures while learning news embeddings. Additionally,
we introduce contrastive learning to capture authenticity-relevant embeddings
across different views. Extensive experiments on two benchmark datasets
demonstrate the effectiveness of our proposed DHy-MFND compared with a broad
range of competing baselines.",2024-12-26T14:05:51Z,http://arxiv.org/abs/2412.19227v1,"Rongping Ye, Xiaobing Pei"
"Completion as Enhancement: A Degradation-Aware Selective Image Guided
  Network for Depth Completion","In this paper, we introduce the Selective Image Guided Network (SigNet), a
novel degradation-aware framework that transforms depth completion into depth
enhancement for the first time. Moving beyond direct completion using
convolutional neural networks (CNNs), SigNet initially densifies sparse depth
data through non-CNN densification tools to obtain coarse yet dense depth. This
approach eliminates the mismatch and ambiguity caused by direct convolution
over irregularly sampled sparse data. Subsequently, SigNet redefines completion
as enhancement, establishing a self-supervised degradation bridge between the
coarse depth and the targeted dense depth for effective RGB-D fusion. To
achieve this, SigNet leverages the implicit degradation to adaptively select
high-frequency components (e.g., edges) of RGB data to compensate for the
coarse depth. This degradation is further integrated into a multi-modal
conditional Mamba, dynamically generating the state parameters to enable
efficient global high-frequency information interaction. We conduct extensive
experiments on the NYUv2, DIML, SUN RGBD, and TOFDC datasets, demonstrating the
state-of-the-art (SOTA) performance of SigNet.",2024-12-26T14:05:01Z,http://arxiv.org/abs/2412.19225v1,"Zhiqiang Yan, Zhengxue Wang, Kun Wang, Jun Li, Jian Yang"
"Interference-Robust Broadband Rapidly-Varying MIMO Communications: A
  Knowledge-Data Dual Driven Framework","A novel time-efficient framework is proposed for improving the robustness of
a broadband multiple-input multiple-output (MIMO) system against unknown
interference under rapidly-varying channels. A mean-squared error (MSE)
minimization problem is formulated by optimizing the beamformers employed.
Since the unknown interference statistics are the premise for solving the
formulated problem, an interference statistics tracking (IST) module is first
designed. The IST module exploits both the time- and spatial-domain
correlations of the interference-plus-noise (IPN) covariance for the future
predictions with data training. Compared to the conventional signal-free space
sampling approach, the IST module can realize zero-pilot and low-latency
estimation. Subsequently, an interference-resistant hybrid beamforming (IR-HBF)
module is presented, which incorporates both the prior knowledge of the
theoretical optimization method as well as the data-fed training. Taking
advantage of the interpretable network structure, the IR-HBF module enables the
simplified mapping from the interference statistics to the beamforming weights.
The simulations are executed in high-mobility scenarios, where the numerical
results unveil that: 1) the proposed IST module attains promising prediction
accuracy compared to the conventional counterparts under different snapshot
sampling errors; and 2) the proposed IR-HBF module achieves lower MSE with
significantly reduced computational complexity.",2024-12-26T13:59:08Z,http://arxiv.org/abs/2412.19221v1,"Jingjing Zhao, Jing Su, Xianchi Lv, Kaiquan Cai, Yanbo Zhu, Yuanwei Liu, Naofal Al-Dhahir"
"Applying the maximum entropy principle to multi-species neural networks
  improves species distribution models","The rapid expansion of citizen science initiatives has led to a significant
growth of biodiversity databases, and particularly presence-only (PO)
observations. PO data are invaluable for understanding species distributions
and their dynamics, but their use in Species Distribution Models (SDM) is
curtailed by sampling biases and the lack of information on absences. Poisson
point processes are widely used for SDMs, with Maxent being one of the most
popular methods. Maxent maximises the entropy of a probability distribution
across sites as a function of predefined transformations of environmental
variables, called features. In contrast, neural networks and deep learning have
emerged as a promising technique for automatic feature extraction from complex
input variables. In this paper, we propose DeepMaxent, which harnesses neural
networks to automatically learn shared features among species, using the
maximum entropy principle. To do so, it employs a normalised Poisson loss where
for each species, presence probabilities across sites are modelled by a neural
network. We evaluate DeepMaxent on a benchmark dataset known for its spatial
sampling biases, using PO data for calibration and presence-absence (PA) data
for validation across six regions with different biological groups and
environmental covariates. Our results indicate that DeepMaxent improves model
performance over Maxent and other state-of-the-art SDMs across regions and
taxonomic groups. The method performs particularly well in regions of uneven
sampling, demonstrating substantial potential to improve species distribution
modelling. The method opens the possibility to learn more robust environmental
features predicting jointly many species and scales to arbitrary large numbers
of sites without an increased memory demand.",2024-12-26T13:47:04Z,http://arxiv.org/abs/2412.19217v1,"Maxime Ryckewaert, Diego Marcos, Christophe Botella, Maximilien Servajean, Pierre Bonnet, Alexis Joly"
"Optimizing Fantasy Sports Team Selection with Deep Reinforcement
  Learning","Fantasy sports, particularly fantasy cricket, have garnered immense
popularity in India in recent years, offering enthusiasts the opportunity to
engage in strategic team-building and compete based on the real-world
performance of professional athletes. In this paper, we address the challenge
of optimizing fantasy cricket team selection using reinforcement learning (RL)
techniques. By framing the team creation process as a sequential
decision-making problem, we aim to develop a model that can adaptively select
players to maximize the team's potential performance. Our approach leverages
historical player data to train RL algorithms, which then predict future
performance and optimize team composition. This not only represents a huge
business opportunity by enabling more accurate predictions of high-performing
teams but also enhances the overall user experience. Through empirical
evaluation and comparison with traditional fantasy team drafting methods, we
demonstrate the effectiveness of RL in constructing competitive fantasy teams.
Our results show that RL-based strategies provide valuable insights into player
selection in fantasy sports.",2024-12-26T13:36:18Z,http://arxiv.org/abs/2412.19215v1,"Shamik Bhattacharjee, Kamlesh Marathe, Hitesh Kapoor, Nilesh Patil"
Primordial Power Spectrum of Five Dimensional Uniform Inflation,"Five dimensional (5D) uniform inflation describes a de Sitter (or
approximate) solution of 5D Einstein equations, with cosmological constant and
a 5D Planck scale $M_* \sim 10^9$ GeV. During the inflationary period all
dimensions (compact and non-compact) expand exponentially in terms of the 5D
proper time. This set-up requires about 40 $e$-folds to expand the fifth
dimension from the fundamental length to the micron size. At the end of 5D
inflation (or at any given moment during the inflationary phase) one can
interpret the solution in terms of 4D fields using 4D Planck units from the
relation $M_p^2 = 2 \pi R M_*^3$, which amounts going to the 4D Einstein frame.
This implies that if the compactification radius $R$ expands $N$ $e$-folds,
then the 3D space would expand $3N/2$ $e$-folds as a result of a uniform 5D
inflation. We reexamine the primordial power spectrum predicted by this model
and show that it is consistent with Planck's measurements of the comic
microwave background. The best-fit to Planck data corresponds to $R \sim
10~\mu$m. A departure of the angular power spectrum predicted by 4D cosmology
is visible at multipole moment $\ell \sim 7$.",2024-12-26T13:24:36Z,http://arxiv.org/abs/2412.19213v1,"Luis A. Anchordoqui, Ignatios Antoniadis"
"Towards Better Spherical Sliced-Wasserstein Distance Learning with
  Data-Adaptive Discriminative Projection Direction","Spherical Sliced-Wasserstein (SSW) has recently been proposed to measure the
discrepancy between spherical data distributions in various fields, such as
geology, medical domains, computer vision, and deep representation learning.
However, in the original SSW, all projection directions are treated equally,
which is too idealistic and cannot accurately reflect the importance of
different projection directions for various data distributions. To address this
issue, we propose a novel data-adaptive Discriminative Spherical
Sliced-Wasserstein (DSSW) distance, which utilizes a projected energy function
to determine the discriminative projection direction for SSW. In our new DSSW,
we introduce two types of projected energy functions to generate the weights
for projection directions with complete theoretical guarantees. The first type
employs a non-parametric deterministic function that transforms the projected
Wasserstein distance into its corresponding weight in each projection
direction. This improves the performance of the original SSW distance with
negligible additional computational overhead. The second type utilizes a neural
network-induced function that learns the projection direction weight through a
parameterized neural network based on data projections. This further enhances
the performance of the original SSW distance with less extra computational
overhead. Finally, we evaluate the performance of our proposed DSSW by
comparing it with several state-of-the-art methods across a variety of machine
learning tasks, including gradient flows, density estimation on real earth
data, and self-supervised learning.",2024-12-26T13:23:37Z,http://arxiv.org/abs/2412.19212v1,"Hongliang Zhang, Shuo Chen, Lei Luo, Jian Yang"
"Large Language Models Meet Graph Neural Networks: A Perspective of Graph
  Mining","Graph mining is an important area in data mining and machine learning that
involves extracting valuable information from graph-structured data. In recent
years, significant progress has been made in this field through the development
of graph neural networks (GNNs). However, GNNs are still deficient in
generalizing to diverse graph data. Aiming to this issue, Large Language Models
(LLMs) could provide new solutions for graph mining tasks with their superior
semantic understanding. In this review, we systematically review the
combination and application techniques of LLMs and GNNs and present a novel
taxonomy for research in this interdisciplinary field, which involves three
main categories: GNN-driving-LLM, LLM-driving-GNN, and GNN-LLM-co-driving.
Within this framework, we reveal the capabilities of LLMs in enhancing graph
feature extraction as well as improving the effectiveness of downstream tasks
such as node classification, link prediction, and community detection. Although
LLMs have demonstrated their great potential in handling graph-structured data,
their high computational requirements and complexity remain challenges. Future
research needs to continue to explore how to efficiently fuse LLMs and GNNs to
achieve more powerful graph learning and reasoning capabilities and provide new
impetus for the development of graph mining techniques.",2024-12-26T13:21:09Z,http://arxiv.org/abs/2412.19211v1,"Yuxin You, Zhen Liu, Xiangchao Wen, Yongtao Zhang, Wei Ai"
Context-Aware Deep Learning for Multi Modal Depression Detection,"In this study, we focus on automated approaches to detect depression from
clinical interviews using multi-modal machine learning (ML). Our approach
differentiates from other successful ML methods such as context-aware analysis
through feature engineering and end-to-end deep neural networks for depression
detection utilizing the Distress Analysis Interview Corpus. We propose a novel
method that incorporates: (1) pre-trained Transformer combined with data
augmentation based on topic modelling for textual data; and (2) deep 1D
convolutional neural network (CNN) for acoustic feature modeling. The
simulation results demonstrate the effectiveness of the proposed method for
training multi-modal deep learning models. Our deep 1D CNN and Transformer
models achieved state-of-the-art performance for audio and text modalities
respectively. Combining them in a multi-modal framework also outperforms
state-of-the-art for the combined setting. Code available at
https://github.com/genandlam/multi-modal-depression-detection",2024-12-26T13:19:26Z,http://arxiv.org/abs/2412.19209v1,"Genevieve Lam, Huang Dongyan, Weisi Lin"
"Open cluster dissolution rate and the initial cluster mass function in
  the solar neighbourhood. Modelling the age and mass distributions of clusters
  observed by Gaia","Context. The dissolution rate of open clusters (OCs) and integration of their
stars into the Milky Way's field population has been previously explored using
their age distribution. With the advent of the Gaia mission, we have an
exceptional opportunity to revisit and enhance these studies with ages and
masses from high quality data. Aims. To build a comprehensive Gaia-based OC
mass catalogue which, combined with the age distribution, allows a deeper
investigation of the disruption experienced by OCs within the solar
neighbourhood. Methods. Masses were determined by comparing luminosity
distributions to theoretical luminosity functions. The limiting and core radii
of the clusters were obtained by fitting the King function to their observed
density profiles. We examined the disruption process through simulations of the
build-up and mass evolution of a population of OCs which were compared to the
observed mass and age distributions. Results. Our analysis yielded an OC mass
distribution with a peak at $log(M)$ = 2.7 dex ($\sim 500 M_{\odot}$), as well
as radii for 1724 OCs. Our simulations showed that using a power-law Initial
Cluster Mass Function (ICMF) no parameters were able to reproduce the observed
mass distribution. Moreover, we find that a skew log-normal ICMF provides a
good match to the observations and that the disruption time of a $10^4
M{_\odot}$ OC is $t_4^{tot} = 2.9 \pm 0.4$ Gyr. Conclusions. Our results
indicate that the OC disruption time $t_4^{tot}$ is about twice longer than
previous estimates based solely on OC age distributions. We find that the shape
of the ICMF for bound OCs differs from that of embedded clusters, which could
imply a low typical star formation efficiency of $\leq 20\%$ in OCs. Our
results also suggest a lower limit of $\sim 60 M{_\odot}$ for bound OCs in the
solar neighbourhood.",2024-12-26T12:54:51Z,http://arxiv.org/abs/2412.19204v1,"Duarte Almeida, André Moitinho, Sandro Moreira"
"GAIS: A Novel Approach to Instance Selection with Graph Attention
  Networks","Instance selection (IS) is a crucial technique in machine learning that aims
to reduce dataset size while maintaining model performance. This paper
introduces a novel method called Graph Attention-based Instance Selection
(GAIS), which leverages Graph Attention Networks (GATs) to identify the most
informative instances in a dataset. GAIS represents the data as a graph and
uses GATs to learn node representations, enabling it to capture complex
relationships between instances. The method processes data in chunks, applies
random masking and similarity thresholding during graph construction, and
selects instances based on confidence scores from the trained GAT model.
Experiments on 13 diverse datasets demonstrate that GAIS consistently
outperforms traditional IS methods in terms of effectiveness, achieving high
reduction rates (average 96\%) while maintaining or improving model
performance. Although GAIS exhibits slightly higher computational costs, its
superior performance in maintaining accuracy with significantly reduced
training data makes it a promising approach for graph-based data selection.",2024-12-26T12:51:14Z,http://arxiv.org/abs/2412.19201v1,"Zahiriddin Rustamov, Ayham Zaitouny, Rafat Damseh, Nazar Zaki"
"Personalized Dynamic Music Emotion Recognition with Dual-Scale
  Attention-Based Meta-Learning","Dynamic Music Emotion Recognition (DMER) aims to predict the emotion of
different moments in music, playing a crucial role in music information
retrieval. The existing DMER methods struggle to capture long-term dependencies
when dealing with sequence data, which limits their performance. Furthermore,
these methods often overlook the influence of individual differences on emotion
perception, even though everyone has their own personalized emotional
perception in the real world. Motivated by these issues, we explore more
effective sequence processing methods and introduce the Personalized DMER
(PDMER) problem, which requires models to predict emotions that align with
personalized perception. Specifically, we propose a Dual-Scale Attention-Based
Meta-Learning (DSAML) method. This method fuses features from a dual-scale
feature extractor and captures both short and long-term dependencies using a
dual-scale attention transformer, improving the performance in traditional
DMER. To achieve PDMER, we design a novel task construction strategy that
divides tasks by annotators. Samples in a task are annotated by the same
annotator, ensuring consistent perception. Leveraging this strategy alongside
meta-learning, DSAML can predict personalized perception of emotions with just
one personalized annotation sample. Our objective and subjective experiments
demonstrate that our method can achieve state-of-the-art performance in both
traditional DMER and PDMER.",2024-12-26T12:47:35Z,http://arxiv.org/abs/2412.19200v1,"Dengming Zhang, Weitao You, Ziheng Liu, Lingyun Sun, Pei Chen"
"Implications for the non-Gaussianity of primordial gravitational waves
  from pulsar timing arrays","The detection of a stochastic signal by recent pulsar timing array (PTA)
collaborations, including NANOGrav, PPTA, EPTA+InPTA, CPTA and MPTA, has opened
a new window to explore gravitational waves (GWs) at nanohertz frequencies.
Motivated by the possibility that such a signal could arise from primordial
gravitational waves (PGWs), we investigate the implications of tensor
non-Gaussianity for the PGW power spectrum. Utilizing PTA data sets, we provide
constraints on local-type tensor non-Gaussianity parameter ${F}_{\mathrm{NL}}$.
We find $|{F}_{\mathrm{NL}}|\lesssim 7.97$ for a log-normal PGW power spectrum.
Our analysis reveals that even moderate tensor non-Gaussianity can lead to
significant deviations from standard predictions, thereby offering a novel
means to test inflationary scenarios and probe the underlying dynamics of the
early Universe. Future multi-band GW observatories, such as LISA, Taiji, and
TianQin, will be instrumental in complementing these efforts and further
refining our understanding of tensor non-Gaussianity.",2024-12-26T12:31:58Z,http://arxiv.org/abs/2412.19196v1,"Zhi-Zhang Peng, You Wu, Lang Liu"
"Game-Theoretically Secure Distributed Protocols for Fair Allocation in
  Coalitional Games","We consider game-theoretically secure distributed protocols for coalition
games that approximate the Shapley value with small multiplicative error. Since
all known existing approximation algorithms for the Shapley value are
randomized, it is a challenge to design efficient distributed protocols among
mutually distrusted players when there is no central authority to generate
unbiased randomness. The game-theoretic notion of maximin security has been
proposed to offer guarantees to an honest player's reward even if all other
players are susceptible to an adversary.
  Permutation sampling is often used in approximation algorithms for the
Shapley value. A previous work in 1994 by Zlotkin et al. proposed a simple
constant-round distributed permutation generation protocol based on commitment
scheme, but it is vulnerable to rushing attacks. The protocol, however, can
detect such attacks.
  In this work, we model the limited resources of an adversary by a violation
budget that determines how many times it can perform such detectable attacks.
Therefore, by repeating the number of permutation samples, an honest player's
reward can be guaranteed to be close to its Shapley value. We explore both high
probability and expected maximin security. We obtain an upper bound on the
number of permutation samples for high probability maximin security, even with
an unknown violation budget. Furthermore, we establish a matching lower bound
for the weaker notion of expected maximin security in specific permutation
generation protocols. We have also performed experiments on both synthetic and
real data to empirically verify our results.",2024-12-26T12:13:21Z,http://arxiv.org/abs/2412.19192v1,"T-H. Hubert Chan, Qipeng Kuang, Quan Xue"
New Physics in the 3-3-1 models,"Two main ingredients of current particle physics
  such as local gauge symmetry and mass generation via the Higgs mechanism
being basic ground of the Standard Model are widely confirmed by
  experimental data. However, some problems such as neutrino masses, dark
matter, baryon asymmetry of Universe have clearly indicated that the Standard
Model cannot be the ultimate theory of nature. To surpass the mentioned
puzzles,
  many extensions of the Standard Model (called beyond Standard Model) have
been proposed. Among beyond Standard Models, the 3-3-1 models have some
intriguing features and they get wide attention. The pioneer models develop in
some directions. In this paper, %some new main versions of the 3-3-1 models and
their consequences are presented.",2024-12-26T11:55:43Z,http://arxiv.org/abs/2412.19188v1,H. N. Long
"Outlier-Bias Removal with Alpha Divergence: A Robust Non-Convex
  Estimator for Linear Regression","Convex and penalized robust methods often suffer from bias induced by large
outliers, limiting their effectiveness in adversarial or heavy-tailed settings.
In this study, we propose a novel approach that eliminates this bias (when
possible) by leveraging a non-convex $M$-estimator based on the alpha
divergence. We address the problem of estimating the parameters vector in high
dimensional linear regression, even when a subset of the data has been
deliberately corrupted by an adversary with full knowledge of the dataset and
its underlying distribution.
  Our primary contribution is to demonstrate that the objective function,
although non-convex, exhibits convexity within a carefully chosen basin of
attraction, enabling robust and unbiased estimation. Additionally, we establish
three key theoretical guarantees for the estimator: (a) a deviation bound that
is minimax optimal up to a logarithmic factor, (b) an improved unbiased bound
when the outliers are large and (c) asymptotic normality as the sample size
increases. Finally, we validate the theoretical findings through empirical
comparisons with state-of-the-art estimators on both synthetic and real-world
datasets, highlighting the proposed method's superior robustness, efficiency,
and ability to mitigate outlier-induced bias.",2024-12-26T11:42:46Z,http://arxiv.org/abs/2412.19183v1,"Ilyes Hammouda, Mohamed Ndaoud, and Abd-Krim Seghouane"
"Unraveling the magnetic and electronic complexity of intermetallic
  ErPd$_2$Si$_2$: Anisotropic thermal expansion, phase transitions, and twofold
  magnetotransport behavior","We present a comprehensive investigation into the physical properties of
intermetallic ErPd$_2$Si$_2$, a compound renowned for its intriguing magnetic
and electronic characteristics. We confirm the tetragonal crystal structure of
ErPd$_2$Si$_2$ within the $I4/mmm$ space group. Notably, we observed
anisotropic thermal expansion, with the lattice constant $a$ expanding and $c$
contracting between 15 K and 300 K. This behavior is attributed to lattice
vibrations and electronic contributions. Heat capacity measurements revealed
three distinct temperature regimes: $T_1 \sim 3.0$ K, $T_\textrm{N} \sim 4.20$
K, and $T_2 \sim 15.31$ K. These correspond to the disappearance of
spin-density waves, the onset of an incommensurate antiferromagnetic (AFM)
structure, and the crystal-field splitting and/or the presence of short-range
spin fluctuations, respectively. Remarkably, the AFM phase transition anomaly
was observed exclusively in low-field magnetization data (120 Oe) at
$T_\textrm{N}$. A high magnetic field ($B =$ 3 T) effectively suppressed this
anomaly, likely due to spin-flop and spin-flip transitions. Furthermore, the
extracted effective PM moments closely matched the expected theoretical value,
suggesting a dominant magnetic contribution from localized 4$f$ spins of Er.
Additionally, significant differences in resistance ($R$) values at low
temperatures under applied $B$ indicated a magnetoresistance (MR) effect with a
minimum value of -4.36\%. Notably, the measured MR effect exhibited anisotropic
behavior, where changes in the strength or direction of the applied $B$ induced
variations in the MR effect. A twofold symmetry of $R$ was discerned at 3 T and
9 T, originating from the orientation of spin moments relative to the applied
$B$. Intriguingly, above $T_\textrm{N}$, short-range spin fluctuations also
displayed a preferred orientation along the $c$-axis due to single-ion
anisotropy.",2024-12-26T11:39:24Z,http://arxiv.org/abs/2412.19181v1,"Kaitong Sun, Si Wu, Guanping Xu, Lingwei Li, Hongyu Chen, Qian Zhao, Muqing Su, Wolfgang Schmidt, Chongde Cao, Hai-Feng Li"
"Mask Approximation Net: Merging Feature Extraction and Distribution
  Learning for Remote Sensing Change Captioning","Remote sensing image change description, as a novel multimodal task in the
field of remote sensing processing, not only enables the detection of changes
in surface conditions but also provides detailed descriptions of these changes,
thereby enhancing human interpretability and interactivity. However, previous
methods mainly employed Convolutional Neural Network (CNN) architectures to
extract bitemporal image features. This approach often leads to an overemphasis
on designing specific network architectures and limits the captured feature
distributions to the current dataset, resulting in poor generalizability and
robustness when applied to other datasets or real-world scenarios. To address
these limitations, this paper proposes a novel approach for remote sensing
image change detection and description that integrates diffusion models, aiming
to shift the focus from conventional feature learning paradigms to data
distribution learning. The proposed method primarily includes a simple
multi-scale change detection module, whose output features are subsequently
refined using a diffusion model. Additionally, we introduce a frequency-guided
complex filter module to handle high-frequency noise during the diffusion
process, which helps to maintain model performance. Finally, we validate the
effectiveness of our proposed method on several remote sensing change detection
description datasets, demonstrating its superior performance. The code
available at MaskApproxNet.",2024-12-26T11:35:57Z,http://arxiv.org/abs/2412.19179v1,"Dongwei Sun, Xiangyong Cao"
Physical nature of quasi-stable structures existing in antimony melt,"Equilibrium antimony melt near the melting temperature is characterised by
structural features that are not present in simple single-component liquids.
The cause of these features may be long-lived structural formations that are
not yet fully understood. The present work provides the detailed
characterization of the structures formed in liquid antimony near the melting
temperature based on the results of quantum chemical calculations and the
available neutron and X-ray diffraction data. The quasi-stable structures in
antimony melt are detected with lifetimes exceeding the structural relaxation
time of this melt. These structures are characterised by a low degree of order
and spatial localisation. It is shown for the first time that the elementary
units of these quasi-stable structures are triplets of atoms with
characteristic lengths of $3.07$\,\AA~and $4.7$\,\AA~and characteristic angles
of $45$ and $90$ degrees. It was found that these triplets can form chains and
percolating clusters up to $\sim15$\,\AA~in length. The characteristic lengths
of these triplets are fully consistent with the correlation lengths associated
with short-range order in the antimony melt as determined by diffraction
experiments.",2024-12-26T11:30:38Z,http://arxiv.org/abs/2412.19177v1,"Artem A. Tsygankov, Bulat N. Galimzyanov, Anatolii V. Mokshin"
"Accelerating Stochastic Gravitational Wave Backgrounds Parameter
  Estimation in Pulsar Timing Arrays with Flow Matching","Pulsar timing arrays (PTAs) are essential tools for detecting the stochastic
gravitational wave background (SGWB), but their analysis faces significant
computational challenges. Traditional methods like Markov-chain Monte Carlo
(MCMC) struggle with high-dimensional parameter spaces where noise parameters
often dominate, while existing deep learning approaches fail to model the
Hellings-Downs (HD) correlation or are validated only on synthetic datasets. We
propose a flow-matching-based continuous normalizing flow (CNF) for efficient
and accurate PTA parameter estimation. By focusing on the 10 most contributive
pulsars from the NANOGrav 15-year dataset, our method achieves posteriors
consistent with MCMC, with a Jensen-Shannon divergence below \(10^{-2}\) nat,
while reducing sampling time from 50 hours to 4 minutes. Powered by a versatile
embedding network and a reweighting loss function, our approach prioritizes the
SGWB parameters and scales effectively for future datasets. It enables precise
reconstruction of SGWB and opens new avenues for exploring vast observational
data and uncovering potential new physics, offering a transformative tool for
advancing gravitational wave astronomy.",2024-12-26T11:02:11Z,http://arxiv.org/abs/2412.19169v1,"Bo Liang, Chang Liu, Tianyu Zhao, Minghui Du, Manjia Liang, Ruijun Shi, Hong Guo, Yuxiang Xu, Li-e Qiang, Peng Xu, Wei-Liang Qian, Ziren Luo"
Master Stability Functions in Complex Networks,"Synchronization is an emergent phenomenon in coupled dynamical networks. The
Master Stability Function (MSF) is a highly elegant and powerful tool for
characterizing the stability of synchronization states. However, a significant
challenge lies in determining the MSF for complex dynamical networks driven by
nonlinear interaction mechanisms. These mechanisms introduce additional
complexity through the intricate connectivity of interacting elements within
the network and the intrinsic dynamics, which are governed by nonlinear
processes with diverse parameters and higher dimensionality of systems. Over
the past 25 years, extensive research has focused on determining the MSF for
pairwise coupled identical systems with diffusive coupling. Our literature
survey highlights two significant advancements in recent years: the
consideration of multilayer networks instead of single-layer networks and the
extension of MSF analysis to incorporate higher-order interactions alongside
pairwise interactions.
  In this review article, we revisit the analysis of the MSF for diffusively
pairwise coupled dynamical systems and extend this framework to more general
coupling schemes. Furthermore, we systematically derive the MSF for multilayer
dynamical networks and single-layer coupled systems by incorporating
higher-order interactions alongside pairwise interactions. The primary focus of
our review is on the analytical derivation and numerical computation of the MSF
for complex dynamical networks. Finally, we demonstrate the application of the
MSF in data science, emphasizing its relevance and potential in this rapidly
evolving field.",2024-12-26T10:47:00Z,http://arxiv.org/abs/2412.19163v1,"Suman Acharyya, Priodyuti Pradhan, Chandrakala Meena"
"Dual Channel Multi-Attention in ViT for Biometric Authentication using
  Forehead Subcutaneous Vein Pattern and Periocular Pattern","Traditional biometric systems, like face and fingerprint recognition, have
encountered significant setbacks due to wearing face masks and hygiene
concerns. To meet the challenges of the partially covered face due to face
masks and hygiene concerns of fingerprint recognition, this paper proposes a
novel dual-channel multi-attention Vision Transformer (ViT) framework for
biometric authentication using forehead subcutaneous vein patterns and
periocular patterns, offering a promising alternative to traditional methods,
capable of performing well even with face masks and without any physical touch.
The proposed framework leverages a dual-channel ViT architecture, designed to
handle two distinct biometric traits. It can capture long-range dependencies of
independent features from the vein and periocular patterns. A custom classifier
is then designed to integrate the independently extracted features, producing a
final class prediction. The performance of the proposed algorithm was
rigorously evaluated using the Forehead Subcutaneous Vein Pattern and
Periocular Biometric Pattern (FSVP-PBP) database. The results demonstrated the
superiority of the algorithm over state-of-the-art methods, achieving
remarkable classification accuracy of $99.3 \pm 0.02\%$ with the combined vein
and periocular patterns.",2024-12-26T10:40:15Z,http://arxiv.org/abs/2412.19160v1,"Arun K. Sharma, Shubhobrata Bhattacharya, Motahar Reza"
Advancements in Terahertz Antenna Design,"The promising way to provide sufficient transmission capacity is by accessing
transmission bands at higher carrier frequencies. This desire for higher
carrier frequency or more bandwidth led the researchers to take advantage of
the terahertz (THz) spectrum. The opportunity for large bandwidth in the THz
band leads to the possibility of easy, high data rate transmission. In spite of
the advantages, the THz band suffers from large free space path loss. In the
development of THz communication systems, the antenna is the most significant
component. The focus is especially on designing highly directive antennas
because they enhance the performance of the overall system by compensating for
the large path loss at THz and thus improving the signal-to-noise ratio. This
chapter presents different types of THz antennas, including planar,
reflectarray, horn antenna, and lens antenna. Emphasis has been made to present
the latest trend of designing THz antennas using carbon-based materials, such
as graphene and carbon nanotubes. The performance of these antennas has been
compared with that of traditional copper-based THz antennas by critically
analyzing their properties. A brief discussion on THz power sources is included
in this chapter for completeness. A comprehensive discussion on different
fabrication techniques has been provided to appraise the reader of the general
fabrication processes of THz components.",2024-12-26T10:21:58Z,http://arxiv.org/abs/2412.19156v1,"Sasmita Dash, Amalendu Patnaik"
"Evolutionary de-homogenization using a generative model for optimizing
  solid-porous infill structures considering the stress concentration issue","The design of porous infill structures presents significant challenges due to
their complex geometric configurations, such as the accurate representation of
geometric boundaries and the control of localized maximum stress. In current
mainstream design methods, such as topology optimization, the analysis is often
performed using pixel or voxel-based element approximations. These
approximations, constrained by the optimization framework, result in
substantial geometric discrepancies between the analysis model and the final
physical model. Such discrepancies can severely impact structural performance,
particularly for localized properties like stress response, where accurate
geometry is critical to mitigating stress concentration. To address these
challenges, we propose evolutionary de-homogenization, which is a design
framework based on the integration of de-homogenization and data-driven
multifidelity optimization. This framework facilitates the hybrid solid-porous
infill design by bridging the gap between low-fidelity analysis and
high-fidelity physical realizations, ensuring both geometric accuracy and
enhanced structural performance. The low-fidelity level utilizes commonly used
density control variables, while the high-fidelity level involves stress
analysis based on structures with precise geometric representations. By
employing a de-homogenization-based mapping method, a side-by-side
correspondence between low-fidelity and high-fidelity results is established.
The low-fidelity control variables are iteratively adjusted to optimize the
high-fidelity results by integrating deep generative model with multi-objective
evolutionary algorithm. Finally, numerical experiments demonstrate the
effectiveness of the proposed method.",2024-12-26T10:18:16Z,http://arxiv.org/abs/2412.19154v1,"Shuzhi Xu, Hiroki Kawabe, Kentaro Yaji"
"To Predict or Not To Predict? Proportionally Masked Autoencoders for
  Tabular Data Imputation","Masked autoencoders (MAEs) have recently demonstrated effectiveness in
tabular data imputation. However, due to the inherent heterogeneity of tabular
data, the uniform random masking strategy commonly used in MAEs can disrupt the
distribution of missingness, leading to suboptimal performance. To address
this, we propose a proportional masking strategy for MAEs. Specifically, we
first compute the statistics of missingness based on the observed proportions
in the dataset, and then generate masks that align with these statistics,
ensuring that the distribution of missingness is preserved after masking.
Furthermore, we argue that simple MLP-based token mixing offers competitive or
often superior performance compared to attention mechanisms while being more
computationally efficient, especially in the tabular domain with the inherent
heterogeneity. Experimental results validate the effectiveness of the proposed
proportional masking strategy across various missing data patterns in tabular
datasets. Code is available at: \url{https://github.com/normal-kim/PMAE}.",2024-12-26T10:12:08Z,http://arxiv.org/abs/2412.19152v1,"Jungkyu Kim, Kibok Lee, Taeyoung Park"
AskChart: Universal Chart Understanding through Textual Enhancement,"Chart understanding tasks such as ChartQA and Chart-to-Text involve
automatically extracting and interpreting key information from charts, enabling
users to query or convert visual data into structured formats. State-of-the-art
approaches primarily focus on visual cues from chart images, failing to
explicitly incorporate rich textual information (e.g., data labels and axis
labels) embedded within the charts. This textual information is vital for
intuitive human comprehension and interpretation of charts. Moreover, existing
models are often large and computationally intensive, limiting their practical
applicability. In this paper, we introduce AskChart, a universal model that
explicitly integrates both textual and visual cues from charts using a Mixture
of Experts (MoE) architecture. AskChart facilitates the learning of enhanced
visual-textual representations of charts for effectively handling multiple
chart understanding tasks, while maintaining a smaller model size. To capture
the synergy between visual and textual modalities, we curate a large-scale
dataset named ChartBank with about 7.5M data samples, which helps align textual
and visual information and facilitates the extraction of visual entities and
text. To effectively train AskChart, we design a three-stage training strategy
to align visual and textual modalities for learning robust visual-textual
representations and optimizing the learning of the MoE layer. Extensive
experiments across five datasets demonstrate the significant performance gains
of AskChart in four chart understanding tasks. Remarkably, AskChart with 4.6B
parameters outperforms state-of-the-art models with 13B parameters by 68.3% in
Open-ended ChartQA and 49.2% in Chart-to-Text tasks, while achieving comparable
performance in ChartQA and Chart-to-Table tasks.",2024-12-26T09:59:43Z,http://arxiv.org/abs/2412.19146v1,"Xudong Yang, Yifan Wu, Yizhang Zhu, Nan Tang, Yuyu Luo"
"Impact of color and mixing proportion of synthetic point clouds on
  semantic segmentation","Semantic segmentation of point clouds is essential for understanding the
built environment, and a large amount of high-quality data is required for
training deep learning models. Despite synthetic point clouds (SPC) having the
potential to compensate for the shortage of real data, how to exploit the
benefits of SPC is still open. Therefore, this study systematically
investigates how color and mixing proportion of SPC impact semantic
segmentation for the first time. First, a new method to mimic the scanning
process and generate SPC based on BIM is proposed, to create a synthetic
dataset with consistent colors of BIM (UniSPC) and a synthetic dataset with
real colors (RealSPC) respectively. Subsequently, by integrating with the S3DIS
dataset, further experiments on PointNet, PointNet++, and DGCNN are conducted.
Meanwhile, benchmark experiments and new evaluation metrics are introduced to
better evaluate the performance of different models. Experiments show that
synthetic color significantly impacts model performance, the performance for
common components of the models trained with pure RealSPC is comparable to
models with real data, and RealSPC contributes average improvements of 14.1% on
overall accuracy and 7.3% on mIoU than UniSPC. Furthermore, the proportion of
SPC also has a significant impact on the performance. In mixing training
experiments, adding more than 70% SPC achieves an average of 3.9% on overall
accuracy and 3.4% on mIoU better than benchmark on three models. It is also
revealed that for large flat elements such as floors, ceilings, and walls, the
SPC can even replace real point clouds without compromising model performance.",2024-12-26T09:58:04Z,http://arxiv.org/abs/2412.19145v1,"Shaojie Zhou, Jia-Rui Lin, Peng Pan, Yuandong Pan, Ioannis Brilakis"
"SILC-EFSA: Self-aware In-context Learning Correction for Entity-level
  Financial Sentiment Analysis","In recent years, fine-grained sentiment analysis in finance has gained
significant attention, but the scarcity of entity-level datasets remains a key
challenge. To address this, we have constructed the largest English and Chinese
financial entity-level sentiment analysis datasets to date. Building on this
foundation, we propose a novel two-stage sentiment analysis approach called
Self-aware In-context Learning Correction (SILC). The first stage involves
fine-tuning a base large language model to generate pseudo-labeled data
specific to our task. In the second stage, we train a correction model using a
GNN-based example retriever, which is informed by the pseudo-labeled data. This
two-stage strategy has allowed us to achieve state-of-the-art performance on
the newly constructed datasets, advancing the field of financial sentiment
analysis. In a case study, we demonstrate the enhanced practical utility of our
data and methods in monitoring the cryptocurrency market. Our datasets and code
are available at https://github.com/NLP-Bin/SILC-EFSA.",2024-12-26T09:53:01Z,http://arxiv.org/abs/2412.19140v1,"Senbin Zhu, Chenyuan He, Hongde Liu, Pengcheng Dong, Hanjie Zhao, Yuchen Yan, Yuxiang Jia, Hongying Zan, Min Peng"
SUTrack: Towards Simple and Unified Single Object Tracking,"In this paper, we propose a simple yet unified single object tracking (SOT)
framework, dubbed SUTrack. It consolidates five SOT tasks (RGB-based,
RGB-Depth, RGB-Thermal, RGB-Event, RGB-Language Tracking) into a unified model
trained in a single session. Due to the distinct nature of the data, current
methods typically design individual architectures and train separate models for
each task. This fragmentation results in redundant training processes,
repetitive technological innovations, and limited cross-modal knowledge
sharing. In contrast, SUTrack demonstrates that a single model with a unified
input representation can effectively handle various common SOT tasks,
eliminating the need for task-specific designs and separate training sessions.
Additionally, we introduce a task-recognition auxiliary training strategy and a
soft token type embedding to further enhance SUTrack's performance with minimal
overhead. Experiments show that SUTrack outperforms previous task-specific
counterparts across 11 datasets spanning five SOT tasks. Moreover, we provide a
range of models catering edge devices as well as high-performance GPUs,
striking a good trade-off between speed and accuracy. We hope SUTrack could
serve as a strong foundation for further compelling research into unified
tracking models. Code and models are available at
github.com/chenxin-dlut/SUTrack.",2024-12-26T09:41:36Z,http://arxiv.org/abs/2412.19138v1,"Xin Chen, Ben Kang, Wanting Geng, Jiawen Zhu, Yi Liu, Dong Wang, Huchuan Lu"
Semantic Residual for Multimodal Unified Discrete Representation,"Recent research in the domain of multimodal unified representations
predominantly employs codebook as representation forms, utilizing Vector
Quantization(VQ) for quantization, yet there has been insufficient exploration
of other quantization representation forms. Our work explores more precise
quantization methods and introduces a new framework, Semantic Residual
Cross-modal Information Disentanglement (SRCID), inspired by the numerical
residual concept inherent to Residual Vector Quantization (RVQ). SRCID employs
semantic residual-based information disentanglement for multimodal data to
better handle the inherent discrepancies between different modalities. Our
method enhances the capabilities of unified multimodal representations and
demonstrates exceptional performance in cross-modal generalization and
cross-modal zero-shot retrieval. Its average results significantly surpass
existing state-of-the-art models, as well as previous attempts with RVQ and
Finite Scalar Quantization (FSQ) based on these modals.",2024-12-26T09:08:52Z,http://arxiv.org/abs/2412.19128v1,"Hai Huang, Shulei Wang, Yan Xia"
"Advanced Knowledge Transfer: Refined Feature Distillation for Zero-Shot
  Quantization in Edge Computing","We introduce AKT (Advanced Knowledge Transfer), a novel method to enhance the
training ability of low-bit quantized (Q) models in the field of zero-shot
quantization (ZSQ). Existing research in ZSQ has focused on generating
high-quality data from full-precision (FP) models. However, these approaches
struggle with reduced learning ability in low-bit quantization due to its
limited information capacity. To overcome this limitation, we propose effective
training strategy compared to data generation. Particularly, we analyzed that
refining feature maps in the feature distillation process is an effective way
to transfer knowledge to the Q model. Based on this analysis, AKT efficiently
transfer core information from the FP model to the Q model. AKT is the first
approach to utilize both spatial and channel attention information in feature
distillation in ZSQ. Our method addresses the fundamental gradient exploding
problem in low-bit Q models. Experiments on CIFAR-10 and CIFAR-100 datasets
demonstrated the effectiveness of the AKT. Our method led to significant
performance enhancement in existing generative models. Notably, AKT achieved
significant accuracy improvements in low-bit Q models, achieving
state-of-the-art in the 3,5bit scenarios on CIFAR-10. The code is available at
https://github.com/Inpyo-Hong/AKT-Advanced-knowledge-Transfer.",2024-12-26T08:52:27Z,http://arxiv.org/abs/2412.19125v1,"Inpyo Hong, Youngwan Jo, Hyojeong Lee, Sunghyun Ahn, Sanghyun Park"
"Evaluating Self-Supervised Learning in Medical Imaging: A Benchmark for
  Robustness, Generalizability, and Multi-Domain Impact","Self-supervised learning (SSL) has emerged as a promising paradigm in medical
imaging, addressing the chronic challenge of limited labeled data in healthcare
settings. While SSL has shown impressive results, existing studies in the
medical domain are often limited in scope, focusing on specific datasets or
modalities, or evaluating only isolated aspects of model performance. This
fragmented evaluation approach poses a significant challenge, as models
deployed in critical medical settings must not only achieve high accuracy but
also demonstrate robust performance and generalizability across diverse
datasets and varying conditions. To address this gap, we present a
comprehensive evaluation of SSL methods within the medical domain, with a
particular focus on robustness and generalizability. Using the MedMNIST dataset
collection as a standardized benchmark, we evaluate 8 major SSL methods across
11 different medical datasets. Our study provides an in-depth analysis of model
performance in both in-domain scenarios and the detection of
out-of-distribution (OOD) samples, while exploring the effect of various
initialization strategies, model architectures, and multi-domain pre-training.
We further assess the generalizability of SSL methods through cross-dataset
evaluations and the in-domain performance with varying label proportions (1%,
10%, and 100%) to simulate real-world scenarios with limited supervision. We
hope this comprehensive benchmark helps practitioners and researchers make more
informed decisions when applying SSL methods to medical applications.",2024-12-26T08:51:56Z,http://arxiv.org/abs/2412.19124v1,"Valay Bundele, Oğuz Ata Çal, Bora Kargi, Karahan Sarıtaş, Kıvanç Tezören, Zohreh Ghaderi, Hendrik Lensch"
Discrete vs. Continuous Trade-offs for Generative Models,"This work explores the theoretical and practical foundations of denoising
diffusion probabilistic models (DDPMs) and score-based generative models, which
leverage stochastic processes and Brownian motion to model complex data
distributions. These models employ forward and reverse diffusion processes
defined through stochastic differential equations (SDEs) to iteratively add and
remove noise, enabling high-quality data generation. By analyzing the
performance bounds of these models, we demonstrate how score estimation errors
propagate through the reverse process and bound the total variation distance
using discrete Girsanov transformations, Pinsker's inequality, and the data
processing inequality (DPI) for an information theoretic lens.",2024-12-26T08:14:27Z,http://arxiv.org/abs/2412.19114v1,"Jathin Korrapati, Tanish Baranwal, Rahul Shah"
"SketchFill: Sketch-Guided Code Generation for Imputing Derived Missing
  Values","Missing value is a critical issue in data science, significantly impacting
the reliability of analyses and predictions. Missing value imputation (MVI) is
a longstanding problem because it highly relies on domain knowledge. Large
language models (LLMs) have emerged as a promising tool for data cleaning,
including MVI for tabular data, offering advanced capabilities for
understanding and generating content. However, despite their promise, existing
LLM techniques such as in-context learning and Chain-of-Thought (CoT) often
fall short in guiding LLMs to perform complex reasoning for MVI, particularly
when imputing derived missing values, which require mathematical formulas and
data relationships across rows and columns. This gap underscores the need for
further advancements in LLM methodologies to enhance their reasoning
capabilities for more reliable imputation outcomes. To fill this gap, we
propose SketchFill, a novel sketch-based method to guide LLMs in generating
accurate formulas to impute missing numerical values. Our experimental results
demonstrate that SketchFill significantly outperforms state-of-the-art methods,
achieving 56.2% higher accuracy than CoT-based methods and 78.8% higher
accuracy than MetaGPT. This sets a new standard for automated data cleaning and
advances the field of MVI for numerical values.",2024-12-26T08:13:34Z,http://arxiv.org/abs/2412.19113v1,"Yunfan Zhang, Changlun Li, Yuyu Luo, Nan Tang"
"Graph Mixture of Experts and Memory-augmented Routers for Multivariate
  Time Series Anomaly Detection","Multivariate time series (MTS) anomaly detection is a critical task that
involves identifying abnormal patterns or events in data that consist of
multiple interrelated time series. In order to better model the complex
interdependence between entities and the various inherent characteristics of
each entity, the GNN based methods are widely adopted by existing methods. In
each layer of GNN, node features aggregate information from their neighboring
nodes to update their information. In doing so, from shallow layer to deep
layer in GNN, original individual node features continue to be weakened and
more structural information,i.e., from short-distance neighborhood to
long-distance neighborhood, continues to be enhanced. However, research to date
has largely ignored the understanding of how hierarchical graph information is
represented and their characteristics that can benefit anomaly detection.
Existing methods simply leverage the output from the last layer of GNN for
anomaly estimation while neglecting the essential information contained in the
intermediate GNN layers. To address such limitations, in this paper, we propose
a Graph Mixture of Experts (Graph-MoE) network for multivariate time series
anomaly detection, which incorporates the mixture of experts (MoE) module to
adaptively represent and integrate hierarchical multi-layer graph information
into entity representations. It is worth noting that our Graph-MoE can be
integrated into any GNN-based MTS anomaly detection method in a plug-and-play
manner. In addition, the memory-augmented routers are proposed in this paper to
capture the correlation temporal information in terms of the global historical
features of MTS to adaptively weigh the obtained entity representations to
achieve successful anomaly estimation. Extensive experiments on five
challenging datasets prove the superiority of our approach and each proposed
module.",2024-12-26T07:49:51Z,http://arxiv.org/abs/2412.19108v1,"Xiaoyu Huang, Weidong Chen, Bo Hu, Zhendong Mao"
"The role of potential energy landscape research in the development of
  new electrolyte solutions","The development of new electrolyte solutions with improved characteristics is
a key challenge for creating high-performance batteries, fuel cells,
supercapacitors, and other electrochemical devices. The study of the potential
energy landscape (PEL) plays an important role in this process, providing
information about the interactions between solution components at the molecular
level. In this work, we review the practice of applying PEL research methods
based on classical and quantum-chemical algorithms to analyze the structure,
dynamics, and thermodynamic properties of electrolyte solutions. Intermolecular
and ion-molecular interactions at the microscopic level, which determine the
macroscopic properties of the electrolyte solution, are considered in detail.
The importance of identifying stable configurations of ions and their solvates
is emphasized. PEL analysis allows for the systematic determination of the most
probable structures and complexes formed in solution, which is important for
understanding ion transport mechanisms. The study of the PEL allows for the
determination of the energy barriers that must be overcome for ion migration,
which is related to the conductivity of the electrolyte. The application of PEL
research methods in combination with experimental data opens up new
possibilities for the rational design of electrolyte solutions with desired
physicochemical properties.",2024-12-26T07:45:29Z,http://arxiv.org/abs/2412.19103v1,Vitaly V. Chaban
"""I've Heard of You!"": Generate Spoken Named Entity Recognition Data for
  Unseen Entities","Spoken named entity recognition (NER) aims to identify named entities from
speech, playing an important role in speech processing. New named entities
appear every day, however, annotating their Spoken NER data is costly. In this
paper, we demonstrate that existing Spoken NER systems perform poorly when
dealing with previously unseen named entities. To tackle this challenge, we
propose a method for generating Spoken NER data based on a named entity
dictionary (NED) to reduce costs. Specifically, we first use a large language
model (LLM) to generate sentences from the sampled named entities and then use
a text-to-speech (TTS) system to generate the speech. Furthermore, we introduce
a noise metric to filter out noisy data. To evaluate our approach, we release a
novel Spoken NER benchmark along with a corresponding NED containing 8,853
entities. Experiment results show that our method achieves state-of-the-art
(SOTA) performance in the in-domain, zero-shot domain adaptation, and fully
zero-shot settings. Our data will be available at
https://github.com/DeepLearnXMU/HeardU.",2024-12-26T07:43:18Z,http://arxiv.org/abs/2412.19102v1,"Jiawei Yu, Xiang Geng, Yuang Li, Mengxin Ren, Wei Tang, Jiahuan Li, Zhibin Lan, Min Zhang, Hao Yang, Shujian Huang, Jinsong Su"
"Reconstruction Target Matters in Masked Image Modeling for Cross-Domain
  Few-Shot Learning","Cross-Domain Few-Shot Learning (CDFSL) requires the model to transfer
knowledge from the data-abundant source domain to data-scarce target domains
for fast adaptation, where the large domain gap makes CDFSL a challenging
problem. Masked Autoencoder (MAE) excels in effectively using unlabeled data
and learning image's global structures, enhancing model generalization and
robustness. However, in the CDFSL task with significant domain shifts, we find
MAE even shows lower performance than the baseline supervised models. In this
paper, we first delve into this phenomenon for an interpretation. We find that
MAE tends to focus on low-level domain information during reconstructing pixels
while changing the reconstruction target to token features could mitigate this
problem. However, not all features are beneficial, as we then find
reconstructing high-level features can hardly improve the model's
transferability, indicating a trade-off between filtering domain information
and preserving the image's global structure. In all, the reconstruction target
matters for the CDFSL task. Based on the above findings and interpretations, we
further propose Domain-Agnostic Masked Image Modeling (DAMIM) for the CDFSL
task. DAMIM includes an Aggregated Feature Reconstruction module to
automatically aggregate features for reconstruction, with balanced learning of
domain-agnostic information and images' global structure, and a Lightweight
Decoder module to further benefit the encoder's generalizability. Experiments
on four CDFSL datasets demonstrate that our method achieves state-of-the-art
performance.",2024-12-26T07:43:01Z,http://arxiv.org/abs/2412.19101v1,"Ran Ma, Yixiong Zou, Yuhua Li, Ruixuan Li"
"TrajGEOS: Trajectory Graph Enhanced Orientation-based Sequential Network
  for Mobility Prediction","Human mobility studies how people move to access their needed resources and
plays a significant role in urban planning and location-based services. As a
paramount task of human mobility modeling, next location prediction is
challenging because of the diversity of users' historical trajectories that
gives rise to complex mobility patterns and various contexts. Deep sequential
models have been widely used to predict the next location by leveraging the
inherent sequentiality of trajectory data. However, they do not fully leverage
the relationship between locations and fail to capture users' multi-level
preferences. This work constructs a trajectory graph from users' historical
traces and proposes a \textbf{Traj}ectory \textbf{G}raph \textbf{E}nhanced
\textbf{O}rientation-based \textbf{S}equential network (TrajGEOS) for
next-location prediction tasks. TrajGEOS introduces hierarchical graph
convolution to capture location and user embeddings. Such embeddings consider
not only the contextual feature of locations but also the relation between
them, and serve as additional features in downstream modules. In addition, we
design an orientation-based module to learn users' mid-term preferences from
sequential modeling modules and their recent trajectories. Extensive
experiments on three real-world LBSN datasets corroborate the value of graph
and orientation-based modules and demonstrate that TrajGEOS outperforms the
state-of-the-art methods on the next location prediction task.",2024-12-26T07:18:38Z,http://arxiv.org/abs/2412.19092v1,"Zhaoping Hu, Zongyuan Huang, Jinming Yang, Tao Yang, Yaohui Jin, Yanyan Xu"
From Coin to Data: The Impact of Object Detection on Digital Numismatics,"In this work we investigate the application of advanced object detection
techniques to digital numismatics, focussing on the analysis of historical
coins. Leveraging models such as Contrastive Language-Image Pre-training
(CLIP), we develop a flexible framework for identifying and classifying
specific coin features using both image and textual descriptions. By examining
two distinct datasets, modern Russian coins featuring intricate ""Saint George
and the Dragon"" designs and degraded 1st millennium AD Southeast Asian coins
bearing Hindu-Buddhist symbols, we evaluate the efficacy of different detection
algorithms in search and classification tasks. Our results demonstrate the
superior performance of larger CLIP models in detecting complex imagery, while
traditional methods excel in identifying simple geometric patterns.
Additionally, we propose a statistical calibration mechanism to enhance the
reliability of similarity scores in low-quality datasets. This work highlights
the transformative potential of integrating state-of-the-art object detection
into digital numismatics, enabling more scalable, precise, and efficient
analysis of historical artifacts. These advancements pave the way for new
methodologies in cultural heritage research, artefact provenance studies, and
the detection of forgeries.",2024-12-26T07:05:53Z,http://arxiv.org/abs/2412.19091v1,"Rafael Cabral, Maria De Iorio, Andrew Harris"
"Integrating Artificial Open Generative Artificial Intelligence into
  Software Supply Chain Security","While new technologies emerge, human errors always looming. Software supply
chain is increasingly complex and intertwined, the security of a service has
become paramount to ensuring the integrity of products, safeguarding data
privacy, and maintaining operational continuity. In this work, we conducted
experiments on the promising open Large Language Models (LLMs) into two main
software security challenges: source code language errors and deprecated code,
with a focus on their potential to replace conventional static and dynamic
security scanners that rely on predefined rules and patterns. Our findings
suggest that while LLMs present some unexpected results, they also encounter
significant limitations, particularly in memory complexity and the management
of new and unfamiliar data patterns. Despite these challenges, the proactive
application of LLMs, coupled with extensive security databases and continuous
updates, holds the potential to fortify Software Supply Chain (SSC) processes
against emerging threats.",2024-12-26T07:03:55Z,http://arxiv.org/abs/2412.19088v1,"Vasileios Alevizos, George A Papakostas, Akebu Simasiku, Dimitra Malliarou, Antonis Messinis, Sabrina Edralin, Clark Xu, Zongliang Yue"
MoPD: Mixture-of-Prompts Distillation for Vision-Language Models,"Soft prompt learning methods are effective for adapting vision-language
models (VLMs) to downstream tasks. Nevertheless, empirical evidence reveals a
tendency of existing methods that they overfit seen classes and exhibit
degraded performance on unseen classes. This limitation is due to the inherent
bias in the training data towards the seen classes. To address this issue, we
propose a novel soft prompt learning method, named Mixture-of-Prompts
Distillation (MoPD), which can effectively transfer useful knowledge from hard
prompts manually hand-crafted (a.k.a. teacher prompts) to the learnable soft
prompt (a.k.a. student prompt), thereby enhancing the generalization ability of
soft prompts on unseen classes. Moreover, the proposed MoPD method utilizes a
gating network that learns to select hard prompts used for prompt distillation.
Extensive experiments demonstrate that the proposed MoPD method outperforms
state-of-the-art baselines especially on on unseen classes.",2024-12-26T06:57:04Z,http://arxiv.org/abs/2412.19087v1,"Yang Chen, Shuai Fu, Yu Zhang"
Investigating the Temporal Dynamics of Cyber Threat Intelligence,"Indicators of Compromise (IoCs) play a crucial role in the rapid detection
and mitigation of cyber threats. However, the existing body of literature lacks
in-depth analytical studies on the temporal aspects of IoC publication,
especially when considering up-to-date datasets related to Common
Vulnerabilities and Exposures (CVEs). This paper addresses this gap by
conducting an analysis of the timeliness and comprehensiveness of Cyber Threat
Intelligence (CTI) pertaining to several recent CVEs. The insights derived from
this study aim to enhance cybersecurity defense strategies, particularly when
dealing with dynamic cyber threats that continually adapt their Tactics,
Techniques, and Procedures (TTPs). Utilizing IoCs sourced from multiple
providers, we scrutinize the IoC publication rate. Our analysis delves into how
various factors, including the inherent nature of a threat, its evolutionary
trajectory, and its observability over time, influence the publication rate of
IoCs. Our preliminary findings emphasize the critical need for cyber defenders
to maintain a constant state of vigilance in updating their IoCs for any given
vulnerability. This vigilance is warranted because the publication rate of IoCs
may exhibit fluctuations over time. We observe a recurring pattern akin to an
epidemic model, with an initial phase following the public disclosure of a
vulnerability characterized by sparse IoC publications, followed by a sudden
surge, and subsequently, a protracted period with a slower rate of IoC
publication.",2024-12-26T06:54:27Z,http://arxiv.org/abs/2412.19086v1,"Angel Kodituwakku, Clark Xu, Daniel Rogers, David K. Ahn, Errin W. Fulp"
"Assessing Pre-trained Models for Transfer Learning through Distribution
  of Spectral Components","Pre-trained model assessment for transfer learning aims to identify the
optimal candidate for the downstream tasks from a model hub, without the need
of time-consuming fine-tuning. Existing advanced works mainly focus on
analyzing the intrinsic characteristics of the entire features extracted by
each pre-trained model or how well such features fit the target labels. This
paper proposes a novel perspective for pre-trained model assessment through the
Distribution of Spectral Components (DISCO). Through singular value
decomposition of features extracted from pre-trained models, we investigate
different spectral components and observe that they possess distinct
transferability, contributing diversely to the fine-tuning performance.
Inspired by this, we propose an assessment method based on the distribution of
spectral components which measures the proportions of their corresponding
singular values. Pre-trained models with features concentrating on more
transferable components are regarded as better choices for transfer learning.
We further leverage the labels of downstream data to better estimate the
transferability of each spectral component and derive the final assessment
criterion. Our proposed method is flexible and can be applied to both
classification and regression tasks. We conducted comprehensive experiments
across three benchmarks and two tasks including image classification and object
detection, demonstrating that our method achieves state-of-the-art performance
in choosing proper pre-trained models from the model hub for transfer learning.",2024-12-26T06:54:22Z,http://arxiv.org/abs/2412.19085v1,"Tengxue Zhang, Yang Shu, Xinyang Chen, Yifei Long, Chenjuan Guo, Bin Yang"
A Microservice Graph Generator with Production Characteristics,"A production microservice application may provide multiple services, queries
of a service may have different call graphs, and a microservice may be shared
across call graphs. It is challenging to improve the resource efficiency of
such complex applications without proper benchmarks, while production traces
are too large to be used in experiments. To this end, we propose a Service
Dependency Graph Generator (DGG) that comprises a Data Handler and a Graph
Generator, for generating the service dependency graphs of benchmarks that
incorporate production-level characteristics from traces. The data handler
first constructs fine-grained call graphs with dynamic interface and repeated
calling features from the trace and merges them into dependency graphs, and
then clusters them into different categories based on the topological and
invocation types. Taking the organized data and the selected category, the
graph generator simulates the process of real microservices invoking downstream
microservices using a random graph model, generates multiple call graphs, and
merges the call graphs to form the small-scale service dependency graph with
production-level characteristics. Case studies show that DGG's generated graphs
are similar to real traces in terms of topologies. Moreover, the resource
scaling based on DGG's fine-grained call graph constructing increases the
resource efficiency by up to 44.8% while ensuring the required QoS.",2024-12-26T06:51:35Z,http://arxiv.org/abs/2412.19083v1,"Fanrong Du, Jiuchen Shi, Quan Chen, Li Li, Minyi Guo"
"Mask Factory: Towards High-quality Synthetic Data Generation for
  Dichotomous Image Segmentation","Dichotomous Image Segmentation (DIS) tasks require highly precise
annotations, and traditional dataset creation methods are labor intensive,
costly, and require extensive domain expertise. Although using synthetic data
for DIS is a promising solution to these challenges, current generative models
and techniques struggle with the issues of scene deviations, noise-induced
errors, and limited training sample variability. To address these issues, we
introduce a novel approach, \textbf{\ourmodel{}}, which provides a scalable
solution for generating diverse and precise datasets, markedly reducing
preparation time and costs. We first introduce a general mask editing method
that combines rigid and non-rigid editing techniques to generate high-quality
synthetic masks. Specially, rigid editing leverages geometric priors from
diffusion models to achieve precise viewpoint transformations under zero-shot
conditions, while non-rigid editing employs adversarial training and
self-attention mechanisms for complex, topologically consistent modifications.
Then, we generate pairs of high-resolution image and accurate segmentation mask
using a multi-conditional control generation method. Finally, our experiments
on the widely-used DIS5K dataset benchmark demonstrate superior performance in
quality and efficiency compared to existing methods. The code is available at
\url{https://qian-hao-tian.github.io/MaskFactory/}.",2024-12-26T06:37:25Z,http://arxiv.org/abs/2412.19080v1,"Haotian Qian, YD Chen, Shengtao Lou, Fahad Shahbaz Khan, Xiaogang Jin, Deng-Ping Fan"
"Graph-Enhanced Dual-Stream Feature Fusion with Pre-Trained Model for
  Acoustic Traffic Monitoring","Microphone array techniques are widely used in sound source localization and
smart city acoustic-based traffic monitoring, but these applications face
significant challenges due to the scarcity of labeled real-world traffic audio
data and the complexity and diversity of application scenarios. The DCASE
Challenge's Task 10 focuses on using multi-channel audio signals to count
vehicles (cars or commercial vehicles) and identify their directions
(left-to-right or vice versa). In this paper, we propose a graph-enhanced
dual-stream feature fusion network (GEDF-Net) for acoustic traffic monitoring,
which simultaneously considers vehicle type and direction to improve detection.
We propose a graph-enhanced dual-stream feature fusion strategy which consists
of a vehicle type feature extraction (VTFE) branch, a vehicle direction feature
extraction (VDFE) branch, and a frame-level feature fusion module to combine
the type and direction feature for enhanced performance. A pre-trained model
(PANNs) is used in the VTFE branch to mitigate data scarcity and enhance the
type features, followed by a graph attention mechanism to exploit temporal
relationships and highlight important audio events within these features. The
frame-level fusion of direction and type features enables fine-grained feature
representation, resulting in better detection performance. Experiments
demonstrate the effectiveness of our proposed method. GEDF-Net is our
submission that achieved 1st place in the DCASE 2024 Challenge Task 10.",2024-12-26T06:28:42Z,http://arxiv.org/abs/2412.19078v1,"Shitong Fan, Feiyang Xiao, Wenbo Wang, Shuhan Qi, Qiaoxi Zhu, Wenwu Wang, Jian Guan"
"Robust Speech and Natural Language Processing Models for Depression
  Screening","Depression is a global health concern with a critical need for increased
patient screening. Speech technology offers advantages for remote screening but
must perform robustly across patients. We have described two deep learning
models developed for this purpose. One model is based on acoustics; the other
is based on natural language processing. Both models employ transfer learning.
Data from a depression-labeled corpus in which 11,000 unique users interacted
with a human-machine application using conversational speech is used. Results
on binary depression classification have shown that both models perform at or
above AUC=0.80 on unseen data with no speaker overlap. Performance is further
analyzed as a function of test subset characteristics, finding that the models
are generally robust over speaker and session variables. We conclude that
models based on these approaches offer promise for generalized automated
depression screening.",2024-12-26T06:05:52Z,http://arxiv.org/abs/2412.19072v1,"Y. Lu, A. Harati, T. Rutowski, R. Oliveira, P. Chlebek, E. Shriberg"
Cross-Demographic Portability of Deep NLP-Based Depression Models,"Deep learning models are rapidly gaining interest for real-world applications
in behavioral health. An important gap in current literature is how well such
models generalize over different populations. We study Natural Language
Processing (NLP) based models to explore portability over two different corpora
highly mismatched in age. The first and larger corpus contains younger
speakers. It is used to train an NLP model to predict depression. When testing
on unseen speakers from the same age distribution, this model performs at
AUC=0.82. We then test this model on the second corpus, which comprises seniors
from a retirement community. Despite the large demographic differences in the
two corpora, we saw only modest degradation in performance for the
senior-corpus data, achieving AUC=0.76. Interestingly, in the senior
population, we find AUC=0.81 for the subset of patients whose health state is
consistent over time. Implications for demographic portability of speech-based
applications are discussed.",2024-12-26T05:54:24Z,http://arxiv.org/abs/2412.19070v1,"Tomek Rutowski, Elizabeth Shriberg, Amir Harati, Yang Lu, Ricardo Oliveira, Piotr Chlebek"
Effective and secure federated online learning to rank,"Online Learning to Rank (OLTR) optimises ranking models using implicit user
feedback, such as clicks. Unlike traditional Learning to Rank (LTR) methods
that rely on a static set of training data with relevance judgements to learn a
ranking model, OLTR methods update the model continually as new data arrives.
Thus, it addresses several drawbacks such as the high cost of human
annotations, potential misalignment between user preferences and human
judgments, and the rapid changes in user query intents. However, OLTR methods
typically require the collection of searchable data, user queries, and clicks,
which poses privacy concerns for users.
  Federated Online Learning to Rank (FOLTR) integrates OLTR within a Federated
Learning (FL) framework to enhance privacy by not sharing raw data. While
promising, FOLTR methods currently lag behind traditional centralised OLTR due
to challenges in ranking effectiveness, robustness with respect to data
distribution across clients, susceptibility to attacks, and the ability to
unlearn client interactions and data. This thesis presents a comprehensive
study on Federated Online Learning to Rank, addressing its effectiveness,
robustness, security, and unlearning capabilities, thereby expanding the
landscape of FOLTR.",2024-12-26T05:53:10Z,http://arxiv.org/abs/2412.19069v1,Shuyi Wang
"Attacking Voice Anonymization Systems with Augmented Feature and Speaker
  Identity Difference","This study focuses on the First VoicePrivacy Attacker Challenge within the
ICASSP 2025 Signal Processing Grand Challenge, which aims to develop speaker
verification systems capable of determining whether two anonymized speech
signals are from the same speaker. However, differences between feature
distributions of original and anonymized speech complicate this task. To
address this challenge, we propose an attacker system that combines Data
Augmentation enhanced feature representation and Speaker Identity Difference
enhanced classifier to improve verification performance, termed DA-SID.
Specifically, data augmentation strategies (i.e., data fusion and SpecAugment)
are utilized to mitigate feature distribution gaps, while probabilistic linear
discriminant analysis (PLDA) is employed to further enhance speaker identity
difference. Our system significantly outperforms the baseline, demonstrating
exceptional effectiveness and robustness against various voice anonymization
systems, ultimately securing a top-5 ranking in the challenge.",2024-12-26T05:52:44Z,http://arxiv.org/abs/2412.19068v1,"Yanzhe Zhang, Zhonghao Bi, Feiyang Xiao, Xuefeng Yang, Qiaoxi Zhu, Jian Guan"
Learning Monocular Depth from Events via Egomotion Compensation,"Event cameras are neuromorphically inspired sensors that sparsely and
asynchronously report brightness changes. Their unique characteristics of high
temporal resolution, high dynamic range, and low power consumption make them
well-suited for addressing challenges in monocular depth estimation (e.g.,
high-speed or low-lighting conditions). However, current existing methods
primarily treat event streams as black-box learning systems without
incorporating prior physical principles, thus becoming over-parameterized and
failing to fully exploit the rich temporal information inherent in event camera
data. To address this limitation, we incorporate physical motion principles to
propose an interpretable monocular depth estimation framework, where the
likelihood of various depth hypotheses is explicitly determined by the effect
of motion compensation. To achieve this, we propose a Focus Cost Discrimination
(FCD) module that measures the clarity of edges as an essential indicator of
focus level and integrates spatial surroundings to facilitate cost estimation.
Furthermore, we analyze the noise patterns within our framework and improve it
with the newly introduced Inter-Hypotheses Cost Aggregation (IHCA) module,
where the cost volume is refined through cost trend prediction and multi-scale
cost consistency constraints. Extensive experiments on real-world and synthetic
datasets demonstrate that our proposed framework outperforms cutting-edge
methods by up to 10\% in terms of the absolute relative error metric, revealing
superior performance in predicting accuracy.",2024-12-26T05:41:18Z,http://arxiv.org/abs/2412.19067v1,"Haitao Meng, Chonghao Zhong, Sheng Tang, Lian JunJia, Wenwei Lin, Zhenshan Bing, Yi Chang, Gang Chen, Alois Knoll"
Coarse-grained binning in Drell-Yan transverse momentum spectra,"We report a study of the determination of the intrinsic transverse momentum
of partons, the intrinsic $k_T$, from the dilepton transverse momentum $p_T$ in
Drell-Yan (DY) production at hadron colliders. The result shows that a good
sensitivity to the intrinsic $k_T$ distribution is achieved by measuring
relative ratios between the cross sections of suitably defined low-$p_T$ and
high-$p_T$ regions. The study is performed through both a pseudo-data test and
an extraction from measurements of the DY process by the CMS collaboration.
Since the methodology does not rely on any dedicated partition of bins, this
$p_T$-ratio observable requires less special treatment in very low $p_T$
regions, and propagates lower systematic uncertainties induced from unfolding
or momentum migration, in contrast with previous proposals of using a
fine-binning measurement of the differential cross section.",2024-12-26T05:13:39Z,http://arxiv.org/abs/2412.19060v1,"Wenxiao Zhan, Siqi Yang, Minghui Liu, Francesco Hautmann, Liang Han"
Faster Semi-streaming Matchings via Alternating Trees,"We design a deterministic algorithm for the $(1+\epsilon)$-approximate
maximum matching problem. Our primary result demonstrates that this problem can
be solved in $O(\epsilon^{-6})$ semi-streaming passes, improving upon the
$O(\epsilon^{-19})$ pass-complexity algorithm by [Fischer, Mitrovi\'c, and
Uitto, STOC'22]. This contributes substantially toward resolving Open
question~2 from [Assadi, SOSA'24]. Leveraging the framework introduced in
[FMU'22], our algorithm achieves an analogous round complexity speed-up for
computing a $(1+\epsilon)$-approximate maximum matching in both the Massively
Parallel Computation (MPC) and CONGEST models.
  The data structures maintained by our algorithm are formulated using blossom
notation and represented through alternating trees. This approach enables a
simplified correctness analysis by treating specific components as if operating
on bipartite graphs, effectively circumventing certain technical intricacies
present in prior work.",2024-12-26T04:59:27Z,http://arxiv.org/abs/2412.19057v1,"Slobodan Mitrović, Anish Mukherjee, Piotr Sankowski, Wen-Horng Sheu"
"Performance Characterization and Optimizations of Traditional ML
  Applications","Even in the era of Deep Learning based methods, traditional machine learning
methods with large data sets continue to attract significant attention.
However, we find an apparent lack of a detailed performance characterization of
these methods in the context of large training datasets. In this work, we study
the system's behavior of a number of traditional ML methods as implemented in
popular free software libraries/modules to identify critical performance
bottlenecks experienced by these applications. The performance characterization
study reveals several interesting insights on the performance of these
applications. Then we evaluate the performance benefits of applying some
well-known optimizations at the levels of caches and the main memory. More
specifically, we test the usefulness of optimizations such as (i) software
prefetching to improve cache performance and (ii) data layout and computation
reordering optimizations to improve locality in DRAM accesses. These
optimizations are implemented as modifications to the well-known scikit-learn
library, and hence can be easily leveraged by application programmers. We
evaluate the impact of the proposed optimizations using a combination of
simulation and execution on a real system. The software prefetching
optimization results in performance benefits varying from 5.2%-27.1% on
different ML applications while the data layout and computation reordering
approaches yield 6.16%-28.0% performance improvement.",2024-12-26T04:13:52Z,http://arxiv.org/abs/2412.19051v1,"Harsh Kumar, R. Govindarajan"
Jasper and Stella: distillation of SOTA embedding models,"A crucial component of many deep learning applications (such as FAQ and RAG)
is dense retrieval, in which embedding models are used to convert raw text to
numerical vectors and then get the most similar text by MIPS (Maximum Inner
Product Search). Some text embedding benchmarks (e.g. MTEB, BEIR, and
AIR-Bench) have been established to evaluate embedding models accurately.
Thanks to these benchmarks, we can use SOTA models; however, the deployment and
application of these models in industry were hampered by their large vector
dimensions and numerous parameters. To alleviate this problem, 1) we present a
distillation technique that can enable a smaller student model to achieve good
performance. 2) Inspired by MRL we present a training approach of reducing the
vector dimensions based on its own vectors or its teacher vectors. 3) We do
simple yet effective alignment training between images and text to make our
model a multimodal encoder. We trained Stella and Jasper models using the
technologies above and achieved high scores on the MTEB leaderboard. We release
the model and data at Hugging Face Hub
(https://huggingface.co/infgrad/jasper_en_vision_language_v1) and the training
logs are at https://api.wandb.ai/links/dunnzhang0/z8jqoqpb.",2024-12-26T04:05:28Z,http://arxiv.org/abs/2412.19048v1,"Dun Zhang, FulongWang"
Revealing the Self: Brainwave-Based Human Trait Identification,"People exhibit unique emotional responses. In the same scenario, the
emotional reactions of two individuals can be either similar or vastly
different. For instance, consider one person's reaction to an invitation to
smoke versus another person's response to a query about their sleep quality.
The identification of these individual traits through the observation of common
physical parameters opens the door to a wide range of applications, including
psychological analysis, criminology, disease prediction, addiction control, and
more. While there has been previous research in the fields of psychometrics,
inertial sensors, computer vision, and audio analysis, this paper introduces a
novel technique for identifying human traits in real time using brainwave data.
To achieve this, we begin with an extensive study of brainwave data collected
from 80 participants using a portable EEG headset. We also conduct a
statistical analysis of the collected data utilizing box plots. Our analysis
uncovers several new insights, leading us to a groundbreaking unified approach
for identifying diverse human traits by leveraging machine learning techniques
on EEG data. Our analysis demonstrates that this proposed solution achieves
high accuracy. Moreover, we explore two deep-learning models to compare the
performance of our solution. Consequently, we have developed an integrated,
real-time trait identification solution using EEG data, based on the insights
from our analysis. To validate our approach, we conducted a rigorous user
evaluation with an additional 20 participants. The outcomes of this evaluation
illustrate both high accuracy and favorable user ratings, emphasizing the
robust potential of our proposed method to serve as a versatile solution for
human trait identification.",2024-12-26T03:27:34Z,http://arxiv.org/abs/2412.19041v1,"Md Mirajul Islam, Md Nahiyan Uddin, Maoyejatun Hasana, Debojit Pandit, Nafis Mahmud Rahman, Sriram Chellappan, Sami Azam, A. B. M. Alim Al Islam"
"Unifying Tree-Reweighted Belief Propagation and Mean Field for Tracking
  Extended Targets","This paper proposes a unified tree-reweighted belief propagation (BP) and
mean field (MF) approach for scalable detection and tracking of extended
targets within the framework of factor graph. The factor graph is partitioned
into a BP region and an MF region so that the messages in each region are
updated according to the corresponding region rules. The BP region exploits the
tree-reweighted BP, which offers improved convergence than the standard BP for
graphs with massive cycles, to resolve data association. The MF region
approximates the posterior densities of the measurement rate, kinematic state
and extent. For linear Gaussian target models and gamma Gaussian inverse
Wishart distributed state density, the unified approach provides a closed-form
recursion for the state density. Hence, the proposed algorithm is more
efficient than particle-based BP algorithms for extended target tracking. This
method also avoids measurement clustering and gating since it solves the data
association problem in a probabilistic fashion. We compare the proposed
approach with algorithms such as the Poisson multi-Bernoulli mixture filter and
the BP-based Poisson multi-Bernoulli filter. Simulation results demonstrate
that the proposed algorithm achieves enhanced tracking performance.",2024-12-26T03:12:53Z,http://arxiv.org/abs/2412.19036v1,"Weizhen Ma, Zhongliang Jing, Peng Dong, Henry Leung"
"Reflection on Purpose Changes Students' Academic Interests: A Scalable
  Intervention in an Online Course Catalog","College students routinely use online course catalogs to explore a variety of
academic offerings. Course catalogs may therefore be an effective place to
encourage reflection on academic choices and interests. To test this, we
embedded a psychological intervention in an online course catalog to encourage
students to reflect on their purpose during course exploration. Results of a
randomized field experiment with over 4,000 students at a large U.S. university
show that a purpose intervention increased students' cognitive engagement in
describing their interests, but reduced search activities. Students became more
interested in courses related to creative arts and social change, but less in
computer and data science. The findings demonstrate the malleability of
students' interests during course exploration and suggest practical strategies
to support purpose reflection and guide students toward deliberate exploration
of their interests in higher education.",2024-12-26T03:12:50Z,http://arxiv.org/abs/2412.19035v1,"Youjie Chen, Pranathi Iyer, Rene F. Kizilcec"
Repository Structure-Aware Training Makes SLMs Better Issue Resolver,"Language models have been applied to various software development tasks, but
the performance varies according to the scale of the models. Large Language
Models (LLMs) outperform Small Language Models (SLMs) in complex tasks like
repository-level issue resolving, but raise concerns about privacy and cost. In
contrast, SLMs are more accessible but under-perform in complex tasks. In this
paper, we introduce ReSAT (Repository Structure-Aware Training), construct
training data based on a large number of issues and corresponding pull requests
from open-source communities to enhance the model's understanding of repository
structure and issue resolving ability. We construct two types of training data:
(1) localization training data, a multi-level progressive localization data to
improve code understanding and localization capability; (2) code edit training
data, which improves context-based code editing capability. The evaluation
results on SWE-Bench-verified and RepoQA demonstrate that ReSAT effectively
enhances SLMs' issue-resolving and repository-level long-context understanding
capabilities.",2024-12-26T03:01:32Z,http://arxiv.org/abs/2412.19031v1,"Zexiong Ma, Shengnan An, Zeqi Lin, Yanzhen Zou, Bing Xie"
"Modality-Projection Universal Model for Comprehensive Full-Body Medical
  Imaging Segmentation","The integration of deep learning in medical imaging has shown great promise
for enhancing diagnostic, therapeutic, and research outcomes. However, applying
universal models across multiple modalities remains challenging due to the
inherent variability in data characteristics. This study aims to introduce and
evaluate a Modality Projection Universal Model (MPUM). MPUM employs a novel
modality-projection strategy, which allows the model to dynamically adjust its
parameters to optimize performance across different imaging modalities. The
MPUM demonstrated superior accuracy in identifying anatomical structures,
enabling precise quantification for improved clinical decision-making. It also
identifies metabolic associations within the brain-body axis, advancing
research on brain-body physiological correlations. Furthermore, MPUM's unique
controller-based convolution layer enables visualization of saliency maps
across all network layers, significantly enhancing the model's
interpretability.",2024-12-26T02:23:27Z,http://arxiv.org/abs/2412.19026v1,"Yixin Chen, Lin Gao, Yajuan Gao, Rui Wang, Jingge Lian, Xiangxi Meng, Yanhua Duan, Leiying Chai, Hongbin Han, Zhaoping Cheng, Zhaoheng Xie"
"Nonparametric Estimation of Matching Efficiency and Elasticity in a Spot
  Gig Work Platform: 2019-2023","This paper provides new evidence on spot gig work platforms for unemployed
workers searching for occupations with minimal educational or experience
requirements in Japan. Using proprietary data from a private online spot work
matching platform, Timee, it examines trends in key variables such as the
numbers of unemployed users, vacancies, hires, and labor market tightness. The
study compares these trends with part-time worker data from the public
employment platform, Hello Work. The private platform shows a significant
market expansion from December 2019 to December 2023. Applying a novel
nonparametric approach, the paper finds greater variability in efficiency and
higher elasticity, with elasticity with respect to the number of users
fluctuating from below 0.7 to above 1.5, and elasticity with respect to the
number of vacancies often exceeding 1.0, which is higher than Hello Work.
Lastly, the study highlights that Tokyo's labor market exhibits higher
efficiency compared to Osaka and Aichi, while elasticities are similar,
indicating less geographical heterogeneity of the spot work compared to Hello
Work.",2024-12-26T02:16:58Z,http://arxiv.org/abs/2412.19024v1,"Hayato Kanayama, Suguru Otani"
"Nuclear matter properties from chiral-scale effective theory including a
  dilatonic scalar meson","Chiral effective theory has become a powerful tool for studying the
low-energy properties of QCD. In this work, we apply an extended chiral
effective theory -- chiral-scale effective theory -- including a dilatonic
scalar meson to study nuclear matter and find that the properties around
saturation density can be well reproduced. Compared to the traditionally used
Walecka-type models in nuclear matter studies, our approach improves the
behavior of symmetry energy and the incompressibility coefficient in describing
empirical data without introducing additional freedoms. Moreover, the predicted
neutron star structures fall within the constraints of GW170817, PSR
J0740+6620, and PSR J0030+0451, while the maximum neutron star mass can reach
about $~3M_{\odot}$ with a pure hadronic phase. Additionally, we find that
symmetry patterns of the effective theory significantly impact neutron star
structures. %In chiral-scale effective theory, effective operators are well
organized by chiral-scale orders and freedoms induced by QCD symmetry patterns.
We believe that introducing this type of theory into nuclear matter studies can
lead to a deeper understanding of QCD, nuclear matter, and compact
astrophysical objects.",2024-12-26T02:13:05Z,http://arxiv.org/abs/2412.19023v1,"Lu-Qi Zhang, Yao Ma, Yong-Liang Ma"
Adaptivity can help exponentially for shadow tomography,"In recent years there has been significant interest in understanding the
statistical complexity of learning from quantum data under the constraint that
one can only make unentangled measurements. While a key challenge in
establishing tight lower bounds in this setting is to deal with the fact that
the measurements can be chosen in an adaptive fashion, a recurring theme has
been that adaptivity offers little advantage over more straightforward,
nonadaptive protocols.
  In this note, we offer a counterpoint to this. We show that for the basic
task of shadow tomography, protocols that use adaptively chosen two-copy
measurements can be exponentially more sample-efficient than any protocol that
uses nonadaptive two-copy measurements.",2024-12-26T02:13:04Z,http://arxiv.org/abs/2412.19022v1,"Sitan Chen, Weiyuan Gong, Zhihan Zhang"
"Brain Ageing Prediction using Isolation Forest Technique and Residual
  Neural Network (ResNet)","Brain aging is a complex and dynamic process, leading to functional and
structural changes in the brain. These changes could lead to the increased risk
of neurodegenerative diseases and cognitive decline. Accurate brain-age
estimation utilizing neuroimaging data has become necessary for detecting
initial signs of neurodegeneration. Here, we propose a novel deep learning
approach using the Residual Neural Network 101 Version 2 (ResNet101V2) model to
predict brain age from MRI scans. To train, validate and test our proposed
model, we used a large dataset of 2102 images which were selected randomly from
the International Consortium for Brain Mapping (ICBM). Next, we applied data
preprocessing techniques, including normalizing the images and using outlier
detection via Isolation Forest method. Then, we evaluated various pre-trained
approaches (namely: MobileNetV2, ResNet50V2, ResNet101V2, Xception). The
results demonstrated that the ResNet101V2 model has higher performance compared
with the other models, attaining MAEs of 0.9136 and 0.8242 years for before and
after using Isolation Forest process. Our method achieved a high accuracy in
brain age estimation in ICBM dataset and it provides a reliable brain age
prediction.",2024-12-26T01:49:21Z,http://arxiv.org/abs/2412.19017v1,"Saadat Behzadi, Danial Sharifrazi, Roohallah Alizadehsani, Mojtaba Lotfaliany, Mohammadreza Mohebbi"
LASER: A new method for locally adaptive nonparametric regression,"In this article, we introduce \textsf{LASER} (Locally Adaptive Smoothing
Estimator for Regression), a computationally efficient locally adaptive
nonparametric regression method that performs variable bandwidth local
polynomial regression. We prove that it adapts (near-)optimally to the local
H\""{o}lder exponent of the underlying regression function
\texttt{simultaneously} at all points in its domain. Furthermore, we show that
there is a single ideal choice of a global tuning parameter under which the
above mentioned local adaptivity holds. Despite the vast literature on
nonparametric regression, instances of practicable methods with provable
guarantees of such a strong notion of local adaptivity are rare. The proposed
method achieves excellent performance across a broad range of numerical
experiments in comparison to popular alternative locally adaptive methods.",2024-12-27T18:59:03Z,http://arxiv.org/abs/2412.19802v1,"Sabyasachi Chatterjee, Subhajit Goswami, Soumendu Sundar Mukherjee"
Concentration of ergotropy in many-body systems,"Ergotropy -- the maximal amount of unitarily extractable work -- measures the
``charge level'' of quantum batteries. We prove that in large many-body
batteries ergotropy exhibits a concentration of measure phenomenon. Namely, the
ergotropy of such systems is almost constant for almost all states sampled from
the Hilbert--Schmidt measure. We establish this by first proving that
ergotropy, as a function of the state, is Lipschitz-continuous with respect to
the Bures distance, and then applying Levy's measure concentration lemma. In
parallel, we showcase the analogous properties of von Neumann entropy,
compiling and adapting known results about its continuity and concentration
properties. Furthermore, we consider the situation with the least amount of
prior information about the state. This corresponds to the quantum version of
the Jeffreys prior distribution -- the Bures measure. In this case, there exist
no analytical bounds guaranteeing exponential concentration of measure.
Nonetheless, we provide numerical evidence that ergotropy, as well as von
Neumann entropy, concentrate also in this case.",2024-12-27T18:58:43Z,http://arxiv.org/abs/2412.19801v1,"Karen V. Hovhannisyan, Rick P. A. Simon, Janet Anders"
"Generalized Grade-of-Membership Estimation for High-dimensional Locally
  Dependent Data","This work focuses on the mixed membership models for multivariate categorical
data widely used for analyzing survey responses and population genetics data.
These grade of membership (GoM) models offer rich modeling power but present
significant estimation challenges for high-dimensional polytomous data. Popular
existing approaches, such as Bayesian MCMC inference, are not scalable and lack
theoretical guarantees in high-dimensional settings. To address this, we first
observe that data from this model can be reformulated as a three-way
(quasi-)tensor, with many subjects responding to many items with varying
numbers of categories. We introduce a novel and simple approach that flattens
the three-way quasi-tensor into a ""fat"" matrix, and then perform a singular
value decomposition of it to estimate parameters by exploiting the singular
subspace geometry. Our fast spectral method can accommodate a broad range of
data distributions with arbitrarily locally dependent noise, which we formalize
as the generalized-GoM models. We establish finite-sample entrywise error
bounds for the generalized-GoM model parameters. This is supported by a new
sharp two-to-infinity singular subspace perturbation theory for locally
dependent and flexibly distributed noise, a contribution of independent
interest. Simulations and applications to data in political surveys, population
genetics, and single-cell sequencing demonstrate our method's superior
performance.",2024-12-27T18:51:15Z,http://arxiv.org/abs/2412.19796v1,"Ling Chen, Chengzhu Huang, Yuqi Gu"
"g-factor theory of Si/SiGe quantum dots: spin-valley and giant
  renormalization effects","Understanding the $g$-factor physics of Si/SiGe quantum dots is crucial for
realizing high-quality spin qubits. While previous work has explained some
aspects of $g$-factor physics in idealized geometries, the results do not
extend to general cases and they miss several important features. Here, we
construct a theory that gives $g$ in terms of readily computable matrix
elements, and can be applied to all Si/SiGe heterostructures of current
interest. As a concrete example, which currently has no $g$-factor
understanding, we study the so-called Wiggle Well structure, containing Ge
concentration oscillations inside the quantum well. Here we find a significant
renormalization of the $g$-factor compared to conventional Si/SiGe quantum
wells. We also uncover a giant $g$-factor suppression of order
$\mathcal{O}(1)$, which arises due to spin-valley coupling, and occurs at
locations of low valley splitting. Our work therefore opens up new avenues for
$g$-factor engineering in Si/SiGe quantum dots.",2024-12-27T18:50:38Z,http://arxiv.org/abs/2412.19795v1,"Benjamin D. Woods, Merritt P. Losert, Robert Joynt, Mark Friesen"
InfAlign: Inference-aware language model alignment,"Language model alignment has become a critical step in training modern
generative language models. The goal of alignment is to finetune a reference
model such that the win rate of a sample from the aligned model over a sample
from the reference model is high, subject to a KL divergence constraint. Today,
we are increasingly using inference-time algorithms (e.g., Best-of-N,
controlled decoding, tree search) to decode from language models rather than
standard sampling. However, the alignment objective does not capture such
inference-time decoding procedures. We show that the existing alignment
framework is sub-optimal in view of such inference-time methods. We then modify
the alignment objective and propose a framework for inference-aware alignment
(IAPO). We prove that for any inference-time decoding algorithm, the optimal
solution that optimizes the inference-time win rate of the aligned policy
against the reference policy is the solution to the typical RLHF problem with a
transformation of the reward. This motivates us to provide the KL-regularized
calibrate-and-transform RL (CTRL) algorithm to solve this problem, which
involves a reward calibration step and a KL-regularized reward maximization
step with a transformation of the calibrated reward. We particularize our study
to two important inference-time strategies: best-of-N sampling and best-of-N
jailbreaking, where N responses are sampled from the model and the one with the
highest or lowest reward is selected. We propose specific transformations for
these strategies and demonstrate that our framework offers significant
improvements over existing state-of-the-art methods for language model
alignment. Empirically, we outperform baselines that are designed without
taking inference-time decoding into consideration by 8-12% and 4-9% on
inference-time win rates over the Anthropic helpfulness and harmlessness dialog
benchmark datasets.",2024-12-27T18:45:36Z,http://arxiv.org/abs/2412.19792v1,"Ananth Balashankar, Ziteng Sun, Jonathan Berant, Jacob Eisenstein, Michael Collins, Adrian Hutter, Jong Lee, Chirag Nagpal, Flavien Prost, Aradhana Sinha, and Ananda Theertha Suresh, Ahmad Beirami"
"Machine Learning for Sentiment Analysis of Imported Food in Trinidad and
  Tobago","This research investigates the performance of various machine learning
algorithms (CNN, LSTM, VADER, and RoBERTa) for sentiment analysis of Twitter
data related to imported food items in Trinidad and Tobago. The study addresses
three primary research questions: the comparative accuracy and efficiency of
the algorithms, the optimal configurations for each model, and the potential
applications of the optimized models in a live system for monitoring public
sentiment and its impact on the import bill. The dataset comprises tweets from
2018 to 2024, divided into imbalanced, balanced, and temporal subsets to assess
the impact of data balancing and the COVID-19 pandemic on sentiment trends. Ten
experiments were conducted to evaluate the models under various configurations.
Results indicated that VADER outperformed the other models in both multi-class
and binary sentiment classifications. The study highlights significant changes
in sentiment trends pre- and post-COVID-19, with implications for import
policies.",2024-12-27T18:25:08Z,http://arxiv.org/abs/2412.19781v1,"Cassandra Daniels, Koffka Khan"
Tensor Network Estimation of Distribution Algorithms,"Tensor networks are a tool first employed in the context of many-body quantum
physics that now have a wide range of uses across the computational sciences,
from numerical methods to machine learning. Methods integrating tensor networks
into evolutionary optimization algorithms have appeared in the recent
literature. In essence, these methods can be understood as replacing the
traditional crossover operation of a genetic algorithm with a tensor
network-based generative model. We investigate these methods from the point of
view that they are Estimation of Distribution Algorithms (EDAs). We find that
optimization performance of these methods is not related to the power of the
generative model in a straightforward way. Generative models that are better
(in the sense that they better model the distribution from which their training
data is drawn) do not necessarily result in better performance of the
optimization algorithm they form a part of. This raises the question of how
best to incorporate powerful generative models into optimization routines. In
light of this we find that adding an explicit mutation operator to the output
of the generative model often improves optimization performance.",2024-12-27T18:22:47Z,http://arxiv.org/abs/2412.19780v1,"John Gardiner, Javier Lopez-Piqueres"
"Symbolic Approximations to Ricci-flat Metrics Via Extrinsic Symmetries
  of Calabi-Yau Hypersurfaces","Ever since Yau's non-constructive existence proof of Ricci-flat metrics on
Calabi-Yau manifolds, finding their explicit construction remains a major
obstacle to development of both string theory and algebraic geometry. Recent
computational approaches employ machine learning to create novel neural
representations for approximating these metrics, offering high accuracy but
limited interpretability. In this paper, we analyse machine learning
approximations to flat metrics of Fermat Calabi-Yau n-folds and some of their
one-parameter deformations in three dimensions in order to discover their new
properties. We formalise cases in which the flat metric has more symmetries
than the underlying manifold, and prove that these symmetries imply that the
flat metric admits a surprisingly compact representation for certain choices of
complex structure moduli. We show that such symmetries uniquely determine the
flat metric on certain loci, for which we present an analytic form. We also
incorporate our theoretical results into neural networks to achieve
state-of-the-art reductions in Ricci curvature for multiple Calabi-Yau
manifolds. We conclude by distilling the ML models to obtain for the first time
closed form expressions for Kahler metrics with near-zero scalar curvature.",2024-12-27T18:19:26Z,http://arxiv.org/abs/2412.19778v1,"Viktor Mirjanić, Challenger Mishra"
"Application of normalizing flows to nuclear many-body perturbation
  theory","Many-body perturbation theory provides a powerful framework to study the
ground state and thermodynamic properties of nuclear matter as well as
associated single-particle potentials and response functions within a
systematic order-by-order expansion. However, computational challenges can
emerge beyond the lowest orders of perturbation theory, especially when
computing both single-particle potentials and response functions, which in
general are complex-valued and require Cauchy principal value calculations of
high-dimensional integrals. We demonstrate that normalizing flows are suitable
for Monte Carlo importance sampling of both regular and irregular functions
appearing in nuclear many-body calculations. Normalizing flows are a class of
machine learning models that can be used to build and sample from complicated
distributions through a bijective mapping from a simple base distribution.
Furthermore, a well-trained model for a certain target integrand can be
efficiently transferred to calculate related integrals with varying physical
conditions. These features can enable more efficient tabulations of nuclear
physics inputs to numerical simulations of supernovae and neutron star mergers
across varying physical conditions and nuclear force models.",2024-12-27T18:18:20Z,http://arxiv.org/abs/2412.19777v1,"Pengsheng Wen, Jeremy W. Holt, Albany Blackburn"
"Analysis of Premature Death Rates in Texas Counties: The Impact of Air
  Quality, Socioeconomic Factors, and COPD Prevalence","Understanding factors contributing to premature mortality is critical for
public health planning. This study examines the relationships between premature
death rates and multiple risk factors across several Texas counties, utilizing
EPA air quality data, Census information, and county health records from recent
years. We analyze the impact of air quality (PM2.5 levels), socioeconomic
factors (median household income), and health conditions (COPD prevalence)
through statistical analysis and modeling techniques. Results reveal COPD
prevalence as a strong predictor of premature death rates, with higher
prevalence associated with a substantial increase in years of potential life
lost. While socioeconomic factors show a significant negative correlation, air
quality demonstrates more complex indirect relationships. These findings
emphasize the need for integrated public health interventions that prioritize
key health conditions while addressing underlying socioeconomic disparities.",2024-12-27T18:12:04Z,http://arxiv.org/abs/2412.19774v1,"Richard Rich, Ernesto Diaz"
"On the uplift of 4D wormholes in Braneworld models and their 5D
  structure","Recent developments in the consistent embedding of general 4D static and
spherically-symmetric spacetimes in arbitrary single-brane braneworld models
[Phys.Rev.D 109 (2024) 4, L041501] initiated the program of studying the bulk
structure of braneworld wormholes. In this article, adopting a completely
generic approach, we derive the general conditions that the metric functions of
any braneworld spacetime must satisfy to describe a wormhole structure in the
bulk. Particular emphasis is placed on clarifying the proper uplift of 4D
wormholes, expressed in terms of various radial coordinates on the brane, and
we demonstrate the important role of the circumferential radius metric function
for the embedding. Additionally, the flare-out conditions for braneworld
wormholes are presented for the first time and are found to differ from the
case of flat extra dimensions. To illustrate the method, we first perform the
uplift into the Randall-Sundrum II braneworld model for three well-known 4D
wormhole spacetimes; the effective braneworld wormhole solutions of
Casadio-Fabbri-Mazzacurati and Bronnikov-Kim, and the Simpson-Visser spacetime.
Subsequently, we study their bulk features by means of curvature invariants,
flare-out conditions, energy conditions and embedding diagrams. Our analysis
reveals that the assumption of a warped extra dimension has non-trivial
implications for the structure of 5D wormholes.",2024-12-27T18:12:03Z,http://arxiv.org/abs/2412.19773v1,"Thomas Pappas, Theodoros Nakas"
"Fortran2CPP: Automating Fortran-to-C++ Migration using LLMs via
  Multi-Turn Dialogue and Dual-Agent Integration","Migrating Fortran code to C++ is a common task for many scientific computing
teams, driven by the need to leverage modern programming paradigms, enhance
cross-platform compatibility, and improve maintainability. Automating this
translation process using large language models (LLMs) has shown promise, but
the lack of high-quality, specialized datasets has hindered their
effectiveness. In this paper, we address this challenge by introducing a novel
multi-turn dialogue dataset, Fortran2CPP, specifically designed for
Fortran-to-C++ code migration. Our dataset, significantly larger than existing
alternatives, is generated using a unique LLM-driven, dual-agent pipeline
incorporating iterative compilation, execution, and code repair to ensure high
quality and functional correctness. To demonstrate the effectiveness of our
dataset, we fine-tuned several open-weight LLMs on Fortran2CPP and evaluated
their performance on two independent benchmarks. Fine-tuning on our dataset led
to remarkable gains, with models achieving up to a 3.31x increase in CodeBLEU
score and a 92\% improvement in compilation success rate. This highlights the
dataset's ability to enhance both the syntactic accuracy and compilability of
the translated C++ code. Our dataset and model have been open-sourced and are
available on our public GitHub
repository\footnote{\url{https://github.com/HPC-Fortran2CPP/Fortran2Cpp}}.",2024-12-27T18:06:25Z,http://arxiv.org/abs/2412.19770v1,"Le Chen, Bin Lei, Dunzhi Zhou, Pei-Hung Lin, Chunhua Liao, Caiwen Ding, Ali Jannesari"
"Teaching materials aligned or unaligned with the principles of the
  Cognitive Theory of Multimedia Learning: the choices made by Physics teachers
  and students","In a recent study [Rev. Bras. Ens. F\'is. vol. 45, 2023], the absence of the
Cognitive Theory of Multimedia Learning (CTML) in the curricula of Physics
teacher education programs at Brazilian public universities was highlighted.
Considering this gap, the present study investigates whether, even without any
formal prior knowledge of CTML principles (Coherence, Signaling, Spatial
Contiguity, Segmentation, Multimedia, and Personalization), Physics teacher
trainees and educators tend to choose, among two formats of multimedia
materials - one aligned with a given CTML principle and the other not - the
materials aligned with these principles. The findings of this case study
revealed that, although most participants generally selected materials aligned
with the mentioned principles, a significant portion did not. These results
underscore the importance of Brazilian universities considering the inclusion
of CTML in Physics teacher education curricula.",2024-12-27T18:02:16Z,http://arxiv.org/abs/2412.19768v1,"Aline N. Braga, Antonio A. M. Neto, Alessandra N. Braga, Silvio C. F. Pereira Filho, Nelson P. C. de Souza, Danilo T. Alves"
"From Ceilings to Walls: Universal Dynamic Perching of Small Aerial
  Robots on Surfaces with Variable Orientations","This work demonstrates universal dynamic perching capabilities for quadrotors
of various sizes and on surfaces with different orientations. By employing a
non-dimensionalization framework and deep reinforcement learning, we
systematically assessed how robot size and surface orientation affect landing
capabilities. We hypothesized that maintaining geometric proportions across
different robot scales ensures consistent perching behavior, which was
validated in both simulation and experimental tests. Additionally, we
investigated the effects of joint stiffness and damping in the landing gear on
perching behaviors and performance. While joint stiffness had minimal impact,
joint damping ratios influenced landing success under vertical approaching
conditions. The study also identified a critical velocity threshold necessary
for successful perching, determined by the robot's maneuverability and leg
geometry. Overall, this research advances robotic perching capabilities,
offering insights into the role of mechanical design and scaling effects, and
lays the groundwork for future drone autonomy and operational efficiency in
unstructured environments.",2024-12-27T17:53:01Z,http://arxiv.org/abs/2412.19765v1,"Bryan Habas, Aaron Brown, Donghyeon Lee, Mitchell Goldman, Bo Cheng"
"Multi-population Differential Evolution for RSS based Cooperative
  Localization in Wireless Sensor Networks with Limited Communication Range","This paper presents a novel approach to deal with the cooperative
localization problem in wireless sensor networks based on received signal
strength measurements. In cooperative scenarios, the cost function of the
localization problem becomes increasingly nonlinear and nonconvex due to the
heightened interaction between sensor nodes, making the estimation of the
positions of the target nodes more challenging. Although most of existing
cooperative localization algorithms assure acceptable localization accuracy,
their computational complexity increases dramatically, which may restrict their
applicability. To reduce the computational complexity and provide competitive
localization accuracy at the same time, we propose a localization algorithm
based on the differential evolution with multiple populations, opposite-based
learning, redirection, and anchoring. In this work, the cooperative
localization cost function is split into several simpler cost functions, each
of which accounts only for one individual target node. Then, each cost function
is solved by a dedicated population of the proposed algorithm. In addition, an
enhanced version of the proposed algorithm which incorporates the population
midpoint scheme for further improvement in the localization accuracy is
devised. Simulation results demonstrate that the proposed algorithms provide
comparative localization accuracy with much lower computational complexity
compared with the state-of-the-art algorithms.",2024-12-27T17:45:10Z,http://arxiv.org/abs/2412.19763v1,"Lismer Andres Caceres Najarro, Iickho Song, Muhammad Salman, Kiseon Kim"
"Enhancing Cognitive Diagnosis by Modeling Learner Cognitive Structure
  State","Cognitive diagnosis represents a fundamental research area within intelligent
education, with the objective of measuring the cognitive status of individuals.
Theoretically, an individual's cognitive state is essentially equivalent to
their cognitive structure state. Cognitive structure state comprises two key
components: knowledge state (KS) and knowledge structure state (KUS). The
knowledge state reflects the learner's mastery of individual concepts, a widely
studied focus within cognitive diagnosis. In contrast, the knowledge structure
state-representing the learner's understanding of the relationships between
concepts-remains inadequately modeled. A learner's cognitive structure is
essential for promoting meaningful learning and shaping academic performance.
Although various methods have been proposed, most focus on assessing KS and
fail to assess KUS. To bridge this gap, we propose an innovative and effective
framework-CSCD (Cognitive Structure State-based Cognitive Diagnosis)-which
introduces a novel framework to modeling learners' cognitive structures in
diagnostic assessments, thereby offering new insights into cognitive structure
modeling. Specifically, we employ an edge-feature-based graph attention network
to represent the learner's cognitive structure state, effectively integrating
KS and KUS. Extensive experiments conducted on real datasets demonstrate the
superior performance of this framework in terms of diagnostic accuracy and
interpretability.",2024-12-27T17:41:39Z,http://arxiv.org/abs/2412.19759v1,"Zhifu Chen, Hengnian Gu, Jin Peng Zhou, Dongdai Zhou"
"""Did my figure do justice to the answer?"" : Towards Multimodal Short
  Answer Grading with Feedback (MMSAF)","Personalized feedback plays a vital role in a student's learning process.
While existing systems are adept at providing feedback over MCQ-based
evaluation, this work focuses more on subjective and open-ended questions,
which is similar to the problem of Automatic Short Answer Grading (ASAG) with
feedback. Additionally, we introduce the Multimodal Short Answer grading with
Feedback (MMSAF) problem over the traditional ASAG feedback problem to address
the scenario where the student answer and reference answer might contain
images. Moreover, we introduce the MMSAF dataset with 2197 data points along
with an automated framework for generating such data sets. Our evaluations on
existing LLMs over this dataset achieved an overall accuracy of 55\% on Level
of Correctness labels, 75\% on Image Relevance labels and a score of 4.27 out
of 5 in correctness level of LLM generated feedback as rated by experts. As per
experts, Pixtral achieved a rating of above 4 out of all metrics, indicating
that it is more aligned to human judgement, and that it is the best solution
for assisting students.",2024-12-27T17:33:39Z,http://arxiv.org/abs/2412.19755v1,"Pritam Sil, Bhaskaran Raman, Pushpak Bhattacharyya"
"IMAGINE: An 8-to-1b 22nm FD-SOI Compute-In-Memory CNN Accelerator With
  an End-to-End Analog Charge-Based 0.15-8POPS/W Macro Featuring
  Distribution-Aware Data Reshaping","Charge-domain compute-in-memory (CIM) SRAMs have recently become an enticing
compromise between computing efficiency and accuracy to process sub-8b
convolutional neural networks (CNNs) at the edge. Yet, they commonly make use
of a fixed dot-product (DP) voltage swing, which leads to a loss in effective
ADC bits due to data-dependent clipping or truncation effects that waste
precious conversion energy and computing accuracy. To overcome this, we present
IMAGINE, a workload-adaptive 1-to-8b CIM-CNN accelerator in 22nm FD-SOI. It
introduces a 1152x256 end-to-end charge-based macro with a multi-bit DP based
on an input-serial, weight-parallel accumulation that avoids power-hungry DACs.
An adaptive swing is achieved by combining a channel-wise DP array split with a
linear in-ADC implementation of analog batch-normalization (ABN), obtaining a
distribution-aware data reshaping. Critical design constraints are relaxed by
including the post-silicon equivalent noise within a CIM-aware CNN training
framework. Measurement results showcase an 8b system-level energy efficiency of
40TOPS/W at 0.3/0.6V, with competitive accuracies on MNIST and CIFAR-10.
Moreover, the peak energy and area efficiencies of the 187kB/mm2 macro
respectively reach up to 0.15-8POPS/W and 2.6-154TOPS/mm2, scaling with the
8-to-1b computing precision. These results exceed previous charge-based designs
by 3-to-5x while being the first work to provide linear in-memory rescaling.",2024-12-27T17:18:15Z,http://arxiv.org/abs/2412.19750v1,"Adrian Kneip, Martin Lefebvre, Pol Maistriaux, David Bol"
"Enhancing Adversarial Robustness of Deep Neural Networks Through
  Supervised Contrastive Learning","Adversarial attacks exploit the vulnerabilities of convolutional neural
networks by introducing imperceptible perturbations that lead to
misclassifications, exposing weaknesses in feature representations and decision
boundaries. This paper presents a novel framework combining supervised
contrastive learning and margin-based contrastive loss to enhance adversarial
robustness. Supervised contrastive learning improves the structure of the
feature space by clustering embeddings of samples within the same class and
separating those from different classes. Margin-based contrastive loss,
inspired by support vector machines, enforces explicit constraints to create
robust decision boundaries with well-defined margins. Experiments on the
CIFAR-100 dataset with a ResNet-18 backbone demonstrate robustness performance
improvements in adversarial accuracy under Fast Gradient Sign Method attacks.",2024-12-27T17:14:52Z,http://arxiv.org/abs/2412.19747v1,"Longwei Wang, Navid Nayyem, Abdullah Rakin"
Pulsed laser as a continuous particle stream,"With the recently introduced particle interpretation of the double-slit
experiment [arXiv:2112.05512 (2024)], all interference phenomena can be
reinterpreted in terms of particle states that couple (bright) or do not couple
(dark) with the detectors. Here, we extend the two-mode analysis to multiple
modes, particularly mode-locked pulsed lasers, and show that collective
particle states of multiple light modes feature a significantly larger number
of dark than bright states. By linking interference in time-independent
multi-slit experiments (same-mode frequencies) to time-dependent multimode
experiments (different frequencies), we demonstrate that pulsed laser light
comprises a continuous photon beam, with photons forming bright states during
pulses and dark states between them. To confirm this, we analyzed mode-locked
lasers, finding that the ratio of the number of bright to dark states aligns
with the pulse-to-interval duration ratio.",2024-12-27T17:14:41Z,http://arxiv.org/abs/2412.19746v1,"Ciro Micheletti Diniz, Franciele Renata Henrique, Bruno Santos de Souza, Lino Misoguti, Paulo Henrique Dias Ferreira, Celso Jorge Villas Bôas"
IR Bounds on Theories with Spontaneously-Broken Lorentz Symmetry,"In nature, some UV features of the dynamics are reflected in IR quantities.
In fully relativistic theories, this connection can be probed through the
analyticity properties of scattering amplitudes, allowing to understand which
IR theories respect the UV assumptions of quantum field theory. The ensuing
analyticity bounds can be usually rephrased as the absence of faster-than-light
propagation for low-energy excitations. While it is interesting to understand
these relations and their IR characterization for theories that have less
idealized properties, it is also more difficult to derive analyticity bounds in
these cases. For theories that spontaneously break Lorentz symmetry, recent
progress was made by considering correlators of conserved currents and their
analyticity properties. In this work, we focus on such theories and close the
gap from the IR side, deriving bounds from the speed of propagation that are
equivalent to the known analyticity bounds. Our bounds require that gapped
excitations have a slower speed than gapless ones, at least for momenta that
are low with respect to the mass gap. These results suggest a way to study the
UV/IR connection in more complex theories.",2024-12-27T17:14:14Z,http://arxiv.org/abs/2412.19745v1,"Francesco Serra, Leonardo G. Trombetta"
"AAM-SEALS: Developing Aerial-Aquatic Manipulators in SEa, Air, and Land
  Simulator","Current simulators lack the ability to accurately model integrated
environments that encompass sea, air, and land. To address this gap, we
introduce Aerial-Aquatic Manipulators (AAMs) in SEa, Air, and Land Simulator
(SEALS), a comprehensive and photorealistic simulator designed for AAMs to
operate and learn in these diverse environments. The development of AAM-SEALS
tackles several significant challenges, including the creation of integrated
controllers for flying, swimming, and manipulation, and the high-fidelity
simulation of aerial dynamics and hydrodynamics leveraging particle physics.
Our evaluation demonstrates smooth operation and photorealistic transitions
across air, water, and their interfaces. We quantitatively validate the
fidelity of particle-based hydrodynamics by comparing position-tracking errors
across real-world and simulated systems. AAM-SEALS promises to benefit a broad
range of robotics communities, including robot learning, aerial robotics,
underwater robotics, mobile manipulation, and robotic simulators. We will
open-source our code and data to foster the advancement of research in these
fields. Please access our project website at: https:
//aam-seals.github.io/aam-seals-v1/",2024-12-27T17:13:14Z,http://arxiv.org/abs/2412.19744v1,"William Wang Yang, Karthikeya Kona, Yashveer Jain, Abhinav Bhamidipati, Tomer Atzili, Xiaomin Lin, Yantian Zha"
"Adaptive Context-Aware Multi-Path Transmission Control for VR/AR
  Content: A Deep Reinforcement Learning Approach","This paper introduces the Adaptive Context-Aware Multi-Path Transmission
Control Protocol (ACMPTCP), an efficient approach designed to optimize the
performance of Multi-Path Transmission Control Protocol (MPTCP) for
data-intensive applications such as augmented and virtual reality (AR/VR)
streaming. ACMPTCP addresses the limitations of conventional MPTCP by
leveraging deep reinforcement learning (DRL) for agile end-to-end path
management and optimal bandwidth allocation, facilitating path realignment
across diverse network environments.",2024-12-27T16:56:12Z,http://arxiv.org/abs/2412.19737v1,"Shakil Ahmed, Saifur Rahman Sabuj, Ashfaq Khokhar"
"Periodically and aperiodically Thue-Morse driven long-range systems:
  from dynamical localization to slow dynamics","We investigate the electric-field driven power-law random banded
matrix(PLRBM) model where a variation in the power-law exponent $\alpha$ yields
a delocalization-to-localization phase transition. We examine the periodically
driven PLRBM model with the help of the Floquet operator. The level spacing
ratio and the generalized participation ratio of the Floquet Hamiltonian reveal
a drive-induced fractal phase accompanied by diffusive transport on the
delocalized side of the undriven PLRBM model. On the localized side, the
time-periodic model remains localized - the average spacing ratio corresponds
to Poisson statistics and logarithmic transport is observed in the dynamics.
Extending our analysis to the aperiodic Thue-Morse (TM) driven system, we find
that the aperiodically driven clean long-range hopping model (clean counterpart
of the PLRBM model) exhibits the phenomenon of \textit{exact dynamical
localization} (EDL) on tuning the drive-parameters at special points. The
disordered time-aperiodic system shows diffusive transport followed by
relaxation to the infinite-temperature state on the delocalized side, and a
prethermal plateau with subdiffusion on the localized side. Additionally, we
compare this with a quasi-periodically driven AAH model that also undergoes a
localization-delocalization transition. Unlike the disordered long-range model,
it features a prolonged prethermal plateau followed by subdiffusion to the
infinite temperature state, even on the delocalized side.",2024-12-27T16:55:47Z,http://arxiv.org/abs/2412.19736v1,"Vatsana Tiwari, Devendra Singh Bhakuni, Auditya Sharma"
"A General Framework of Brain Region Detection And Genetic Variants
  Selection in Imaging Genetics","Imaging genetics is a growing field that employs structural or functional
neuroimaging techniques to study individuals with genetic risk variants
potentially linked to specific illnesses. This area presents considerable
challenges to statisticians due to the heterogeneous information and different
data forms it involves. In addition, both imaging and genetic data are
typically high-dimensional, creating a ""big data squared"" problem. Moreover,
brain imaging data contains extensive spatial information. Simply vectorizing
tensor images and treating voxels as independent features can lead to
computational issues and disregard spatial structure. This paper presents a
novel statistical method for imaging genetics modeling while addressing all
these challenges. We explore a Canonical Correlation Analysis based linear
model for the joint modeling of brain imaging, genetic information, and
clinical phenotype, enabling the simultaneous detection of significant brain
regions and selection of important genetic variants associated with the
phenotype outcome. Scalable algorithms are developed to tackle the ""big data
squared"" issue. We apply the proposed method to explore the reaction speed, an
indicator of cognitive functions, and its associations with brain MRI and
genetic factors using the UK Biobank database. Our study reveals a notable
connection between the caudate nucleus region of brain and specific significant
SNPs, along with their respective regulated genes, and the reaction speed.",2024-12-27T16:54:11Z,http://arxiv.org/abs/2412.19735v1,"Siqiang Su, Zhenghao Li, Long Feng, Ting Li"
"Dynamics, data and reconstruction","Data-driven learning is prevalent in many fields of science, mathematics and
engineering. The goal of data-driven learning of dynamical systems is to
interpret timeseries as a continuous observation of an underlying dynamical
system. This task is not well-posed for a variety of reasons. A dynamical
system may have multiple sub-systems co-existing within it. The nature of the
dataset depends on the portion of the phase space being viewed, and may thus my
confined to a sub-system. Secondly these sub-systems may be topologically
inter-weaved, so may be inseparable computationally. Thirdly, two timeseries
sampled separately from different dynamical systems may be close or even
indistinguishable. So there is no unqiue source for the timeseries. We show how
these ambiguities are circumvented if one considers dynamical systems and
measurement maps collectively. This is made possible in a category theoretical
framework, in which reconstruction is unique up to equivalences. We introduce
two categories of observed dynamical systems and timeseries-data. These are
related to the well known category of dynamical systems via functors. This
enables a functorial interpretation of the task of reconstruction as well.",2024-12-27T16:49:52Z,http://arxiv.org/abs/2412.19734v1,"Suddhasattwa Das, Tomoharu Suda"
"Generative Pretrained Embedding and Hierarchical Irregular Time Series
  Representation for Daily Living Activity Recognition","Within the evolving landscape of smart homes, the precise recognition of
daily living activities using ambient sensor data stands paramount. This paper
not only aims to bolster existing algorithms by evaluating two distinct
pretrained embeddings suited for ambient sensor activations but also introduces
a novel hierarchical architecture. We delve into an architecture anchored on
Transformer Decoder-based pre-trained embeddings, reminiscent of the GPT
design, and contrast it with the previously established state-of-the-art (SOTA)
ELMo embeddings for ambient sensors. Our proposed hierarchical structure
leverages the strengths of each pre-trained embedding, enabling the discernment
of activity dependencies and sequence order, thereby enhancing classification
precision. To further refine recognition, we incorporate into our proposed
architecture an hour-of-the-day embedding. Empirical evaluations underscore the
preeminence of the Transformer Decoder embedding in classification endeavors.
Additionally, our innovative hierarchical design significantly bolsters the
efficacy of both pre-trained embeddings, notably in capturing inter-activity
nuances. The integration of temporal aspects subtly but distinctively augments
classification, especially for time-sensitive activities. In conclusion, our
GPT-inspired hierarchical approach, infused with temporal insights, outshines
the SOTA ELMo benchmark.",2024-12-27T16:43:52Z,http://arxiv.org/abs/2412.19732v1,"Damien Bouchabou, Sao Mai Nguyen"
"Fully-relativistic evolution of vacuum tensor inhomogeneities during
  inflation","We present a complete method for the initialisation and extraction of
first-order inflationary tensor perturbations for fully relativistic
simulations which incorporate gravitational back-reaction. We outline a
correspondence between the Cosmological Perturbation Theory (CPT) framework and
the numerical relativity BSSN variables in the appropriate limit. We describe a
generation method for stochastic tensoral initial conditions, inspired by the
standard scalar initial condition used from inflation and implemented in
lattice cosmology. We discuss the implementation of this procedure in the
GRChombo/GRTeclyn code, and demonstrate the detailed quantitative
correspondence between the linearised and fully-nonlinear solutions in the
perturbative limit, through the evolution of the background and the tensor
power spectrum. We also validate the methodology by showing that energy and
momentum constraints are introduced and preserved to second-order or better. We
provide some preliminary indicative results probing tensoral non-Gaussianity
using the skewness and kurtosis. The computational pipeline presented here will
be used to study the emergence of a primordial tensor bispectra and
cross-spectra that incorporate the effect of nonlinear gravitational couplings
with the metric, which has potential applications for the analysis of
next-generation CMB surveys.",2024-12-27T16:42:59Z,http://arxiv.org/abs/2412.19731v1,"Ericka Florio, E. Paul S. Shellard"
"Learning to Forget: Bayesian Time Series Forecasting using Recurrent
  Sparse Spectrum Signature Gaussian Processes","The signature kernel is a kernel between time series of arbitrary length and
comes with strong theoretical guarantees from stochastic analysis. It has found
applications in machine learning such as covariance functions for Gaussian
processes. A strength of the underlying signature features is that they provide
a structured global description of a time series. However, this property can
quickly become a curse when local information is essential and forgetting is
required; so far this has only been addressed with ad-hoc methods such as
slicing the time series into subsegments. To overcome this, we propose a
principled, data-driven approach by introducing a novel forgetting mechanism
for signatures. This allows the model to dynamically adapt its context length
to focus on more recent information. To achieve this, we revisit the recently
introduced Random Fourier Signature Features, and develop Random Fourier
Decayed Signature Features (RFDSF) with Gaussian processes (GPs). This results
in a Bayesian time series forecasting algorithm with variational inference,
that offers a scalable probabilistic algorithm that processes and transforms a
time series into a joint predictive distribution over time steps in one pass
using recurrence. For example, processing a sequence of length $10^4$ steps in
$\approx 10^{-2}$ seconds and in $&lt; 1\text{GB}$ of GPU memory. We demonstrate
that it outperforms other GP-based alternatives and competes with
state-of-the-art probabilistic time series forecasting algorithms.",2024-12-27T16:31:09Z,http://arxiv.org/abs/2412.19727v1,"Csaba Tóth, Masaki Adachi, Michael A. Osborne, Harald Oberhauser"
EEG-Reptile: An Automatized Reptile-Based Meta-Learning Library for BCIs,"Meta-learning, i.e., ""learning to learn"", is a promising approach to enable
efficient BCI classifier training with limited amounts of data. It can
effectively use collections of in some way similar classification tasks, with
rapid adaptation to new tasks where only minimal data are available. However,
applying meta-learning to existing classifiers and BCI tasks requires
significant effort. To address this issue, we propose EEG-Reptile, an automated
library that leverages meta-learning to improve classification accuracy of
neural networks in BCIs and other EEG-based applications. It utilizes the
Reptile meta-learning algorithm to adapt neural network classifiers of EEG data
to the inter-subject domain, allowing for more efficient fine-tuning for a new
subject on a small amount of data. The proposed library incorporates an
automated hyperparameter tuning module, a data management pipeline, and an
implementation of the Reptile meta-learning algorithm. EEG-Reptile automation
level allows using it without deep understanding of meta-learning. We
demonstrate the effectiveness of EEG-Reptile on two benchmark datasets (BCI IV
2a, Lee2019 MI) and three neural network architectures (EEGNet, FBCNet,
EEG-Inception). Our library achieved improvement in both zero-shot and few-shot
learning scenarios compared to traditional transfer learning approaches.",2024-12-27T16:24:31Z,http://arxiv.org/abs/2412.19725v1,"Daniil A. Berdyshev, Artem M. Grachev, Sergei L. Shishkin, Bogdan L. Kozyrskiy"
"OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse
  Task Synthesis","Graphical User Interface (GUI) agents powered by Vision-Language Models
(VLMs) have demonstrated human-like computer control capability. Despite their
utility in advancing digital automation, a critical bottleneck persists:
collecting high-quality trajectory data for training. Common practices for
collecting such data rely on human supervision or synthetic data generation
through executing pre-defined tasks, which are either resource-intensive or
unable to guarantee data quality. Moreover, these methods suffer from limited
data diversity and significant gaps between synthetic data and real-world
environments. To address these challenges, we propose OS-Genesis, a novel GUI
data synthesis pipeline that reverses the conventional trajectory collection
process. Instead of relying on pre-defined tasks, OS-Genesis enables agents
first to perceive environments and perform step-wise interactions, then
retrospectively derive high-quality tasks to enable trajectory-level
exploration. A trajectory reward model is then employed to ensure the quality
of the generated trajectories. We demonstrate that training GUI agents with
OS-Genesis significantly improves their performance on highly challenging
online benchmarks. In-depth analysis further validates OS-Genesis's efficiency
and its superior data quality and diversity compared to existing synthesis
methods. Our codes, data, and checkpoints are available at
\href{https://qiushisun.github.io/OS-Genesis-Home/}{OS-Genesis Homepage}.",2024-12-27T16:21:58Z,http://arxiv.org/abs/2412.19723v1,"Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, Ben Kao, Guohao Li, Junxian He, Yu Qiao, Zhiyong Wu"
Sharpening Neural Implicit Functions with Frequency Consolidation Priors,"Signed Distance Functions (SDFs) are vital implicit representations to
represent high fidelity 3D surfaces. Current methods mainly leverage a neural
network to learn an SDF from various supervisions including signed distances,
3D point clouds, or multi-view images. However, due to various reasons
including the bias of neural network on low frequency content, 3D unaware
sampling, sparsity in point clouds, or low resolutions of images, neural
implicit representations still struggle to represent geometries with high
frequency components like sharp structures, especially for the ones learned
from images or point clouds. To overcome this challenge, we introduce a method
to sharpen a low frequency SDF observation by recovering its high frequency
components, pursuing a sharper and more complete surface. Our key idea is to
learn a mapping from a low frequency observation to a full frequency coverage
in a data-driven manner, leading to a prior knowledge of shape consolidation in
the frequency domain, dubbed frequency consolidation priors. To better
generalize a learned prior to unseen shapes, we introduce to represent
frequency components as embeddings and disentangle the embedding of the low
frequency component from the embedding of the full frequency component. This
disentanglement allows the prior to generalize on an unseen low frequency
observation by simply recovering its full frequency embedding through a
test-time self-reconstruction. Our evaluations under widely used benchmarks or
real scenes show that our method can recover high frequency component and
produce more accurate surfaces than the latest methods. The code, data, and
pre-trained models are available at \url{https://github.com/chenchao15/FCP}.",2024-12-27T16:18:46Z,http://arxiv.org/abs/2412.19720v1,"Chao Chen, Yu-Shen Liu, Zhizhong Han"
"Text2Insight: Transform natural language text into insights seamlessly
  using multi-model architecture","The growing demand for dynamic, user-centric data analysis and visualization
is evident across domains like healthcare, finance, and research. Traditional
visualization tools often fail to meet individual user needs due to their
static and predefined nature. To address this gap, Text2Insight is introduced
as an innovative solution that delivers customized data analysis and
visualizations based on user-defined natural language requirements. Leveraging
a multi-model architecture, Text2Insight transforms user inputs into actionable
insights and dynamic visualizations.
  The methodology begins with analyzing the input dataset to extract structural
details such as columns and values. A pre-trained Llama3 model converts the
user's natural language query into an SQL query, which is further refined using
a Named Entity Recognition (NER) model for accuracy. A chart predictor
determines the most suitable visualization type, while the Llama3 model
generates insights based on the SQL query's results. The output is a
user-friendly and visually informative chart. To enhance analysis capabilities,
the system integrates a question-answering model and a predictive model using
the BERT framework. These models provide insights into historical data and
predict future trends.
  Performance evaluation of Text2Insight demonstrates its effectiveness,
achieving high accuracy (99%), precision (100%), recall (99%), and F1-score
(99%), with a BLEU score of 0.5. The question-answering model attained an
accuracy of 89% and the predictive model achieved 70% accuracy. These results
validate Text2Insight as a robust and viable solution for transforming natural
language text into dynamic, user-specific data analysis and visualizations.",2024-12-27T16:17:22Z,http://arxiv.org/abs/2412.19718v1,Pradeep Sain
"ProKAN: Progressive Stacking of Kolmogorov-Arnold Networks for Efficient
  Liver Segmentation","The growing need for accurate and efficient 3D identification of tumors,
particularly in liver segmentation, has spurred considerable research into deep
learning models. While many existing architectures offer strong performance,
they often face challenges such as overfitting and excessive computational
costs. An adjustable and flexible architecture that strikes a balance between
time efficiency and model complexity remains an unmet requirement. In this
paper, we introduce proKAN, a progressive stacking methodology for
Kolmogorov-Arnold Networks (KANs) designed to address these challenges. Unlike
traditional architectures, proKAN dynamically adjusts its complexity by
progressively adding KAN blocks during training, based on overfitting behavior.
This approach allows the network to stop growing when overfitting is detected,
preventing unnecessary computational overhead while maintaining high accuracy.
Additionally, proKAN utilizes KAN's learnable activation functions modeled
through B-splines, which provide enhanced flexibility in learning complex
relationships in 3D medical data. Our proposed architecture achieves
state-of-the-art performance in liver segmentation tasks, outperforming
standard Multi-Layer Perceptrons (MLPs) and fixed KAN architectures. The
dynamic nature of proKAN ensures efficient training times and high accuracy
without the risk of overfitting. Furthermore, proKAN provides better
interpretability by allowing insight into the decision-making process through
its learnable coefficients. The experimental results demonstrate a significant
improvement in accuracy, Dice score, and time efficiency, making proKAN a
compelling solution for 3D medical image segmentation tasks.",2024-12-27T16:14:06Z,http://arxiv.org/abs/2412.19713v1,"Bhavesh Gyanchandani, Aditya Oza, Abhinav Roy"
"Causal machine learning for heterogeneous treatment effects in the
  presence of missing outcome data","When estimating heterogeneous treatment effects, missing outcome data can
complicate treatment effect estimation, causing certain subgroups of the
population to be poorly represented. In this work, we discuss this commonly
overlooked problem and consider the impact that missing at random (MAR) outcome
data has on causal machine learning estimators for the conditional average
treatment effect (CATE). We then propose two de-biased machine learning
estimators for the CATE, the mDR-learner and mEP-learner, which address the
issue of under-representation by integrating inverse probability of censoring
weights into the DR-learner and EP-learner respectively. We show that under
reasonable conditions, these estimators are oracle efficient, and illustrate
their favorable performance through simulated data settings, comparing them to
existing CATE estimators, including comparison to estimators which use common
missing data techniques. Guidance on the implementation of these estimators is
provided and we present an example of their application using the ACTG175
trial, exploring treatment effect heterogeneity when comparing Zidovudine
mono-therapy against alternative antiretroviral therapies among HIV-1-infected
individuals.",2024-12-27T16:10:03Z,http://arxiv.org/abs/2412.19711v1,"Matthew Pryce, Karla Diaz-Ordaz, Ruth H. Keogh, Stijn Vansteelandt"
High precision spectroscopy of trilobite Rydberg molecules,"We perform three-photon photoassociation to obtain high resolution spectra of
$^{87}$Rb trilobite dimers for the principal quantum numbers $n = 22,24,25,26$,
and 27. The large binding energy of the molecules in combination with a
relative spectroscopic resolution of $10^{-4}$ provides a rigorous benchmark
for existing theoretical models. A recently developed Green's function
framework, which circumvents the convergence issues that afflicted previous
studies,, is employed to theoretically reproduce the vibrational spectrum of
the molecule with high accuracy. The relatively large molecular binding energy
are primarily determined by the low energy $S$-wave electron-atom scattering
length, thereby allowing us to extract the $^3S_1$ scattering phase shift with
unprecedented accuracy, at low energy regimes inaccessible to free electrons.",2024-12-27T16:04:17Z,http://arxiv.org/abs/2412.19710v1,"Markus Exner, Rohan Srikumar, Richard Blättner, Matthew T. Eiles, Peter Schmelcher, Herwig Ott"
Toward Adaptive Reasoning in Large Language Models with Thought Rollback,"Large language models (LLMs) have been routinely used to solve various tasks
using step-by-step reasoning. However, the structure of intermediate reasoning
steps, or thoughts, is rigid and unidirectional, such as chains, trees, or
acyclic-directed graphs. Consequently, the resulting inflexible and
forward-only reasoning may not address challenging tasks and fail when the LLM
frequently gives false responses, i.e., ``hallucinations''. This paper proposes
a new reasoning framework, called Thought Rollback (TR), allowing LLMs to
adaptively build thought structure while maintaining effective reasoning toward
problem-solving under ``hallucinations''. The core mechanism of TR is rolling
back thoughts, which allows LLMs to perform error analysis on thoughts, and
thus roll back to any previously mistaken thought for revision. Subsequently,
by including such trial-and-error in the prompt to guide the LLM, each rollback
leads to one more reliable reasoning path. Therefore, starting with a simple
prompt without human annotations, LLM with TR adaptively and gradually explores
thoughts for a correct solution. Comprehensive experiments on mathematical
problems and multi-task reasoning demonstrate the state-of-the-art performance
of TR in terms of problem-solving rate and interaction cost. For instance, the
solving rate of GPT-4 with TR outperforms the current best by $9\%$ on the MATH
dataset.",2024-12-27T16:02:34Z,http://arxiv.org/abs/2412.19707v1,"Sijia Chen, Baochun Li"
"An Integrated Optimization and Deep Learning Pipeline for Predicting
  Live Birth Success in IVF Using Feature Optimization and Transformer-Based
  Models","In vitro fertilization (IVF) is a widely utilized assisted reproductive
technology, yet predicting its success remains challenging due to the
multifaceted interplay of clinical, demographic, and procedural factors. This
study develops a robust artificial intelligence (AI) pipeline aimed at
predicting live birth outcomes in IVF treatments. The pipeline uses anonymized
data from 2010 to 2018, obtained from the Human Fertilization and Embryology
Authority (HFEA). We evaluated the prediction performance of live birth success
as a binary outcome (success/failure) by integrating different feature
selection methods, such as principal component analysis (PCA) and particle
swarm optimization (PSO), with different traditional machine learning-based
classifiers including random forest (RF) and decision tree, as well as deep
learning-based classifiers including custom transformer-based model and a tab
transformer model with an attention mechanism. Our research demonstrated that
the best performance was achieved by combining PSO for feature selection with
the TabTransformer-based deep learning model, yielding an accuracy of 99.50%
and an AUC of 99.96%, highlighting its significant performance to predict live
births. This study establishes a highly accurate AI pipeline for predicting
live birth outcomes in IVF, demonstrating its potential to enhance personalized
fertility treatments.",2024-12-27T15:46:59Z,http://arxiv.org/abs/2412.19696v1,"Arezoo Borji, Hossam Haick, Birgit Pohn, Antonia Graf, Jana Zakall, S M Ragib Shahriar Islam, Gernot Kronreif, Daniel Kovatchki, Heinz Strohmer, Sepideh Hatamikia"
"From prediction to explanation: managing influential negative reviews
  through explainable AI","The profound impact of online reviews on consumer decision-making has made it
crucial for businesses to manage negative reviews. Recent advancements in
artificial intelligence (AI) technology have offered businesses novel and
effective ways to manage and analyze substantial consumer feedback. In response
to the growing demand for explainablility and transparency in AI applications,
this study proposes a novel explainable AI (XAI) algorithm aimed at identifying
influential negative reviews. The experiments conducted on 101,338 restaurant
reviews validate the algorithm's effectiveness and provides understandable
explanations from both the feature-level and word-level perspectives. By
leveraging this algorithm, businesses can gain actionable insights for
predicting, perceiving, and strategically responding to online negative
feedback, fostering improved customer service and mitigating the potential
damage caused by negative reviews.",2024-12-27T15:37:19Z,http://arxiv.org/abs/2412.19692v1,Rongping Shen
"A Review on the Integration of Artificial Intelligence and Medical
  Imaging in IVF Ovarian Stimulation","Artificial intelligence (AI) has emerged as a powerful tool to enhance
decision-making and optimize treatment protocols in in vitro fertilization
(IVF). In particular, AI shows significant promise in supporting
decision-making during the ovarian stimulation phase of the IVF process. This
review evaluates studies focused on the applications of AI combined with
medical imaging in ovarian stimulation, examining methodologies, outcomes, and
current limitations. Our analysis of 13 studies on this topic reveals that,
reveal that while AI algorithms demonstrated notable potential in predicting
optimal hormonal dosages, trigger timing, and oocyte retrieval outcomes, the
medical imaging data utilized predominantly came from two-dimensional (2D)
ultrasound which mainly involved basic quantifications, such as follicle size
and number, with limited use of direct feature extraction or advanced image
analysis techniques. This points to an underexplored opportunity where advanced
image analysis approaches, such as deep learning, and more diverse imaging
modalities, like three-dimensional (3D) ultrasound, could unlock deeper
insights. Additionally, the lack of explainable AI (XAI) in most studies raises
concerns about the transparency and traceability of AI-driven decisions - key
factors for clinical adoption and trust. Furthermore, many studies relied on
single-center designs and small datasets, which limit the generalizability of
their findings. This review highlights the need for integrating advanced
imaging analysis techniques with explainable AI methodologies, as well as the
importance of leveraging multicenter collaborations and larger datasets.
Addressing these gaps has the potential to enhance ovarian stimulation
management, paving the way for efficient, personalized, and data-driven
treatment pathways that improve IVF outcomes.",2024-12-27T15:29:08Z,http://arxiv.org/abs/2412.19688v1,"Jana Zakall, Birgit Pohn, Antonia Graf, Daniel Kovatchki, Arezoo Borji, Ragib Shahriar Islam, Hossam Haick, Heinz Strohmer, Sepideh Hatamikia"
"Combining Machine Learning with Recurrence Analysis for resonance
  detection","The width of a resonance in a nearly integrable system, i.e. in a
non-integrable system where chaotic motion is still not prominent, can tell us
how a perturbation parameter is driving the system away from integrability.
Although the tool that we are presenting here can be used is quite generic and
can be used in a variety of systems, our particular interest lies in binary
compact object systems known as extreme mass ratio inspirals (EMRIs). In an
EMRI a lighter compact object, like a black hole or a neutron star, inspirals
into a supermassive black hole due to gravitational radiation reaction. During
this inspiral the lighter object crosses resonances, which are still not very
well modeled. Measuring the width of resonances in EMRI models allows us to
estimate the importance of each perturbation parameter able to drive the system
away from resonances and decide whether its impact should be included in EMRI
waveform modeling or not. To tackle this issue in our study we show first that
recurrence quantifiers of orbits carry imprints of resonant behavior,
regardless of the system's dimensionality. As a next step, we apply a long
short-term memory machine learning architecture to automate the resonance
detection procedure. Our analysis is developed on a simple standard map and
gradually we extend it to more complicated systems until finally we employ it
in a generic deformed Kerr spacetime known in the literature as the
Johannsen-Psaltis spacetime.",2024-12-27T15:20:57Z,http://arxiv.org/abs/2412.19683v1,"Ondřej Zelenka, Ondřej Kopáček, Georgios Lukes-Gerakopoulos"
"A Hybrid Technique for Plant Disease Identification and Localisation in
  Real-time","Over the past decade, several image-processing methods and algorithms have
been proposed for identifying plant diseases based on visual data. DNN (Deep
Neural Networks) have recently become popular for this task. Both traditional
image processing and DNN-based methods encounter significant performance issues
in real-time detection owing to computational limitations and a broad spectrum
of plant disease features. This article proposes a novel technique for
identifying and localising plant disease based on the Quad-Tree decomposition
of an image and feature learning simultaneously. The proposed algorithm
significantly improves accuracy and faster convergence in high-resolution
images with relatively low computational load. Hence it is ideal for deploying
the algorithm in a standalone processor in a remotely operated image
acquisition and disease detection system, ideally mounted on drones and robots
working on large agricultural fields. The technique proposed in this article is
hybrid as it exploits the advantages of traditional image processing methods
and DNN-based models at different scales, resulting in faster inference. The F1
score is approximately 0.80 for four disease classes corresponding to potato
and tomato crops.",2024-12-27T15:20:45Z,http://arxiv.org/abs/2412.19682v1,"Mahendra Kumar Gohil, Anirudha Bhattacharjee, Rwik Rana, Kishan Lal, Samir Kumar Biswas, Nachiketa Tiwari, Bishakh Bhattacharya"
Identifying clusters in Czekanowski's diagram,"Visualizing data through Czekanowski's diagram has as its aim the
illustration of the relationships between objects. Often, obvious clusters of
observations are directly visible. However, it is not straightforward to
precisely delineate these clusters. This paper presents the development of the
package RMaCzek, which now includes features for cluster identification in
Czekanowski diagrams.",2024-12-27T15:04:06Z,http://arxiv.org/abs/2412.19679v1,"Krzysztof Bartoszek, Ying Luo"
Deep ReLU networks -- injectivity capacity upper bounds,"We study deep ReLU feed forward neural networks (NN) and their injectivity
abilities. The main focus is on \emph{precisely} determining the so-called
injectivity capacity. For any given hidden layers architecture, it is defined
as the minimal ratio between number of network's outputs and inputs which
ensures unique recoverability of the input from a realizable output. A strong
recent progress in precisely studying single ReLU layer injectivity properties
is here moved to a deep network level. In particular, we develop a program that
connects deep $l$-layer net injectivity to an $l$-extension of the $\ell_0$
spherical perceptrons, thereby massively generalizing an isomorphism between
studying single layer injectivity and the capacity of the so-called
(1-extension) $\ell_0$ spherical perceptrons discussed in [82]. \emph{Random
duality theory} (RDT) based machinery is then created and utilized to
statistically handle properties of the extended $\ell_0$ spherical perceptrons
and implicitly of the deep ReLU NNs. A sizeable set of numerical evaluations is
conducted as well to put the entire RDT machinery in practical use. From these
we observe a rapidly decreasing tendency in needed layers' expansions, i.e., we
observe a rapid \emph{expansion saturation effect}. Only $4$ layers of depth
are sufficient to closely approach level of no needed expansion -- a result
that fairly closely resembles observations made in practical experiments and
that has so far remained completely untouchable by any of the existing
mathematical methodologies.",2024-12-27T14:57:40Z,http://arxiv.org/abs/2412.19677v1,Mihailo Stojnic
"Optimizing Local-Global Dependencies for Accurate 3D Human Pose
  Estimation","Transformer-based methods have recently achieved significant success in 3D
human pose estimation, owing to their strong ability to model long-range
dependencies. However, relying solely on the global attention mechanism is
insufficient for capturing the fine-grained local details, which are crucial
for accurate pose estimation. To address this, we propose SSR-STF, a
dual-stream model that effectively integrates local features with global
dependencies to enhance 3D human pose estimation. Specifically, we introduce
SSRFormer, a simple yet effective module that employs the skeleton selective
refine attention (SSRA) mechanism to capture fine-grained local dependencies in
human pose sequences, complementing the global dependencies modeled by the
Transformer. By adaptively fusing these two feature streams, SSR-STF can better
learn the underlying structure of human poses, overcoming the limitations of
traditional methods in local feature extraction. Extensive experiments on the
Human3.6M and MPI-INF-3DHP datasets demonstrate that SSR-STF achieves
state-of-the-art performance, with P1 errors of 37.4 mm and 13.2 mm
respectively, outperforming existing methods in both accuracy and
generalization. Furthermore, the motion representations learned by our model
prove effective in downstream tasks such as human mesh recovery. Codes are
available at https://github.com/poker-xu/SSR-STF.",2024-12-27T14:54:12Z,http://arxiv.org/abs/2412.19676v1,"Guangsheng Xu, Guoyi Zhang, Lejia Ye, Shuwei Gan, Xiaohu Zhang, Xia Yang"
"DLScanner: A parameter space scanner package assisted by deep learning
  methods","In this paper, we introduce a scanner package enhanced by deep learning (DL)
techniques. The proposed package addresses two significant challenges
associated with previously developed DL-based methods: slow convergence in
high-dimensional scans and the limited generalization of the DL network when
mapping random points to the target space. To tackle the first issue, we
utilize a similarity learning network that maps sampled points into a
representation space. In this space, in-target points are grouped together
while out-target points are effectively pushed apart. This approach enhances
the scan convergence by refining the representation of sampled points. The
second challenge is mitigated by integrating a dynamic sampling strategy.
Specifically, we employ a VEGAS mapping to adaptively suggest new points for
the DL network while also improving the mapping when more points are collected.
Our proposed framework demonstrates substantial gains in both performance and
efficiency compared to other scanning methods.",2024-12-27T14:52:42Z,http://arxiv.org/abs/2412.19675v1,"A. Hammad, Raymundo Ramos"
Spectral form factors for curved spacetimes with horizon,"The spectral form factor is believed to provide a special type of behavior
called ""dip-ramp-plateau"" in chaotic quantum systems which originates from the
random matrix theory. A similar behavior could be observed for deterministic
systems, ranging from the Riemann zeta function to the scattering amplitudes of
different types. It has been shown recently, the same behavior is observed for
the spectral form factor when the normal modes of a scalar massless field
theory in the brickwall model of the BTZ black hole are substituted as
eigenvalues of some quantum Hamiltonian. At the same time, the level spacing
distribution of these eigenvalues differs from that associated with the random
matrix theory ensembles. In this paper, we generalize these results considering
the recently proposed generalized spectral form factor for the de Sitter and
BTZ spacetimes. We study the details of this complex-valued form factor for
integrable quantum systems and for backgrounds with a horizon comparing it with
the random matrix theory behavior. As a result, we confirm that the scalar
field normal modes once again exhibit features of chaos.",2024-12-27T14:43:35Z,http://arxiv.org/abs/2412.19672v1,"Dmitry S. Ageev, Vasilii V. Pushkarev, Anastasia N. Zueva"
"Conjugation, loop and closure invariants of the iterated-integrals
  signature","Given a feature set for the shape of a closed loop, it is natural to ask
which features in that set do not change when the starting point of the path is
moved. For example, in two dimensions, the area enclosed by the path does not
depend on the starting point. In the present article, we characterize such loop
invariants among all those features known as interated integrals of a given
path. Furthermore, we relate these to conjugation invariants, which are a
canonical object of study when treating (tree reduced) paths as a group with
multiplication given by the concatenation. Finally, closure invariants are a
third class in this context which is of particular relevance when studying
piecewise linear trajectories, e.g. given by linear interpolation of time
series.
  Keywords: invariant features; concatenation of paths; combinatorial
necklaces; shuffle algebra; free Lie algebra; signed area; signed volume;
tree-like equivalence.",2024-12-27T14:32:30Z,http://arxiv.org/abs/2412.19670v1,"Joscha Diehl, Rosa Preiß, Jeremy Reizenstein"
"Toward Scalable Multirobot Control: Fast Policy Learning in Distributed
  MPC","Distributed model predictive control (DMPC) is promising in achieving optimal
cooperative control in multirobot systems (MRS). However, real-time DMPC
implementation relies on numerical optimization tools to periodically calculate
local control sequences online. This process is computationally demanding and
lacks scalability for large-scale, nonlinear MRS. This article proposes a novel
distributed learning-based predictive control (DLPC) framework for scalable
multirobot control. Unlike conventional DMPC methods that calculate open-loop
control sequences, our approach centers around a computationally fast and
efficient distributed policy learning algorithm that generates explicit
closed-loop DMPC policies for MRS without using numerical solvers. The policy
learning is executed incrementally and forward in time in each prediction
interval through an online distributed actor-critic implementation. The control
policies are successively updated in a receding-horizon manner, enabling fast
and efficient policy learning with the closed-loop stability guarantee. The
learned control policies could be deployed online to MRS with varying robot
scales, enhancing scalability and transferability for large-scale MRS.
Furthermore, we extend our methodology to address the multirobot safe learning
challenge through a force field-inspired policy learning approach. We validate
our approach's effectiveness, scalability, and efficiency through extensive
experiments on cooperative tasks of large-scale wheeled robots and multirotor
drones. Our results demonstrate the rapid learning and deployment of DMPC
policies for MRS with scales up to 10,000 units.",2024-12-27T14:31:52Z,http://arxiv.org/abs/2412.19669v1,"Xinglong Zhang, Wei Pan, Cong Li, Xin Xu, Xiangke Wang, Ronghua Zhang, Dewen Hu"
"CAD-GPT: Synthesising CAD Construction Sequence with Spatial
  Reasoning-Enhanced Multimodal LLMs","Computer-aided design (CAD) significantly enhances the efficiency, accuracy,
and innovation of design processes by enabling precise 2D and 3D modeling,
extensive analysis, and optimization. Existing methods for creating CAD models
rely on latent vectors or point clouds, which are difficult to obtain and
costly to store. Recent advances in Multimodal Large Language Models (MLLMs)
have inspired researchers to use natural language instructions and images for
CAD model construction. However, these models still struggle with inferring
accurate 3D spatial location and orientation, leading to inaccuracies in
determining the spatial 3D starting points and extrusion directions for
constructing geometries. This work introduces CAD-GPT, a CAD synthesis method
with spatial reasoning-enhanced MLLM that takes either a single image or a
textual description as input. To achieve precise spatial inference, our
approach introduces a 3D Modeling Spatial Mechanism. This method maps 3D
spatial positions and 3D sketch plane rotation angles into a 1D linguistic
feature space using a specialized spatial unfolding mechanism, while
discretizing 2D sketch coordinates into an appropriate planar space to enable
precise determination of spatial starting position, sketch orientation, and 2D
sketch coordinate translations. Extensive experiments demonstrate that CAD-GPT
consistently outperforms existing state-of-the-art methods in CAD model
synthesis, both quantitatively and qualitatively.",2024-12-27T14:19:36Z,http://arxiv.org/abs/2412.19663v1,"Siyu Wang, Cailian Chen, Xinyi Le, Qimin Xu, Lei Xu, Yanzhou Zhang, Jie Yang"
Quantum Cluster State Model with Haagerup Fusion Category Symmetry,"We propose a (1+1)D lattice model, inspired by a weak Hopf algebra
generalization of the cluster state model, which realizes Haagerup fusion
category symmetry and features a tensor product Hilbert space. The construction
begins with a reconstruction of the Haagerup weak Hopf algebra $H_3$ from the
Haagerup fusion category, ensuring that the representation category of $H_3$ is
equivalent to Haagerup fusion category. Utilizing the framework of symmetry
topological field theory (SymTFT), we develop an ultra-thin weak Hopf quantum
double model, characterized by a smooth topological boundary condition. We show
that this model supports Haagerup fusion category symmetry. Finally, we solve
the ground state of the model in terms of a weak Hopf matrix product state,
which serves as a natural generalization of the cluster state, embodying
Haagerup fusion category symmetry.",2024-12-27T14:05:15Z,http://arxiv.org/abs/2412.19657v1,Zhian Jia
"Asymmetrical Reciprocity-based Federated Learning for Resolving
  Disparities in Medical Diagnosis","Geographic health disparities pose a pressing global challenge, particularly
in underserved regions of low- and middle-income nations. Addressing this issue
requires a collaborative approach to enhance healthcare quality, leveraging
support from medically more developed areas. Federated learning emerges as a
promising tool for this purpose. However, the scarcity of medical data and
limited computation resources in underserved regions make collaborative
training of powerful machine learning models challenging. Furthermore, there
exists an asymmetrical reciprocity between underserved and developed regions.
To overcome these challenges, we propose a novel cross-silo federated learning
framework, named FedHelp, aimed at alleviating geographic health disparities
and fortifying the diagnostic capabilities of underserved regions.
Specifically, FedHelp leverages foundational model knowledge via one-time API
access to guide the learning process of underserved small clients, addressing
the challenge of insufficient data. Additionally, we introduce a novel
asymmetric dual knowledge distillation module to manage the issue of asymmetric
reciprocity, facilitating the exchange of necessary knowledge between developed
large clients and underserved small clients. We validate the effectiveness and
utility of FedHelp through extensive experiments on both medical image
classification and segmentation tasks. The experimental results demonstrate
significant performance improvement compared to state-of-the-art baselines,
particularly benefiting clients in underserved regions.",2024-12-27T13:59:58Z,http://arxiv.org/abs/2412.19654v1,"Jiaqi Wang, Ziyi Yin, Quanzeng You, Lingjuan Lyu, Fenglong Ma"
"Toward Modality Gap: Vision Prototype Learning for Weakly-supervised
  Semantic Segmentation with CLIP","The application of Contrastive Language-Image Pre-training (CLIP) in Weakly
Supervised Semantic Segmentation (WSSS) research powerful cross-modal semantic
understanding capabilities. Existing methods attempt to optimize input text
prompts for improved alignment of images and text, by finely adjusting text
prototypes to facilitate semantic matching. Nevertheless, given the modality
gap between text and vision spaces, the text prototypes employed by these
methods have not effectively established a close correspondence with
pixel-level vision features. In this work, our theoretical analysis indicates
that the inherent modality gap results in misalignment of text and region
features, and that this gap cannot be sufficiently reduced by minimizing
contrast loss in CLIP. To mitigate the impact of the modality gap, we propose a
Vision Prototype Learning (VPL) framework, by introducing more representative
vision prototypes. The core of this framework is to learn class-specific vision
prototypes in vision space with the help of text prototypes, for capturing
high-quality localization maps. Moreover, we propose a regional semantic
contrast module that contrasts regions embedding with corresponding prototypes,
leading to more comprehensive and robust feature learning. Experimental results
show that our proposed framework achieves state-of-the-art performance on two
benchmark datasets.",2024-12-27T13:55:11Z,http://arxiv.org/abs/2412.19650v1,"Zhongxing Xu, Feilong Tang, Zhe Chen, Yingxue Su, Zhiyi Zhao, Ge Zhang, Jionglong Su, Zongyuan Ge"
"Distributed Download from an External Data Source in Faulty Majority
  Settings","We extend the study of retrieval problems in distributed networks, focusing
on improving the efficiency and resilience of protocols in the \emph{Data
Retrieval (DR) Model}. The DR Model consists of a complete network (i.e., a
clique) with $k$ peers, up to $\beta k$ of which may be Byzantine (for $\beta
\in [0, 1)$), and a trusted \emph{External Data Source} comprising an array $X$
of $n$ bits ($n \gg k$) that the peers can query. Additionally, the peers can
also send messages to each other. In this work, we focus on the Download
problem that requires all peers to learn $X$. Our primary goal is to minimize
the maximum number of queries made by any honest peer and additionally optimize
time.
  We begin with a randomized algorithm for the Download problem that achieves
optimal query complexity up to a logarithmic factor. For the stronger dynamic
adversary that can change the set of Byzantine peers from one round to the
next, we achieve the optimal time complexity in peer-to-peer communication but
with larger messages. In broadcast communication where all peers (including
Byzantine peers) are required to send the same message to all peers, with
larger messages, we achieve almost optimal time and query complexities for a
dynamic adversary. Finally, in a more relaxed crash fault model, where peers
stop responding after crashing, we address the Download problem in both
synchronous and asynchronous settings. Using a deterministic protocol, we
obtain nearly optimal results for both query complexity and message sizes in
these scenarios.",2024-12-27T13:55:00Z,http://arxiv.org/abs/2412.19649v1,"John Augustine, Soumyottam Chatterjee, Valerie King, Manish Kumar, Shachar Meir, David Peleg"
"Chimera: A Block-Based Neural Architecture Search Framework for
  Event-Based Object Detection","Event-based cameras are sensors that simulate the human eye, offering
advantages such as high-speed robustness and low power consumption. Established
Deep Learning techniques have shown effectiveness in processing event data.
Chimera is a Block-Based Neural Architecture Search (NAS) framework
specifically designed for Event-Based Object Detection, aiming to create a
systematic approach for adapting RGB-domain processing methods to the event
domain. The Chimera design space is constructed from various macroblocks,
including Attention blocks, Convolutions, State Space Models, and
MLP-mixer-based architectures, which provide a valuable trade-off between local
and global processing capabilities, as well as varying levels of complexity.
The results on the PErson Detection in Robotics (PEDRo) dataset demonstrated
performance levels comparable to leading state-of-the-art models, alongside an
average parameter reduction of 1.6 times.",2024-12-27T13:50:44Z,http://arxiv.org/abs/2412.19646v1,"Diego A. Silva, Ahmed Elsheikh, Kamilya Smagulova, Mohammed E. Fouda, Ahmed M. Eltawil"
"VideoMaker: Zero-shot Customized Video Generation with the Inherent
  Force of Video Diffusion Models","Zero-shot customized video generation has gained significant attention due to
its substantial application potential. Existing methods rely on additional
models to extract and inject reference subject features, assuming that the
Video Diffusion Model (VDM) alone is insufficient for zero-shot customized
video generation. However, these methods often struggle to maintain consistent
subject appearance due to suboptimal feature extraction and injection
techniques. In this paper, we reveal that VDM inherently possesses the force to
extract and inject subject features. Departing from previous heuristic
approaches, we introduce a novel framework that leverages VDM's inherent force
to enable high-quality zero-shot customized video generation. Specifically, for
feature extraction, we directly input reference images into VDM and use its
intrinsic feature extraction process, which not only provides fine-grained
features but also significantly aligns with VDM's pre-trained knowledge. For
feature injection, we devise an innovative bidirectional interaction between
subject features and generated content through spatial self-attention within
VDM, ensuring that VDM has better subject fidelity while maintaining the
diversity of the generated video.Experiments on both customized human and
object video generation validate the effectiveness of our framework.",2024-12-27T13:49:25Z,http://arxiv.org/abs/2412.19645v1,"Tao Wu, Yong Zhang, Xiaodong Cun, Zhongang Qi, Junfu Pu, Huanzhang Dou, Guangcong Zheng, Ying Shan, Xi Li"
Xmodel-2 Technical Report,"Xmodel-2 is a 1.2-billion-parameter large language model designed
specifically for reasoning tasks. Its architecture enables different model
scales to share a unified set of hyperparameters, allowing for extensive
experimentation on smaller models and seamless transfer of optimal
configurations to larger models. To maximize training efficiency and stability,
Xmodel-2 employs the WSD learning rate scheduler from MiniCPM. Pretrained on
1.5 trillion tokens from diverse sources, Xmodel-2 achieves state-of-the-art
performance in complex reasoning and agent-based tasks, while maintaining low
training costs. These results highlight the potential of efficient model design
and training strategies in advancing reasoning capabilities. Model checkpoints
and code are publicly available on GitHub at
https://github.com/XiaoduoAILab/Xmodel-2",2024-12-27T13:32:10Z,http://arxiv.org/abs/2412.19638v1,"Wang Qun, Liu Yang, Lin Qingquan, Qu Zhijiu, Jiang Ling"
ReNeg: Learning Negative Embedding with Reward Guidance,"In text-to-image (T2I) generation applications, negative embeddings have
proven to be a simple yet effective approach for enhancing generation quality.
Typically, these negative embeddings are derived from user-defined negative
prompts, which, while being functional, are not necessarily optimal. In this
paper, we introduce ReNeg, an end-to-end method designed to learn improved
Negative embeddings guided by a Reward model. We employ a reward feedback
learning framework and integrate classifier-free guidance (CFG) into the
training process, which was previously utilized only during inference, thus
enabling the effective learning of negative embeddings. We also propose two
strategies for learning both global and per-sample negative embeddings.
Extensive experiments show that the learned negative embedding significantly
outperforms null-text and handcrafted counterparts, achieving substantial
improvements in human preference alignment. Additionally, the negative
embedding learned within the same text embedding space exhibits strong
generalization capabilities. For example, using the same CLIP text encoder, the
negative embedding learned on SD1.5 can be seamlessly transferred to
text-to-image or even text-to-video models such as ControlNet, ZeroScope, and
VideoCrafter2, resulting in consistent performance improvements across the
board.",2024-12-27T13:31:55Z,http://arxiv.org/abs/2412.19637v1,"Xiaomin Li, Yixuan Liu, Takashi Isobe, Xu Jia, Qinpeng Cui, Dong Zhou, Dong Li, You He, Huchuan Lu, Zhongdao Wang, Emad Barsoum"
Deep Linear Hawkes Processes,"Marked temporal point processes (MTPPs) are used to model sequences of
different types of events with irregular arrival times, with broad applications
ranging from healthcare and social networks to finance. We address shortcomings
in existing point process models by drawing connections between modern deep
state-space models (SSMs) and linear Hawkes processes (LHPs), culminating in an
MTPP that we call the deep linear Hawkes process (DLHP). The DLHP modifies the
linear differential equations in deep SSMs to be stochastic jump differential
equations, akin to LHPs. After discretizing, the resulting recurrence can be
implemented efficiently using a parallel scan. This brings parallelism and
linear scaling to MTPP models. This contrasts with attention-based MTPPs, which
scale quadratically, and RNN-based MTPPs, which do not parallelize across the
sequence length. We show empirically that DLHPs match or outperform existing
models across a broad range of metrics on eight real-world datasets. Our
proposed DLHP model is the first instance of the unique architectural
capabilities of SSMs being leveraged to construct a new class of MTPP models.",2024-12-27T13:23:58Z,http://arxiv.org/abs/2412.19634v1,"Yuxin Chang, Alex Boyd, Cao Xiao, Taha Kass-Hout, Parminder Bhatia, Padhraic Smyth, Andrew Warrington"
IMTP: Search-based Code Generation for In-memory Tensor Programs,"Processing-in-DRAM (DRAM-PIM) has emerged as a promising technology for
accelerating memory-intensive operations in modern applications, such as Large
Language Models (LLMs). Despite its potential, current software stacks for
DRAM-PIM face significant challenges, including reliance on hand-tuned
libraries that hinder programmability, limited support for high-level
abstractions, and the lack of systematic optimization frameworks. To address
these limitations, we present IMTP, a search-based optimizing tensor compiler
for UPMEM. Key features of IMTP include: (1) automated searches of the joint
search space for host and kernel tensor programs, (2) PIM-aware optimizations
for efficiently handling boundary conditions, and (3) improved search
algorithms for the expanded search space of UPMEM systems. Our experimental
results on UPMEM hardware demonstrate performance gains of up to 8.21x for
various UPMEM benchmark kernels and 5.33x for GPT-J layers. To the best of our
knowledge, IMTP is the first tensor compiler to provide fully automated,
autotuning-integrated code generation support for a DRAM-PIM system. By
bridging the gap between high-level tensor computation abstractions and
low-level hardware-specific requirements, IMTP establishes a foundation for
advancing DRAM-PIM programmability and enabling streamlined optimization.",2024-12-27T13:19:35Z,http://arxiv.org/abs/2412.19630v1,"Yongwon Shin, Dookyung Kang, Hyojin Sung"
"Gradient Weight-normalized Low-rank Projection for Efficient LLM
  Training","Large Language Models (LLMs) have shown remarkable performance across various
tasks, but the escalating demands on computational resources pose significant
challenges, particularly in the extensive utilization of full fine-tuning for
downstream tasks. To address this, parameter-efficient fine-tuning (PEFT)
methods have been developed, but they often underperform compared to full
fine-tuning and struggle with memory efficiency. In this work, we introduce
Gradient Weight-Normalized Low-Rank Projection (GradNormLoRP), a novel approach
that enhances both parameter and memory efficiency while maintaining comparable
performance to full fine-tuning. GradNormLoRP normalizes the weight matrix to
improve gradient conditioning, facilitating better convergence during
optimization. Additionally, it applies low-rank approximations to the weight
and gradient matrices, significantly reducing memory usage during training.
Extensive experiments demonstrate that our 8-bit GradNormLoRP reduces optimizer
memory usage by up to 89.5% and enables the pre-training of large LLMs, such as
LLaMA 7B, on consumer-level GPUs like the NVIDIA RTX 4090, without additional
inference costs. Moreover, GradNormLoRP outperforms existing low-rank methods
in fine-tuning tasks. For instance, when fine-tuning the RoBERTa model on all
GLUE tasks with a rank of 8, GradNormLoRP achieves an average score of 80.65,
surpassing LoRA's score of 79.23. These results underscore GradNormLoRP as a
promising alternative for efficient LLM pre-training and fine-tuning. Source
code and Appendix:
https://github.com/Jhhuangkay/Gradient-Weight-normalized-Low-rank-Projection-for-Efficient-LLM-Training",2024-12-27T12:23:39Z,http://arxiv.org/abs/2412.19616v1,"Jia-Hong Huang, Yixian Shen, Hongyi Zhu, Stevan Rudinac, Evangelos Kanoulas"
Anisotropic Band Flattening in Twisted Bilayer of M-Valley MXenes,"Experimental studies on moir\'e materials have predominantly focused on
twisted hexagonal lattice with low-energy states near the $\Gamma$- or
K-points. These materials, characterized by isotropic low-energy dispersion,
are fundamentally distinct from those with anisotropic properties. Here we
introduce a series of semiconducting transition metal carbides (MXenes)
$M_2$C$T_2$ ($M$ = Ti, Zr, Hf, Sc, Y; $T$ = O, F, Cl) as a novel platform for
M-valley moir\'e materials. Take Ti$_2$CO$_2$ and Zr$_2$CO$_2$ as
representative examples, large-scale \emph{ab initio} calculations show that
their AB-stacked twisted homobilayer features three three-fold rotational
symmetry related M-valleys with time-reserval symmetry and giant anisotropic
band flattening. We derive a simplified moir\'e Hamiltonian for these systems
and conduct a detailed analysis of their band structures, where the origins of
anisotropic band flattening are clearly elucidated. This research broadens the
scope of moir\'e materials, where the valley- and spin-degenerate
two-dimensional array of quasi-one-dimensional system could serve as a
potential platform for realizing many interesting correlated phases.",2024-12-27T12:20:15Z,http://arxiv.org/abs/2412.19613v1,"Kejie Bao, Huan Wang, Zhaochen Liu, jing Wang"
"Enhancing Fine-grained Image Classification through Attentive Batch
  Training","Fine-grained image classification, which is a challenging task in computer
vision, requires precise differentiation among visually similar object
categories. In this paper, we propose 1) a novel module called Residual
Relationship Attention (RRA) that leverages the relationships between images
within each training batch to effectively integrate visual feature vectors of
batch images and 2) a novel technique called Relationship Position Encoding
(RPE), which encodes the positions of relationships between original images in
a batch and effectively preserves the relationship information between images
within the batch. Additionally, we design a novel framework, namely
Relationship Batch Integration (RBI), which utilizes RRA in conjunction with
RPE, allowing the discernment of vital visual features that may remain elusive
when examining a singular image representative of a particular class. Through
extensive experiments, our proposed method demonstrates significant
improvements in the accuracy of different fine-grained classifiers, with an
average increase of $(+2.78\%)$ and $(+3.83\%)$ on the CUB200-2011 and Stanford
Dog datasets, respectively, while achieving a state-of-the-art results
$(95.79\%)$ on the Stanford Dog dataset. Despite not achieving the same level
of improvement as in fine-grained image classification, our method still
demonstrates its prowess in leveraging general image classification by
attaining a state-of-the-art result of $(93.71\%)$ on the Tiny-Imagenet
dataset. Furthermore, our method serves as a plug-in refinement module and can
be easily integrated into different networks.",2024-12-27T12:07:58Z,http://arxiv.org/abs/2412.19606v1,"Duy M. Le, Bao Q. Bui, Anh Tran, Cong Tran, Cuong Pham"
"ViDTA: Enhanced Drug-Target Affinity Prediction via Virtual Graph Nodes
  and Attention-based Feature Fusion","Drug-target interaction is fundamental in understanding how drugs affect
biological systems, and accurately predicting drug-target affinity (DTA) is
vital for drug discovery. Recently, deep learning methods have emerged as a
significant approach for estimating the binding strength between drugs and
target proteins. However, existing methods simply utilize the drug's local
information from molecular topology rather than global information.
Additionally, the features of drugs and proteins are usually fused with a
simple concatenation operation, limiting their effectiveness. To address these
challenges, we proposed ViDTA, an enhanced DTA prediction framework. We
introduce virtual nodes into the Graph Neural Network (GNN)-based drug feature
extraction network, which acts as a global memory to exchange messages more
efficiently. By incorporating virtual graph nodes, we seamlessly integrate
local and global features of drug molecular structures, expanding the GNN's
receptive field. Additionally, we propose an attention-based linear feature
fusion network for better capturing the interaction information between drugs
and proteins. Experimental results evaluated on various benchmarks including
Davis, Metz, and KIBA demonstrate that our proposed ViDTA outperforms the
state-of-the-art baselines.",2024-12-27T11:19:10Z,http://arxiv.org/abs/2412.19589v1,"Minghui Li, Zikang Guo, Yang Wu, Peijin Guo, Yao Shi, Shengshan Hu, Wei Wan, Shengqing Hu"
"Goal-oriented Communications based on Recursive Early Exit Neural
  Networks","This paper presents a novel framework for goal-oriented semantic
communications leveraging recursive early exit models. The proposed approach is
built on two key components. First, we introduce an innovative early exit
strategy that dynamically partitions computations, enabling samples to be
offloaded to a server based on layer-wise recursive prediction dynamics that
detect samples for which the confidence is not increasing fast enough over
layers. Second, we develop a Reinforcement Learning-based online optimization
framework that jointly determines early exit points, computation splitting, and
offloading strategies, while accounting for wireless conditions, inference
accuracy, and resource costs. Numerical evaluations in an edge inference
scenario demonstrate the method's adaptability and effectiveness in striking an
excellent trade-off between performance, latency, and resource efficiency.",2024-12-27T11:14:11Z,http://arxiv.org/abs/2412.19587v1,"Jary Pomponi, Mattia Merluzzi, Alessio Devoto, Mateus Pontes Mota, Paolo Di Lorenzo, Simone Scardapane"
"Ultralight Signal Classification Model for Automatic Modulation
  Recognition","The growing complexity of radar signals demands responsive and accurate
detection systems that can operate efficiently on resource-constrained edge
devices. Existing models, while effective, often rely on substantial
computational resources and large datasets, making them impractical for edge
deployment. In this work, we propose an ultralight hybrid neural network
optimized for edge applications, delivering robust performance across
unfavorable signal-to-noise ratios (mean accuracy of 96.3% at 0 dB) using less
than 100 samples per class, and significantly reducing computational overhead.",2024-12-27T11:03:26Z,http://arxiv.org/abs/2412.19585v1,"Alessandro Daniele Genuardi Oquendo, Agustín Matías Galante Cerviño, Nilotpal Sinha, Luc Andrea, Sam Mugel, Román Orús"
"A Comparative Study of Machine Unlearning Techniques for Image and Text
  Classification Models","Machine Unlearning has emerged as a critical area in artificial intelligence,
addressing the need to selectively remove learned data from machine learning
models in response to data privacy regulations. This paper provides a
comprehensive comparative analysis of six state-of-theart unlearning techniques
applied to image and text classification tasks. We evaluate their performance,
efficiency, and compliance with regulatory requirements, highlighting their
strengths and limitations in practical scenarios. By systematically analyzing
these methods, we aim to provide insights into their applicability,
challenges,and tradeoffs, fostering advancements in the field of ethical and
adaptable machine learning.",2024-12-27T10:58:55Z,http://arxiv.org/abs/2412.19583v1,"Omar M. Safa, Mahmoud M. Abdelaziz, Mustafa Eltawy, Mohamed Mamdouh, Moamen Gharib, Salaheldin Eltenihy, Nagia M. Ghanem, Mohamed M. Ismail"
"An Actionable Hierarchical Scene Representation Enhancing Autonomous
  Inspection Missions in Unknown Environments","In this article, we present the Layered Semantic Graphs (LSG), a novel
actionable hierarchical scene graph, fully integrated with a multi-modal
mission planner, the FLIE: A First-Look based Inspection and Exploration
planner. The novelty of this work stems from aiming to address the task of
maintaining an intuitive and multi-resolution scene representation, while
simultaneously offering a tractable foundation for planning and scene
understanding during an ongoing inspection mission of apriori unknown
targets-of-interest in an unknown environment. The proposed LSG scheme is
composed of locally nested hierarchical graphs, at multiple layers of
abstraction, with the abstract concepts grounded on the functionality of the
integrated FLIE planner. Furthermore, LSG encapsulates real-time semantic
segmentation models that offer extraction and localization of desired semantic
elements within the hierarchical representation. This extends the capability of
the inspection planner, which can then leverage LSG to make an informed
decision to inspect a particular semantic of interest. We also emphasize the
hierarchical and semantic path-planning capabilities of LSG, which can extend
inspection missions by improving situational awareness for human operators in
an unknown environment. The validity of the proposed scheme is proven through
extensive evaluations of the proposed architecture in simulations, as well as
experimental field deployments on a Boston Dynamics Spot quadruped robot in
urban outdoor environment settings.",2024-12-27T10:57:17Z,http://arxiv.org/abs/2412.19582v1,"Vignesh Kottayam Viswanathan, Mario Alberto Valdes Saucedo, Sumeet Gajanan Satpute, Christoforos Kanellakis, George Nikolakopoulos"
Stamps and Mathematics,"This study examines the potential of using math-themed postage stamps in
mathematics lessons as a tool to engage students and integrate the subject with
history, art, and culture. Since the first mathematical stamps appeared in the
early 20th century, featuring prominent scholars like Carl Friedrich Gauss and
Isaac Newton, they serve not only as philatelic artifacts but also as
historical carriers of knowledge. The paper presents several practical projects
to interest students, such as creating their own math stamps, investigating the
price trends of math-themed stamps, and developing a timeline of mathematical
discoveries depicted in philatelic issues. The proposed projects develop
students' mathematical skills in areas such as percentage calculations, general
arithmetic, working with time intervals, and statistical analysis. Students can
analyze shapes, symmetry, and patterns on stamps, study principles of
proportion, and explore geometric figures. Using stamps broadens students'
horizons, providing an opportunity to become familiar with renowned
mathematicians from different eras, countries, and cultures. This also offers
students a new perspective on the subject, presenting mathematical discoveries
as part of the world's cultural heritage. Postage stamps dedicated to
mathematics can become a powerful tool for visualizing theoretical knowledge,
stimulating interest in mathematics, and encouraging independent research among
students.",2024-12-27T10:52:54Z,http://arxiv.org/abs/2412.19579v1,Nataliya M. Ivanova
"Graph-attention-based Casual Discovery with Trust Region-navigated
  Clipping Policy Optimization","In many domains of empirical sciences, discovering the causal structure
within variables remains an indispensable task. Recently, to tackle with
unoriented edges or latent assumptions violation suffered by conventional
methods, researchers formulated a reinforcement learning (RL) procedure for
causal discovery, and equipped REINFORCE algorithm to search for the
best-rewarded directed acyclic graph. The two keys to the overall performance
of the procedure are the robustness of RL methods and the efficient encoding of
variables. However, on the one hand, REINFORCE is prone to local convergence
and unstable performance during training. Neither trust region policy
optimization, being computationally-expensive, nor proximal policy optimization
(PPO), suffering from aggregate constraint deviation, is decent alternative for
combinatory optimization problems with considerable individual subactions. We
propose a trust region-navigated clipping policy optimization method for causal
discovery that guarantees both better search efficiency and steadiness in
policy optimization, in comparison with REINFORCE, PPO and our prioritized
sampling-guided REINFORCE implementation. On the other hand, to boost the
efficient encoding of variables, we propose a refined graph attention encoder
called SDGAT that can grasp more feature information without priori
neighbourhood information. With these improvements, the proposed method
outperforms former RL method in both synthetic and benchmark datasets in terms
of output results and optimization robustness.",2024-12-27T10:50:43Z,http://arxiv.org/abs/2412.19578v1,"Shixuan Liu, Yanghe Feng, Keyu Wu, Guangquan Cheng, Jincai Huang, Zhong Liu"
"Error estimate based adaptive quadrature for layer potentials over
  axisymmetric surfaces","Layer potentials represent solutions to partial differential equations in an
integral equation formulation. When numerically evaluating layer potentials at
evaluation points close to the domain boundary, specialized quadrature
techniques are required for accuracy because of rapid variations in the
integrand. To efficiently achieve a specified error tolerance, we introduce an
adaptive quadrature method with automatic parameter adjustment for axisymmetric
surfaces, facilitated by error estimation. Notably, while each surface must be
axisymmetric, the integrand itself need not be, allowing for applications with
complex geometries featuring multiple axisymmetric bodies.
  The proposed quadrature method utilizes so-called interpolatory
semi-analytical quadrature in conjunction with a singularity swap technique in
the azimuthal angle. In the polar angle, such a technique is used as needed,
depending on the integral kernel, combined with an adaptive subdivision of the
integration interval. The method is tied to a regular quadrature method that
employs a trapezoidal rule in the azimuthal angle and a Gauss-Legendre
quadrature rule in the polar angle, which will be used whenever deemed
sufficiently accurate, as determined by a quadrature error estimate [C.
Sorgentone and A.-K. Tornberg, Advances in Computational Mathematics, 49
(2023), p. 87].
  Error estimates for both numerical integration and interpolation are derived
using complex analysis, and are used to determine the adaptive panel
subdivision given the evaluation point and desired accuracy. Numerical examples
are presented to demonstrate the method's efficacy.",2024-12-27T10:44:34Z,http://arxiv.org/abs/2412.19575v1,"David Krantz, Anna-Karin Tornberg"
"Reinforced Label Denoising for Weakly-Supervised Audio-Visual Video
  Parsing","Audio-visual video parsing (AVVP) aims to recognize audio and visual event
labels with precise temporal boundaries, which is quite challenging since audio
or visual modality might include only one event label with only the overall
video labels available. Existing label denoising models often treat the
denoising process as a separate preprocessing step, leading to a disconnect
between label denoising and AVVP tasks. To bridge this gap, we present a novel
joint reinforcement learning-based label denoising approach (RLLD). This
approach enables simultaneous training of both label denoising and video
parsing models through a joint optimization strategy. We introduce a novel
AVVP-validation and soft inter-reward feedback mechanism that directly guides
the learning of label denoising policy. Extensive experiments on AVVP tasks
demonstrate the superior performance of our proposed method compared to label
denoising techniques. Furthermore, by incorporating our label denoising method
into other AVVP models, we find that it can further enhance parsing results.",2024-12-27T10:05:56Z,http://arxiv.org/abs/2412.19563v1,"Yongbiao Gao, Xiangcheng Sun, Guohua Lv, Deng Yu, Sijiu Niu"
"Hindsight Planner: A Closed-Loop Few-Shot Planner for Embodied
  Instruction Following","This work focuses on building a task planner for Embodied Instruction
Following (EIF) using Large Language Models (LLMs). Previous works typically
train a planner to imitate expert trajectories, treating this as a supervised
task. While these methods achieve competitive performance, they often lack
sufficient robustness. When a suboptimal action is taken, the planner may
encounter an out-of-distribution state, which can lead to task failure. In
contrast, we frame the task as a Partially Observable Markov Decision Process
(POMDP) and aim to develop a robust planner under a few-shot assumption. Thus,
we propose a closed-loop planner with an adaptation module and a novel
hindsight method, aiming to use as much information as possible to assist the
planner. Our experiments on the ALFRED dataset indicate that our planner
achieves competitive performance under a few-shot assumption. For the first
time, our few-shot agent's performance approaches and even surpasses that of
the full-shot supervised agent.",2024-12-27T10:05:45Z,http://arxiv.org/abs/2412.19562v1,"Yuxiao Yang, Shenao Zhang, Zhihan Liu, Huaxiu Yao, Zhaoran Wang"
"Structural Similarity in Deep Features: Image Quality Assessment Robust
  to Geometrically Disparate Reference","Image Quality Assessment (IQA) with references plays an important role in
optimizing and evaluating computer vision tasks. Traditional methods assume
that all pixels of the reference and test images are fully aligned. Such
Aligned-Reference IQA (AR-IQA) approaches fail to address many real-world
problems with various geometric deformations between the two images. Although
significant effort has been made to attack Geometrically-Disparate-Reference
IQA (GDR-IQA) problem, it has been addressed in a task-dependent fashion, for
example, by dedicated designs for image super-resolution and retargeting, or by
assuming the geometric distortions to be small that can be countered by
translation-robust filters or by explicit image registrations. Here we rethink
this problem and propose a unified, non-training-based Deep Structural
Similarity (DeepSSIM) approach to address the above problems in a single
framework, which assesses structural similarity of deep features in a simple
but efficient way and uses an attention calibration strategy to alleviate
attention deviation. The proposed method, without application-specific design,
achieves state-of-the-art performance on AR-IQA datasets and meanwhile shows
strong robustness to various GDR-IQA test cases. Interestingly, our test also
shows the effectiveness of DeepSSIM as an optimization tool for training image
super-resolution, enhancement and restoration, implying an even wider
generalizability. \footnote{Source code will be made public after the review is
completed.",2024-12-27T09:51:23Z,http://arxiv.org/abs/2412.19553v1,"Keke Zhang, Weiling Chen, Tiesong Zhao, Zhou Wang"
"Learning states enhanced knowledge tracing: Simulating the diversity in
  real-world learning process","The Knowledge Tracing (KT) task focuses on predicting a learner's future
performance based on the historical interactions. The knowledge state plays a
key role in learning process. However, considering that the knowledge state is
influenced by various learning factors in the interaction process, such as the
exercises similarities, responses reliability and the learner's learning state.
Previous models still face two major limitations. First, due to the exercises
differences caused by various complex reasons and the unreliability of
responses caused by guessing behavior, it is hard to locate the historical
interaction which is most relevant to the current answered exercise. Second,
the learning state is also a key factor to influence the knowledge state, which
is always ignored by previous methods. To address these issues, we propose a
new method named Learning State Enhanced Knowledge Tracing (LSKT). Firstly, to
simulate the potential differences in interactions, inspired by Item Response
Theory~(IRT) paradigm, we designed three different embedding methods ranging
from coarse-grained to fine-grained views and conduct comparative analysis on
them. Secondly, we design a learning state extraction module to capture the
changing learning state during the learning process of the learner. In turn,
with the help of the extracted learning state, a more detailed knowledge state
could be captured. Experimental results on four real-world datasets show that
our LSKT method outperforms the current state-of-the-art methods.",2024-12-27T09:41:25Z,http://arxiv.org/abs/2412.19550v1,"Shanshan Wang, Xueying Zhang, Keyang Wang, Xun Yang, Xingyi Zhang"
"Unprejudiced Training Auxiliary Tasks Makes Primary Better: A Multi-Task
  Learning Perspective","Human beings can leverage knowledge from relative tasks to improve learning
on a primary task. Similarly, multi-task learning methods suggest using
auxiliary tasks to enhance a neural network's performance on a specific primary
task. However, previous methods often select auxiliary tasks carefully but
treat them as secondary during training. The weights assigned to auxiliary
losses are typically smaller than the primary loss weight, leading to
insufficient training on auxiliary tasks and ultimately failing to support the
main task effectively. To address this issue, we propose an uncertainty-based
impartial learning method that ensures balanced training across all tasks.
Additionally, we consider both gradients and uncertainty information during
backpropagation to further improve performance on the primary task. Extensive
experiments show that our method achieves performance comparable to or better
than state-of-the-art approaches. Moreover, our weighting strategy is effective
and robust in enhancing the performance of the primary task regardless the
noise auxiliary tasks' pseudo labels.",2024-12-27T09:27:18Z,http://arxiv.org/abs/2412.19547v1,"Yuanze Li, Chun-Mei Feng, Qilong Wang, Guanglei Yang, Wangmeng Zuo"
Quantiles under ambiguity and risk sharing,"Choquet capacities and integrals are central concepts in decision making
under ambiguity or model uncertainty, pioneered by Schmeidler. Motivated by
risk optimization problems for quantiles under ambiguity, we study the subclass
of Choquet integrals, called Choquet quantiles, which generalizes the usual
(probabilistic) quantiles, also known as Value-at-Risk in finance, from
probabilities to capacities. Choquet quantiles share many features with
probabilistic quantiles, in terms of axiomatic representation, optimization
formulas, and risk sharing. We characterize Choquet quantiles via only one
axiom, called ordinality. We prove that the inf-convolution of Choquet
quantiles is again a Choquet quantile, leading to explicit optimal allocations
in risk sharing problems for quantile agents under ambiguity. A new class of
risk measures, Choquet Expected Shortfall, is introduced, which enjoys most
properties of the coherent risk measure Expected Shortfall. Our theory is
complemented by optimization algorithms, numerical examples, and a stylized
illustration with financial data.",2024-12-27T09:22:19Z,http://arxiv.org/abs/2412.19546v1,"Peng Liu, Tiantian Mao, Ruodu Wang"
"Enhancing Media Literacy: The Effectiveness of (Human) Annotations and
  Bias Visualizations on Bias Detection","Marking biased texts is a practical approach to increase media bias awareness
among news consumers. However, little is known about the generalizability of
such awareness to new topics or unmarked news articles, and the role of
machine-generated bias labels in enhancing awareness remains unclear. This
study tests how news consumers may be trained and pre-bunked to detect media
bias with bias labels obtained from different sources ( (Human or AI) and in
various manifestations. We conducted two experiments with 470 and 846
participants, exposing them to various bias-labeling conditions. We
subsequently tested how much bias they could identify in unlabeled news
materials on new topics. The results show that both Human (t(467) = 4.55, p &lt;
.001, d = 0.42) and AI labels (t(467) = 2.49, p = .039, d = 0.23) increased
correct detection compared to the control group. Human labels demonstrate
larger effect sizes and higher statistical significance. The control group
(t(467) = 4.51, p &lt; .001, d = 0.21) also improves performance through mere
exposure to study materials. We also find that participants trained with marked
biased phrases detected bias most reliably (F(834,1) = 44.00, p &lt; .001,
{\eta}2part = 0.048). Our experimental framework provides theoretical
implications for systematically assessing the generalizability of learning
effects in identifying media bias. These findings also provide practical
implications for developing news-reading platforms that offer bias indicators
and designing media literacy curricula to enhance media bias awareness.",2024-12-27T09:19:22Z,http://arxiv.org/abs/2412.19545v1,"Timo Spinde, Fei Wu, Wolfgang Gaissmaier, Gianluca Demartini, Helge Giese"
"TARGA: Targeted Synthetic Data Generation for Practical Reasoning over
  Structured Data","Semantic parsing, which converts natural language questions into logic forms,
plays a crucial role in reasoning within structured environments. However,
existing methods encounter two significant challenges: reliance on extensive
manually annotated datasets and limited generalization capability to unseen
examples. To tackle these issues, we propose Targeted Synthetic Data Generation
(TARGA), a practical framework that dynamically generates high-relevance
synthetic data without manual annotation. Starting from the pertinent entities
and relations of a given question, we probe for the potential relevant queries
through layer-wise expansion and cross-layer combination. Then we generate
corresponding natural language questions for these constructed queries to
jointly serve as the synthetic demonstrations for in-context learning.
Experiments on multiple knowledge base question answering (KBQA) datasets
demonstrate that TARGA, using only a 7B-parameter model, substantially
outperforms existing non-fine-tuned methods that utilize close-sourced model,
achieving notable improvements in F1 scores on GrailQA(+7.7) and
KBQA-Agent(+12.2). Furthermore, TARGA also exhibits superior sample efficiency,
robustness, and generalization capabilities under non-I.I.D. settings.",2024-12-27T09:16:39Z,http://arxiv.org/abs/2412.19544v1,"Xiang Huang, Jiayu Shen, Shanshan Huang, Sitao Cheng, Xiaxia Wang, Yuzhong Qu"
Diverse Rare Sample Generation with Pretrained GANs,"Deep generative models are proficient in generating realistic data but
struggle with producing rare samples in low density regions due to their
scarcity of training datasets and the mode collapse problem. While recent
methods aim to improve the fidelity of generated samples, they often reduce
diversity and coverage by ignoring rare and novel samples. This study proposes
a novel approach for generating diverse rare samples from high-resolution image
datasets with pretrained GANs. Our method employs gradient-based optimization
of latent vectors within a multi-objective framework and utilizes normalizing
flows for density estimation on the feature space. This enables the generation
of diverse rare images, with controllable parameters for rarity, diversity, and
similarity to a reference image. We demonstrate the effectiveness of our
approach both qualitatively and quantitatively across various datasets and GANs
without retraining or fine-tuning the pretrained GANs.",2024-12-27T09:10:30Z,http://arxiv.org/abs/2412.19543v1,"Subeen Lee, Jiyeon Han, Soyeon Kim, Jaesik Choi"
Interacted Object Grounding in Spatio-Temporal Human-Object Interactions,"Spatio-temporal Human-Object Interaction (ST-HOI) understanding aims at
detecting HOIs from videos, which is crucial for activity understanding.
However, existing whole-body-object interaction video benchmarks overlook the
truth that open-world objects are diverse, that is, they usually provide
limited and predefined object classes. Therefore, we introduce a new open-world
benchmark: Grounding Interacted Objects (GIO) including 1,098 interacted
objects class and 290K interacted object boxes annotation. Accordingly, an
object grounding task is proposed expecting vision systems to discover
interacted objects. Even though today's detectors and grounding methods have
succeeded greatly, they perform unsatisfactorily in localizing diverse and rare
objects in GIO. This profoundly reveals the limitations of current vision
systems and poses a great challenge. Thus, we explore leveraging
spatio-temporal cues to address object grounding and propose a 4D
question-answering framework (4D-QA) to discover interacted objects from
diverse videos. Our method demonstrates significant superiority in extensive
experiments compared to current baselines. Data and code will be publicly
available at https://github.com/DirtyHarryLYL/HAKE-AVA.",2024-12-27T09:08:46Z,http://arxiv.org/abs/2412.19542v1,"Xiaoyang Liu, Boran Wen, Xinpeng Liu, Zizheng Zhou, Hongwei Fan, Cewu Lu, Lizhuang Ma, Yulong Chen, Yong-Lu Li"
"Efficient Computation of the Non-convex Quasi-norm Ball Projection with
  Iterative Reweighted Approach","In this study, we focus on computing the projection onto the $\ell_p$
quasi-norm ball, which is challenging due to the non-convex and non-Lipschitz
nature inherent in the $\ell_p$ quasi-norm with $0&lt;p&lt;1$. We propose a novel
localized approximation method that yields a Lipschitz continuous concave
surrogate function for the $\ell_p$ quasi-norm with improved approximation
quality. Building on this approximation, we enhance the state-of-the-art
iterative reweighted algorithm proposed by Yang et al. (J Mach Learn Res
23:1-31, 2022) by constructing tighter subproblems. This improved algorithm
solves the $\ell_p$ quasinorm ball projection problem through a series of
tractable projections onto the weighted $\ell_1$ norm balls. Convergence
analyses and numerical studies demonstrate the global convergence and superior
computational efficiency of the proposed method.",2024-12-27T09:07:56Z,http://arxiv.org/abs/2412.19541v1,"Qi An, Jiao Wang, Zequn Niu, Nana Zhang"
"Scalable Hierarchical Reinforcement Learning for Hyper Scale Multi-Robot
  Task Planning","To improve the efficiency of warehousing system and meet huge customer
orders, we aim to solve the challenges of dimension disaster and dynamic
properties in hyper scale multi-robot task planning (MRTP) for robotic mobile
fulfillment system (RMFS). Existing research indicates that hierarchical
reinforcement learning (HRL) is an effective method to reduce these challenges.
Based on that, we construct an efficient multi-stage HRL-based multi-robot task
planner for hyper scale MRTP in RMFS, and the planning process is represented
with a special temporal graph topology. To ensure optimality, the planner is
designed with a centralized architecture, but it also brings the challenges of
scaling up and generalization that require policies to maintain performance for
various unlearned scales and maps. To tackle these difficulties, we first
construct a hierarchical temporal attention network (HTAN) to ensure basic
ability of handling inputs with unfixed lengths, and then design multi-stage
curricula for hierarchical policy learning to further improve the scaling up
and generalization ability while avoiding catastrophic forgetting.
Additionally, we notice that policies with hierarchical structure suffer from
unfair credit assignment that is similar to that in multi-agent reinforcement
learning, inspired of which, we propose a hierarchical reinforcement learning
algorithm with counterfactual rollout baseline to improve learning performance.
Experimental results demonstrate that our planner outperform other
state-of-the-art methods on various MRTP instances in both simulated and
real-world RMFS. Also, our planner can successfully scale up to hyper scale
MRTP instances in RMFS with up to 200 robots and 1000 retrieval racks on
unlearned maps while keeping superior performance over other methods.",2024-12-27T09:07:11Z,http://arxiv.org/abs/2412.19538v1,"Xuan Zhou, Xiang Shi, Lele Zhang, Chen Chen, Hongbo Li, Lin Ma, Fang Deng, Jie Chen"
"Finger in Camera Speaks Everything: Unconstrained Air-Writing for
  Real-World","Air-writing is a challenging task that combines the fields of computer vision
and natural language processing, offering an intuitive and natural approach for
human-computer interaction. However, current air-writing solutions face two
primary challenges: (1) their dependency on complex sensors (e.g., Radar, EEGs
and others) for capturing precise handwritten trajectories, and (2) the absence
of a video-based air-writing dataset that covers a comprehensive vocabulary
range. These limitations impede their practicality in various real-world
scenarios, including the use on devices like iPhones and laptops. To tackle
these challenges, we present the groundbreaking air-writing Chinese character
video dataset (AWCV-100K-UCAS2024), serving as a pioneering benchmark for
video-based air-writing. This dataset captures handwritten trajectories in
various real-world scenarios using commonly accessible RGB cameras, eliminating
the need for complex sensors. AWCV-100K-UCAS2024 includes 8.8 million video
frames, encompassing the complete set of 3,755 characters from the GB2312-80
level-1 set (GB1). Furthermore, we introduce our baseline approach, the
video-based character recognizer (VCRec). VCRec adeptly extracts fingertip
features from sparse visual cues and employs a spatio-temporal sequence module
for analysis. Experimental results showcase the superior performance of VCRec
compared to existing models in recognizing air-written characters, both
quantitatively and qualitatively. This breakthrough paves the way for enhanced
human-computer interaction in real-world contexts. Moreover, our approach
leverages affordable RGB cameras, enabling its applicability in a diverse range
of scenarios. The code and data examples will be made public at
https://github.com/wmeiqi/AWCV.",2024-12-27T09:04:04Z,http://arxiv.org/abs/2412.19537v1,"Meiqi Wu, Kaiqi Huang, Yuanqiang Cai, Shiyu Hu, Yuzhong Zhao, Weiqiang Wang"
"P3S-Diffusion:A Selective Subject-driven Generation Framework via Point
  Supervision","Recent research in subject-driven generation increasingly emphasizes the
importance of selective subject features. Nevertheless, accurately selecting
the content in a given reference image still poses challenges, especially when
selecting the similar subjects in an image (e.g., two different dogs). Some
methods attempt to use text prompts or pixel masks to isolate specific
elements. However, text prompts often fall short in precisely describing
specific content, and pixel masks are often expensive. To address this, we
introduce P3S-Diffusion, a novel architecture designed for context-selected
subject-driven generation via point supervision. P3S-Diffusion leverages
minimal cost label (e.g., points) to generate subject-driven images. During
fine-tuning, it can generate an expanded base mask from these points, obviating
the need for additional segmentation models. The mask is employed for
inpainting and aligning with subject representation. The P3S-Diffusion
preserves fine features of the subjects through Multi-layers Condition
Injection. Enhanced by the Attention Consistency Loss for improved training,
extensive experiments demonstrate its excellent feature preservation and image
generation capabilities.",2024-12-27T08:59:01Z,http://arxiv.org/abs/2412.19533v1,"Junjie Hu, Shuyong Gao, Lingyi Hong, Qishan Wang, Yuzhou Zhao, Yan Wang, Wenqiang Zhang"
Is Your Text-to-Image Model Robust to Caption Noise?,"In text-to-image (T2I) generation, a prevalent training technique involves
utilizing Vision Language Models (VLMs) for image re-captioning. Even though
VLMs are known to exhibit hallucination, generating descriptive content that
deviates from the visual reality, the ramifications of such caption
hallucinations on T2I generation performance remain under-explored. Through our
empirical investigation, we first establish a comprehensive dataset comprising
VLM-generated captions, and then systematically analyze how caption
hallucination influences generation outcomes. Our findings reveal that (1) the
disparities in caption quality persistently impact model outputs during
fine-tuning. (2) VLMs confidence scores serve as reliable indicators for
detecting and characterizing noise-related patterns in the data distribution.
(3) even subtle variations in caption fidelity have significant effects on the
quality of learned representations. These findings collectively emphasize the
profound impact of caption quality on model performance and highlight the need
for more sophisticated robust training algorithm in T2I. In response to these
observations, we propose a approach leveraging VLM confidence score to mitigate
caption noise, thereby enhancing the robustness of T2I models against
hallucination in caption.",2024-12-27T08:53:37Z,http://arxiv.org/abs/2412.19531v1,"Weichen Yu, Ziyan Yang, Shanchuan Lin, Qi Zhao, Jianyi Wang, Liangke Gui, Matt Fredrikson, Lu Jiang"
"The Value of AI Advice: Personalized and Value-Maximizing AI Advisors
  Are Necessary to Reliably Benefit Experts and Organizations","Despite advances in AI's performance and interpretability, AI advisors can
undermine experts' decisions and increase the time and effort experts must
invest to make decisions. Consequently, AI systems deployed in high-stakes
settings often fail to consistently add value across contexts and can even
diminish the value that experts alone provide. Beyond harm in specific domains,
such outcomes impede progress in research and practice, underscoring the need
to understand when and why different AI advisors add or diminish value. To
bridge this gap, we stress the importance of assessing the value AI advice
brings to real-world contexts when designing and evaluating AI advisors.
Building on this perspective, we characterize key pillars -- pathways through
which AI advice impacts value -- and develop a framework that incorporates
these pillars to create reliable, personalized, and value-adding advisors. Our
results highlight the need for system-level, value-driven development of AI
advisors that advise selectively, are tailored to experts' unique behaviors,
and are optimized for context-specific trade-offs between decision improvements
and advising costs. They also reveal how the lack of inclusion of these pillars
in the design of AI advising systems may be contributing to the failures
observed in practical applications.",2024-12-27T08:50:54Z,http://arxiv.org/abs/2412.19530v1,"Nicholas Wolczynski, Maytal Saar-Tsechansky, Tong Wang"
"Nonconvex Stochastic Optimization under Heavy-Tailed Noises: Optimal
  Convergence without Gradient Clipping","Recently, the study of heavy-tailed noises in first-order nonconvex
stochastic optimization has gotten a lot of attention since it was recognized
as a more realistic condition as suggested by many empirical observations.
Specifically, the stochastic noise (the difference between the stochastic and
true gradient) is considered only to have a finite $\mathfrak{p}$-th moment
where $\mathfrak{p}\in\left(1,2\right]$ instead of assuming it always satisfies
the classical finite variance assumption. To deal with this more challenging
setting, people have proposed different algorithms and proved them to converge
at an optimal $\mathcal{O}(T^{\frac{1-\mathfrak{p}}{3\mathfrak{p}-2}})$ rate
for smooth objectives after $T$ iterations. Notably, all these new-designed
algorithms are based on the same technique - gradient clipping. Naturally, one
may want to know whether the clipping method is a necessary ingredient and the
only way to guarantee convergence under heavy-tailed noises. In this work, by
revisiting the existing Batched Normalized Stochastic Gradient Descent with
Momentum (Batched NSGDM) algorithm, we provide the first convergence result
under heavy-tailed noises but without gradient clipping. Concretely, we prove
that Batched NSGDM can achieve the optimal
$\mathcal{O}(T^{\frac{1-\mathfrak{p}}{3\mathfrak{p}-2}})$ rate even under the
relaxed smooth condition. More interestingly, we also establish the first
$\mathcal{O}(T^{\frac{1-\mathfrak{p}}{2\mathfrak{p}}})$ convergence rate in the
case where the tail index $\mathfrak{p}$ is unknown in advance, which is
arguably the common scenario in practice.",2024-12-27T08:46:46Z,http://arxiv.org/abs/2412.19529v1,"Zijian Liu, Zhengyuan Zhou"
Magnon-Phonon Coupling in Layered Antiferromagnet,"We present a fully analytical model of hybridization between magnon, and
phonons observed experimentally in magneto-Raman scattering in van der Waals
(vdW) antiferromagnets (AFM). Here, the representative material, FePS3, has
been shown to be a quasi-two-dimensional-Ising antiferromagnet, with additional
features of spin-phonon coupling in the Raman spectra emerging below the N\'eel
temperature (TN) of approximately 120 K. Using magneto-Raman spectroscopy as an
optical probe of magnetic structure, we show that one of these Raman-active
modes in the magnetically ordered state is a magnon with a frequency of 3.7 THz
(~ 122 cm-1). In addition, one magnon band and three phonon bands are coupled
via the magneto-elastic coupling evidenced by anti-crossing in the complete
spectra. We consider a simple model involving only in-plane nearest neighbor
exchange couplings (designed to give rise to a similar magnetic structure) and
perpendicular anisotropy in presence of an out-of-plane magnetic field. Exact
diagonalization of the Hamiltonian leads to energy bands which show that the
interaction term gives rise to avoided crossings between the hybridized magnon
and phonon branches. Realizing magnon-phonon coupling in two-dimensional (2D)
AFMs is important for the verification of the theoretical predictions on exotic
quantum transport phenomena like spin-caloritronics, topological magnonics,
etc.",2024-12-27T08:38:33Z,http://arxiv.org/abs/2412.19526v1,"Somsubhra Ghosh, Mainak Palit, Sujan Maity, Subhadeep Datta"
"Lévy Score Function and Score-Based Particle Algorithm for Nonlinear
  Lévy--Fokker--Planck Equations","The score function for the diffusion process, also known as the gradient of
the log-density, is a basic concept to characterize the probability flow with
important applications in the score-based diffusion generative modelling and
the simulation of It\^{o} stochastic differential equations. However, neither
the probability flow nor the corresponding score function for the
diffusion-jump process are known. This paper delivers mathematical derivation,
numerical algorithm, and error analysis focusing on the corresponding score
function in non-Gaussian systems with jumps and discontinuities represented by
the nonlinear L\'{e}vy--Fokker--Planck equations. We propose the L\'{e}vy score
function for such stochastic equations, which features a nonlocal
double-integral term, and we develop its training algorithm by minimizing the
proposed loss function from samples. Based on the equivalence of the
probability flow with deterministic dynamics, we develop a self-consistent
score-based transport particle algorithm to sample the interactive L\'{e}vy
stochastic process at discrete time grid points. We provide error bound for the
Kullback--Leibler divergence between the numerical and true probability density
functions by overcoming the nonlocal challenges in the L\'{e}vy score. The full
error analysis with the Monte Carlo error and the time discretization error is
furthermore established. To show the usefulness and efficiency of our approach,
numerical examples from applications in biology and finance are tested.",2024-12-27T08:23:04Z,http://arxiv.org/abs/2412.19520v1,"Yuanfei Huang, Chengyu Liu, Xiang Zhou"
"Dust to Tower: Coarse-to-Fine Photo-Realistic Scene Reconstruction from
  Sparse Uncalibrated Images","Photo-realistic scene reconstruction from sparse-view, uncalibrated images is
highly required in practice. Although some successes have been made, existing
methods are either Sparse-View but require accurate camera parameters (i.e.,
intrinsic and extrinsic), or SfM-free but need densely captured images. To
combine the advantages of both methods while addressing their respective
weaknesses, we propose Dust to Tower (D2T), an accurate and efficient
coarse-to-fine framework to optimize 3DGS and image poses simultaneously from
sparse and uncalibrated images. Our key idea is to first construct a coarse
model efficiently and subsequently refine it using warped and inpainted images
at novel viewpoints. To do this, we first introduce a Coarse Construction
Module (CCM) which exploits a fast Multi-View Stereo model to initialize a 3D
Gaussian Splatting (3DGS) and recover initial camera poses. To refine the 3D
model at novel viewpoints, we propose a Confidence Aware Depth Alignment (CADA)
module to refine the coarse depth maps by aligning their confident parts with
estimated depths by a Mono-depth model. Then, a Warped Image-Guided Inpainting
(WIGI) module is proposed to warp the training images to novel viewpoints by
the refined depth maps, and inpainting is applied to fulfill the ``holes"" in
the warped images caused by view-direction changes, providing high-quality
supervision to further optimize the 3D model and the camera poses. Extensive
experiments and ablation studies demonstrate the validity of D2T and its design
choices, achieving state-of-the-art performance in both tasks of novel view
synthesis and pose estimation while keeping high efficiency. Codes will be
publicly available.",2024-12-27T08:19:34Z,http://arxiv.org/abs/2412.19518v1,"Xudong Cai, Yongcai Wang, Zhaoxin Fan, Deng Haoran, Shuo Wang, Wanting Li, Deying Li, Lun Luo, Minhang Wang, Jintao Xu"
"Estimation of System Parameters Including Repeated Cross-Sectional Data
  through Emulator-Informed Deep Generative Model","Differential equations (DEs) are crucial for modeling the evolution of
natural or engineered systems. Traditionally, the parameters in DEs are
adjusted to fit data from system observations. However, in fields such as
politics, economics, and biology, available data are often independently
collected at distinct time points from different subjects (i.e., repeated
cross-sectional (RCS) data). Conventional optimization techniques struggle to
accurately estimate DE parameters when RCS data exhibit various
heterogeneities, leading to a significant loss of information. To address this
issue, we propose a new estimation method called the emulator-informed
deep-generative model (EIDGM), designed to handle RCS data. Specifically, EIDGM
integrates a physics-informed neural network-based emulator that immediately
generates DE solutions and a Wasserstein generative adversarial network-based
parameter generator that can effectively mimic the RCS data. We evaluated EIDGM
on exponential growth, logistic population models, and the Lorenz system,
demonstrating its superior ability to accurately capture parameter
distributions. Additionally, we applied EIDGM to an experimental dataset of
Amyloid beta 40 and beta 42, successfully capturing diverse parameter
distribution shapes. This shows that EIDGM can be applied to model a wide range
of systems and extended to uncover the operating principles of systems based on
limited data.",2024-12-27T08:19:23Z,http://arxiv.org/abs/2412.19517v1,"Hyunwoo Cho, Sung Woong Cho, Hyeontae Jo, Hyung Ju Hwang"
"Real-time classification of EEG signals using Machine Learning
  deployment","The prevailing educational methods predominantly rely on traditional
classroom instruction or online delivery, often limiting the teachers' ability
to engage effectively with all the students simultaneously. A more intrinsic
method of evaluating student attentiveness during lectures can enable the
educators to tailor the course materials and their teaching styles in order to
better meet the students' needs. The aim of this paper is to enhance teaching
quality in real time, thereby fostering a higher student engagement in the
classroom activities. By monitoring the students' electroencephalography (EEG)
signals and employing machine learning algorithms, this study proposes a
comprehensive solution for addressing this challenge. Machine learning has
emerged as a powerful tool for simplifying the analysis of complex variables,
enabling the effective assessment of the students' concentration levels based
on specific parameters. However, the real-time impact of machine learning
models necessitates a careful consideration as their deployment is concerned.
This study proposes a machine learning-based approach for predicting the level
of students' comprehension with regard to a certain topic. A browser interface
was introduced that accesses the values of the system's parameters to determine
a student's level of concentration on a chosen topic. The deployment of the
proposed system made it necessary to address the real-time challenges faced by
the students, consider the system's cost, and establish trust in its efficacy.
This paper presents the efforts made for approaching this pertinent issue
through the implementation of innovative technologies and provides a framework
for addressing key considerations for future research directions.",2024-12-27T08:14:28Z,http://arxiv.org/abs/2412.19515v1,"Swati Chowdhuri, Satadip Saha, Samadrita Karmakar, Ankur Chanda"
"Confidence v.s. Critique: A Decomposition of Self-Correction Capability
  for LLMs","Large Language Models (LLMs) can correct their self-generated responses, but
a decline in accuracy after self-correction is also witnessed. To have a deeper
understanding of self-correction, we endeavor to decompose, evaluate, and
analyze the self-correction behaviors of LLMs. By enumerating and analyzing
answer correctness before and after self-correction, we decompose the
self-correction capability into confidence (being confident to correct answers)
and critique (turning wrong answers to correct) capabilities, and propose two
metrics from a probabilistic perspective to measure these 2 capabilities, along
with another metric for overall self-correction capability evaluation. Based on
our decomposition and evaluation metrics, we conduct extensive experiments and
draw some empirical conclusions. For example, we find different models can
exhibit distinct behaviors: some models are confident while others are more
critical. We also find the trade-off between the two capabilities (i.e.
improving one can lead to a decline in the other) when manipulating model
self-correction behavior by prompts or in-context learning. Further, we find a
simple yet efficient strategy to improve self-correction capability by
transforming Supervision Fine-Tuning (SFT) data format, and our strategy
outperforms vanilla SFT in both capabilities and achieves much higher accuracy
after self-correction. Our code will be publicly available on GitHub.",2024-12-27T08:09:11Z,http://arxiv.org/abs/2412.19513v1,"Zhe Yang, Yichang Zhang, Yudong Wang, Ziyao Xu, Junyang Lin, Zhifang Sui"
"Uncertainty quantification for improving radiomic-based models in
  radiation pneumonitis prediction","Background and Objective: Radiation pneumonitis (RP) is a side effect of
thoracic radiation therapy. Recently, Machine learning (ML) models enhanced
with radiomic and dosiomic features provide better predictions by incorporating
spatial information beyond DVHs. However, to improve the clinical decision
process, we propose to use uncertainty quantification (UQ) to improve the
confidence in model prediction. This study evaluates the impact of post hoc UQ
methods on the discriminative performance and calibration of ML models for RP
prediction. Methods: This study evaluated four ML models: logistic regression
(LR), support vector machines (SVM), extreme gradient boosting (XGB), and
random forest (RF), using radiomic, dosiomic, and dosimetric features to
predict RP. We applied UQ methods, including Patt scaling, isotonic regression,
Venn-ABERS predictor, and Conformal Prediction, to quantify uncertainty. Model
performance was assessed through Area Under the Receiver Operating
Characteristic curve (AUROC), Area Under the Precision-Recall Curve (AUPRC),
and Adaptive Calibration Error (ACE) using Leave-One-Out Cross-Validation
(LOO-CV). Results: UQ methods enhanced predictive performance, particularly for
high-certainty predictions, while also improving calibration. Radiomic and
dosiomic features increased model accuracy but introduced calibration
challenges, especially for non-linear models like XGB and RF. Performance gains
from UQ methods were most noticeable at higher certainty thresholds.
Conclusion: Integrating UQ into ML models with radiomic and dosiomic features
improves both predictive accuracy and calibration, supporting more reliable
clinical decision-making. The findings emphasize the value of UQ methods in
enhancing applicability of predictive models for RP in healthcare settings.",2024-12-27T08:01:42Z,http://arxiv.org/abs/2412.19511v1,"Chanon Puttanawarut, Romen Samuel Wabina, Nat Sirirutbunkajorn"
"Parameter Efficient Fine-Tuning for Deep Learning-Based Full-Waveform
  Inversion","Seismic full waveform inversion (FWI) has seen promising advancements through
deep learning. Existing approaches typically focus on task-specific models
trained and evaluated in isolation that lead to limited generalization across
different geological scenarios. In this work we introduce a task-agnostic
foundational model for FWI that captures general features across tasks. We
first demonstrate that full fine-tuning of this foundational model outperforms
task-specific models built from scratch by delivering superior performance
across multiple benchmarks. Building upon this we employ parameter-efficient
fine-tuning (PEFT) to further reduce computational overhead. By fine-tuning
only a small fraction of the model parameters PEFT achieves comparable results
to full fine-tuning while significantly lowering memory and computational
requirements. Additionally, PEFT excels in out-of-distribution tasks where it
outperforms both full fine-tuning and task-specific models. These findings
establish the value of foundational modeling for FWI and highlight PEFT as an
effective strategy for efficient and scalable adaptation across diverse tasks.",2024-12-27T08:00:12Z,http://arxiv.org/abs/2412.19510v1,"Koustav Ghosal, Abhranta Panigrahi, Arnav Chavan, ArunSingh, Deepak Gupta"
Hybrid Local Causal Discovery,"Local causal discovery aims to learn and distinguish the direct causes and
effects of a target variable from observed data. Existing constraint-based
local causal discovery methods use AND or OR rules in constructing the local
causal skeleton, but using either rule alone is prone to produce cascading
errors in the learned local causal skeleton, and thus impacting the inference
of local causal relationships. On the other hand, directly applying score-based
global causal discovery methods to local causal discovery may randomly return
incorrect results due to the existence of local equivalence classes. To address
the above issues, we propose a Hybrid Local Causal Discovery algorithm, called
HLCD. Specifically, HLCD initially utilizes a constraint-based approach
combined with the OR rule to obtain a candidate skeleton and then employs a
score-based method to eliminate redundant portions in the candidate skeleton.
Furthermore, during the local causal orientation phase, HLCD distinguishes
between V-structures and equivalence classes by comparing the local structure
scores between the two, thereby avoiding orientation interference caused by
local equivalence classes. We conducted extensive experiments with seven
state-of-the-art competitors on 14 benchmark Bayesian network datasets, and the
experimental results demonstrate that HLCD significantly outperforms existing
local causal discovery algorithms.",2024-12-27T07:53:59Z,http://arxiv.org/abs/2412.19507v1,"Zhaolong Ling, Honghui Peng, Yiwen Zhang, Peng Zhou, Xingyu Wu, Kui Yu, Xindong Wu"
"DrivingWorld: ConstructingWorld Model for Autonomous Driving via Video
  GPT","Recent successes in autoregressive (AR) generation models, such as the GPT
series in natural language processing, have motivated efforts to replicate this
success in visual tasks. Some works attempt to extend this approach to
autonomous driving by building video-based world models capable of generating
realistic future video sequences and predicting ego states. However, prior
works tend to produce unsatisfactory results, as the classic GPT framework is
designed to handle 1D contextual information, such as text, and lacks the
inherent ability to model the spatial and temporal dynamics essential for video
generation. In this paper, we present DrivingWorld, a GPT-style world model for
autonomous driving, featuring several spatial-temporal fusion mechanisms. This
design enables effective modeling of both spatial and temporal dynamics,
facilitating high-fidelity, long-duration video generation. Specifically, we
propose a next-state prediction strategy to model temporal coherence between
consecutive frames and apply a next-token prediction strategy to capture
spatial information within each frame. To further enhance generalization
ability, we propose a novel masking strategy and reweighting strategy for token
prediction to mitigate long-term drifting issues and enable precise control.
Our work demonstrates the ability to produce high-fidelity and consistent video
clips of over 40 seconds in duration, which is over 2 times longer than
state-of-the-art driving world models. Experiments show that, in contrast to
prior works, our method achieves superior visual quality and significantly more
accurate controllable future video generation. Our code is available at
https://github.com/YvanYin/DrivingWorld.",2024-12-27T07:44:07Z,http://arxiv.org/abs/2412.19505v1,"Xiaotao Hu, Wei Yin, Mingkai Jia, Junyuan Deng, Xiaoyang Guo, Qian Zhang, Xiaoxiao Long, Ping Tan"
Hear the Scene: Audio-Enhanced Text Spotting,"Recent advancements in scene text spotting have focused on end-to-end
methodologies that heavily rely on precise location annotations, which are
often costly and labor-intensive to procure. In this study, we introduce an
innovative approach that leverages only transcription annotations for training
text spotting models, substantially reducing the dependency on elaborate
annotation processes. Our methodology employs a query-based paradigm that
facilitates the learning of implicit location features through the interaction
between text queries and image embeddings. These features are later refined
during the text recognition phase using an attention activation map. Addressing
the challenges associated with training a weakly-supervised model from scratch,
we implement a circular curriculum learning strategy to enhance model
convergence. Additionally, we introduce a coarse-to-fine cross-attention
localization mechanism for more accurate text instance localization. Notably,
our framework supports audio-based annotation, which significantly diminishes
annotation time and provides an inclusive alternative for individuals with
disabilities. Our approach achieves competitive performance against existing
benchmarks, demonstrating that high accuracy in text spotting can be attained
without extensive location annotations.",2024-12-27T07:44:05Z,http://arxiv.org/abs/2412.19504v1,"Jing Li, Bo Wang"
"RobotDiffuse: Motion Planning for Redundant Manipulator based on
  Diffusion Model","Redundant manipulators, with their higher Degrees of Freedom (DOFs), offer
enhanced kinematic performance and versatility, making them suitable for
applications like manufacturing, surgical robotics, and human-robot
collaboration. However, motion planning for these manipulators is challenging
due to increased DOFs and complex, dynamic environments. While traditional
motion planning algorithms struggle with high-dimensional spaces, deep
learning-based methods often face instability and inefficiency in complex
tasks. This paper introduces RobotDiffuse, a diffusion model-based approach for
motion planning in redundant manipulators. By integrating physical constraints
with a point cloud encoder and replacing the U-Net structure with an
encoder-only transformer, RobotDiffuse improves the model's ability to capture
temporal dependencies and generate smoother, more coherent motion plans. We
validate the approach using a complex simulator, and release a new dataset with
35M robot poses and 0.14M obstacle avoidance scenarios. Experimental results
demonstrate the effectiveness of RobotDiffuse and the promise of diffusion
models for motion planning tasks. The code can be accessed at
https://github.com/ACRoboT-buaa/RobotDiffuse.",2024-12-27T07:34:54Z,http://arxiv.org/abs/2412.19500v1,"Xiaohan Zhang, Xudong Mou, Rui Wang, Tianyu Wo, Ningbo Gu, Tiejun Wang, Cangbai Xu, Xudong Liu"
Casevo: A Cognitive Agents and Social Evolution Simulator,"In this paper, we introduce a multi-agent simulation framework Casevo
(Cognitive Agents and Social Evolution Simulator), that integrates large
language models (LLMs) to simulate complex social phenomena and decision-making
processes. Casevo is designed as a discrete-event simulator driven by agents
with features such as Chain of Thoughts (CoT), Retrieval-Augmented Generation
(RAG), and Customizable Memory Mechanism. Casevo enables dynamic social
modeling, which can support various scenarios such as social network analysis,
public opinion dynamics, and behavior prediction in complex social systems. To
demonstrate the effectiveness of Casevo, we utilize one of the U.S. 2020
midterm election TV debates as a simulation example. Our results show that
Casevo facilitates more realistic and flexible agent interactions, improving
the quality of dynamic social phenomena simulation. This work contributes to
the field by providing a robust system for studying large-scale, high-fidelity
social behaviors with advanced LLM-driven agents, expanding the capabilities of
traditional agent-based modeling (ABM). The open-source code repository address
of casevo is https://github.com/rgCASS/casevo.",2024-12-27T07:33:49Z,http://arxiv.org/abs/2412.19498v1,"Zexun Jiang, Yafang Shi, Maoxu Li, Hongjiang Xiao, Yunxiao Qin, Qinglan Wei, Ye Wang, Yuan Zhang"
"Disparate Model Performance and Stability in Machine Learning Clinical
  Support for Diabetes and Heart Diseases","Machine Learning (ML) algorithms are vital for supporting clinical
decision-making in biomedical informatics. However, their predictive
performance can vary across demographic groups, often due to the
underrepresentation of historically marginalized populations in training
datasets. The investigation reveals widespread sex- and age-related inequities
in chronic disease datasets and their derived ML models. Thus, a novel
analytical framework is introduced, combining systematic arbitrariness with
traditional metrics like accuracy and data complexity. The analysis of data
from over 25,000 individuals with chronic diseases revealed mild sex-related
disparities, favoring predictive accuracy for males, and significant
age-related differences, with better accuracy for younger patients. Notably,
older patients showed inconsistent predictive accuracy across seven datasets,
linked to higher data complexity and lower model performance. This highlights
that representativeness in training data alone does not guarantee equitable
outcomes, and model arbitrariness must be addressed before deploying models in
clinical settings.",2024-12-27T07:31:14Z,http://arxiv.org/abs/2412.19495v1,"Ioannis Bilionis, Ricardo C. Berrios, Luis Fernandez-Luque, Carlos Castillo"
Towards Open-Vocabulary Remote Sensing Image Semantic Segmentation,"Recently, deep learning based methods have revolutionized remote sensing
image segmentation. However, these methods usually rely on a pre-defined
semantic class set, thus needing additional image annotation and model training
when adapting to new classes. More importantly, they are unable to segment
arbitrary semantic classes. In this work, we introduce Open-Vocabulary Remote
Sensing Image Semantic Segmentation (OVRSISS), which aims to segment arbitrary
semantic classes in remote sensing images. To address the lack of OVRSISS
datasets, we develop LandDiscover50K, a comprehensive dataset of 51,846 images
covering 40 diverse semantic classes. In addition, we propose a novel framework
named GSNet that integrates domain priors from special remote sensing models
and versatile capabilities of general vision-language models. Technically,
GSNet consists of a Dual-Stream Image Encoder (DSIE), a Query-Guided Feature
Fusion (QGFF), and a Residual Information Preservation Decoder (RIPD). DSIE
first captures comprehensive features from both special models and general
models in dual streams. Then, with the guidance of variable vocabularies, QGFF
integrates specialist and generalist features, enabling them to complement each
other. Finally, RIPD is proposed to aggregate multi-source features for more
accurate mask predictions. Experiments show that our method outperforms other
methods by a large margin, and our proposed LandDiscover50K improves the
performance of OVRSISS methods. The proposed dataset and method will be made
publicly available at https://github.com/yecy749/GSNet.",2024-12-27T07:20:30Z,http://arxiv.org/abs/2412.19492v1,"Chengyang Ye, Yunzhi Zhuge, Pingping Zhang"
"Multi-label Classification using Deep Multi-order Context-aware Kernel
  Networks","Multi-label classification is a challenging task in pattern recognition. Many
deep learning methods have been proposed and largely enhanced classification
performance. However, most of the existing sophisticated methods ignore context
in the models' learning process. Since context may provide additional cues to
the learned models, it may significantly boost classification performances. In
this work, we make full use of context information (namely geometrical
structure of images) in order to learn better context-aware similarities
(a.k.a. kernels) between images. We reformulate context-aware kernel design as
a feed-forward network that outputs explicit kernel mapping features. Our
obtained context-aware kernel network further leverages multiple orders of
patch neighbors within different distances, resulting into a more
discriminating Deep Multi-order Context-aware Kernel Network (DMCKN) for
multi-label classification. We evaluate the proposed method on the challenging
Corel5K and NUS-WIDE benchmarks, and empirical results show that our method
obtains competitive performances against the related state-of-the-art, and both
quantitative and qualitative performances corroborate its effectiveness and
superiority for multi-label image classification.",2024-12-27T07:16:11Z,http://arxiv.org/abs/2412.19491v1,"Mingyuan Jiu, Hailong Zhu, Hichem Sahbi"
UniBrain: A Unified Model for Cross-Subject Brain Decoding,"Brain decoding aims to reconstruct original stimuli from fMRI signals,
providing insights into interpreting mental content. Current approaches rely
heavily on subject-specific models due to the complex brain processing
mechanisms and the variations in fMRI signals across individuals. Therefore,
these methods greatly limit the generalization of models and fail to capture
cross-subject commonalities. To address this, we present UniBrain, a unified
brain decoding model that requires no subject-specific parameters. Our approach
includes a group-based extractor to handle variable fMRI signal lengths, a
mutual assistance embedder to capture cross-subject commonalities, and a
bilevel feature alignment scheme for extracting subject-invariant features. We
validate our UniBrain on the brain decoding benchmark, achieving comparable
performance to current state-of-the-art subject-specific models with extremely
fewer parameters. We also propose a generalization benchmark to encourage the
community to emphasize cross-subject commonalities for more general brain
decoding. Our code is available at https://github.com/xiaoyao3302/UniBrain.",2024-12-27T07:03:47Z,http://arxiv.org/abs/2412.19487v1,"Zicheng Wang, Zhen Zhao, Luping Zhou, Parashkev Nachev"
Learning Radiance Fields from a Single Snapshot Compressive Image,"In this paper, we explore the potential of Snapshot Compressive Imaging (SCI)
technique for recovering the underlying 3D scene structure from a single
temporal compressed image. SCI is a cost-effective method that enables the
recording of high-dimensional data, such as hyperspectral or temporal
information, into a single image using low-cost 2D imaging sensors. To achieve
this, a series of specially designed 2D masks are usually employed, reducing
storage and transmission requirements and offering potential privacy
protection. Inspired by this, we take one step further to recover the encoded
3D scene information leveraging powerful 3D scene representation capabilities
of neural radiance fields (NeRF). Specifically, we propose SCINeRF, in which we
formulate the physical imaging process of SCI as part of the training of NeRF,
allowing us to exploit its impressive performance in capturing complex scene
structures. In addition, we further integrate the popular 3D Gaussian Splatting
(3DGS) framework and propose SCISplat to improve 3D scene reconstruction
quality and training/rendering speed by explicitly optimizing point clouds into
3D Gaussian representations. To assess the effectiveness of our method, we
conduct extensive evaluations using both synthetic data and real data captured
by our SCI system. Experimental results demonstrate that our proposed approach
surpasses the state-of-the-art methods in terms of image reconstruction and
novel view synthesis. Moreover, our method also exhibits the ability to render
high frame-rate multi-view consistent images in real time by leveraging SCI and
the rendering capabilities of 3DGS. Codes will be available at:
https://github.com/WU- CVGL/SCISplat.",2024-12-27T06:40:44Z,http://arxiv.org/abs/2412.19483v1,"Yunhao Li, Xiang Liu, Xiaodong Wang, Xin Yuan, Peidong Liu"
"Pre-training, Fine-tuning and Re-ranking: A Three-Stage Framework for
  Legal Question Answering","Legal question answering (QA) has attracted increasing attention from people
seeking legal advice, which aims to retrieve the most applicable answers from a
large-scale database of question-answer pairs. Previous methods mainly use a
dual-encoder architecture to learn dense representations of both questions and
answers. However, these methods could suffer from lacking domain knowledge and
sufficient labeled training data. In this paper, we propose a three-stage
(\underline{p}re-training, \underline{f}ine-tuning and \underline{r}e-ranking)
framework for \underline{l}egal \underline{QA} (called PFR-LQA), which promotes
the fine-grained text representation learning and boosts the performance of
dense retrieval with the dual-encoder architecture. Concretely, we first
conduct domain-specific pre-training on legal questions and answers through a
self-supervised training objective, allowing the pre-trained model to be
adapted to the legal domain. Then, we perform task-specific fine-tuning of the
dual-encoder on legal question-answer pairs by using the supervised learning
objective, leading to a high-quality dual-encoder for the specific downstream
QA task. Finally, we employ a contextual re-ranking objective to further refine
the output representations of questions produced by the document encoder, which
uses contextual similarity to increase the discrepancy between the anchor and
hard negative samples for better question re-ranking. We conduct extensive
experiments on a manually annotated legal QA dataset. Experimental results show
that our PFR-LQA method achieves better performance than the strong competitors
for legal question answering.",2024-12-27T06:33:42Z,http://arxiv.org/abs/2412.19482v1,"Shiwen Ni, Hao Cheng, Min Yang"
Generative Adversarial Network on Motion-Blur Image Restoration,"In everyday life, photographs taken with a camera often suffer from motion
blur due to hand vibrations or sudden movements. This phenomenon can
significantly detract from the quality of the images captured, making it an
interesting challenge to develop a deep learning model that utilizes the
principles of adversarial networks to restore clarity to these blurred pixels.
In this project, we will focus on leveraging Generative Adversarial Networks
(GANs) to effectively deblur images affected by motion blur. A GAN-based
Tensorflow model is defined, training and evaluating by GoPro dataset which
comprises paired street view images featuring both clear and blurred versions.
This adversarial training process between Discriminator and Generator helps to
produce increasingly realistic images over time. Peak Signal-to-Noise Ratio
(PSNR) and Structural Similarity Index Measure (SSIM) are the two evaluation
metrics used to provide quantitative measures of image quality, allowing us to
evaluate the effectiveness of the deblurring process. Mean PSNR in 29.1644 and
mean SSIM in 0.7459 with average 4.6921 seconds deblurring time are achieved in
this project. The blurry pixels are sharper in the output of GAN model shows a
good image restoration effect in real world applications.",2024-12-27T06:12:50Z,http://arxiv.org/abs/2412.19479v1,Zhengdong Li
"An Overview of Machine Learning-Driven Resource Allocation in IoT
  Networks","In the wake of disruptive IoT technologies generating massive amounts of
diverse data, Machine Learning (ML) will play a crucial role in bringing
intelligence to Internet of Things (IoT) networks. This paper provides a
comprehensive analysis of the current state of resource allocation within IoT
networks, focusing specifically on two key categories: Low-Power IoT Networks
and Mobile IoT Networks. We delve into the resource allocation strategies that
are crucial for optimizing network performance and energy efficiency in these
environments. Furthermore, the paper explores the transformative role of
Machine Learning (ML), Deep Learning (DL), and Reinforcement Learning (RL) in
enhancing IoT functionalities. We highlight a range of applications and use
cases where these advanced technologies can significantly improve
decision-making and optimization processes. In addition to the opportunities
presented by ML, DL, and RL, we also address the potential challenges that
organizations may face when implementing these technologies in IoT settings.
These challenges include crucial accuracy, low flexibility and adaptability,
and high computational cost, etc. Finally, the paper identifies promising
avenues for future research, emphasizing the need for innovative solutions to
overcome existing hurdles and improve the integration of ML, DL, and RL into
IoT networks. By providing this holistic perspective, we aim to contribute to
the ongoing discourse on resource allocation strategies and the application of
intelligent technologies in the IoT landscape.",2024-12-27T06:11:28Z,http://arxiv.org/abs/2412.19478v1,Zhengdong Li
"Meta-Learning-Based Delayless Subband Adaptive Filter using Complex
  Self-Attention for Active Noise Control","Active noise control typically employs adaptive filtering to generate
secondary noise, where the least mean square algorithm is the most widely used.
However, traditional updating rules are linear and exhibit limited
effectiveness in addressing nonlinear environments and nonstationary noise. To
tackle this challenge, we reformulate the active noise control problem as a
meta-learning problem and propose a meta-learning-based delayless subband
adaptive filter with deep neural networks. The core idea is to utilize a neural
network as an adaptive algorithm that can adapt to different environments and
types of noise. The neural network will train under noisy observations,
implying that it recognizes the optimized updating rule without true labels. A
single-headed attention recurrent neural network is devised with learnable
feature embedding to update the adaptive filter weight efficiently, enabling
accurate computation of the secondary source to attenuate the unwanted primary
noise. In order to relax the time constraint on updating the adaptive filter
weights, the delayless subband architecture is employed, which will allow the
system to be updated less frequently as the downsampling factor increases. In
addition, the delayless subband architecture does not introduce additional time
delays in active noise control systems. A skip updating strategy is introduced
to decrease the updating frequency further so that machines with limited
resources have more possibility to board our meta-learning-based model.
Extensive multi-condition training ensures generalization and robustness
against various types of noise and environments. Simulation results demonstrate
that our meta-learning-based model achieves superior noise reduction
performance compared to traditional methods.",2024-12-27T05:51:40Z,http://arxiv.org/abs/2412.19471v1,"Pengxing Feng, Hing Cheung So"
"Optimizing Helmet Detection with Hybrid YOLO Pipelines: A Detailed
  Analysis","Helmet detection is crucial for advancing protection levels in public road
traffic dynamics. This problem statement translates to an object detection
task. Therefore, this paper compares recent You Only Look Once (YOLO) models in
the context of helmet detection in terms of reliability and computational load.
Specifically, YOLOv8, YOLOv9, and the newly released YOLOv11 have been used.
Besides, a modified architectural pipeline that remarkably improves the overall
performance has been proposed in this manuscript. This hybridized YOLO model
(h-YOLO) has been pitted against the independent models for analysis that
proves h-YOLO is preferable for helmet detection over plain YOLO models. The
models were tested using a range of standard object detection benchmarks such
as recall, precision, and mAP (Mean Average Precision). In addition, training
and testing times were recorded to provide the overall scope of the models in a
real-time detection scenario.",2024-12-27T05:26:12Z,http://arxiv.org/abs/2412.19467v1,"Vaikunth M, Dejey D, Vishaal C, Balamurali S"
"MNet-SAt: A Multiscale Network with Spatial-enhanced Attention for
  Segmentation of Polyps in Colonoscopy","Objective: To develop a novel deep learning framework for the automated
segmentation of colonic polyps in colonoscopy images, overcoming the
limitations of current approaches in preserving precise polyp boundaries,
incorporating multi-scale features, and modeling spatial dependencies that
accurately reflect the intricate and diverse morphology of polyps. Methods: To
address these limitations, we propose a novel Multiscale Network with
Spatial-enhanced Attention (MNet-SAt) for polyp segmentation in colonoscopy
images. This framework incorporates four key modules: Edge-Guided Feature
Enrichment (EGFE) preserves edge information for improved boundary quality;
Multi-Scale Feature Aggregator (MSFA) extracts and aggregates multi-scale
features across channel spatial dimensions, focusing on salient regions;
Spatial-Enhanced Attention (SEAt) captures spatial-aware global dependencies
within the multi-scale aggregated features, emphasizing the region of interest;
and Channel-Enhanced Atrous Spatial Pyramid Pooling (CE-ASPP) resamples and
recalibrates attentive features across scales. Results: We evaluated MNet-SAt
on the Kvasir-SEG and CVC-ClinicDB datasets, achieving Dice Similarity
Coefficients of 96.61% and 98.60%, respectively. Conclusion: Both quantitative
(DSC) and qualitative assessments highlight MNet-SAt's superior performance and
generalization capabilities compared to existing methods. Significance:
MNet-SAt's high accuracy in polyp segmentation holds promise for improving
clinical workflows in early polyp detection and more effective treatment,
contributing to reduced colorectal cancer mortality rates.",2024-12-27T05:17:29Z,http://arxiv.org/abs/2412.19464v1,"Chandravardhan Singh Raghaw, Aryan Yadav, Jasmer Singh Sanjotra, Shalini Dangi, Nagendra Kumar"
Pulse-induced memory-like effect in cyclotron motion?,"We study how a charged particle moving in a uniform magnetic field along its
standard circular path (cyclotron motion) reacts to a short-duration,
homogeneous, uniform electric field pulse injected in the plane perpendicular
to the magnetic field. A `permanent' change in the radius of the initial circle
and a shift of its centre is noted at later times, after the pulse is switched
off. The magnitude of the velocity undergoes a change too, akin to a `velocity
kick'. In summary, our results suggest a pulse-induced `electromagnetic
memory-like effect', which is not quite a `wave memory', but, nevertheless, has
similar features within a simple, non-relativistic context.",2024-12-27T05:13:53Z,http://arxiv.org/abs/2412.19460v1,Sayan Kar
A Prototype Unit for Image De-raining using Time-Lapse Data,"We address the challenge of single-image de-raining, a task that involves
recovering rain-free background information from a single rain image. While
recent advancements have utilized real-world time-lapse data for training,
enabling the estimation of consistent backgrounds and realistic rain streaks,
these methods often suffer from computational and memory consumption, limiting
their applicability in real-world scenarios. In this paper, we introduce a
novel solution: the Rain Streak Prototype Unit (RsPU). The RsPU efficiently
encodes rain streak-relevant features as real-time prototypes derived from
time-lapse data, eliminating the need for excessive memory resources. Our
de-raining network combines encoder-decoder networks with the RsPU, allowing us
to learn and encapsulate diverse rain streak-relevant features as concise
prototypes, employing an attention-based approach. To ensure the effectiveness
of our approach, we propose a feature prototype loss encompassing cohesion and
divergence components. This loss function captures both the compactness and
diversity aspects of the prototypical rain streak features within the RsPU. Our
method evaluates various de-raining benchmarks, accompanied by comprehensive
ablation studies. We show that it can achieve competitive results in various
rain images compared to state-of-the-art methods.",2024-12-27T05:04:56Z,http://arxiv.org/abs/2412.19459v1,"Jaehoon Cho, Minjung Yoo, Jini Yang, Sunok Kim"
Focusing Image Generation to Mitigate Spurious Correlations,"Instance features in images exhibit spurious correlations with background
features, affecting the training process of deep neural classifiers. This leads
to insufficient attention to instance features by the classifier, resulting in
erroneous classification outcomes. In this paper, we propose a data
augmentation method called Spurious Correlations Guided Synthesis (SCGS) that
mitigates spurious correlations through image generation model. This approach
does not require expensive spurious attribute (group) labels for the training
data and can be widely applied to other debiasing methods. Specifically, SCGS
first identifies the incorrect attention regions of a pre-trained classifier on
the training images, and then uses an image generation model to generate new
training data based on these incorrect attended regions. SCGS increases the
diversity and scale of the dataset to reduce the impact of spurious
correlations on classifiers. Changes in the classifier's attention regions and
experimental results on three different domain datasets demonstrate that this
method is effective in reducing the classifier's reliance on spurious
correlations.",2024-12-27T04:48:56Z,http://arxiv.org/abs/2412.19457v1,"Xuewei Li, Zhenzhen Nie, Mei Yu, Zijian Zhang, Jie Gao, Tianyi Xu, Zhiqiang Liu"
"NijiGAN: Transform What You See into Anime with Contrastive
  Semi-Supervised Learning and Neural Ordinary Differential Equations","Generative AI has transformed the animation industry. Several models have
been developed for image-to-image translation, particularly focusing on
converting real-world images into anime through unpaired translation.
Scenimefy, a notable approach utilizing contrastive learning, achieves high
fidelity anime scene translation by addressing limited paired data through
semi-supervised training. However, it faces limitations due to its reliance on
paired data from a fine-tuned StyleGAN in the anime domain, often producing
low-quality datasets. Additionally, Scenimefy's high parameter architecture
presents opportunities for computational optimization. This research introduces
NijiGAN, a novel model incorporating Neural Ordinary Differential Equations
(NeuralODEs), which offer unique advantages in continuous transformation
modeling compared to traditional residual networks. NijiGAN successfully
transforms real-world scenes into high fidelity anime visuals using half of
Scenimefy's parameters. It employs pseudo-paired data generated through
Scenimefy for supervised training, eliminating dependence on low-quality paired
data and improving the training process. Our comprehensive evaluation includes
ablation studies, qualitative, and quantitative analysis comparing NijiGAN to
similar models. The testing results demonstrate that NijiGAN produces
higher-quality images compared to AnimeGAN, as evidenced by a Mean Opinion
Score (MOS) of 2.192, it surpasses AnimeGAN's MOS of 2.160. Furthermore, our
model achieved a Frechet Inception Distance (FID) score of 58.71, outperforming
Scenimefy's FID score of 60.32. These results demonstrate that NijiGAN achieves
competitive performance against existing state-of-the-arts, especially
Scenimefy as the baseline model.",2024-12-27T04:46:44Z,http://arxiv.org/abs/2412.19455v1,"Kevin Putra Santoso, Anny Yuniarti, Dwiyasa Nakula, Dimas Prihady Setyawan, Adam Haidar Azizi, Jeany Aurellia P. Dewati, Farah Dhia Fadhila, Maria T. Elvara Bumbungan"
"Feature Alignment-Based Knowledge Distillation for Efficient Compression
  of Large Language Models","This study proposes a knowledge distillation algorithm based on large
language models and feature alignment, aiming to effectively transfer the
knowledge of large pre-trained models into lightweight student models, thereby
reducing computational costs while maintaining high model performance.
Different from the traditional soft label distillation method, this method
introduces a multi-layer feature alignment strategy to deeply align the
intermediate features and attention mechanisms of the teacher model and the
student model, maximally retaining the semantic expression ability and context
modeling ability of the teacher model. In terms of method design, a multi-task
loss function is constructed, including feature matching loss, attention
alignment loss, and output distribution matching loss, to ensure multi-level
information transfer through joint optimization. The experiments were
comprehensively evaluated on the GLUE data set and various natural language
processing tasks. The results show that the proposed model performs very close
to the state-of-the-art GPT-4 model in terms of evaluation indicators such as
perplexity, BLEU, ROUGE, and CER. At the same time, it far exceeds baseline
models such as DeBERTa, XLNet, and GPT-3, showing significant performance
improvements and computing efficiency advantages. Research results show that
the feature alignment distillation strategy is an effective model compression
method that can significantly reduce computational overhead and storage
requirements while maintaining model capabilities. Future research can be
further expanded in the directions of self-supervised learning, cross-modal
feature alignment, and multi-task transfer learning to provide more flexible
and efficient solutions for the deployment and optimization of deep learning
models.",2024-12-27T04:37:06Z,http://arxiv.org/abs/2412.19449v1,"Shuo Wang, Chihang Wang, Jia Gao, Zhen Qi, Hongye Zheng, Xiaoxuan Liao"
Towards Simple and Provable Parameter-Free Adaptive Gradient Methods,"Optimization algorithms such as AdaGrad and Adam have significantly advanced
the training of deep models by dynamically adjusting the learning rate during
the optimization process. However, adhoc tuning of learning rates poses a
challenge, leading to inefficiencies in practice. To address this issue, recent
research has focused on developing ""learning-rate-free"" or ""parameter-free""
algorithms that operate effectively without the need for learning rate tuning.
Despite these efforts, existing parameter-free variants of AdaGrad and Adam
tend to be overly complex and/or lack formal convergence guarantees. In this
paper, we present AdaGrad++ and Adam++, novel and simple parameter-free
variants of AdaGrad and Adam with convergence guarantees. We prove that
AdaGrad++ achieves comparable convergence rates to AdaGrad in convex
optimization without predefined learning rate assumptions. Similarly, Adam++
matches the convergence rate of Adam without relying on any conditions on the
learning rates. Experimental results across various deep learning tasks
validate the competitive performance of AdaGrad++ and Adam++.",2024-12-27T04:22:02Z,http://arxiv.org/abs/2412.19444v1,"Yuanzhe Tao, Huizhuo Yuan, Xun Zhou, Yuan Cao, Quanquan Gu"
"Comparative Performance Analysis of Quantum Machine Learning
  Architectures for Credit Card Fraud Detection","As financial fraud becomes increasingly complex, effective detection methods
are essential. Quantum Machine Learning (QML) introduces certain capabilities
that may enhance both accuracy and efficiency in this area. This study examines
how different quantum feature map and ansatz configurations affect the
performance of three QML-based classifiers-the Variational Quantum Classifier
(VQC), the Sampler Quantum Neural Network (SQNN), and the Estimator Quantum
Neural Network (EQNN)-when applied to two non-standardized financial fraud
datasets. Different quantum feature map and ansatz configurations are
evaluated, revealing distinct performance patterns. The VQC consistently
demonstrates strong classification results, achieving an F1 score of 0.88,
while the SQNN also delivers promising outcomes. In contrast, the EQNN
struggles to produce robust results, emphasizing the challenges presented by
non-standardized data. These findings highlight the importance of careful model
configuration in QML-based financial fraud detection. By showing how specific
feature maps and ansatz choices influence predictive success, this work guides
researchers and practitioners in refining QML approaches for complex financial
applications.",2024-12-27T04:17:34Z,http://arxiv.org/abs/2412.19441v1,"Mansour El Alami, Nouhaila Innan, Muhammad Shafique, Mohamed Bennai"
DeepSeek-V3 Technical Report,"We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with
671B total parameters with 37B activated for each token. To achieve efficient
inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent
Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated
in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free
strategy for load balancing and sets a multi-token prediction training
objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion
diverse and high-quality tokens, followed by Supervised Fine-Tuning and
Reinforcement Learning stages to fully harness its capabilities. Comprehensive
evaluations reveal that DeepSeek-V3 outperforms other open-source models and
achieves performance comparable to leading closed-source models. Despite its
excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its
full training. In addition, its training process is remarkably stable.
Throughout the entire training process, we did not experience any irrecoverable
loss spikes or perform any rollbacks. The model checkpoints are available at
https://github.com/deepseek-ai/DeepSeek-V3.",2024-12-27T04:03:16Z,http://arxiv.org/abs/2412.19437v1,"DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, Zizheng Pan"
"Low-Rank Contextual Reinforcement Learning from Heterogeneous Human
  Feedback","Reinforcement learning from human feedback (RLHF) has become a cornerstone
for aligning large language models with human preferences. However, the
heterogeneity of human feedback, driven by diverse individual contexts and
preferences, poses significant challenges for reward learning. To address this,
we propose a Low-rank Contextual RLHF (LoCo-RLHF) framework that integrates
contextual information to better model heterogeneous feedback while maintaining
computational efficiency. Our approach builds on a contextual preference model,
leveraging the intrinsic low-rank structure of the interaction between user
contexts and query-answer pairs to mitigate the high dimensionality of feature
representations. Furthermore, we address the challenge of distributional shifts
in feedback through our Pessimism in Reduced Subspace (PRS) policy, inspired by
pessimistic offline reinforcement learning techniques. We theoretically
demonstrate that our policy achieves a tighter sub-optimality gap compared to
existing methods. Extensive experiments validate the effectiveness of
LoCo-RLHF, showcasing its superior performance in personalized RLHF settings
and its robustness to distribution shifts.",2024-12-27T04:02:46Z,http://arxiv.org/abs/2412.19436v1,"Seong Jin Lee, Will Wei Sun, Yufeng Liu"
"Residual Feature-Reutilization Inception Network for Image
  Classification","Capturing feature information effectively is of great importance in the field
of computer vision. With the development of convolutional neural networks
(CNNs), concepts like residual connection and multiple scales promote continual
performance gains in diverse deep learning vision tasks. In this paper, we
propose a novel CNN architecture that it consists of residual
feature-reutilization inceptions (ResFRI) or split-residual
feature-reutilization inceptions (Split-ResFRI). And it is composed of four
convolutional combinations of different structures connected by specially
designed information interaction passages, which are utilized to extract
multi-scale feature information and effectively increase the receptive field of
the model. Moreover, according to the network structure designed above,
Split-ResFRI can adjust the segmentation ratio of the input information,
thereby reducing the number of parameters and guaranteeing the model
performance. Specifically, in experiments based on popular vision datasets,
such as CIFAR10 ($97.94$\%), CIFAR100 ($85.91$\%) and Tiny Imagenet
($70.54$\%), we obtain state-of-the-art results compared with other modern
models under the premise that the model size is approximate and no additional
data is used.",2024-12-27T03:55:25Z,http://arxiv.org/abs/2412.19433v1,"Yuanpeng He, Wenjie Song, Lijian Li, Tianxiang Zhan, Wenpin Jiao"
"Seed-Driven Stepwise Crystallization (SDSC) for Growing Rutile GeO2
  Films via MOCVD","Germanium dioxide (r-GeO2) is an emerging new ultrawide bandgap (UWBG)
semiconductor with significant potential for power electronics, thanks to its
large-size substrate compatibility and ambipolar doping capability. However,
phase segregation during metal-organic chemical vapor deposition (MOCVD) on
substrates like r-TiO2 has posed a significant barrier to achieving
high-quality films. Conventional optimization of growth parameters has been
found so far not very insufficient in film coverage and film quality. To
address this, a seed-driven stepwise crystallization (SDSC) growth approach was
employed in this study, featuring multiple sequential deposition steps on a
pre-templated substrate enriched with r-GeO2 seeds. The process began with an
initial 180-minute deposition to establish r-GeO2 nucleation seeds, followed by
a sequence of shorter deposition steps (90, 60, 60, 60, 60, and 60 minutes).
This stepwise growth strategy progressively increased the crystalline coverage
to 57.4%, 77.49%, 79.73%, 93.27%, 99.17%, and ultimately 100%. Concurrently,
the crystalline quality improved substantially, evidenced by a ~30% reduction
in the Full Width at Half Maximum (FWHM) of X-ray diffraction rocking curves.
These findings demonstrate the potential of the SDSC approach for overcoming
phase segregation and achieving high-quality, large-area r-GeO2 films.",2024-12-27T03:47:35Z,http://arxiv.org/abs/2412.19429v1,"Imteaz Rahaman, Botong Li, Bobby Duersch, Hunter D. Ellis, Kai Fu"
"Temporal Context Consistency Above All: Enhancing Long-Term Anticipation
  by Learning and Enforcing Temporal Constraints","This paper proposes a method for long-term action anticipation (LTA), the
task of predicting action labels and their duration in a video given the
observation of an initial untrimmed video interval. We build on an
encoder-decoder architecture with parallel decoding and make two key
contributions. First, we introduce a bi-directional action context regularizer
module on the top of the decoder that ensures temporal context coherence in
temporally adjacent segments. Second, we learn from classified segments a
transition matrix that models the probability of transitioning from one action
to another and the sequence is optimized globally over the full prediction
interval. In addition, we use a specialized encoder for the task of action
segmentation to increase the quality of the predictions in the observation
interval at inference time, leading to a better understanding of the past. We
validate our methods on four benchmark datasets for LTA, the EpicKitchen-55,
EGTEA+, 50Salads and Breakfast demonstrating superior or comparable performance
to state-of-the-art methods, including probabilistic models and also those
based on Large Language Models, that assume trimmed video as input. The code
will be released upon acceptance.",2024-12-27T03:29:10Z,http://arxiv.org/abs/2412.19424v1,"Alberto Maté, Mariella Dimiccoli"
Revisiting PCA for time series reduction in temporal dimension,"Revisiting PCA for Time Series Reduction in Temporal Dimension; Jiaxin Gao,
Wenbo Hu, Yuntian Chen; Deep learning has significantly advanced time series
analysis (TSA), enabling the extraction of complex patterns for tasks like
classification, forecasting, and regression. Although dimensionality reduction
has traditionally focused on the variable space-achieving notable success in
minimizing data redundancy and computational complexity-less attention has been
paid to reducing the temporal dimension. In this study, we revisit Principal
Component Analysis (PCA), a classical dimensionality reduction technique, to
explore its utility in temporal dimension reduction for time series data. It is
generally thought that applying PCA to the temporal dimension would disrupt
temporal dependencies, leading to limited exploration in this area. However,
our theoretical analysis and extensive experiments demonstrate that applying
PCA to sliding series windows not only maintains model performance, but also
enhances computational efficiency. In auto-regressive forecasting, the temporal
structure is partially preserved through windowing, and PCA is applied within
these windows to denoise the time series while retaining their statistical
information. By preprocessing time-series data with PCA, we reduce the temporal
dimensionality before feeding it into TSA models such as Linear, Transformer,
CNN, and RNN architectures. This approach accelerates training and inference
and reduces resource consumption. Notably, PCA improves Informer training and
inference speed by up to 40% and decreases GPU memory usage of TimesNet by 30%,
without sacrificing model accuracy. Comparative analysis against other
reduction methods further highlights the effectiveness of PCA in improving the
efficiency of TSA models.",2024-12-27T03:17:26Z,http://arxiv.org/abs/2412.19423v1,"Jiaxin Gao, Wenbo Hu, Yuntian Chen"
"Gx2Mol: De Novo Generation of Hit-like Molecules from Gene Expression
  Profiles via Deep Learning","De novo generation of hit-like molecules is a challenging task in the drug
discovery process. Most methods in previous studies learn the semantics and
syntax of molecular structures by analyzing molecular graphs or simplified
molecular input line entry system (SMILES) strings; however, they do not take
into account the drug responses of the biological systems consisting of genes
and proteins. In this study we propose a deep generative model, Gx2Mol, which
utilizes gene expression profiles to generate molecular structures with
desirable phenotypes for arbitrary target proteins. In the algorithm, a
variational autoencoder is employed as a feature extractor to learn the latent
feature distribution of the gene expression profiles. Then, a long short-term
memory is leveraged as the chemical generator to produce syntactically valid
SMILES strings that satisfy the feature conditions of the gene expression
profile extracted by the feature extractor. Experimental results and case
studies demonstrate that the proposed Gx2Mol model can produce new molecules
with potential bioactivities and drug-like properties.",2024-12-27T03:16:56Z,http://arxiv.org/abs/2412.19422v1,"Chen Li, Yuki Matsukiyo, Yoshihiro Yamanishi"
"A Matrix Logic Approach to Efficient Frequent Itemset Discovery in Large
  Data Sets","This paper proposes a frequent itemset mining algorithm based on the Boolean
matrix method, aiming to solve the storage and computational bottlenecks of
traditional frequent pattern mining algorithms in high-dimensional and
large-scale transaction databases. By representing the itemsets in the
transaction database as Boolean matrices, the algorithm uses Boolean logic
operations such as AND and OR to efficiently calculate the support of the
itemsets, avoiding the generation and storage of a large number of candidates
itemsets in traditional algorithms. The algorithm recursively mines frequent
itemsets through matrix operations and can flexibly adapt to different data
scales and support thresholds. In the experiment, the public Groceries dataset
was selected, and the running efficiency test and frequent itemset mining
effect test were designed to evaluate the algorithm's performance indicators
such as running time, memory usage, and number of frequent itemsets under
different transaction numbers and support thresholds. The experimental results
show that the algorithm can efficiently mine a large number of frequent
itemsets when the support threshold is low, and focus on strong association
rules with high support when the threshold is high. In addition, the changing
trends of running time and memory usage show that the Boolean matrix method can
still maintain good running efficiency when the number of transactions
increases significantly and has high scalability and robustness. Future
research can improve memory optimization and matrix block operations, and
combine distributed computing and deep learning models to further enhance the
algorithm's applicability and real-time processing capabilities in
ultra-large-scale data environments. The algorithm has broad application
potential and development prospects in the fields of market analysis,
recommendation systems, and network security.",2024-12-27T03:13:13Z,http://arxiv.org/abs/2412.19420v1,"Xuan Li, Tingyi Ruan, Yankaiqi Li, Quanchao Lu, Xiaoxuan Sun"
"Introduction to Graph Neural Networks: A Starting Point for Machine
  Learning Engineers","Graph neural networks are deep neural networks designed for graphs with
attributes attached to nodes or edges. The number of research papers in the
literature concerning these models is growing rapidly due to their impressive
performance on a broad range of tasks. This survey introduces graph neural
networks through the encoder-decoder framework and provides examples of
decoders for a range of graph analytic tasks. It uses theory and numerous
experiments on homogeneous graphs to illustrate the behavior of graph neural
networks for different training sizes and degrees of graph complexity.",2024-12-27T03:13:02Z,http://arxiv.org/abs/2412.19419v1,"James H. Tanis, Chris Giannella, Adrian V. Mariano"
"Generalized Uncertainty-Based Evidential Fusion with Hybrid Multi-Head
  Attention for Weak-Supervised Temporal Action Localization","Weakly supervised temporal action localization (WS-TAL) is a task of
targeting at localizing complete action instances and categorizing them with
video-level labels. Action-background ambiguity, primarily caused by background
noise resulting from aggregation and intra-action variation, is a significant
challenge for existing WS-TAL methods. In this paper, we introduce a hybrid
multi-head attention (HMHA) module and generalized uncertainty-based evidential
fusion (GUEF) module to address the problem. The proposed HMHA effectively
enhances RGB and optical flow features by filtering redundant information and
adjusting their feature distribution to better align with the WS-TAL task.
Additionally, the proposed GUEF adaptively eliminates the interference of
background noise by fusing snippet-level evidences to refine uncertainty
measurement and select superior foreground feature information, which enables
the model to concentrate on integral action instances to achieve better action
localization and classification performance. Experimental results conducted on
the THUMOS14 dataset demonstrate that our method outperforms state-of-the-art
methods. Our code is available in
\url{https://github.com/heyuanpengpku/GUEF/tree/main}.",2024-12-27T03:04:57Z,http://arxiv.org/abs/2412.19418v1,"Yuanpeng He, Lijian Li, Tianxiang Zhan, Wenpin Jiao, Chi-Man Pun"
Multi-scale Latent Point Consistency Models for 3D Shape Generation,"Consistency Models (CMs) have significantly accelerated the sampling process
in diffusion models, yielding impressive results in synthesizing
high-resolution images. To explore and extend these advancements to
point-cloud-based 3D shape generation, we propose a novel Multi-scale Latent
Point Consistency Model (MLPCM). Our MLPCM follows a latent diffusion framework
and introduces hierarchical levels of latent representations, ranging from
point-level to super-point levels, each corresponding to a different spatial
resolution. We design a multi-scale latent integration module along with 3D
spatial attention to effectively denoise the point-level latent representations
conditioned on those from multiple super-point levels. Additionally, we propose
a latent consistency model, learned through consistency distillation, that
compresses the prior into a one-step generator. This significantly improves
sampling efficiency while preserving the performance of the original teacher
model. Extensive experiments on standard benchmarks ShapeNet and ShapeNet-Vol
demonstrate that MLPCM achieves a 100x speedup in the generation process, while
surpassing state-of-the-art diffusion models in terms of both shape quality and
diversity.",2024-12-27T02:41:33Z,http://arxiv.org/abs/2412.19413v1,"Bi'an Du, Wei Hu, Renjie Liao"
MINIMA: Modality Invariant Image Matching,"Image matching for both cross-view and cross-modality plays a critical role
in multimodal perception. In practice, the modality gap caused by different
imaging systems/styles poses great challenges to the matching task. Existing
works try to extract invariant features for specific modalities and train on
limited datasets, showing poor generalization. In this paper, we present
MINIMA, a unified image matching framework for multiple cross-modal cases.
Without pursuing fancy modules, our MINIMA aims to enhance universal
performance from the perspective of data scaling up. For such purpose, we
propose a simple yet effective data engine that can freely produce a large
dataset containing multiple modalities, rich scenarios, and accurate matching
labels. Specifically, we scale up the modalities from cheap but rich RGB-only
matching data, by means of generative models. Under this setting, the matching
labels and rich diversity of the RGB dataset are well inherited by the
generated multimodal data. Benefiting from this, we construct MD-syn, a new
comprehensive dataset that fills the data gap for general multimodal image
matching. With MD-syn, we can directly train any advanced matching pipeline on
randomly selected modality pairs to obtain cross-modal ability. Extensive
experiments on in-domain and zero-shot matching tasks, including $19$
cross-modal cases, demonstrate that our MINIMA can significantly outperform the
baselines and even surpass modality-specific methods. The dataset and code are
available at https://github.com/LSXI7/MINIMA .",2024-12-27T02:39:50Z,http://arxiv.org/abs/2412.19412v1,"Xingyu Jiang, Jiangwei Ren, Zizhuo Li, Xin Zhou, Dingkang Liang, Xiang Bai"
Efficient Feature Mapping Using a Collaborative Team of AUVs,"We present the results of experiments performed using a team of small
autonomous underwater vehicles (AUVs) to determine the location of an isobath.
The primary contributions of this work are (1) the development of a novel
objective function for level set estimation that utilizes a rigorous assessment
of uncertainty, and (2) a description of the practical challenges and
corresponding solutions needed to implement our approach in the field using a
team of AUVs. We combine path planning techniques and an approach to
decentralization from prior work that yields theoretical performance
guarantees. Experimentation with a team of AUVs provides empirical evidence
that the desirable performance guarantees can be preserved in practice even in
the presence of limitations that commonly arise in underwater robotics,
including slow and intermittent acoustic communications and limited
computational resources.",2024-12-27T02:22:52Z,http://arxiv.org/abs/2412.19409v1,"Benjamin Biggs, Daniel J. Stilwell, Harun Yetkin, James McMahon"
"MLLM-SUL: Multimodal Large Language Model for Semantic Scene
  Understanding and Localization in Traffic Scenarios","Multimodal large language models (MLLMs) have shown satisfactory effects in
many autonomous driving tasks. In this paper, MLLMs are utilized to solve joint
semantic scene understanding and risk localization tasks, while only relying on
front-view images. In the proposed MLLM-SUL framework, a dual-branch visual
encoder is first designed to extract features from two resolutions, and rich
visual information is conducive to the language model describing risk objects
of different sizes accurately. Then for the language generation, LLaMA model is
fine-tuned to predict scene descriptions, containing the type of driving
scenario, actions of risk objects, and driving intentions and suggestions of
ego-vehicle. Ultimately, a transformer-based network incorporating a regression
token is trained to locate the risk objects. Extensive experiments on the
existing DRAMA-ROLISP dataset and the extended DRAMA-SRIS dataset demonstrate
that our method is efficient, surpassing many state-of-the-art image-based and
video-based methods. Specifically, our method achieves 80.1% BLEU-1 score and
298.5% CIDEr score in the scene understanding task, and 59.6% accuracy in the
localization task. Codes and datasets are available at
https://github.com/fjq-tongji/MLLM-SUL.",2024-12-27T02:05:38Z,http://arxiv.org/abs/2412.19406v1,"Jiaqi Fan, Jianhua Wu, Jincheng Gao, Jianhao Yu, Yafei Wang, Hongqing Chu, Bingzhao Gao"
Spectral-Temporal Fusion Representation for Person-in-Bed Detection,"This study is based on the ICASSP 2025 Signal Processing Grand Challenge's
Accelerometer-Based Person-in-Bed Detection Challenge, which aims to determine
bed occupancy using accelerometer signals. The task is divided into two tracks:
""in bed"" and ""not in bed"" segmented detection, and streaming detection, facing
challenges such as individual differences, posture variations, and external
disturbances. We propose a spectral-temporal fusion-based feature
representation method with mixup data augmentation, and adopt Intersection over
Union (IoU) loss to optimize detection accuracy. In the two tracks, our method
achieved outstanding results of 100.00% and 95.55% in detection scores,
securing first place and third place, respectively.",2024-12-27T02:05:09Z,http://arxiv.org/abs/2412.19404v1,"Xuefeng Yang, Shiheng Zhang, Jian Guan, Feiyang Xiao, Wei Lu, Qiaoxi Zhu"
"Fully Data-driven but Interpretable Human Behavioural Modelling with
  Differentiable Discrete Choice Model","Discrete choice models are essential for modelling various decision-making
processes in human behaviour. However, the specification of these models has
depended heavily on domain knowledge from experts, and the fully automated but
interpretable modelling of complex human behaviours has been a long-standing
challenge. In this paper, we introduce the differentiable discrete choice model
(Diff-DCM), a fully data-driven method for the interpretable modelling,
learning, prediction, and control of complex human behaviours, which is
realised by differentiable programming. Solely from input features and choice
outcomes without any prior knowledge, Diff-DCM can estimate interpretable
closed-form utility functions that reproduce observed behaviours. Comprehensive
experiments with both synthetic and real-world data demonstrate that Diff-DCM
can be applied to various types of data and requires only a small amount of
computational resources for the estimations, which can be completed within tens
of seconds on a laptop without any accelerators. In these experiments, we also
demonstrate that, using its differentiability, Diff-DCM can provide useful
insights into human behaviours, such as an optimal intervention path for
effective behavioural changes. This study provides a strong basis for the fully
automated and reliable modelling, prediction, and control of human behaviours.",2024-12-27T01:53:18Z,http://arxiv.org/abs/2412.19403v1,"Fumiyasu Makinoshima, Tatsuya Mitomi, Fumiya Makihara, Eigo Segawa"
"A Generalized Einstein Relation for Markovian Friction Coefficients from
  Molecular Trajectories","We present a generalized Einstein relation for the friction coefficients
associated with an underlying memory kernel in terms of observable time
correlation functions. There is considerable freedom in the correlations
involved, and this allows the expression to be tailored to the particular
system to achieve numerical stability. We demonstrate this by recovering the
site-specific friction coefficients from trajectories of a freely diffusing
model trimer, and we show that the accuracy is greatly improved over
established Volterra inversion methods for kernel extraction.",2024-12-27T01:20:37Z,http://arxiv.org/abs/2412.19398v1,"J. M. Hall, M. G. Guenza"
"Comparing Few to Rank Many: Active Human Preference Learning using
  Randomized Frank-Wolfe","We study learning of human preferences from a limited comparison feedback.
This task is ubiquitous in machine learning. Its applications such as
reinforcement learning from human feedback, have been transformational. We
formulate this problem as learning a Plackett-Luce model over a universe of $N$
choices from $K$-way comparison feedback, where typically $K \ll N$. Our
solution is the D-optimal design for the Plackett-Luce objective. The design
defines a data logging policy that elicits comparison feedback for a small
collection of optimally chosen points from all ${N \choose K}$ feasible
subsets. The main algorithmic challenge in this work is that even fast methods
for solving D-optimal designs would have $O({N \choose K})$ time complexity. To
address this issue, we propose a randomized Frank-Wolfe (FW) algorithm that
solves the linear maximization sub-problems in the FW method on randomly chosen
variables. We analyze the algorithm, and evaluate it empirically on synthetic
and open-source NLP datasets.",2024-12-27T01:10:17Z,http://arxiv.org/abs/2412.19396v1,"Kiran Koshy Thekumparampil, Gaurush Hiranandani, Kousha Kalantari, Shoham Sabach, Branislav Kveton"
"Two-echelon Electric Vehicle Routing Problem in Parcel Delivery: A
  Literature Review","Multi-echelon parcel delivery systems using electric vehicles (EVs) are
crucial for managing urban logistics complexity and promoting sustainability.
In multi-echelon systems, particularly within two-stage systems, larger
vehicles transport parcels from a central depot to satellite hubs, where
smaller EVs pick up the parcels and carry out last-mile deliveries. This system
could increase efficiency, reduce emissions, and improve service reliability.
The two-echelon electric vehicle routing problem (2E-EVRP), an extension of the
traditional two-echelon vehicle routing problem (2E-VRP), addresses EV-specific
challenges such as battery constraints and recharging stations to tackle
environmental impacts, urban congestion, and e-commerce demands. While
effectively reducing costs, energy use, and emissions, the 2E-EVRP faces
modeling challenges due to multi-echelon structures, EV limitations, and
recharging station selection. This paper systematically reviews 2E-EVRP
literature, analyzing key studies. It proposes a classification scheme to
categorize the papers based on the problem variants, objectives, constraints,
and solution methods. It identifies gaps such as delivery tardiness,
environmental trade-offs, multi-objective optimization, multiple depots, split
deliveries, and time-dependent travel conditions. Future research directions
include aligning models with urban policies, integrating parcel lockers,
enabling same-day delivery, and incorporating advanced technologies like
autonomous vehicles. Methodological advancements suggest using machine
learning, reinforcement learning, and simulation-based approaches to enhance
dynamic routing and real-time decision-making. These directions aim to expand
the 2E-EVRP applicability, addressing theoretical and practical challenges in
sustainable urban logistics for future works.",2024-12-27T01:05:59Z,http://arxiv.org/abs/2412.19395v1,"Nima Moradi, Niloufar Mirzavand Boroujeni, Navid Aftabi, Amin Aslani"
"Asymptotically Optimal Search for a Change Point Anomaly under a
  Composite Hypothesis Model","We address the problem of searching for a change point in an anomalous
process among a finite set of M processes. Specifically, we address a composite
hypothesis model in which each process generates measurements following a
common distribution with an unknown parameter (vector). This parameter belongs
to either a normal or abnormal space depending on the current state of the
process. Before the change point, all processes, including the anomalous one,
are in a normal state; after the change point, the anomalous process
transitions to an abnormal state. Our goal is to design a sequential search
strategy that minimizes the Bayes risk by balancing sample complexity and
detection accuracy. We propose a deterministic search algorithm with the
following notable properties. First, we analytically demonstrate that when the
distributions of both normal and abnormal processes are unknown, the algorithm
is asymptotically optimal in minimizing the Bayes risk as the error probability
approaches zero. In the second setting, where the parameter under the null
hypothesis is known, the algorithm achieves asymptotic optimality with improved
detection time based on the true normal state. Simulation results are presented
to validate the theoretical findings.",2024-12-27T00:44:34Z,http://arxiv.org/abs/2412.19392v1,"Liad Lea Didi, Tomer Gafni, Kobi Cohen"
"An In-Depth Analysis of Adversarial Discriminative Domain Adaptation for
  Digit Classification","Domain adaptation is an active area of research driven by the growing demand
for robust machine learning models that perform well on real-world data.
Adversarial learning for deep neural networks (DNNs) has emerged as a promising
approach to improving generalization ability, particularly for image
classification. In this paper, we implement a specific adversarial learning
technique known as Adversarial Discriminative Domain Adaptation (ADDA) and
replicate digit classification experiments from the original ADDA paper. We
extend their findings by examining a broader range of domain shifts and provide
a detailed analysis of in-domain classification accuracy post-ADDA. Our results
demonstrate that ADDA significantly improves accuracy across certain domain
shifts with minimal impact on in-domain performance. Furthermore, we provide
qualitative analysis and propose potential explanations for ADDA's limitations
in less successful domain shifts. Code is at
https://github.com/eugenechoi2004/COS429_FINAL .",2024-12-27T00:36:40Z,http://arxiv.org/abs/2412.19391v1,"Eugene Choi, Julian Rodriguez, Edmund Young"
The lack of influence of the scalar hair on the DC conductivity,"Recently obtained black hole solutions within the framework of
beyond-Horndeski theories, which have the advantage of featuring primary hair,
are generalized in the presence of two axionic fields. In order to induce a
momentum dissipation, the axionic field solutions are homogeneously distributed
along the horizon coordinates of the planar base manifold. We show that,
despite the explicit dependence of the scalar field and the metric on the
primary hair, this latter does not directly affect the calculation of transport
properties. Its influence is indirect, modifying the horizon location, but the
transport properties themselves do not explicitly depend on the hair parameter.
We take a step further and show that even within a more general class of
beyond-Horndeski theories, where the scalar field depends linearly on the hair
parameter, the scalar hair still has no direct impact on the DC conductivity.
This result underscores the robustness of our earlier findings, and seem to
confirm that the transport properties remain unaffected by the explicit
presence of the hair parameter.",2024-12-27T00:28:58Z,http://arxiv.org/abs/2412.19388v1,Ulises Hernandez-Vera
"Preventive Energy Management for Distribution Systems Under Uncertain
  Events: A Deep Reinforcement Learning Approach","As power systems become more complex with the continuous integration of
intelligent distributed energy resources (DERs), new risks and uncertainties
arise. Consequently, to enhance system resiliency, it is essential to account
for various uncertain events when implementing the optimization problem for the
energy management system (EMS). This paper presents a preventive EMS
considering the probability of failure (PoF) of each system component across
different scenarios. A conditional-value-at-risk (CVaR)-based framework is
proposed to integrate the uncertainties of the distribution network. Loads are
classified into critical, semi-critical, and non-critical categories to
prioritize essential loads during generation resource shortages. A proximal
policy optimization (PPO)-based reinforcement learning (RL) agent is used to
solve the formulated problem and generate the control decisions. The proposed
framework is evaluated on a notional MVDC ship system and a modified IEEE
30-bus system, where the results demonstrate that the PPO agent can
successfully optimize the objective function while maintaining the network and
operational constraints. For validation, the RL-based method is benchmarked
against a traditional optimization approach, further highlighting its
effectiveness and robustness. This comparison shows that RL agents can offer
more resiliency against future uncertain events compared to the traditional
solution methods due to their adaptability and learning capacity.",2024-12-26T23:50:28Z,http://arxiv.org/abs/2412.19382v1,"Md Isfakul Anam, Tuyen Vu, Jianhua Zhang"
"Minimal Batch Adaptive Learning Policy Engine for Real-Time Mid-Price
  Forecasting in High-Frequency Trading","High-frequency trading (HFT) has transformed modern financial markets, making
reliable short-term price forecasting models essential. In this study, we
present a novel approach to mid-price forecasting using Level 1 limit order
book (LOB) data from NASDAQ, focusing on 100 U.S. stocks from the S&amp;P 500 index
during the period from September to November 2022. Expanding on our previous
work with Radial Basis Function Neural Networks (RBFNN), which leveraged
automated feature importance techniques based on mean decrease impurity (MDI)
and gradient descent (GD), we introduce the Adaptive Learning Policy Engine
(ALPE) - a reinforcement learning (RL)-based agent designed for batch-free,
immediate mid-price forecasting. ALPE incorporates adaptive epsilon decay to
dynamically balance exploration and exploitation, outperforming a diverse range
of highly effective machine learning (ML) and deep learning (DL) models in
forecasting performance.",2024-12-26T22:49:53Z,http://arxiv.org/abs/2412.19372v1,"Adamantios Ntakaris, Gbenga Ibikunle"
"BeSplat -- Gaussian Splatting from a Single Blurry Image and Event
  Stream","Novel view synthesis has been greatly enhanced by the development of radiance
field methods. The introduction of 3D Gaussian Splatting (3DGS) has effectively
addressed key challenges, such as long training times and slow rendering
speeds, typically associated with Neural Radiance Fields (NeRF), while
maintaining high-quality reconstructions. In this work (BeSplat), we
demonstrate the recovery of sharp radiance field (Gaussian splats) from a
single motion-blurred image and its corresponding event stream. Our method
jointly learns the scene representation via Gaussian Splatting and recovers the
camera motion through Bezier SE(3) formulation effectively, minimizing
discrepancies between synthesized and real-world measurements of both blurry
image and corresponding event stream. We evaluate our approach on both
synthetic and real datasets, showcasing its ability to render view-consistent,
sharp images from the learned radiance field and the estimated camera
trajectory. To the best of our knowledge, ours is the first work to address
this highly challenging ill-posed problem in a Gaussian Splatting framework
with the effective incorporation of temporal information captured using the
event stream.",2024-12-26T22:35:29Z,http://arxiv.org/abs/2412.19370v1,"Gopi Raju Matta, Reddypalli Trisha, Kaushik Mitra"
"Central limit theorems for vector-valued composite functionals with
  smoothing and applications","This paper focuses on vector-valued composite functionals, which may be
nonlinear in probability. Our primary goal is to establish central limit
theorems for these functionals when mixed estimators are employed. Our study is
relevant to the evaluation and comparison of risk in decision-making contexts
and extends to functionals that arise in machine learning methods. A
generalized family of composite risk functionals is presented, which
encompasses most of the known coherent risk measures including systemic
measures of risk. The paper makes two main contributions. First, we analyze
vector-valued functionals, providing a framework for evaluating
high-dimensional risks. This framework facilitates the comparison of multiple
risk measures, as well as the estimation and asymptotic analysis of systemic
risk and its optimal value in decision-making problems. Second, we derive novel
central limit theorems for optimized composite functionals when mixed types of
estimators: empirical and smoothed estimators are used. We provide verifiable
sufficient conditions for the central limit formulae and show their
applicability to several popular measures of risk.",2024-12-26T22:23:11Z,http://arxiv.org/abs/2412.19367v1,"Huhui Chen, Darinka Dentcheva, Yang Lin, Gregory J. Stock"
Constructive approximate transport maps with normalizing flows,"We study an approximate controllability problem for the continuity equation
and its application to constructing transport maps with normalizing flows.
Specifically, we construct time-dependent controls $\theta=(w, a, b)$ in the
vector field $w(a^\top x + b)_+$ to approximately transport a known base
density $\rho_{\mathrm{B}}$ to a target density $\rho_*$. The approximation
error is measured in relative entropy, and $\theta$ are constructed piecewise
constant, with bounds on the number of switches being provided. Our main result
relies on an assumption on the relative tail decay of $\rho_*$ and
$\rho_{\mathrm{B}}$, and provides hints on characterizing the reachable space
of the continuity equation in relative entropy.",2024-12-26T22:09:33Z,http://arxiv.org/abs/2412.19366v1,"Antonio Álvarez-López, Borjan Geshkovski, Domènec Ruiz-Balet"
Large Language Models for Market Research: A Data-augmentation Approach,"Large Language Models (LLMs) have transformed artificial intelligence by
excelling in complex natural language processing tasks. Their ability to
generate human-like text has opened new possibilities for market research,
particularly in conjoint analysis, where understanding consumer preferences is
essential but often resource-intensive. Traditional survey-based methods face
limitations in scalability and cost, making LLM-generated data a promising
alternative. However, while LLMs have the potential to simulate real consumer
behavior, recent studies highlight a significant gap between LLM-generated and
human data, with biases introduced when substituting between the two. In this
paper, we address this gap by proposing a novel statistical data augmentation
approach that efficiently integrates LLM-generated data with real data in
conjoint analysis. Our method leverages transfer learning principles to debias
the LLM-generated data using a small amount of human data. This results in
statistically robust estimators with consistent and asymptotically normal
properties, in contrast to naive approaches that simply substitute human data
with LLM-generated data, which can exacerbate bias. We validate our framework
through an empirical study on COVID-19 vaccine preferences, demonstrating its
superior ability to reduce estimation error and save data and costs by 24.9\%
to 79.8\%. In contrast, naive approaches fail to save data due to the inherent
biases in LLM-generated data compared to human data. Another empirical study on
sports car choices validates the robustness of our results. Our findings
suggest that while LLM-generated data is not a direct substitute for human
responses, it can serve as a valuable complement when used within a robust
statistical framework.",2024-12-26T22:06:29Z,http://arxiv.org/abs/2412.19363v1,"Mengxin Wang, Dennis J. Zhang, Heng Zhang"
"Evaluating Convolutional Neural Networks for COVID-19 classification in
  chest X-ray images","Coronavirus Disease 2019 (COVID-19) pandemic rapidly spread globally,
impacting the lives of billions of people. The effective screening of infected
patients is a critical step to struggle with COVID-19, and treating the
patients avoiding this quickly disease spread. The need for automated and
scalable methods has increased due to the unavailability of accurate automated
toolkits. Recent researches using chest X-ray images suggest they include
relevant information about the COVID-19 virus. Hence, applying machine learning
techniques combined with radiological imaging promises to identify this disease
accurately. It is straightforward to collect these images once it is spreadly
shared and analyzed in the world. This paper presents a method for automatic
COVID-19 detection using chest Xray images through four convolutional neural
networks, namely: AlexNet, VGG-11, SqueezeNet, and DenseNet-121. This method
had been providing accurate diagnostics for positive or negative COVID-19
classification. We validate our experiments using a ten-fold cross-validation
procedure over the training and test sets. Our findings include the shallow
fine-tuning and data augmentation strategies that can assist in dealing with
the low number of positive COVID-19 images publicly available. The accuracy for
all CNNs is higher than 97.00%, and the SqueezeNet model achieved the best
result with 99.20%.",2024-12-26T22:05:30Z,http://arxiv.org/abs/2412.19362v1,"Leonardo Gabriel Ferreira Rodrigues, Danilo Ferreira da Silva, Larissa Ferreira Rodrigues, João Fernando Mari"
Dynamic Skill Adaptation for Large Language Models,"We present Dynamic Skill Adaptation (DSA), an adaptive and dynamic framework
to adapt novel and complex skills to Large Language Models (LLMs). Compared
with previous work which learns from human-curated and static data in random
orders, we propose to first automatically generate and organize the training
data by mimicking the learning pathways of human and then dynamically tailor
the training data based on the training dynamics. Specifically, inspired by the
learning structures and teaching strategies in the human education system, we
first construct a skill graph by decomposing complex skills into sub-skills and
arranging them based on their dependencies in human syllables. For every skill,
we utilize LLMs to generate both textbook-like data which contains detailed
descriptions of skills for pre-training and exercise-like data which targets at
explicitly utilizing the skills to solve problems for instruction-tuning.
Furthermore, during the instruction-tuning, we dynamically update the training
data which down-weight easy-to-learn examples, generate more complex examples,
and filter out data with errors. Experiments on large language models such as
LLAMA and Mistral demonstrate the effectiveness of our proposed methods in
adapting math reasoning skills and social study skills.",2024-12-26T22:04:23Z,http://arxiv.org/abs/2412.19361v1,"Jiaao Chen, Diyi Yang"
Microscopic imprints of learned solutions in adaptive resistor networks,"In physical networks trained using supervised learning, physical parameters
are adjusted to produce desired responses to inputs. An example is electrical
contrastive local learning networks of nodes connected by edges that are
resistors that adjust their conductances during training. When an edge
conductance changes, it upsets the current balance of every node. In response,
physics adjusts the node voltages to minimize the dissipated power. Learning in
these systems is therefore a coupled double-optimization process, in which the
network descends both a cost landscape in the high-dimensional space of edge
conductances, and a physical landscape -- the power -- in the high-dimensional
space of node voltages. Because of this coupling, the physical landscape of a
trained network contains information about the learned task. Here we
demonstrate that all the physical information relevant to the trained
input-output relation can be captured by a susceptibility, an experimentally
measurable quantity. We supplement our theoretical results with simulations to
show that the susceptibility is positively correlated with functional
importance and that we can extract physical insight into how the system
performs the task from the conductances of highly susceptible edges.",2024-12-26T21:36:23Z,http://arxiv.org/abs/2412.19356v1,"Marcel Guzman, Felipe Martins, Menachem Stern, Andrea J. Liu"
"Quantum-Inspired Weight-Constrained Neural Network: Reducing Variable
  Numbers by 100x Compared to Standard Neural Networks","Although quantum machine learning has shown great promise, the practical
application of quantum computers remains constrained in the noisy
intermediate-scale quantum era. To take advantage of quantum machine learning,
we investigate the underlying mathematical principles of these quantum models
and adapt them to classical machine learning frameworks. Specifically, we
develop a classical weight-constrained neural network that generates weights
based on quantum-inspired insights. We find that this approach can reduce the
number of variables in a classical neural network by a factor of 135 while
preserving its learnability. In addition, we develop a dropout method to
enhance the robustness of quantum machine learning models, which are highly
susceptible to adversarial attacks. This technique can also be applied to
improve the adversarial resilience of the classical weight-constrained neural
network, which is essential for industry applications, such as self-driving
vehicles. Our work offers a novel approach to reduce the complexity of large
classical neural networks, addressing a critical challenge in machine learning.",2024-12-26T21:35:12Z,http://arxiv.org/abs/2412.19355v1,"Shaozhi Li, M Sabbir Salek, Binayyak Roy, Yao Wang, Mashrur Chowdhury"
"SuperSalt: Equivariant Neural Network Force Fields for Multicomponent
  Molten Salts System","Molten salts are crucial for clean energy applications, yet exploring their
thermophysical properties across diverse chemical space remains challenging. We
present the development of a machine learning interatomic potential (MLIP)
called SuperSalt, which targets 11-cation chloride melts and captures the
essential physics of molten salts with near-DFT accuracy. Using an efficient
workflow that integrates systems of one, two, and 11 components, the SuperSalt
potential can accurately predict thermophysical properties such as density,
bulk modulus, thermal expansion, and heat capacity. Our model is validated
across a broad chemical space, demonstrating excellent transferability. We
further illustrate how Bayesian optimization combined with SuperSalt can
accelerate the discovery of optimal salt compositions with desired properties.
This work provides a foundation for future studies that allows easy extensions
to more complex systems, such as those containing additional elements.
SuperSalt represents a shift towards a more universal, efficient, and accurate
modeling of molten salts for advanced energy applications.",2024-12-26T21:32:08Z,http://arxiv.org/abs/2412.19353v1,"Chen Shen, Siamak Attarian, Yixuan Zhang, Hongbin Zhang, Mark Asta, Izabela Szlufarska, Dane Morgan"
"Federated Hybrid Training and Self-Adversarial Distillation: Towards
  Robust Edge Networks","Federated learning (FL) is a distributed training technology that enhances
data privacy in mobile edge networks by allowing data owners to collaborate
without transmitting raw data to the edge server. However, data heterogeneity
and adversarial attacks pose challenges to develop an unbiased and robust
global model for edge deployment. To address this, we propose Federated hyBrid
Adversarial training and self-adversarial disTillation (FedBAT), a new
framework designed to improve both robustness and generalization of the global
model. FedBAT seamlessly integrates hybrid adversarial training and
self-adversarial distillation into the conventional FL framework from data
augmentation and feature distillation perspectives. From a data augmentation
perspective, we propose hybrid adversarial training to defend against
adversarial attacks by balancing accuracy and robustness through a weighted
combination of standard and adversarial training. From a feature distillation
perspective, we introduce a novel augmentation-invariant adversarial
distillation method that aligns local adversarial features of augmented images
with their corresponding unbiased global clean features. This alignment can
effectively mitigate bias from data heterogeneity while enhancing both the
robustness and generalization of the global model. Extensive experimental
results across multiple datasets demonstrate that FedBAT yields comparable or
superior performance gains in improving robustness while maintaining accuracy
compared to several baselines.",2024-12-26T21:32:08Z,http://arxiv.org/abs/2412.19354v1,"Yu Qiao, Apurba Adhikary, Kitae Kim, Eui-Nam Huh, Zhu Han, Choong Seon Hong"
ETTA: Elucidating the Design Space of Text-to-Audio Models,"Recent years have seen significant progress in Text-To-Audio (TTA) synthesis,
enabling users to enrich their creative workflows with synthetic audio
generated from natural language prompts. Despite this progress, the effects of
data, model architecture, training objective functions, and sampling strategies
on target benchmarks are not well understood. With the purpose of providing a
holistic understanding of the design space of TTA models, we set up a
large-scale empirical experiment focused on diffusion and flow matching models.
Our contributions include: 1) AF-Synthetic, a large dataset of high quality
synthetic captions obtained from an audio understanding model; 2) a systematic
comparison of different architectural, training, and inference design choices
for TTA models; 3) an analysis of sampling methods and their Pareto curves with
respect to generation quality and inference speed. We leverage the knowledge
obtained from this extensive analysis to propose our best model dubbed
Elucidated Text-To-Audio (ETTA). When evaluated on AudioCaps and MusicCaps,
ETTA provides improvements over the baselines trained on publicly available
data, while being competitive with models trained on proprietary data. Finally,
we show ETTA's improved ability to generate creative audio following complex
and imaginative captions -- a task that is more challenging than current
benchmarks.",2024-12-26T21:13:12Z,http://arxiv.org/abs/2412.19351v1,"Sang-gil Lee, Zhifeng Kong, Arushi Goel, Sungwon Kim, Rafael Valle, Bryan Catanzaro"
"On the Expressiveness and Length Generalization of Selective State-Space
  Models on Regular Languages","Selective state-space models (SSMs) are an emerging alternative to the
Transformer, offering the unique advantage of parallel training and sequential
inference. Although these models have shown promising performance on a variety
of tasks, their formal expressiveness and length generalization properties
remain underexplored. In this work, we provide insight into the workings of
selective SSMs by analyzing their expressiveness and length generalization
performance on regular language tasks, i.e., finite-state automaton (FSA)
emulation. We address certain limitations of modern SSM-based architectures by
introducing the Selective Dense State-Space Model (SD-SSM), the first selective
SSM that exhibits perfect length generalization on a set of various regular
language tasks using a single layer. It utilizes a dictionary of dense
transition matrices, a softmax selection mechanism that creates a convex
combination of dictionary matrices at each time step, and a readout consisting
of layer normalization followed by a linear map. We then proceed to evaluate
variants of diagonal selective SSMs by considering their empirical performance
on commutative and non-commutative automata. We explain the experimental
results with theoretical considerations. Our code is available at
https://github.com/IBM/selective-dense-state-space-model.",2024-12-26T20:53:04Z,http://arxiv.org/abs/2412.19350v1,"Aleksandar Terzić, Michael Hersche, Giacomo Camposampiero, Thomas Hofmann, Abu Sebastian, Abbas Rahimi"
"Semi-Supervised Learning from Small Annotated Data and Large Unlabeled
  Data for Fine-grained PICO Entity Recognition","Objective: Extracting PICO elements -- Participants, Intervention,
Comparison, and Outcomes -- from clinical trial literature is essential for
clinical evidence retrieval, appraisal, and synthesis. Existing approaches do
not distinguish the attributes of PICO entities. This study aims to develop a
named entity recognition (NER) model to extract PICO entities with fine
granularities.
  Materials and Methods: Using a corpus of 2,511 abstracts with PICO mentions
from 4 public datasets, we developed a semi-supervised method to facilitate the
training of a NER model, FinePICO, by combining limited annotated data of PICO
entities and abundant unlabeled data. For evaluation, we divided the entire
dataset into two subsets: a smaller group with annotations and a larger group
without annotations. We then established the theoretical lower and upper
performance bounds based on the performance of supervised learning models
trained solely on the small, annotated subset and on the entire set with
complete annotations, respectively. Finally, we evaluated FinePICO on both the
smaller annotated subset and the larger, initially unannotated subset. We
measured the performance of FinePICO using precision, recall, and F1.
  Results: Our method achieved precision/recall/F1 of 0.567/0.636/0.60,
respectively, using a small set of annotated samples, outperforming the
baseline model (F1: 0.437) by more than 16\%. The model demonstrates
generalizability to a different PICO framework and to another corpus, which
consistently outperforms the benchmark in diverse experimental settings
(p-value \textless0.001).
  Conclusion: This study contributes a generalizable and effective
semi-supervised approach to named entity recognition leveraging large unlabeled
data together with small, annotated data. It also initially supports
fine-grained PICO extraction.",2024-12-26T20:24:35Z,http://arxiv.org/abs/2412.19346v1,"Fangyi Chen, Gongbo Zhang, Yilu Fang, Yifan Peng, Chunhua Weng"
"A Reinforcement Learning-Based Task Mapping Method to Improve the
  Reliability of Clustered Manycores","The increasing scale of manycore systems poses significant challenges in
managing reliability while meeting performance demands. Simultaneously, these
systems become more susceptible to different aging mechanisms such as
negative-bias temperature instability (NBTI), hot carrier injection (HCI), and
thermal cycling (TC), as well as the electromigration (EM) phenomenon. In this
paper, we propose a reinforcement learning (RL)-based task mapping method to
improve the reliability of manycore systems considering the aforementioned
aging mechanisms, which consists of three steps including bin packing,
task-to-bin mapping, and task-to-core mapping. In the initial step, a
density-based spatial application with noise (DBSCAN) clustering method is
employed to compose some clusters (bins) based on the cores temperature. Then,
the Q-learning algorithm is used for the two latter steps, to map the arrived
task on a core such that the minimum thermal variation is occurred among all
the bins. Compared to the state-of-the-art works, the proposed method is
performed during runtime without requiring any parameter to be calculated
offline. The effectiveness of the proposed technique is evaluated on 16, 32,
and 64 cores systems using SPLASH2 and PARSEC benchmark suite applications. The
results demonstrate up to 27% increase in the mean time to failure (MTTF)
compared to the state-of-the-art task mapping techniques.",2024-12-26T20:08:10Z,http://arxiv.org/abs/2412.19340v1,"Fatemeh Hossein-Khani, Omid Akbari"
Modular quantum extreme reservoir computing,"The connectivity between qubits plays a crucial role in the performance of
quantum extreme reservoir computing (QERC), particularly regarding long-range
and inter-modular connections. We demonstrate that sufficiently long-range
connections within a single module can achieve performance comparable to fully
connected networks in supervised learning tasks. Further analysis of
inter-modular connection schemes -- such as boundary, parallel, and arbitrary
links -- shows that even a small number of well-placed connections can
significantly enhance QERC performance. These findings suggest that modular
QERC architectures, which could be more easily implemented on two-dimensional
quantum chips or through the integration of small quantum systems, provide an
effective approach for machine learning tasks.",2024-12-26T19:15:09Z,http://arxiv.org/abs/2412.19336v1,"Hon Wai Lau, Aoi Hayashi, Akitada Sakurai, William John Munro, Kae Nemoto"
"DPmoire: A tool for constructing accurate machine learning force fields
  in moiré systems","In moir\'e systems, the impact of lattice relaxation on electronic band
structures is significant, yet the computational demands of first-principles
relaxation are prohibitively high due to the large number of atoms involved. To
address this challenge, We introduce a robust methodology for the construction
of machine learning potentials specifically tailored for moir\'e structures and
present an open-source software package DPmoire designed to facilitate this
process. Utilizing this package, we have developed machine learning force
fields (MLFFs) for MX$_2$ (M = Mo, W; X = S, Se, Te) materials. Our approach
not only streamlines the computational process but also ensures accurate
replication of the detailed electronic and structural properties typically
observed in density functional theory (DFT) relaxations. The MLFFs were
rigorously validated against standard DFT results, confirming their efficacy in
capturing the complex interplay of atomic interactions within these layered
materials. This development not only enhances our ability to explore the
physical properties of moir\'e systems with reduced computational overhead but
also opens new avenues for the study of relaxation effects and their impact on
material properties in two-dimensional layered structures.",2024-12-26T19:00:40Z,http://arxiv.org/abs/2412.19333v1,"Jiaxuan Liu, Zhong Fang, Hongming Weng, Quansheng Wu"
"CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language
  Models","Recent advances in Large Vision-Language Models (LVLMs) have sparked
significant progress in general-purpose vision tasks through visual instruction
tuning. While some works have demonstrated the capability of LVLMs to generate
segmentation masks that align phrases with natural language descriptions in a
single image, they struggle with segmentation-grounded comparisons across
multiple images, particularly at finer granularities such as object parts. In
this paper, we introduce the new task of part-focused semantic co-segmentation,
which seeks to identify and segment common and unique objects and parts across
images. To address this task, we present CALICO, the first LVLM that can
segment and reason over multiple masks across images, enabling object
comparison based on their constituent parts. CALICO features two proposed
components, a novel Correspondence Extraction Module, which captures
semantic-rich information to identify part-level correspondences between
objects, and a Correspondence Adaptation Module, which embeds this information
into the LVLM to facilitate multi-image understanding in a parameter-efficient
manner. To support training and evaluation, we curate MixedParts, a
comprehensive multi-image segmentation dataset containing $\sim$2.4M samples
across $\sim$44K images with diverse object and part categories. Experimental
results show CALICO, finetuned on only 0.3% of its architecture, achieves
robust performance in part-focused semantic co-segmentation.",2024-12-26T18:59:37Z,http://arxiv.org/abs/2412.19331v1,"Kiet A. Nguyen, Adheesh Juvekar, Tianjiao Yu, Muntasir Wahed, Ismini Lourentzou"
Identifying Split Vacancies with Foundation Models and Electrostatics,"Point defects are ubiquitous in solid-state compounds, dictating many
functional properties such as conductivity, catalytic activity and carrier
recombination. Over the past decade, the prevalence of metastable defect
geometries and their importance to relevant properties has been increasingly
recognised. A particularly striking example of this is split vacancies, where
an isolated atomic vacancy transforms to a stoichiometry-conserving complex of
two vacancies and an interstitial ($V_X \rightarrow [V_X + X_i + V_X]$), which
can be accompanied by a dramatic lowering of the defect energy and change in
behaviour. Such species are particularly challenging to identify from
computation, due to the `non-local' nature of this reconstruction. Here, I
present an approach for efficiently identifying such species in solid-state
compounds, through tiered screening which combines geometric analysis,
electrostatic energies and foundation machine learning (ML) models. This
approach allows the screening of all compounds in the Materials Project
database (including all entries in the ICSD, along with several thousand
predicted metastable materials), identifying thousands of split vacancy
configurations, hitherto unknown. This study highlights both the potential
utility of machine-learning potentials for defect investigations, with
important caveats, and the importance of global optimisation approaches for
correctly identifying stable defect geometries.",2024-12-26T18:58:52Z,http://arxiv.org/abs/2412.19330v1,Seán R. Kavanagh
"Deep learning and whole-brain networks for biomarker discovery: modeling
  the dynamics of brain fluctuations in resting-state and cognitive tasks","Background: Brain network models offer insights into brain dynamics, but the
utility of model-derived bifurcation parameters as biomarkers remains
underexplored. Objective: This study evaluates bifurcation parameters from a
whole-brain network model as biomarkers for distinguishing brain states
associated with resting-state and task-based cognitive conditions. Methods:
Synthetic BOLD signals were generated using a supercritical Hopf brain network
model to train deep learning models for bifurcation parameter prediction.
Inference was performed on Human Connectome Project data, including both
resting-state and task-based conditions. Statistical analyses assessed the
separability of brain states based on bifurcation parameter distributions.
Results: Bifurcation parameter distributions differed significantly across task
and resting-state conditions ($p &lt; 0.0001$ for all but one comparison).
Task-based brain states exhibited higher bifurcation values compared to rest.
Conclusion: Bifurcation parameters effectively differentiate cognitive and
resting states, warranting further investigation as biomarkers for brain state
characterization and neurological disorder assessment.",2024-12-26T18:58:38Z,http://arxiv.org/abs/2412.19329v1,"Facundo Roffet, Gustavo Deco, Claudio Delrieux, Gustavo Patow"
"Resolving the Ambiguity of Complete-to-Partial Point Cloud Registration
  for Image-Guided Liver Surgery with Patches-to-Partial Matching","In image-guided liver surgery, the initial rigid alignment between
preoperative and intraoperative data, often represented as point clouds, is
crucial for providing sub-surface information from preoperative CT/MRI images
to the surgeon during the procedure. Currently, this alignment is typically
performed using semi-automatic methods, which, while effective to some extent,
are prone to errors that demand manual correction. Point cloud
correspondence-based registration methods are promising to serve as a fully
automatic solution. However, they may struggle in scenarios with limited
intraoperative surface visibility, a common challenge in liver surgery,
particularly in laparoscopic procedures, which we refer to as
complete-to-partial ambiguity. We first illustrate this ambiguity by evaluating
the performance of state-of-the-art learning-based point cloud registration
methods on our carefully constructed in silico and in vitro datasets. Then, we
propose a patches-to-partial matching strategy as a plug-and-play module to
resolve the ambiguity, which can be seamlessly integrated into learning-based
registration methods without disrupting their end-to-end structure. It has
proven effective and efficient in improving registration performance for cases
with limited intraoperative visibility. The constructed benchmark and the
proposed module establish a solid foundation for advancing applications of
point cloud correspondence-based registration methods in image-guided liver
surgery.",2024-12-26T18:58:29Z,http://arxiv.org/abs/2412.19328v1,"Zixin Yang, Jon S. Heiselman, Cheng Han, Kelly Merrell, Richard Simon, Cristian. A. Linte"
"Task Preference Optimization: Improving Multimodal Large Language Models
  with Vision Task Alignment","Current multimodal large language models (MLLMs) struggle with fine-grained
or precise understanding of visuals though they give comprehensive perception
and reasoning in a spectrum of vision applications. Recent studies either
develop tool-using or unify specific visual tasks into the autoregressive
framework, often at the expense of overall multimodal performance. To address
this issue and enhance MLLMs with visual tasks in a scalable fashion, we
propose Task Preference Optimization (TPO), a novel method that utilizes
differentiable task preferences derived from typical fine-grained visual tasks.
TPO introduces learnable task tokens that establish connections between
multiple task-specific heads and the MLLM. By leveraging rich visual labels
during training, TPO significantly enhances the MLLM's multimodal capabilities
and task-specific performance. Through multi-task co-training within TPO, we
observe synergistic benefits that elevate individual task performance beyond
what is achievable through single-task training methodologies. Our
instantiation of this approach with VideoChat and LLaVA demonstrates an overall
14.6% improvement in multimodal performance compared to baseline models.
Additionally, MLLM-TPO demonstrates robust zero-shot capabilities across
various tasks, performing comparably to state-of-the-art supervised models. The
code will be released at https://github.com/OpenGVLab/TPO",2024-12-26T18:56:05Z,http://arxiv.org/abs/2412.19326v1,"Ziang Yan, Zhilin Li, Yinan He, Chenting Wang, Kunchang Li, Xinhao Li, Xiangyu Zeng, Zilei Wang, Yali Wang, Yu Qiao, Limin Wang, Yi Wang"
"Performance Control in Early Exiting to Deploy Large Models at the Same
  Cost of Smaller Ones","Early Exiting (EE) is a promising technique for speeding up inference by
adaptively allocating compute resources to data points based on their
difficulty. The approach enables predictions to exit at earlier layers for
simpler samples while reserving more computation for challenging ones. In this
study, we first present a novel perspective on the EE approach, showing that
larger models deployed with EE can achieve higher performance than smaller
models while maintaining similar computational costs. As existing EE approaches
rely on confidence estimation at each exit point, we further study the impact
of overconfidence on the controllability of the compute-performance trade-off.
We introduce Performance Control Early Exiting (PCEE), a method that enables
accuracy thresholding by basing decisions not on a data point's confidence but
on the average accuracy of samples with similar confidence levels from a
held-out validation set. In our experiments, we show that PCEE offers a simple
yet computationally efficient approach that provides better control over
performance than standard confidence-based approaches, and allows us to scale
up model sizes to yield performance gain while reducing the computational cost.",2024-12-26T18:54:32Z,http://arxiv.org/abs/2412.19325v1,"Mehrnaz Mofakhami, Reza Bayat, Ioannis Mitliagkas, Joao Monteiro, Valentina Zantedeschi"
"A novel framework for MCDM based on Z numbers and soft likelihood
  function","The optimization on the structure of process of information management under
uncertain environment has attracted lots of attention from researchers around
the world. Nevertheless, how to obtain accurate and rational evaluation from
assessments produced by experts is still an open problem. Specially,
intuitionistic fuzzy set provides an effective solution in handling
indeterminate information. And Yager proposes a novel method for fusion of
probabilistic evidence to handle uncertain and conflicting information lately
which is called soft likelihood function. This paper devises a novel framework
of soft likelihood function based on information volume of fuzzy membership and
credibility measure for extracting truly useful and valuable information from
uncertainty. An application is provided to verify the validity and correctness
of the proposed framework. Besides, the comparisons with other existing methods
further demonstrate the superiority of the novel framework of soft likelihood
function.",2024-12-26T18:47:19Z,http://arxiv.org/abs/2412.19321v1,Yuanpeng He
Adaptive Conformal Inference by Betting,"Conformal prediction is a valuable tool for quantifying predictive
uncertainty of machine learning models. However, its applicability relies on
the assumption of data exchangeability, a condition which is often not met in
real-world scenarios. In this paper, we consider the problem of adaptive
conformal inference without any assumptions about the data generating process.
Existing approaches for adaptive conformal inference are based on optimizing
the pinball loss using variants of online gradient descent. A notable
shortcoming of such approaches is in their explicit dependence on and
sensitivity to the choice of the learning rates. In this paper, we propose a
different approach for adaptive conformal inference that leverages
parameter-free online convex optimization techniques. We prove that our method
controls long-term miscoverage frequency at a nominal level and demonstrate its
convincing empirical performance without any need of performing cumbersome
parameter tuning.",2024-12-26T18:42:08Z,http://arxiv.org/abs/2412.19318v1,"Aleksandr Podkopaev, Darren Xu, Kuang-Chih Lee"
"xSRL: Safety-Aware Explainable Reinforcement Learning -- Safety as a
  Product of Explainability","Reinforcement learning (RL) has shown great promise in simulated
environments, such as games, where failures have minimal consequences. However,
the deployment of RL agents in real-world systems such as autonomous vehicles,
robotics, UAVs, and medical devices demands a higher level of safety and
transparency, particularly when facing adversarial threats. Safe RL algorithms
have been developed to address these concerns by optimizing both task
performance and safety constraints. However, errors are inevitable, and when
they occur, it is essential that the RL agents can also explain their actions
to human operators. This makes trust in the safety mechanisms of RL systems
crucial for effective deployment. Explainability plays a key role in building
this trust by providing clear, actionable insights into the agent's
decision-making process, ensuring that safety-critical decisions are well
understood. While machine learning (ML) has seen significant advances in
interpretability and visualization, explainability methods for RL remain
limited. Current tools fail to address the dynamic, sequential nature of RL and
its needs to balance task performance with safety constraints over time. The
re-purposing of traditional ML methods, such as saliency maps, is inadequate
for safety-critical RL applications where mistakes can result in severe
consequences. To bridge this gap, we propose xSRL, a framework that integrates
both local and global explanations to provide a comprehensive understanding of
RL agents' behavior. xSRL also enables developers to identify policy
vulnerabilities through adversarial attacks, offering tools to debug and patch
agents without retraining. Our experiments and user studies demonstrate xSRL's
effectiveness in increasing safety in RL systems, making them more reliable and
trustworthy for real-world deployment. Code is available at
https://github.com/risal-shefin/xSRL.",2024-12-26T18:19:04Z,http://arxiv.org/abs/2412.19311v1,"Risal Shahriar Shefin, Md Asifur Rahman, Thai Le, Sarra Alqahtani"
"Perceive, Query &amp; Reason: Enhancing Video QA with Question-Guided
  Temporal Queries","Video Question Answering (Video QA) is a challenging video understanding task
that requires models to comprehend entire videos, identify the most relevant
information based on contextual cues from a given question, and reason
accurately to provide answers. Recent advancements in Multimodal Large Language
Models (MLLMs) have transformed video QA by leveraging their exceptional
commonsense reasoning capabilities. This progress is largely driven by the
effective alignment between visual data and the language space of MLLMs.
However, for video QA, an additional space-time alignment poses a considerable
challenge for extracting question-relevant information across frames. In this
work, we investigate diverse temporal modeling techniques to integrate with
MLLMs, aiming to achieve question-guided temporal modeling that leverages
pre-trained visual and textual alignment in MLLMs. We propose T-Former, a novel
temporal modeling method that creates a question-guided temporal bridge between
frame-wise visual perception and the reasoning capabilities of LLMs. Our
evaluation across multiple video QA benchmarks demonstrates that T-Former
competes favorably with existing temporal modeling approaches and aligns with
recent advancements in video QA.",2024-12-26T17:53:14Z,http://arxiv.org/abs/2412.19304v1,"Roberto Amoroso, Gengyuan Zhang, Rajat Koner, Lorenzo Baraldi, Rita Cucchiara, Volker Tresp"
RecLM: Recommendation Instruction Tuning,"Modern recommender systems aim to deeply understand users' complex
preferences through their past interactions. While deep collaborative filtering
approaches using Graph Neural Networks (GNNs) excel at capturing user-item
relationships, their effectiveness is limited when handling sparse data or
zero-shot scenarios, primarily due to constraints in ID-based embedding
functions. To address these challenges, we propose a model-agnostic
recommendation instruction-tuning paradigm that seamlessly integrates large
language models with collaborative filtering. Our proposed Recommendation
Language Model (RecLM) enhances the capture of user preference diversity
through a carefully designed reinforcement learning reward function that
facilitates self-augmentation of language models. Comprehensive evaluations
demonstrate significant advantages of our approach across various settings, and
its plug-and-play compatibility with state-of-the-art recommender systems
results in notable performance enhancements.",2024-12-26T17:51:54Z,http://arxiv.org/abs/2412.19302v1,"Yangqin Jiang, Yuhao Yang, Lianghao Xia, Da Luo, Kangyi Lin, Chao Huang"
RAG with Differential Privacy,"Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to
provide *Large Language Models* (LLM) with fresh and relevant context,
mitigating the risk of hallucinations and improving the overall quality of
responses in environments with large and fast moving knowledge bases. However,
the integration of external documents into the generation process raises
significant privacy concerns. Indeed, when added to a prompt, it is not
possible to guarantee a response will not inadvertently expose confidential
data, leading to potential breaches of privacy and ethical dilemmas. This paper
explores a practical solution to this problem suitable to general knowledge
extraction from personal data. It shows *differentially private token
generation* is a viable approach to private RAG.",2024-12-26T17:34:26Z,http://arxiv.org/abs/2412.19291v1,Nicolas Grislain
"ViPCap: Retrieval Text-Based Visual Prompts for Lightweight Image
  Captioning","Recent lightweight image captioning models using retrieved data mainly focus
on text prompts. However, previous works only utilize the retrieved text as
text prompts, and the visual information relies only on the CLIP visual
embedding. Because of this issue, there is a limitation that the image
descriptions inherent in the prompt are not sufficiently reflected in the
visual embedding space. To tackle this issue, we propose ViPCap, a novel
retrieval text-based visual prompt for lightweight image captioning. ViPCap
leverages the retrieved text with image information as visual prompts to
enhance the ability of the model to capture relevant visual information. By
mapping text prompts into the CLIP space and generating multiple randomized
Gaussian distributions, our method leverages sampling to explore randomly
augmented distributions and effectively retrieves the semantic features that
contain image information. These retrieved features are integrated into the
image and designated as the visual prompt, leading to performance improvements
on the datasets such as COCO, Flickr30k, and NoCaps. Experimental results
demonstrate that ViPCap significantly outperforms prior lightweight captioning
models in efficiency and effectiveness, demonstrating the potential for a
plug-and-play solution.",2024-12-26T17:29:38Z,http://arxiv.org/abs/2412.19289v1,"Taewhan Kim, Soeun Lee, Si-Woo Kim, Dong-Jin Kim"
"Time Series Foundational Models: Their Role in Anomaly Detection and
  Prediction","Time series foundational models (TSFM) have gained prominence in time series
forecasting, promising state-of-the-art performance across various
applications. However, their application in anomaly detection and prediction
remains underexplored, with growing concerns regarding their black-box nature,
lack of interpretability and applicability. This paper critically evaluates the
efficacy of TSFM in anomaly detection and prediction tasks. We systematically
analyze TSFM across multiple datasets, including those characterized by the
absence of discernible patterns, trends and seasonality. Our analysis shows
that while TSFMs can be extended for anomaly detection and prediction,
traditional statistical and deep learning models often match or outperform TSFM
in these tasks. Additionally, TSFMs require high computational resources but
fail to capture sequential dependencies effectively or improve performance in
few-shot or zero-shot scenarios. \noindent The preprocessed datasets, codes to
reproduce the results and supplementary materials are available at
https://github.com/smtmnfg/TSFM.",2024-12-26T17:15:30Z,http://arxiv.org/abs/2412.19286v1,"Chathurangi Shyalika, Harleen Kaur Bagga, Ahan Bhatt, Renjith Prasad, Alaa Al Ghazo, Amit Sheth"
"Thermal amplification and melting of phases in spin-orbit-coupled spin-1
  Bose-Einstein condensates","We implement Hartree-Fock-Bogoliubov theory with Popov approximation for a
homogeneous Raman-induced spin-orbit-coupled spin-1 Bose-Einstein condensate
and investigate the effects of finite temperature ($T$) on the ground-state
phase diagram. We calculate the roton gap as a function of Raman coupling
($\Omega$) or quadratic Zeeman field strength ($\epsilon$) to extract the
critical points separating the supersolid stripe phase from the plane wave or
zero-momentum phase at finite temperatures. We present a few representative
finite-temperature phase diagrams for the system in the $T-\Omega$ and
$T-\epsilon$ planes. Our observations indicate that the supersolid stripe phase
melts at finite temperatures. We also discuss the contrasting roles of quantum
and thermal fluctuations in shifting the phase boundary separating the
supersolid stripe from the plane-wave phase.",2024-12-26T17:03:05Z,http://arxiv.org/abs/2412.19285v1,"Ritu, Rajat, Arko Roy, Sandeep Gautam"
"PearSAN: A Machine Learning Method for Inverse Design using Pearson
  Correlated Surrogate Annealing","PearSAN is a machine learning-assisted optimization algorithm applicable to
inverse design problems with large design spaces, where traditional optimizers
struggle. The algorithm leverages the latent space of a generative model for
rapid sampling and employs a Pearson correlated surrogate model to predict the
figure of merit of the true design metric. As a showcase example, PearSAN is
applied to thermophotovoltaic (TPV) metasurface design by matching the working
bands between a thermal radiator and a photovoltaic cell. PearSAN can work with
any pretrained generative model with a discretized latent space, making it easy
to integrate with VQ-VAEs and binary autoencoders. Its novel Pearson
correlational loss can be used as both a latent regularization method, similar
to batch and layer normalization, and as a surrogate training loss. We compare
both to previous energy matching losses, which are shown to enforce poor
regularization and performance, even with upgraded affine parameters. PearSAN
achieves a state-of-the-art maximum design efficiency of 97%, and is at least
an order of magnitude faster than previous methods, with an improved maximum
figure-of-merit gain.",2024-12-26T17:02:19Z,http://arxiv.org/abs/2412.19284v1,"Michael Bezick, Blake A. Wilson, Vaishnavi Iyer, Yuheng Chen, Vladimir M. Shalaev, Sabre Kais, Alexander V. Kildishev, Alexandra Boltasseva, Brad Lackey"
Probing medium response via strangeness enhancement around quenched jets,"Jet-induced medium excitation is a crucial part of jet interactions with the
quark-gluon plasma (QGP) in relativistic heavy-ion collisions, and has recently
been confirmed by experiment for the first time. Based on the AMPT model
simulation, we propose the strangeness enhancement around quenched jets as a
novel signature of jet-induced medium excitation. By applying the jet-particle
correlation techniques, we calculate jet-induced particle yields around the
jets and find a significant enhancement of the strange-to-non-strange-hadron
ratio and the double-to-single-strange-hadron ratio correlated with jets in
relativistic nucleus-nucleus collisions relative to proton-proton collisions.
This enhancement increases with both the strength of jet-QGP interactions and
the radial distance from jet axis. These observations align with the features
of jet-induced medium excitation and parton coalescence in hadron formation,
and await experimental validation in the future measurements.",2024-12-26T17:00:56Z,http://arxiv.org/abs/2412.19283v1,"Ao Luo, Shanshan Cao, Guang-You Qin"
Improving Generalization for AI-Synthesized Voice Detection,"AI-synthesized voice technology has the potential to create realistic human
voices for beneficial applications, but it can also be misused for malicious
purposes. While existing AI-synthesized voice detection models excel in
intra-domain evaluation, they face challenges in generalizing across different
domains, potentially becoming obsolete as new voice generators emerge. Current
solutions use diverse data and advanced machine learning techniques (e.g.,
domain-invariant representation, self-supervised learning), but are limited by
predefined vocoders and sensitivity to factors like background noise and
speaker identity. In this work, we introduce an innovative disentanglement
framework aimed at extracting domain-agnostic artifact features related to
vocoders. Utilizing these features, we enhance model learning in a flat loss
landscape, enabling escape from suboptimal solutions and improving
generalization. Extensive experiments on benchmarks show our approach
outperforms state-of-the-art methods, achieving up to 5.12% improvement in the
equal error rate metric in intra-domain and 7.59% in cross-domain evaluations.",2024-12-26T16:45:20Z,http://arxiv.org/abs/2412.19279v1,"Hainan Ren, Lin Li, Chun-Hao Liu, Xin Wang, Shu Hu"
"Flat panel laser displays enabled by large-scale visible photonic
  integrated circuits","Laser-based displays are highly sought after for their superior brightness
and color performance, especially in advanced applications like augmented
reality (AR). However, their broader adoption has been hindered by bulky
projector designs and complex optical module assemblies. Here, we introduce a
new laser display architecture enabled by large-scale visible photonic
integrated circuits (PICs) to address these challenges. Unlike previous
projector-style laser displays, this architecture features an ultra-thin,
flat-panel form factor, replacing bulky free-space illumination modules with a
single, high-performance photonic chip. Centimeter-scale PIC devices, which
integrate thousands of distinct optical components on-chip, are carefully
tailored to achieve high display uniformity, contrast, and efficiency. We
demonstrate a 2 mm-thick flat-panel laser display combining the PIC with a
liquid-crystal-on-silicon (LCoS) panel, achieving 211% of the color gamut and
more than 80% volume reduction compared to traditional LCoS displays. We
further showcase its application in a see-through AR system. Our work
represents a major advancement in the integration of nanophotonics with display
technology, enabling a range of new display concepts, from high-performance
immersive displays to slim-panel 3D holography.",2024-12-26T16:31:00Z,http://arxiv.org/abs/2412.19274v1,"Zhujun Shi, Risheng Cheng, Guohua Wei, Steven A. Hickman, Min Chul Shin, Peter Topalian, Lei Wang, Dusan Coso, Brian Le, Lizzy Lee, Sean Braxton, Alexander Koshelev, Maxwell F. Parsons, Rahul Agarwal, Barry Silverstein, Yun Wang, Giuseppe Calafiore"
Optimizing Multi-Stage Language Models for Effective Text Retrieval,"Efficient text retrieval is critical for applications such as legal document
analysis, particularly in specialized contexts like Japanese legal systems.
Existing retrieval methods often underperform in such domain-specific
scenarios, necessitating tailored approaches. In this paper, we introduce a
novel two-phase text retrieval pipeline optimized for Japanese legal datasets.
Our method leverages advanced language models to achieve state-of-the-art
performance, significantly improving retrieval efficiency and accuracy. To
further enhance robustness and adaptability, we incorporate an ensemble model
that integrates multiple retrieval strategies, resulting in superior outcomes
across diverse tasks. Extensive experiments validate the effectiveness of our
approach, demonstrating strong performance on both Japanese legal datasets and
widely recognized benchmarks like MS-MARCO. Our work establishes new standards
for text retrieval in domain-specific and general contexts, providing a
comprehensive solution for addressing complex queries in legal and multilingual
environments.",2024-12-26T16:05:19Z,http://arxiv.org/abs/2412.19265v1,"Quang Hoang Trung, Le Trung Hoang, Nguyen Van Hoang Phuc"
"MEDEC: A Benchmark for Medical Error Detection and Correction in
  Clinical Notes","Several studies showed that Large Language Models (LLMs) can answer medical
questions correctly, even outperforming the average human score in some medical
exams. However, to our knowledge, no study has been conducted to assess the
ability of language models to validate existing or generated medical text for
correctness and consistency. In this paper, we introduce MEDEC
(https://github.com/abachaa/MEDEC), the first publicly available benchmark for
medical error detection and correction in clinical notes, covering five types
of errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal
Organism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes
from three US hospital systems that were not previously seen by any LLM. The
dataset has been used for the MEDIQA-CORR shared task to evaluate seventeen
participating systems [Ben Abacha et al., 2024]. In this paper, we describe the
data creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,
Claude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and
correcting medical errors requiring both medical knowledge and reasoning
capabilities. We also conducted a comparative study where two medical doctors
performed the same task on the MEDEC test set. The results showed that MEDEC is
a sufficiently challenging benchmark to assess the ability of models to
validate existing or generated notes and to correct medical errors. We also
found that although recent LLMs have a good performance in error detection and
correction, they are still outperformed by medical doctors in these tasks. We
discuss the potential factors behind this gap, the insights from our
experiments, the limitations of current evaluation metrics, and share potential
pointers for future research.",2024-12-26T15:54:10Z,http://arxiv.org/abs/2412.19260v1,"Asma Ben Abacha, Wen-wai Yim, Yujuan Fu, Zhaoyi Sun, Meliha Yetisgen, Fei Xia, Thomas Lin"
"Prospects for detecting the dark matter particles and primordial black
  holes with the Hongmeng mission using the 21 cm global spectrum at cosmic
  dawn","Dark matter is believed to account for a significant portion of the mass in
the universe, exerting a critical influence on the formation and evolution of
cosmic structures. This research delves into the processes of annihilation and
decay of dark matter particles, which generate observable signals that deepen
our comprehension of their characteristics and behaviors. Furthermore, the
study explores the potential role of primordial black holes, with a focus on
the emissions of Hawking radiation that could offer valuable insights into
their distribution and size range. A key aspect of this investigation revolves
around the 21 cm signal, a vital tool for scrutinizing the effects of dark
matter particles and primordial black hole phenomena on the intergalactic
medium. The upcoming Hongmeng mission, featuring a lunar orbital interferometer
array, is poised to revolutionize our ability to observe the 21 cm signal. By
conducting measurements devoid of atmospheric disturbances, the mission will
significantly boost sensitivity to subtle signals associated with dark matter
particle annihilation, decay, and primordial black hole emissions. This study
assesses the expected performance of the Hongmeng mission in detecting these
telltale signs and aims to unveil fresh insights into the nature and
interactions of dark matter particles and primordial black hole emissions
through a meticulous analysis of the global 21 cm spectrum. The mission holds
immense promise for reshaping our understanding of the universe's concealed
components.",2024-12-26T15:47:51Z,http://arxiv.org/abs/2412.19257v1,"Meng-Lin Zhao, Sai Wang, Xin Zhang"
Multi-matrix Factorization Attention,"We propose novel attention architectures, Multi-matrix Factorization
Attention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard
Multi-Head Attention (MHA), including SOTA methods like MLA, fail to maintain
as strong performance under stringent Key-Value cache (KV cache) constraints.
MFA enhances model capacity by efficiently scaling up both the number and
dimension of attention heads through low-rank matrix factorization in the
Query-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory
requirements by repurposing the key cache as value through value projection
re-parameterization. MFA's design enables strong model capacity when working
under tight KV cache budget, while MFA-KR is suitable for even harsher KV cache
limits with minor performance trade-off. Notably, in our extensive and
large-scale experiments, the proposed architecture outperforms MLA and performs
comparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,
respectively.",2024-12-26T15:45:45Z,http://arxiv.org/abs/2412.19255v1,"Jingcheng Hu, Houyi Li, Yinmin Zhang, Zili Wang, Shuigeng Zhou, Xiangyu Zhang, Heung-Yeung Shum"
"Leveraging Self-Training and Variational Autoencoder for Agitation
  Detection in People with Dementia Using Wearable Sensors","Dementia is a neurodegenerative disorder that has been growing among elder
people over the past decades. This growth profoundly impacts the quality of
life for patients and caregivers due to the symptoms arising from it. Agitation
and aggression (AA) are some of the symptoms of people with severe dementia
(PwD) in long-term care or hospitals. AA not only causes discomfort but also
puts the patients or others at potential risk. Existing monitoring solutions
utilizing different wearable sensors integrated with Artificial Intelligence
(AI) offer a way to detect AA early enough for timely and adequate medical
intervention. However, most studies are limited by the availability of
accurately labeled datasets, which significantly affects the efficacy of such
solutions in real-world scenarios. This study presents a novel comprehensive
approach to detect AA in PwD using physiological data from the Empatica E4
wristbands. The research creates a diverse dataset, consisting of three
distinct datasets gathered from 14 participants across multiple hospitals in
Canada. These datasets have not been extensively explored due to their limited
labeling. We propose a novel approach employing self-training and a variational
autoencoder (VAE) to detect AA in PwD effectively. The proposed approach aims
to learn the representation of the features extracted using the VAE and then
uses a semi-supervised block to generate labels, classify events, and detect
AA. We demonstrate that combining Self-Training and Variational Autoencoder
mechanism significantly improves model performance in classifying AA in PwD.
Among the tested techniques, the XGBoost classifier achieved the highest
accuracy of 90.16\%. By effectively addressing the challenge of limited labeled
data, the proposed system not only learns new labels but also proves its
superiority in detecting AA.",2024-12-26T15:34:25Z,http://arxiv.org/abs/2412.19254v1,"Abeer Badawi, Somayya Elmoghazy, Samira Choudhury, Khalid Elgazzar, Amer Burhan"
"Localized exploration in contextual dynamic pricing achieves
  dimension-free regret","We study the problem of contextual dynamic pricing with a linear demand
model. We propose a novel localized exploration-then-commit (LetC) algorithm
which starts with a pure exploration stage, followed by a refinement stage that
explores near the learned optimal pricing policy, and finally enters a pure
exploitation stage. The algorithm is shown to achieve a minimax optimal,
dimension-free regret bound when the time horizon exceeds a polynomial of the
covariate dimension. Furthermore, we provide a general theoretical framework
that encompasses the entire time spectrum, demonstrating how to balance
exploration and exploitation when the horizon is limited. The analysis is
powered by a novel critical inequality that depicts the
exploration-exploitation trade-off in dynamic pricing, mirroring its existing
counterpart for the bias-variance trade-off in regularized regression. Our
theoretical results are validated by extensive experiments on synthetic and
real-world data.",2024-12-26T15:29:58Z,http://arxiv.org/abs/2412.19252v1,"Jinhang Chai, Yaqi Duan, Jianqing Fan, Kaizheng Wang"
"Causal Speech Enhancement with Predicting Semantics based on Quantized
  Self-supervised Learning Features","Real-time speech enhancement (SE) is essential to online speech
communication. Causal SE models use only the previous context while predicting
future information, such as phoneme continuation, may help performing causal
SE. The phonetic information is often represented by quantizing latent features
of self-supervised learning (SSL) models. This work is the first to incorporate
SSL features with causality into an SE model. The causal SSL features are
encoded and combined with spectrogram features using feature-wise linear
modulation to estimate a mask for enhancing the noisy input speech.
Simultaneously, we quantize the causal SSL features using vector quantization
to represent phonetic characteristics as semantic tokens. The model not only
encodes SSL features but also predicts the future semantic tokens in multi-task
learning (MTL). The experimental results using VoiceBank + DEMAND dataset show
that our proposed method achieves 2.88 in PESQ, especially with semantic
prediction MTL, in which we confirm that the semantic prediction played an
important role in causal SE.",2024-12-26T15:08:36Z,http://arxiv.org/abs/2412.19248v1,"Emiru Tsunoo, Yuki Saito, Wataru Nakata, Hiroshi Saruwatari"
Sentiment trading with large language models,"We investigate the efficacy of large language models (LLMs) in sentiment
analysis of U.S. financial news and their potential in predicting stock market
returns. We analyze a dataset comprising 965,375 news articles that span from
January 1, 2010, to June 30, 2023; we focus on the performance of various LLMs,
including BERT, OPT, FINBERT, and the traditional Loughran-McDonald dictionary
model, which has been a dominant methodology in the finance literature. The
study documents a significant association between LLM scores and subsequent
daily stock returns. Specifically, OPT, which is a GPT-3 based LLM, shows the
highest accuracy in sentiment prediction with an accuracy of 74.4%, slightly
ahead of BERT (72.5%) and FINBERT (72.2%). In contrast, the Loughran-McDonald
dictionary model demonstrates considerably lower effectiveness with only 50.1%
accuracy. Regression analyses highlight a robust positive impact of OPT model
scores on next-day stock returns, with coefficients of 0.274 and 0.254 in
different model specifications. BERT and FINBERT also exhibit predictive
relevance, though to a lesser extent. Notably, we do not observe a significant
relationship between the Loughran-McDonald dictionary model scores and stock
returns, challenging the efficacy of this traditional method in the current
financial context. In portfolio performance, the long-short OPT strategy excels
with a Sharpe ratio of 3.05, compared to 2.11 for BERT and 2.07 for FINBERT
long-short strategies. Strategies based on the Loughran-McDonald dictionary
yield the lowest Sharpe ratio of 1.23. Our findings emphasize the superior
performance of advanced LLMs, especially OPT, in financial market prediction
and portfolio management, marking a significant shift in the landscape of
financial analysis tools with implications to financial regulation and policy
analysis.",2024-12-26T15:01:24Z,http://arxiv.org/abs/2412.19245v1,"Kemal Kirtac, Guido Germano"
"6Diffusion: IPv6 Target Generation Using a Diffusion Model with
  Global-Local Attention Mechanisms for Internet-wide IPv6 Scanning","Due to the vast address space of IPv6, the brute-force scanning methods
originally applicable to IPv4 are no longer suitable for proactive scanning of
IPv6. The recently proposed target generation algorithms have a low hit rate
for existing IPv6 target generation algorithms, primarily because they do not
accurately fit the distribution patterns of active IPv6 addresses. This paper
introduces a diffusion model-based IPv6 target generation algorithm called
6Diffusion. 6Diffusion first maps addresses to vector space for language
modeling, adds noise to active IPv6 addresses in the forward process, diffusing
them throughout the entire IPv6 address space, and then performs a reverse
process to gradually denoise and recover to active IPv6 addresses. We use the
DDIM sampler to increase the speed of generating candidate sets. At the same
time, we introduce the GLF-MSA (Global-Local Fusion Multi-Head Self-Attention)
mechanism to adapt to the top-down global allocation pattern of IPv6 addresses
and the local characteristics of IPv6 address segments, thus better learning
the deep-level features of active IPv6 addresses. Experimental results show
that compared to existing methods, 6Diffusion can generate higher quality
candidate sets and outperforms state-of-the-art target generation algorithms
across multiple metrics.",2024-12-26T14:57:58Z,http://arxiv.org/abs/2412.19243v1,"Nabo He, DanDan Li, Xiaohong Huang"
Functional structural equation modeling with latent variables,"Handling latent variables in Structural Equation Models (SEMs) in a case
where both the latent variables and their corresponding indicators in the
measurement error part of the model are random curves presents significant
challenges, especially with sparse data. In this paper, we develop a novel
family of Functional Structural Equation Models (FSEMs) that incorporate latent
variables modeled as Gaussian Processes (GPs). The introduced FSEMs are built
upon functional regression models having response variables modeled as
underlying GPs. The model flexibly adapts to cases when the random curves'
realizations are observed only over a sparse subset of the domain, and the
inferential framework is based on a restricted maximum likelihood approach. The
advantage of this framework lies in its ability and flexibility in handling
various data scenarios, including regularly and irregularly spaced points and
thus missing data. To extract smooth estimates for the functional parameters,
we employ a penalized likelihood approach that selects the smoothing parameters
using a cross-validation method. We evaluate the performance of the proposed
model using simulation studies and a real data example, which suggests that our
model performs well in practice. The uncertainty associated with the estimates
of the functional coefficients is also assessed by constructing confidence
regions for each estimate. The goodness of fit indices that are commonly used
to evaluate the fit of SEMs are developed for the FSEMs introduced in this
paper. Overall, the proposed method is a promising approach for modeling
functional data in SEMs with functional latent variables.",2024-12-26T14:57:14Z,http://arxiv.org/abs/2412.19242v1,"Fatemeh Asgari, Valeria Vitelli, Uta Sailer"
"Latenrgy: Model Agnostic Latency and Energy Consumption Prediction for
  Binary Classifiers","Machine learning systems increasingly drive innovation across scientific
fields and industry, yet challenges in compute overhead, specifically during
inference, limit their scalability and sustainability. Responsible AI
guardrails, essential for ensuring fairness, transparency, and privacy, further
exacerbate these computational demands. This study addresses critical gaps in
the literature, chiefly the lack of generalized predictive techniques for
latency and energy consumption, limited cross-comparisons of classifiers, and
unquantified impacts of RAI guardrails on inference performance. Using Theory
Construction Methodology, this work constructed a model-agnostic theoretical
framework for predicting latency and energy consumption in binary
classification models during inference. The framework synthesizes classifier
characteristics, dataset properties, and RAI guardrails into a unified
analytical instrument. Two predictive equations are derived that capture the
interplay between these factors while offering generalizability across diverse
classifiers. The proposed framework provides foundational insights for
designing efficient, responsible ML systems. It enables researchers to
benchmark and optimize inference performance and assists practitioners in
deploying scalable solutions. Finally, this work establishes a theoretical
foundation for balancing computational efficiency with ethical AI principles,
paving the way for future empirical validation and broader applications.",2024-12-26T14:51:24Z,http://arxiv.org/abs/2412.19241v1,Jason M. Pittman
FineVQ: Fine-Grained User Generated Content Video Quality Assessment,"The rapid growth of user-generated content (UGC) videos has produced an
urgent need for effective video quality assessment (VQA) algorithms to monitor
video quality and guide optimization and recommendation procedures. However,
current VQA models generally only give an overall rating for a UGC video, which
lacks fine-grained labels for serving video processing and recommendation
applications. To address the challenges and promote the development of UGC
videos, we establish the first large-scale Fine-grained Video quality
assessment Database, termed FineVD, which comprises 6104 UGC videos with
fine-grained quality scores and descriptions across multiple dimensions. Based
on this database, we propose a Fine-grained Video Quality assessment (FineVQ)
model to learn the fine-grained quality of UGC videos, with the capabilities of
quality rating, quality scoring, and quality attribution. Extensive
experimental results demonstrate that our proposed FineVQ can produce
fine-grained video-quality results and achieve state-of-the-art performance on
FineVD and other commonly used UGC-VQA datasets. Both Both FineVD and FineVQ
will be made publicly available.",2024-12-26T14:44:47Z,http://arxiv.org/abs/2412.19238v1,"Huiyu Duan, Qiang Hu, Jiarui Wang, Liu Yang, Zitong Xu, Lu Liu, Xiongkuo Min, Chunlei Cai, Tianxiao Ye, Xiaoyun Zhang, Guangtao Zhai"
SeaMo: A Multi-Seasonal and Multimodal Remote Sensing Foundation Model,"Remote Sensing (RS) data contains a wealth of multi-dimensional information
crucial for Earth observation. Owing to its vast volume, diverse sources, and
temporal properties, RS data is highly suitable for the development of large
Visual Foundation Models (VFMs). VFMs act as robust feature extractors,
learning from extensive RS data, and are subsequently fine-tuned for deployment
in various geoscientific tasks. However, current VFMs in the RS domain are
predominantly pretrained and tailored exclusively for specific characteristics
of RS imagery, neglecting the potential of utilizing the multi-dimensional
properties of RS data. Therefore, in this work, we propose SeaMo, a pioneering
visual foundation model that integrates multi-seasonal and multimodal
information in the RS field. SeaMo is designed to harness multiple properties
of RS data. Within the masked image modeling framework, we employ non-aligned
cropping techniques to extract spatial properties, use multi-source inputs for
multimodal integration, and incorporate temporal-multimodal fusion blocks for
effective assimilation of multi-seasonal data. SeaMo explicitly models the
multi-dimensional properties of RS data, making the model more comprehensive,
robust, and versatile. We applied SeaMo to several downstream geoscience tasks,
which demonstrated exceptional performance. Extensive ablation studies were
conducted to validate the model's superiority.",2024-12-26T14:40:38Z,http://arxiv.org/abs/2412.19237v1,"Xuyang Li, Danfeng Hong, Chenyu Li, Jocelyn Chanussot"
"Are Two Hidden Layers Still Enough for the Physics-Informed Neural
  Networks?","The article discusses the development of various methods and techniques for
initializing and training neural networks with a single hidden layer, as well
as training a separable physics-informed neural network consisting of neural
networks with a single hidden layer to solve physical problems described by
ordinary differential equations (ODEs) and partial differential equations
(PDEs). A method for strictly deterministic initialization of a neural network
with one hidden layer for solving physical problems described by an ODE is
proposed. Modifications to existing methods for weighting the loss function are
given, as well as new methods developed for training strictly
deterministic-initialized neural networks to solve ODEs (detaching, additional
weighting based on the second derivative, predicted solution-based weighting,
relative residuals). An algorithm for physics-informed data-driven
initialization of a neural network with one hidden layer is proposed. A neural
network with pronounced generalizing properties is presented, whose
generalizing abilities of which can be precisely controlled by adjusting
network parameters. A metric for measuring the generalization of such neural
network has been introduced. A gradient-free neuron-by-neuron fitting method
has been developed for adjusting the parameters of a single-hidden-layer neural
network, which does not require the use of an optimizer or solver for its
implementation. The proposed methods have been extended to 2D problems using
the separable physics-informed neural networks approach. Numerous experiments
have been carried out to develop the above methods and approaches. Experiments
on physical problems, such as solving various ODEs and PDEs, have demonstrated
that these methods for initializing and training neural networks with one or
two hidden layers (SPINN) achieve competitive accuracy and, in some cases,
state-of-the-art results.",2024-12-26T14:30:54Z,http://arxiv.org/abs/2412.19235v1,"Vasiliy A. Es'kin, Alexey O. Malkhanov, Mikhail E. Smorkalov"
"Virtual Nodes Can Help: Tackling Distribution Shifts in Federated Graph
  Learning","Federated Graph Learning (FGL) enables multiple clients to jointly train
powerful graph learning models, e.g., Graph Neural Networks (GNNs), without
sharing their local graph data for graph-related downstream tasks, such as
graph property prediction. In the real world, however, the graph data can
suffer from significant distribution shifts across clients as the clients may
collect their graph data for different purposes. In particular, graph
properties are usually associated with invariant label-relevant substructures
(i.e., subgraphs) across clients, while label-irrelevant substructures can
appear in a client-specific manner. The issue of distribution shifts of graph
data hinders the efficiency of GNN training and leads to serious performance
degradation in FGL. To tackle the aforementioned issue, we propose a novel FGL
framework entitled FedVN that eliminates distribution shifts through
client-specific graph augmentation strategies with multiple learnable Virtual
Nodes (VNs). Specifically, FedVN lets the clients jointly learn a set of shared
VNs while training a global GNN model. To eliminate distribution shifts, each
client trains a personalized edge generator that determines how the VNs connect
local graphs in a client-specific manner. Furthermore, we provide theoretical
analyses indicating that FedVN can eliminate distribution shifts of graph data
across clients. Comprehensive experiments on four datasets under five settings
demonstrate the superiority of our proposed FedVN over nine baselines.",2024-12-26T14:16:15Z,http://arxiv.org/abs/2412.19229v1,"Xingbo Fu, Zihan Chen, Yinhan He, Song Wang, Binchi Zhang, Chen Chen, Jundong Li"
"Learning Cross-Domain Representations for Transferable Drug
  Perturbations on Single-Cell Transcriptional Responses","Phenotypic drug discovery has attracted widespread attention because of its
potential to identify bioactive molecules. Transcriptomic profiling provides a
comprehensive reflection of phenotypic changes in cellular responses to
external perturbations. In this paper, we propose XTransferCDR, a novel
generative framework designed for feature decoupling and transferable
representation learning across domains. Given a pair of perturbed expression
profiles, our approach decouples the perturbation representations from basal
states through domain separation encoders and then cross-transfers them in the
latent space. The transferred representations are then used to reconstruct the
corresponding perturbed expression profiles via a shared decoder. This
cross-transfer constraint effectively promotes the learning of transferable
drug perturbation representations. We conducted extensive evaluations of our
model on multiple datasets, including single-cell transcriptional responses to
drugs and single- and combinatorial genetic perturbations. The experimental
results show that XTransferCDR achieved better performance than current
state-of-the-art methods, showcasing its potential to advance phenotypic drug
discovery.",2024-12-26T14:09:16Z,http://arxiv.org/abs/2412.19228v1,"Hui Liu, Shikai Jin"
Multi-view Fake News Detection Model Based on Dynamic Hypergraph,"With the rapid development of online social networks and the inadequacies in
content moderation mechanisms, the detection of fake news has emerged as a
pressing concern for the public. Various methods have been proposed for fake
news detection, including text-based approaches as well as a series of
graph-based approaches. However, the deceptive nature of fake news renders
text-based approaches less effective. Propagation tree-based methods focus on
the propagation process of individual news, capturing pairwise relationships
but lacking the capability to capture high-order complex relationships. Large
heterogeneous graph-based approaches necessitate the incorporation of
substantial additional information beyond news text and user data, while
hypergraph-based approaches rely on predefined hypergraph structures. To tackle
these issues, we propose a novel dynamic hypergraph-based multi-view fake news
detection model (DHy-MFND) that learns news embeddings across three distinct
views: text-level, propagation tree-level, and hypergraph-level. By employing
hypergraph structures to model complex high-order relationships among multiple
news pieces and introducing dynamic hypergraph structure learning, we optimize
predefined hypergraph structures while learning news embeddings. Additionally,
we introduce contrastive learning to capture authenticity-relevant embeddings
across different views. Extensive experiments on two benchmark datasets
demonstrate the effectiveness of our proposed DHy-MFND compared with a broad
range of competing baselines.",2024-12-26T14:05:51Z,http://arxiv.org/abs/2412.19227v1,"Rongping Ye, Xiaobing Pei"
"VINEVI: A Virtualized Network Vision Architecture for Smart Monitoring
  of Heterogeneous Applications and Infrastructures","Monitoring heterogeneous infrastructures and applications is essential to
cope with user requirements properly, but it still lacks enhancements. The
well-known state-of-the-art methods and tools do not support seamless
monitoring of bare-metal, low-cost infrastructures, neither hosted nor
virtualized services with fine-grained details. This work proposes VIrtualized
NEtwork VIsion architecture (VINEVI), an intelligent method for seamless
monitoring heterogeneous infrastructures and applications. The VINEVI
architecture advances state of the art with a node-embedded traffic
classification agent placing physical and virtualized infrastructures enabling
real-time traffic classification. VINEVI combines this real-time traffic
classification with well-known tools such as Prometheus and Victoria Metrics to
monitor the entire stack from the hardware to the virtualized applications.
Experimental results showcased that VINEVI architecture allowed seamless
heterogeneous infrastructure monitoring with a higher level of detail beyond
literature. Also, our node-embedded real-time Internet traffic classifier
evolved with flexibility the methods with monitoring heterogeneous
infrastructures seamlessly.",2024-12-26T14:05:14Z,http://arxiv.org/abs/2412.19226v1,"Rodrigo Moreira, Hugo G. V. O. da Cunha, Larissa F. Rodrigues Moreira, Flávio de Oliveira Silva"
"Completion as Enhancement: A Degradation-Aware Selective Image Guided
  Network for Depth Completion","In this paper, we introduce the Selective Image Guided Network (SigNet), a
novel degradation-aware framework that transforms depth completion into depth
enhancement for the first time. Moving beyond direct completion using
convolutional neural networks (CNNs), SigNet initially densifies sparse depth
data through non-CNN densification tools to obtain coarse yet dense depth. This
approach eliminates the mismatch and ambiguity caused by direct convolution
over irregularly sampled sparse data. Subsequently, SigNet redefines completion
as enhancement, establishing a self-supervised degradation bridge between the
coarse depth and the targeted dense depth for effective RGB-D fusion. To
achieve this, SigNet leverages the implicit degradation to adaptively select
high-frequency components (e.g., edges) of RGB data to compensate for the
coarse depth. This degradation is further integrated into a multi-modal
conditional Mamba, dynamically generating the state parameters to enable
efficient global high-frequency information interaction. We conduct extensive
experiments on the NYUv2, DIML, SUN RGBD, and TOFDC datasets, demonstrating the
state-of-the-art (SOTA) performance of SigNet.",2024-12-26T14:05:01Z,http://arxiv.org/abs/2412.19225v1,"Zhiqiang Yan, Zhengxue Wang, Kun Wang, Jun Li, Jian Yang"
"Transformer-Based Wireless Capsule Endoscopy Bleeding Tissue Detection
  and Classification","Informed by the success of the transformer model in various computer vision
tasks, we design an end-to-end trainable model for the automatic detection and
classification of bleeding and non-bleeding frames extracted from Wireless
Capsule Endoscopy (WCE) videos. Based on the DETR model, our model uses the
Resnet50 for feature extraction, the transformer encoder-decoder for bleeding
and non-bleeding region detection, and a feedforward neural network for
classification. Trained in an end-to-end approach on the Auto-WCEBleedGen
Version 1 challenge training set, our model performs both detection and
classification tasks as a single unit. Our model achieves an accuracy, recall,
and F1-score classification percentage score of 98.28, 96.79, and 98.37
respectively, on the Auto-WCEBleedGen version 1 validation set. Further, we
record an average precision (AP @ 0.5), mean-average precision (mAP) of 0.7447
and 0.7328 detection results. This earned us a 3rd place position in the
challenge. Our code is publicly available via
https://github.com/BasitAlawode/WCEBleedGen.",2024-12-26T13:49:39Z,http://arxiv.org/abs/2412.19218v1,"Basit Alawode, Shibani Hamza, Adarsh Ghimire, Divya Velayudhan"
"Applying the maximum entropy principle to multi-species neural networks
  improves species distribution models","The rapid expansion of citizen science initiatives has led to a significant
growth of biodiversity databases, and particularly presence-only (PO)
observations. PO data are invaluable for understanding species distributions
and their dynamics, but their use in Species Distribution Models (SDM) is
curtailed by sampling biases and the lack of information on absences. Poisson
point processes are widely used for SDMs, with Maxent being one of the most
popular methods. Maxent maximises the entropy of a probability distribution
across sites as a function of predefined transformations of environmental
variables, called features. In contrast, neural networks and deep learning have
emerged as a promising technique for automatic feature extraction from complex
input variables. In this paper, we propose DeepMaxent, which harnesses neural
networks to automatically learn shared features among species, using the
maximum entropy principle. To do so, it employs a normalised Poisson loss where
for each species, presence probabilities across sites are modelled by a neural
network. We evaluate DeepMaxent on a benchmark dataset known for its spatial
sampling biases, using PO data for calibration and presence-absence (PA) data
for validation across six regions with different biological groups and
environmental covariates. Our results indicate that DeepMaxent improves model
performance over Maxent and other state-of-the-art SDMs across regions and
taxonomic groups. The method performs particularly well in regions of uneven
sampling, demonstrating substantial potential to improve species distribution
modelling. The method opens the possibility to learn more robust environmental
features predicting jointly many species and scales to arbitrary large numbers
of sites without an increased memory demand.",2024-12-26T13:47:04Z,http://arxiv.org/abs/2412.19217v1,"Maxime Ryckewaert, Diego Marcos, Christophe Botella, Maximilien Servajean, Pierre Bonnet, Alexis Joly"
"On the convergence of continuous-time fictitious play in $3\times3$
  games via the geometrical approach","As the earliest and one of the most fundamental learning dynamics for
computing NE, fictitious play (FP) has being receiving incessant research
attention and finding games where FP would converge (games with FPP) is one
central question in related fields. In this paper, we identify a new class of
games with FPP, i.e., $3\times3$ games without IIP, based on the geometrical
approach by leveraging the location of NE and the partition of best response
region. During the process, we devise a new projection mapping to reduce a
high-dimensional dynamical system to a planar system. And to overcome the
non-smoothness of the systems, we redefine the concepts of saddle and sink NE,
which are proven to exist and help prove the convergence of CFP by separating
the projected space into two parts. Furthermore, we show that our projection
mapping can be extended to higher-dimensional and degenerate games.",2024-12-26T13:42:58Z,http://arxiv.org/abs/2412.19216v1,"Zhouming Wu, Yifen Mu, Xiaoguang Yang"
"Optimizing Fantasy Sports Team Selection with Deep Reinforcement
  Learning","Fantasy sports, particularly fantasy cricket, have garnered immense
popularity in India in recent years, offering enthusiasts the opportunity to
engage in strategic team-building and compete based on the real-world
performance of professional athletes. In this paper, we address the challenge
of optimizing fantasy cricket team selection using reinforcement learning (RL)
techniques. By framing the team creation process as a sequential
decision-making problem, we aim to develop a model that can adaptively select
players to maximize the team's potential performance. Our approach leverages
historical player data to train RL algorithms, which then predict future
performance and optimize team composition. This not only represents a huge
business opportunity by enabling more accurate predictions of high-performing
teams but also enhances the overall user experience. Through empirical
evaluation and comparison with traditional fantasy team drafting methods, we
demonstrate the effectiveness of RL in constructing competitive fantasy teams.
Our results show that RL-based strategies provide valuable insights into player
selection in fantasy sports.",2024-12-26T13:36:18Z,http://arxiv.org/abs/2412.19215v1,"Shamik Bhattacharjee, Kamlesh Marathe, Hitesh Kapoor, Nilesh Patil"
"Towards Better Spherical Sliced-Wasserstein Distance Learning with
  Data-Adaptive Discriminative Projection Direction","Spherical Sliced-Wasserstein (SSW) has recently been proposed to measure the
discrepancy between spherical data distributions in various fields, such as
geology, medical domains, computer vision, and deep representation learning.
However, in the original SSW, all projection directions are treated equally,
which is too idealistic and cannot accurately reflect the importance of
different projection directions for various data distributions. To address this
issue, we propose a novel data-adaptive Discriminative Spherical
Sliced-Wasserstein (DSSW) distance, which utilizes a projected energy function
to determine the discriminative projection direction for SSW. In our new DSSW,
we introduce two types of projected energy functions to generate the weights
for projection directions with complete theoretical guarantees. The first type
employs a non-parametric deterministic function that transforms the projected
Wasserstein distance into its corresponding weight in each projection
direction. This improves the performance of the original SSW distance with
negligible additional computational overhead. The second type utilizes a neural
network-induced function that learns the projection direction weight through a
parameterized neural network based on data projections. This further enhances
the performance of the original SSW distance with less extra computational
overhead. Finally, we evaluate the performance of our proposed DSSW by
comparing it with several state-of-the-art methods across a variety of machine
learning tasks, including gradient flows, density estimation on real earth
data, and self-supervised learning.",2024-12-26T13:23:37Z,http://arxiv.org/abs/2412.19212v1,"Hongliang Zhang, Shuo Chen, Lei Luo, Jian Yang"
"Large Language Models Meet Graph Neural Networks: A Perspective of Graph
  Mining","Graph mining is an important area in data mining and machine learning that
involves extracting valuable information from graph-structured data. In recent
years, significant progress has been made in this field through the development
of graph neural networks (GNNs). However, GNNs are still deficient in
generalizing to diverse graph data. Aiming to this issue, Large Language Models
(LLMs) could provide new solutions for graph mining tasks with their superior
semantic understanding. In this review, we systematically review the
combination and application techniques of LLMs and GNNs and present a novel
taxonomy for research in this interdisciplinary field, which involves three
main categories: GNN-driving-LLM, LLM-driving-GNN, and GNN-LLM-co-driving.
Within this framework, we reveal the capabilities of LLMs in enhancing graph
feature extraction as well as improving the effectiveness of downstream tasks
such as node classification, link prediction, and community detection. Although
LLMs have demonstrated their great potential in handling graph-structured data,
their high computational requirements and complexity remain challenges. Future
research needs to continue to explore how to efficiently fuse LLMs and GNNs to
achieve more powerful graph learning and reasoning capabilities and provide new
impetus for the development of graph mining techniques.",2024-12-26T13:21:09Z,http://arxiv.org/abs/2412.19211v1,"Yuxin You, Zhen Liu, Xiangchao Wen, Yongtao Zhang, Wei Ai"
Context-Aware Deep Learning for Multi Modal Depression Detection,"In this study, we focus on automated approaches to detect depression from
clinical interviews using multi-modal machine learning (ML). Our approach
differentiates from other successful ML methods such as context-aware analysis
through feature engineering and end-to-end deep neural networks for depression
detection utilizing the Distress Analysis Interview Corpus. We propose a novel
method that incorporates: (1) pre-trained Transformer combined with data
augmentation based on topic modelling for textual data; and (2) deep 1D
convolutional neural network (CNN) for acoustic feature modeling. The
simulation results demonstrate the effectiveness of the proposed method for
training multi-modal deep learning models. Our deep 1D CNN and Transformer
models achieved state-of-the-art performance for audio and text modalities
respectively. Combining them in a multi-modal framework also outperforms
state-of-the-art for the combined setting. Code available at
https://github.com/genandlam/multi-modal-depression-detection",2024-12-26T13:19:26Z,http://arxiv.org/abs/2412.19209v1,"Genevieve Lam, Huang Dongyan, Weisi Lin"
"Developing Explainable Machine Learning Model using Augmented Concept
  Activation Vector","Machine learning models use high dimensional feature spaces to map their
inputs to the corresponding class labels. However, these features often do not
have a one-to-one correspondence with physical concepts understandable by
humans, which hinders the ability to provide a meaningful explanation for the
decisions made by these models. We propose a method for measuring the
correlation between high-level concepts and the decisions made by a machine
learning model. Our method can isolate the impact of a given high-level concept
and accurately measure it quantitatively. Additionally, this study aims to
determine the prevalence of frequent patterns in machine learning models, which
often occur in imbalanced datasets. We have successfully applied the proposed
method to fundus images and managed to quantitatively measure the impact of
radiomic patterns on the model decisions.",2024-12-26T13:18:16Z,http://arxiv.org/abs/2412.19208v1,"Reza Hassanpour, Kasim Oztoprak, Niels Netten, Tony Busker, Mortaza S. Bargh, Sunil Choenni, Beyza Kizildag, Leyla Sena Kilinc"
NADER: Neural Architecture Design via Multi-Agent Collaboration,"Designing effective neural architectures poses a significant challenge in
deep learning. While Neural Architecture Search (NAS) automates the search for
optimal architectures, existing methods are often constrained by predetermined
search spaces and may miss critical neural architectures. In this paper, we
introduce NADER (Neural Architecture Design via multi-agEnt collaboRation), a
novel framework that formulates neural architecture design (NAD) as a LLM-based
multi-agent collaboration problem. NADER employs a team of specialized agents
to enhance a base architecture through iterative modification. Current
LLM-based NAD methods typically operate independently, lacking the ability to
learn from past experiences, which results in repeated mistakes and inefficient
exploration. To address this issue, we propose the Reflector, which effectively
learns from immediate feedback and long-term experiences. Additionally, unlike
previous LLM-based methods that use code to represent neural architectures, we
utilize a graph-based representation. This approach allows agents to focus on
design aspects without being distracted by coding. We demonstrate the
effectiveness of NADER in discovering high-performing architectures beyond
predetermined search spaces through extensive experiments on benchmark tasks,
showcasing its advantages over state-of-the-art methods. The codes will be
released soon.",2024-12-26T13:07:03Z,http://arxiv.org/abs/2412.19206v1,"Zekang Yang, Wang Zeng, Sheng Jin, Chen Qian, Ping Luo, Wentao Liu"
"GAIS: A Novel Approach to Instance Selection with Graph Attention
  Networks","Instance selection (IS) is a crucial technique in machine learning that aims
to reduce dataset size while maintaining model performance. This paper
introduces a novel method called Graph Attention-based Instance Selection
(GAIS), which leverages Graph Attention Networks (GATs) to identify the most
informative instances in a dataset. GAIS represents the data as a graph and
uses GATs to learn node representations, enabling it to capture complex
relationships between instances. The method processes data in chunks, applies
random masking and similarity thresholding during graph construction, and
selects instances based on confidence scores from the trained GAT model.
Experiments on 13 diverse datasets demonstrate that GAIS consistently
outperforms traditional IS methods in terms of effectiveness, achieving high
reduction rates (average 96\%) while maintaining or improving model
performance. Although GAIS exhibits slightly higher computational costs, its
superior performance in maintaining accuracy with significantly reduced
training data makes it a promising approach for graph-based data selection.",2024-12-26T12:51:14Z,http://arxiv.org/abs/2412.19201v1,"Zahiriddin Rustamov, Ayham Zaitouny, Rafat Damseh, Nazar Zaki"
"Personalized Dynamic Music Emotion Recognition with Dual-Scale
  Attention-Based Meta-Learning","Dynamic Music Emotion Recognition (DMER) aims to predict the emotion of
different moments in music, playing a crucial role in music information
retrieval. The existing DMER methods struggle to capture long-term dependencies
when dealing with sequence data, which limits their performance. Furthermore,
these methods often overlook the influence of individual differences on emotion
perception, even though everyone has their own personalized emotional
perception in the real world. Motivated by these issues, we explore more
effective sequence processing methods and introduce the Personalized DMER
(PDMER) problem, which requires models to predict emotions that align with
personalized perception. Specifically, we propose a Dual-Scale Attention-Based
Meta-Learning (DSAML) method. This method fuses features from a dual-scale
feature extractor and captures both short and long-term dependencies using a
dual-scale attention transformer, improving the performance in traditional
DMER. To achieve PDMER, we design a novel task construction strategy that
divides tasks by annotators. Samples in a task are annotated by the same
annotator, ensuring consistent perception. Leveraging this strategy alongside
meta-learning, DSAML can predict personalized perception of emotions with just
one personalized annotation sample. Our objective and subjective experiments
demonstrate that our method can achieve state-of-the-art performance in both
traditional DMER and PDMER.",2024-12-26T12:47:35Z,http://arxiv.org/abs/2412.19200v1,"Dengming Zhang, Weitao You, Ziheng Liu, Lingyun Sun, Pei Chen"
Multi-Attribute Constraint Satisfaction via Language Model Rewriting,"Obeying precise constraints on top of multiple external attributes is a
common computational problem underlying seemingly different domains, from
controlled text generation to protein engineering. Existing language model (LM)
controllability methods for multi-attribute constraint satisfaction often rely
on specialized architectures or gradient-based classifiers, limiting their
flexibility to work with arbitrary black-box evaluators and pretrained models.
Current general-purpose large language models, while capable, cannot achieve
fine-grained multi-attribute control over external attributes. Thus, we create
Multi-Attribute Constraint Satisfaction (MACS), a generalized method capable of
finetuning language models on any sequential domain to satisfy user-specified
constraints on multiple external real-value attributes. Our method trains LMs
as editors by sampling diverse multi-attribute edit pairs from an initial set
of paraphrased outputs. During inference, LM iteratively improves upon its
previous solution to satisfy constraints for all attributes by leveraging our
designed constraint satisfaction reward. We additionally experiment with
reward-weighted behavior cloning to further improve the constraint satisfaction
rate of LMs. To evaluate our approach, we present a new Fine-grained Constraint
Satisfaction (FineCS) benchmark, featuring two challenging tasks: (1) Text
Style Transfer, where the goal is to simultaneously modify the sentiment and
complexity of reviews, and (2) Protein Design, focusing on modulating
fluorescence and stability of Green Fluorescent Proteins (GFP). Our empirical
results show that MACS achieves the highest threshold satisfaction in both
FineCS tasks, outperforming strong domain-specific baselines. Our work opens
new avenues for generalized and real-value multi-attribute control, with
implications for diverse applications spanning NLP and bioinformatics.",2024-12-26T12:36:39Z,http://arxiv.org/abs/2412.19198v1,"Ashutosh Baheti, Debanjana Chakraborty, Faeze Brahman, Ronan Le Bras, Ximing Lu, Nouha Dziri, Yejin Choi, Mark Riedl, Maarten Sap"
Provably Efficient Exploration in Reward Machines with Low Regret,"We study reinforcement learning (RL) for decision processes with
non-Markovian reward, in which high-level knowledge of the task in the form of
reward machines is available to the learner. We consider probabilistic reward
machines with initially unknown dynamics, and investigate RL under the
average-reward criterion, where the learning performance is assessed through
the notion of regret. Our main algorithmic contribution is a model-based RL
algorithm for decision processes involving probabilistic reward machines that
is capable of exploiting the structure induced by such machines. We further
derive high-probability and non-asymptotic bounds on its regret and demonstrate
the gain in terms of regret over existing algorithms that could be applied, but
obliviously to the structure. We also present a regret lower bound for the
studied setting. To the best of our knowledge, the proposed algorithm
constitutes the first attempt to tailor and analyze regret specifically for RL
with probabilistic reward machines.",2024-12-26T12:25:04Z,http://arxiv.org/abs/2412.19194v1,"Hippolyte Bourel, Anders Jonsson, Odalric-Ambrym Maillard, Chenxiao Ma, Mohammad Sadegh Talebi"
"Biology Instructions: A Dataset and Benchmark for Multi-Omics Sequence
  Understanding Capability of Large Language Models","Large language models have already demonstrated their formidable capabilities
in general domains, ushering in a revolutionary transformation. However,
exploring and exploiting the extensive knowledge of these models to comprehend
multi-omics biology remains underexplored. To fill this research gap, we first
introduce Biology-Instructions, the first large-scale multi-omics biological
sequences-related instruction-tuning dataset including DNA, RNA, proteins, and
multi-molecules, designed to bridge the gap between large language models
(LLMs) and complex biological sequences-related tasks. This dataset can enhance
the versatility of LLMs by integrating diverse biological sequenced-based
prediction tasks with advanced reasoning capabilities, while maintaining
conversational fluency. Additionally, we reveal significant performance
limitations in even state-of-the-art LLMs on biological sequence-related
multi-omics tasks without specialized pre-training and instruction-tuning. We
further develop a strong baseline called ChatMultiOmics with a novel
three-stage training pipeline, demonstrating the powerful ability to understand
biology by using Biology-Instructions. Biology-Instructions and ChatMultiOmics
are publicly available and crucial resources for enabling more effective
integration of LLMs with multi-omics sequence analysis.",2024-12-26T12:12:23Z,http://arxiv.org/abs/2412.19191v1,"Haonan He, Yuchen Ren, Yining Tang, Ziyang Xu, Junxian Li, Minghao Yang, Di Zhang, Dong Yuan, Tao Chen, Shufei Zhang, Yuqiang Li, Nanqing Dong, Wanli Ouyang, Dongzhan Zhou, Peng Ye"
An End-to-End Depth-Based Pipeline for Selfie Image Rectification,"Portraits or selfie images taken from a close distance typically suffer from
perspective distortion. In this paper, we propose an end-to-end deep
learning-based rectification pipeline to mitigate the effects of perspective
distortion. We learn to predict the facial depth by training a deep CNN. The
estimated depth is utilized to adjust the camera-to-subject distance by moving
the camera farther, increasing the camera focal length, and reprojecting the 3D
image features to the new perspective. The reprojected features are then fed to
an inpainting module to fill in the missing pixels. We leverage a
differentiable renderer to enable end-to-end training of our depth estimation
and feature extraction nets to improve the rectified outputs. To boost the
results of the inpainting module, we incorporate an auxiliary module to predict
the horizontal movement of the camera which decreases the area that requires
hallucination of challenging face parts such as ears. Unlike previous works, we
process the full-frame input image at once without cropping the subject's face
and processing it separately from the rest of the body, eliminating the need
for complex post-processing steps to attach the face back to the subject's
body. To train our network, we utilize the popular game engine Unreal Engine to
generate a large synthetic face dataset containing various subjects, head
poses, expressions, eyewear, clothes, and lighting. Quantitative and
qualitative results show that our rectification pipeline outperforms previous
methods, and produces comparable results with a time-consuming 3D GAN-based
method while being more than 260 times faster.",2024-12-26T11:57:54Z,http://arxiv.org/abs/2412.19189v1,"Ahmed Alhawwary, Phong Nguyen-Ha, Janne Mustaniemi, Janne Heikkilä"
New Physics in the 3-3-1 models,"Two main ingredients of current particle physics
  such as local gauge symmetry and mass generation via the Higgs mechanism
being basic ground of the Standard Model are widely confirmed by
  experimental data. However, some problems such as neutrino masses, dark
matter, baryon asymmetry of Universe have clearly indicated that the Standard
Model cannot be the ultimate theory of nature. To surpass the mentioned
puzzles,
  many extensions of the Standard Model (called beyond Standard Model) have
been proposed. Among beyond Standard Models, the 3-3-1 models have some
intriguing features and they get wide attention. The pioneer models develop in
some directions. In this paper, %some new main versions of the 3-3-1 models and
their consequences are presented.",2024-12-26T11:55:43Z,http://arxiv.org/abs/2412.19188v1,H. N. Long
"Multi-Head Attention Driven Dynamic Visual-Semantic Embedding for
  Enhanced Image-Text Matching","With the rapid development of multimodal learning, the image-text matching
task, as a bridge connecting vision and language, has become increasingly
important. Based on existing research, this study proposes an innovative visual
semantic embedding model, Multi-Headed Consensus-Aware Visual-Semantic
Embedding (MH-CVSE). This model introduces a multi-head self-attention
mechanism based on the consensus-aware visual semantic embedding model (CVSE)
to capture information in multiple subspaces in parallel, significantly
enhancing the model's ability to understand and represent the complex
relationship between images and texts. In addition, we adopt a parameterized
feature fusion strategy to flexibly integrate feature information at different
levels, further improving the model's expressive power. In terms of loss
function design, the MH-CVSE model adopts a dynamic weight adjustment strategy
to dynamically adjust the weight according to the loss value itself, so that
the model can better balance the contribution of different loss terms during
training. At the same time, we introduce a cosine annealing learning rate
strategy to help the model converge more stably in the later stages of
training. Extensive experimental verification on the Flickr30k dataset shows
that the MH-CVSE model achieves better performance than previous methods in
both bidirectional image and text retrieval tasks, fully demonstrating its
effectiveness and superiority.",2024-12-26T11:46:22Z,http://arxiv.org/abs/2412.19184v1,Wenjing Chen
"Unraveling the magnetic and electronic complexity of intermetallic
  ErPd$_2$Si$_2$: Anisotropic thermal expansion, phase transitions, and twofold
  magnetotransport behavior","We present a comprehensive investigation into the physical properties of
intermetallic ErPd$_2$Si$_2$, a compound renowned for its intriguing magnetic
and electronic characteristics. We confirm the tetragonal crystal structure of
ErPd$_2$Si$_2$ within the $I4/mmm$ space group. Notably, we observed
anisotropic thermal expansion, with the lattice constant $a$ expanding and $c$
contracting between 15 K and 300 K. This behavior is attributed to lattice
vibrations and electronic contributions. Heat capacity measurements revealed
three distinct temperature regimes: $T_1 \sim 3.0$ K, $T_\textrm{N} \sim 4.20$
K, and $T_2 \sim 15.31$ K. These correspond to the disappearance of
spin-density waves, the onset of an incommensurate antiferromagnetic (AFM)
structure, and the crystal-field splitting and/or the presence of short-range
spin fluctuations, respectively. Remarkably, the AFM phase transition anomaly
was observed exclusively in low-field magnetization data (120 Oe) at
$T_\textrm{N}$. A high magnetic field ($B =$ 3 T) effectively suppressed this
anomaly, likely due to spin-flop and spin-flip transitions. Furthermore, the
extracted effective PM moments closely matched the expected theoretical value,
suggesting a dominant magnetic contribution from localized 4$f$ spins of Er.
Additionally, significant differences in resistance ($R$) values at low
temperatures under applied $B$ indicated a magnetoresistance (MR) effect with a
minimum value of -4.36\%. Notably, the measured MR effect exhibited anisotropic
behavior, where changes in the strength or direction of the applied $B$ induced
variations in the MR effect. A twofold symmetry of $R$ was discerned at 3 T and
9 T, originating from the orientation of spin moments relative to the applied
$B$. Intriguingly, above $T_\textrm{N}$, short-range spin fluctuations also
displayed a preferred orientation along the $c$-axis due to single-ion
anisotropy.",2024-12-26T11:39:24Z,http://arxiv.org/abs/2412.19181v1,"Kaitong Sun, Si Wu, Guanping Xu, Lingwei Li, Hongyu Chen, Qian Zhao, Muqing Su, Wolfgang Schmidt, Chongde Cao, Hai-Feng Li"
"Mask Approximation Net: Merging Feature Extraction and Distribution
  Learning for Remote Sensing Change Captioning","Remote sensing image change description, as a novel multimodal task in the
field of remote sensing processing, not only enables the detection of changes
in surface conditions but also provides detailed descriptions of these changes,
thereby enhancing human interpretability and interactivity. However, previous
methods mainly employed Convolutional Neural Network (CNN) architectures to
extract bitemporal image features. This approach often leads to an overemphasis
on designing specific network architectures and limits the captured feature
distributions to the current dataset, resulting in poor generalizability and
robustness when applied to other datasets or real-world scenarios. To address
these limitations, this paper proposes a novel approach for remote sensing
image change detection and description that integrates diffusion models, aiming
to shift the focus from conventional feature learning paradigms to data
distribution learning. The proposed method primarily includes a simple
multi-scale change detection module, whose output features are subsequently
refined using a diffusion model. Additionally, we introduce a frequency-guided
complex filter module to handle high-frequency noise during the diffusion
process, which helps to maintain model performance. Finally, we validate the
effectiveness of our proposed method on several remote sensing change detection
description datasets, demonstrating its superior performance. The code
available at MaskApproxNet.",2024-12-26T11:35:57Z,http://arxiv.org/abs/2412.19179v1,"Dongwei Sun, Xiangyong Cao"
"Reversed in Time: A Novel Temporal-Emphasized Benchmark for Cross-Modal
  Video-Text Retrieval","Cross-modal (e.g. image-text, video-text) retrieval is an important task in
information retrieval and multimodal vision-language understanding field.
Temporal understanding makes video-text retrieval more challenging than
image-text retrieval. However, we find that the widely used video-text
benchmarks have shortcomings in comprehensively assessing abilities of models,
especially in temporal understanding, causing large-scale image-text
pre-trained models can already achieve comparable zero-shot performance with
video-text pre-trained models. In this paper, we introduce RTime, a novel
temporal-emphasized video-text retrieval dataset. We first obtain videos of
actions or events with significant temporality, and then reverse these videos
to create harder negative samples. We then recruit annotators to judge the
significance and reversibility of candidate videos, and write captions for
qualified videos. We further adopt GPT-4 to extend more captions based on
human-written captions. Our RTime dataset currently consists of 21k videos with
10 captions per video, totalling about 122 hours. Based on RTime, we propose
three retrieval benchmark tasks: RTime-Origin, RTime-Hard, and RTime-Binary. We
further enhance the use of harder-negatives in model training, and benchmark a
variety of video-text models on RTime. Extensive experiment analysis proves
that RTime indeed poses new and higher challenges to video-text retrieval. We
release our RTime
dataset\footnote{\url{https://github.com/qyr0403/Reversed-in-Time}} to further
advance video-text retrieval and multimodal understanding research.",2024-12-26T11:32:00Z,http://arxiv.org/abs/2412.19178v1,"Yang Du, Yuqi Liu, Qin Jin"
Physical nature of quasi-stable structures existing in antimony melt,"Equilibrium antimony melt near the melting temperature is characterised by
structural features that are not present in simple single-component liquids.
The cause of these features may be long-lived structural formations that are
not yet fully understood. The present work provides the detailed
characterization of the structures formed in liquid antimony near the melting
temperature based on the results of quantum chemical calculations and the
available neutron and X-ray diffraction data. The quasi-stable structures in
antimony melt are detected with lifetimes exceeding the structural relaxation
time of this melt. These structures are characterised by a low degree of order
and spatial localisation. It is shown for the first time that the elementary
units of these quasi-stable structures are triplets of atoms with
characteristic lengths of $3.07$\,\AA~and $4.7$\,\AA~and characteristic angles
of $45$ and $90$ degrees. It was found that these triplets can form chains and
percolating clusters up to $\sim15$\,\AA~in length. The characteristic lengths
of these triplets are fully consistent with the correlation lengths associated
with short-range order in the antimony melt as determined by diffraction
experiments.",2024-12-26T11:30:38Z,http://arxiv.org/abs/2412.19177v1,"Artem A. Tsygankov, Bulat N. Galimzyanov, Anatolii V. Mokshin"
"Towards Popularity-Aware Recommendation: A Multi-Behavior Enhanced
  Framework with Orthogonality Constraint","Top-$K$ recommendation involves inferring latent user preferences and
generating personalized recommendations accordingly, which is now ubiquitous in
various decision systems. Nonetheless, recommender systems usually suffer from
severe \textit{popularity bias}, leading to the over-recommendation of popular
items. Such a bias deviates from the central aim of reflecting user preference
faithfully, compromising both customer satisfaction and retailer profits.
Despite the prevalence, existing methods tackling popularity bias still have
limitations due to the considerable accuracy-debias tradeoff and the
sensitivity to extensive parameter selection, further exacerbated by the
extreme sparsity in positive user-item interactions.
  In this paper, we present a \textbf{Pop}ularity-aware top-$K$ recommendation
algorithm integrating multi-behavior \textbf{S}ide \textbf{I}nformation
(PopSI), aiming to enhance recommendation accuracy and debias performance
simultaneously. Specifically, by leveraging multiple user feedback that mirrors
similar user preferences and formulating it as a three-dimensional tensor,
PopSI can utilize all slices to capture the desiring user preferences
effectively. Subsequently, we introduced a novel orthogonality constraint to
refine the estimated item feature space, enforcing it to be invariant to item
popularity features thereby addressing our model's sensitivity to popularity
bias. Comprehensive experiments on real-world e-commerce datasets demonstrate
the general improvements of PopSI over state-of-the-art debias methods with a
marginal accuracy-debias tradeoff and scalability to practical applications.
The source code for our algorithm and experiments is available at
\url{https://github.com/Eason-sys/PopSI}.",2024-12-26T11:06:49Z,http://arxiv.org/abs/2412.19172v1,"Yishan Han, Biao Xu, Yao Wang, Shanxing Gao"
"High-Precision Schottky Diagnostics for Low-SNR Betatron Tune
  Measurement in Ramping Synchrotrons","This paper presents a novel Schottky diagnostics-based method for real-time
betatron tune measurement in ramping synchrotrons, exemplified by the Shanghai
Advanced Proton Therapy (SAPT) facility. The proposed approach achieves high
precision under challenging conditions, including low frequency resolution and
signal-to-noise ratios (SNR) as low as -15 dB within the bandwidth of a
narrowband detector. By employing Short-Time Fourier Transform (STFT) analysis
with automatically optimized time windows, the method effectively addresses the
rapid increase in revolution frequency from 4 MHz to 7.5 MHz over 0.35 seconds,
assuming constant beam properties within each window. Monte Carlo
macro-particle simulations are employed to generate Schottky signals, which are
subsequently combined with real noise collected from an analog-to-digital
converter to emulate practical conditions. The betatron tune measurement
procedure integrates longitudinal signal exclusion, spectrum smoothing, and
spectral multiplication to reliably extract transverse Schottky spectra buried
in noise, to enable precise betatron tune determination. Experimental results
demonstrate that the proposed method surpasses existing approaches in
precision, accuracy, and robustness, while meeting stringent design
requirements. This innovative approach addresses key limitations of Schottky
diagnostics for betatron tune measurement in ramping synchrotrons, providing a
foundation for applications such as proton therapy.",2024-12-26T11:05:21Z,http://arxiv.org/abs/2412.19171v1,"Peihan Sun, Manzhou Zhang, Renxian Yuan, Deming Li, Jian Dong, Ying Shi"
"Accelerating Stochastic Gravitational Wave Backgrounds Parameter
  Estimation in Pulsar Timing Arrays with Flow Matching","Pulsar timing arrays (PTAs) are essential tools for detecting the stochastic
gravitational wave background (SGWB), but their analysis faces significant
computational challenges. Traditional methods like Markov-chain Monte Carlo
(MCMC) struggle with high-dimensional parameter spaces where noise parameters
often dominate, while existing deep learning approaches fail to model the
Hellings-Downs (HD) correlation or are validated only on synthetic datasets. We
propose a flow-matching-based continuous normalizing flow (CNF) for efficient
and accurate PTA parameter estimation. By focusing on the 10 most contributive
pulsars from the NANOGrav 15-year dataset, our method achieves posteriors
consistent with MCMC, with a Jensen-Shannon divergence below \(10^{-2}\) nat,
while reducing sampling time from 50 hours to 4 minutes. Powered by a versatile
embedding network and a reweighting loss function, our approach prioritizes the
SGWB parameters and scales effectively for future datasets. It enables precise
reconstruction of SGWB and opens new avenues for exploring vast observational
data and uncovering potential new physics, offering a transformative tool for
advancing gravitational wave astronomy.",2024-12-26T11:02:11Z,http://arxiv.org/abs/2412.19169v1,"Bo Liang, Chang Liu, Tianyu Zhao, Minghui Du, Manjia Liang, Ruijun Shi, Hong Guo, Yuxiang Xu, Li-e Qiang, Peng Xu, Wei-Liang Qian, Ziren Luo"
GFG -- Gender-Fair Generation: A CALAMITA Challenge,"Gender-fair language aims at promoting gender equality by using terms and
expressions that include all identities and avoid reinforcing gender
stereotypes. Implementing gender-fair strategies is particularly challenging in
heavily gender-marked languages, such as Italian. To address this, the
Gender-Fair Generation challenge intends to help shift toward gender-fair
language in written communication. The challenge, designed to assess and
monitor the recognition and generation of gender-fair language in both mono-
and cross-lingual scenarios, includes three tasks: (1) the detection of
gendered expressions in Italian sentences, (2) the reformulation of gendered
expressions into gender-fair alternatives, and (3) the generation of
gender-fair language in automatic translation from English to Italian. The
challenge relies on three different annotated datasets: the GFL-it corpus,
which contains Italian texts extracted from administrative documents provided
by the University of Brescia; GeNTE, a bilingual test set for gender-neutral
rewriting and translation built upon a subset of the Europarl dataset; and
Neo-GATE, a bilingual test set designed to assess the use of non-binary
neomorphemes in Italian for both fair formulation and translation tasks.
Finally, each task is evaluated with specific metrics: average of F1-score
obtained by means of BERTScore computed on each entry of the datasets for task
1, an accuracy measured with a gender-neutral classifier, and a
coverage-weighted accuracy for tasks 2 and 3.",2024-12-26T10:58:40Z,http://arxiv.org/abs/2412.19168v1,"Simona Frenda, Andrea Piergentili, Beatrice Savoldi, Marco Madeddu, Martina Rosola, Silvia Casola, Chiara Ferrando, Viviana Patti, Matteo Negri, Luisa Bentivogli"
"Accelerating global search of adsorbate molecule position using
  machine-learning interatomic potentials with active learning","We present an algorithm for accelerating the search of molecule's adsorption
site based on global optimization of surface adsorbate geometries. Our approach
uses a machine-learning interatomic potential (moment tensor potential) to
approximate the potential energy surface and an active learning algorithm for
the automatic construction of an optimal training dataset. To validate our
methodology, we compare the results across various well-known catalytic systems
with surfaces of different crystallographic orientations and adsorbate
geometries, including CO/Pd(111), NO/Pd(100), NH$_3$/Cu(100),
C$_6$H$_6$/Ag(111), and CH$_2$CO/Rh(211). In the all cases, we observed an
agreement of our results with the literature.",2024-12-26T10:44:55Z,http://arxiv.org/abs/2412.19162v1,"Olga Klimanova, Nikita Rybin, Alexander Shapeev"
"Dual Channel Multi-Attention in ViT for Biometric Authentication using
  Forehead Subcutaneous Vein Pattern and Periocular Pattern","Traditional biometric systems, like face and fingerprint recognition, have
encountered significant setbacks due to wearing face masks and hygiene
concerns. To meet the challenges of the partially covered face due to face
masks and hygiene concerns of fingerprint recognition, this paper proposes a
novel dual-channel multi-attention Vision Transformer (ViT) framework for
biometric authentication using forehead subcutaneous vein patterns and
periocular patterns, offering a promising alternative to traditional methods,
capable of performing well even with face masks and without any physical touch.
The proposed framework leverages a dual-channel ViT architecture, designed to
handle two distinct biometric traits. It can capture long-range dependencies of
independent features from the vein and periocular patterns. A custom classifier
is then designed to integrate the independently extracted features, producing a
final class prediction. The performance of the proposed algorithm was
rigorously evaluated using the Forehead Subcutaneous Vein Pattern and
Periocular Biometric Pattern (FSVP-PBP) database. The results demonstrated the
superiority of the algorithm over state-of-the-art methods, achieving
remarkable classification accuracy of $99.3 \pm 0.02\%$ with the combined vein
and periocular patterns.",2024-12-26T10:40:15Z,http://arxiv.org/abs/2412.19160v1,"Arun K. Sharma, Shubhobrata Bhattacharya, Motahar Reza"
"Mobile Robots through Task-Based Human Instructions using Incremental
  Curriculum Learning","This paper explores the integration of incremental curriculum learning (ICL)
with deep reinforcement learning (DRL) techniques to facilitate mobile robot
navigation through task-based human instruction. By adopting a curriculum that
mirrors the progressive complexity encountered in human learning, our approach
systematically enhances robots' ability to interpret and execute complex
instructions over time. We explore the principles of DRL and its synergy with
ICL, demonstrating how this combination not only improves training efficiency
but also equips mobile robots with the generalization capability required for
navigating through dynamic indoor environments. Empirical results indicate that
robots trained with our ICL-enhanced DRL framework outperform those trained
without curriculum learning, highlighting the benefits of structured learning
progressions in robotic training.",2024-12-26T10:38:40Z,http://arxiv.org/abs/2412.19159v1,"Muhammad A. Muttaqien, Ayanori Yorozu, Akihisa Ohya"
"Referencing Where to Focus: Improving VisualGrounding with Referential
  Query","Visual Grounding aims to localize the referring object in an image given a
natural language expression. Recent advancements in DETR-based visual grounding
methods have attracted considerable attention, as they directly predict the
coordinates of the target object without relying on additional efforts, such as
pre-generated proposal candidates or pre-defined anchor boxes. However,
existing research primarily focuses on designing stronger multi-modal decoder,
which typically generates learnable queries by random initialization or by
using linguistic embeddings. This vanilla query generation approach inevitably
increases the learning difficulty for the model, as it does not involve any
target-related information at the beginning of decoding. Furthermore, they only
use the deepest image feature during the query learning process, overlooking
the importance of features from other levels. To address these issues, we
propose a novel approach, called RefFormer. It consists of the query adaption
module that can be seamlessly integrated into CLIP and generate the referential
query to provide the prior context for decoder, along with a task-specific
decoder. By incorporating the referential query into the decoder, we can
effectively mitigate the learning difficulty of the decoder, and accurately
concentrate on the target object. Additionally, our proposed query adaption
module can also act as an adapter, preserving the rich knowledge within CLIP
without the need to tune the parameters of the backbone network. Extensive
experiments demonstrate the effectiveness and efficiency of our proposed
method, outperforming state-of-the-art approaches on five visual grounding
benchmarks.",2024-12-26T10:19:20Z,http://arxiv.org/abs/2412.19155v1,"Yabing Wang, Zhuotao Tian, Qingpei Guo, Zheng Qin, Sanping Zhou, Ming Yang, Le Wang"
"To Predict or Not To Predict? Proportionally Masked Autoencoders for
  Tabular Data Imputation","Masked autoencoders (MAEs) have recently demonstrated effectiveness in
tabular data imputation. However, due to the inherent heterogeneity of tabular
data, the uniform random masking strategy commonly used in MAEs can disrupt the
distribution of missingness, leading to suboptimal performance. To address
this, we propose a proportional masking strategy for MAEs. Specifically, we
first compute the statistics of missingness based on the observed proportions
in the dataset, and then generate masks that align with these statistics,
ensuring that the distribution of missingness is preserved after masking.
Furthermore, we argue that simple MLP-based token mixing offers competitive or
often superior performance compared to attention mechanisms while being more
computationally efficient, especially in the tabular domain with the inherent
heterogeneity. Experimental results validate the effectiveness of the proposed
proportional masking strategy across various missing data patterns in tabular
datasets. Code is available at: \url{https://github.com/normal-kim/PMAE}.",2024-12-26T10:12:08Z,http://arxiv.org/abs/2412.19152v1,"Jungkyu Kim, Kibok Lee, Taeyoung Park"
Generating Editable Head Avatars with 3D Gaussian GANs,"Generating animatable and editable 3D head avatars is essential for various
applications in computer vision and graphics. Traditional 3D-aware generative
adversarial networks (GANs), often using implicit fields like Neural Radiance
Fields (NeRF), achieve photorealistic and view-consistent 3D head synthesis.
However, these methods face limitations in deformation flexibility and
editability, hindering the creation of lifelike and easily modifiable 3D heads.
We propose a novel approach that enhances the editability and animation control
of 3D head avatars by incorporating 3D Gaussian Splatting (3DGS) as an explicit
3D representation. This method enables easier illumination control and improved
editability. Central to our approach is the Editable Gaussian Head (EG-Head)
model, which combines a 3D Morphable Model (3DMM) with texture maps, allowing
precise expression control and flexible texture editing for accurate animation
while preserving identity. To capture complex non-facial geometries like hair,
we use an auxiliary set of 3DGS and tri-plane features. Extensive experiments
demonstrate that our approach delivers high-quality 3D-aware synthesis with
state-of-the-art controllability. Our code and models are available at
https://github.com/liguohao96/EGG3D.",2024-12-26T10:10:03Z,http://arxiv.org/abs/2412.19149v1,"Guohao Li, Hongyu Yang, Yifang Men, Di Huang, Weixin Li, Ruijie Yang, Yunhong Wang"
AskChart: Universal Chart Understanding through Textual Enhancement,"Chart understanding tasks such as ChartQA and Chart-to-Text involve
automatically extracting and interpreting key information from charts, enabling
users to query or convert visual data into structured formats. State-of-the-art
approaches primarily focus on visual cues from chart images, failing to
explicitly incorporate rich textual information (e.g., data labels and axis
labels) embedded within the charts. This textual information is vital for
intuitive human comprehension and interpretation of charts. Moreover, existing
models are often large and computationally intensive, limiting their practical
applicability. In this paper, we introduce AskChart, a universal model that
explicitly integrates both textual and visual cues from charts using a Mixture
of Experts (MoE) architecture. AskChart facilitates the learning of enhanced
visual-textual representations of charts for effectively handling multiple
chart understanding tasks, while maintaining a smaller model size. To capture
the synergy between visual and textual modalities, we curate a large-scale
dataset named ChartBank with about 7.5M data samples, which helps align textual
and visual information and facilitates the extraction of visual entities and
text. To effectively train AskChart, we design a three-stage training strategy
to align visual and textual modalities for learning robust visual-textual
representations and optimizing the learning of the MoE layer. Extensive
experiments across five datasets demonstrate the significant performance gains
of AskChart in four chart understanding tasks. Remarkably, AskChart with 4.6B
parameters outperforms state-of-the-art models with 13B parameters by 68.3% in
Open-ended ChartQA and 49.2% in Chart-to-Text tasks, while achieving comparable
performance in ChartQA and Chart-to-Table tasks.",2024-12-26T09:59:43Z,http://arxiv.org/abs/2412.19146v1,"Xudong Yang, Yifan Wu, Yizhang Zhu, Nan Tang, Yuyu Luo"
"Impact of color and mixing proportion of synthetic point clouds on
  semantic segmentation","Semantic segmentation of point clouds is essential for understanding the
built environment, and a large amount of high-quality data is required for
training deep learning models. Despite synthetic point clouds (SPC) having the
potential to compensate for the shortage of real data, how to exploit the
benefits of SPC is still open. Therefore, this study systematically
investigates how color and mixing proportion of SPC impact semantic
segmentation for the first time. First, a new method to mimic the scanning
process and generate SPC based on BIM is proposed, to create a synthetic
dataset with consistent colors of BIM (UniSPC) and a synthetic dataset with
real colors (RealSPC) respectively. Subsequently, by integrating with the S3DIS
dataset, further experiments on PointNet, PointNet++, and DGCNN are conducted.
Meanwhile, benchmark experiments and new evaluation metrics are introduced to
better evaluate the performance of different models. Experiments show that
synthetic color significantly impacts model performance, the performance for
common components of the models trained with pure RealSPC is comparable to
models with real data, and RealSPC contributes average improvements of 14.1% on
overall accuracy and 7.3% on mIoU than UniSPC. Furthermore, the proportion of
SPC also has a significant impact on the performance. In mixing training
experiments, adding more than 70% SPC achieves an average of 3.9% on overall
accuracy and 3.4% on mIoU better than benchmark on three models. It is also
revealed that for large flat elements such as floors, ceilings, and walls, the
SPC can even replace real point clouds without compromising model performance.",2024-12-26T09:58:04Z,http://arxiv.org/abs/2412.19145v1,"Shaojie Zhou, Jia-Rui Lin, Peng Pan, Yuandong Pan, Ioannis Brilakis"
LibAFL-DiFuzz: Advanced Architecture Enabling Directed Fuzzing,"Directed fuzzing performs best for targeted program testing via estimating
the impact of each input in reaching predefined program points. But due to
insufficient analysis of the program structure and lack of flexibility and
configurability it can lose efficiency. In this paper, we enhance directed
fuzzing with context weights for graph nodes and resolve indirect edges during
call graph construction. We construct flexible tool for directed fuzzing with
components able to be easily combined with other techniques. We implement
proposed method in three separate modules: DiFuzzLLVM library for graph
construction and indirect calls resolving, DiFuzz static analysis tool for
processing program graphs and computing proximity metrics, and LibAFL-DiFuzz
directed fuzzer based on LibAFL fuzzing library. We create additional LibAFL
modules for enabling custom power scheduling and static instrumentation. We
evaluate indirect calls resolving and get increase in directed fuzzing
efficiency for reaching deeper target points. We evaluate context weights
contribution and get benefits in TTE and scheduling iterations number. We
evaluate our fuzzer in comparison with AFLGo and BEACON, and reveal speedup in
time to exposure on several benchmarks. Furthermore, our tool implements some
important usability features that are not available in mentioned tools: target
points detection, multiple target points support, etc.",2024-12-26T09:54:57Z,http://arxiv.org/abs/2412.19143v1,"Darya Parygina, Timofey Mezhuev, Daniil Kuts"
"CLIP-GS: Unifying Vision-Language Representation with 3D Gaussian
  Splatting","Recent works in 3D multimodal learning have made remarkable progress.
However, typically 3D multimodal models are only capable of handling point
clouds. Compared to the emerging 3D representation technique, 3D Gaussian
Splatting (3DGS), the spatially sparse point cloud cannot depict the texture
information of 3D objects, resulting in inferior reconstruction capabilities.
This limitation constrains the potential of point cloud-based 3D multimodal
representation learning. In this paper, we present CLIP-GS, a novel multimodal
representation learning framework grounded in 3DGS. We introduce the GS
Tokenizer to generate serialized gaussian tokens, which are then processed
through transformer layers pre-initialized with weights from point cloud
models, resulting in the 3DGS embeddings. CLIP-GS leverages contrastive loss
between 3DGS and the visual-text embeddings of CLIP, and we introduce an image
voting loss to guide the directionality and convergence of gradient
optimization. Furthermore, we develop an efficient way to generate triplets of
3DGS, images, and text, facilitating CLIP-GS in learning unified multimodal
representations. Leveraging the well-aligned multimodal representations,
CLIP-GS demonstrates versatility and outperforms point cloud-based models on
various 3D tasks, including multimodal retrieval, zero-shot, and few-shot
classification.",2024-12-26T09:54:25Z,http://arxiv.org/abs/2412.19142v1,"Siyu Jiao, Haoye Dong, Yuyang Yin, Zequn Jie, Yinlong Qian, Yao Zhao, Humphrey Shi, Yunchao Wei"
"How Panel Layouts Define Manga: Insights from Visual Ablation
  Experiments","Today, manga has gained worldwide popularity. However, the question of how
various elements of manga, such as characters, text, and panel layouts, reflect
the uniqueness of a particular work, or even define it, remains an unexplored
area. In this paper, we aim to quantitatively and qualitatively analyze the
visual characteristics of manga works, with a particular focus on panel layout
features. As a research method, we used facing page images of manga as input to
train a deep learning model for predicting manga titles, examining
classification accuracy to quantitatively analyze these features. Specifically,
we conducted ablation studies by limiting page image information to panel
frames to analyze the characteristics of panel layouts. Through a series of
quantitative experiments using all 104 works, 12 genres, and 10,122 facing page
images from the Manga109 dataset, as well as qualitative analysis using
Grad-CAM, our study demonstrates that the uniqueness of manga works is strongly
reflected in their panel layouts.",2024-12-26T09:53:37Z,http://arxiv.org/abs/2412.19141v1,"Siyuan Feng, Teruya Yoshinaga, Katsuhiko Hayashi, Koki Washio, Hidetaka Kamigaito"
"SILC-EFSA: Self-aware In-context Learning Correction for Entity-level
  Financial Sentiment Analysis","In recent years, fine-grained sentiment analysis in finance has gained
significant attention, but the scarcity of entity-level datasets remains a key
challenge. To address this, we have constructed the largest English and Chinese
financial entity-level sentiment analysis datasets to date. Building on this
foundation, we propose a novel two-stage sentiment analysis approach called
Self-aware In-context Learning Correction (SILC). The first stage involves
fine-tuning a base large language model to generate pseudo-labeled data
specific to our task. In the second stage, we train a correction model using a
GNN-based example retriever, which is informed by the pseudo-labeled data. This
two-stage strategy has allowed us to achieve state-of-the-art performance on
the newly constructed datasets, advancing the field of financial sentiment
analysis. In a case study, we demonstrate the enhanced practical utility of our
data and methods in monitoring the cryptocurrency market. Our datasets and code
are available at https://github.com/NLP-Bin/SILC-EFSA.",2024-12-26T09:53:01Z,http://arxiv.org/abs/2412.19140v1,"Senbin Zhu, Chenyuan He, Hongde Liu, Pengcheng Dong, Hanjie Zhao, Yuchen Yan, Yuxiang Jia, Hongying Zan, Min Peng"
PlanLLM: Video Procedure Planning with Refinable Large Language Models,"Video procedure planning, i.e., planning a sequence of action steps given the
video frames of start and goal states, is an essential ability for embodied AI.
Recent works utilize Large Language Models (LLMs) to generate enriched action
step description texts to guide action step decoding. Although LLMs are
introduced, these methods decode the action steps into a closed-set of one-hot
vectors, limiting the model's capability of generalizing to new steps or tasks.
Additionally, fixed action step descriptions based on world-level commonsense
may contain noise in specific instances of visual states. In this paper, we
propose PlanLLM, a cross-modal joint learning framework with LLMs for video
procedure planning. We propose an LLM-Enhanced Planning module which fully uses
the generalization ability of LLMs to produce free-form planning output and to
enhance action step decoding. We also propose Mutual Information Maximization
module to connect world-level commonsense of step descriptions and
sample-specific information of visual states, enabling LLMs to employ the
reasoning ability to generate step sequences. With the assistance of LLMs, our
method can both closed-set and open vocabulary procedure planning tasks. Our
PlanLLM achieves superior performance on three benchmarks, demonstrating the
effectiveness of our designs.",2024-12-26T09:51:05Z,http://arxiv.org/abs/2412.19139v1,"Dejie Yang, Zijing Zhao, YangLiu"
SUTrack: Towards Simple and Unified Single Object Tracking,"In this paper, we propose a simple yet unified single object tracking (SOT)
framework, dubbed SUTrack. It consolidates five SOT tasks (RGB-based,
RGB-Depth, RGB-Thermal, RGB-Event, RGB-Language Tracking) into a unified model
trained in a single session. Due to the distinct nature of the data, current
methods typically design individual architectures and train separate models for
each task. This fragmentation results in redundant training processes,
repetitive technological innovations, and limited cross-modal knowledge
sharing. In contrast, SUTrack demonstrates that a single model with a unified
input representation can effectively handle various common SOT tasks,
eliminating the need for task-specific designs and separate training sessions.
Additionally, we introduce a task-recognition auxiliary training strategy and a
soft token type embedding to further enhance SUTrack's performance with minimal
overhead. Experiments show that SUTrack outperforms previous task-specific
counterparts across 11 datasets spanning five SOT tasks. Moreover, we provide a
range of models catering edge devices as well as high-performance GPUs,
striking a good trade-off between speed and accuracy. We hope SUTrack could
serve as a strong foundation for further compelling research into unified
tracking models. Code and models are available at
github.com/chenxin-dlut/SUTrack.",2024-12-26T09:41:36Z,http://arxiv.org/abs/2412.19138v1,"Xin Chen, Ben Kang, Wanting Geng, Jiawen Zhu, Yi Liu, Dong Wang, Huchuan Lu"
"Extended Cross-Modality United Learning for Unsupervised
  Visible-Infrared Person Re-identification","Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)
aims to learn modality-invariant features from unlabeled cross-modality
datasets and reduce the inter-modality gap. However, the existing methods lack
cross-modality clustering or excessively pursue cluster-level association,
which makes it difficult to perform reliable modality-invariant features
learning. To deal with this issue, we propose a Extended Cross-Modality United
Learning (ECUL) framework, incorporating Extended Modality-Camera Clustering
(EMCC) and Two-Step Memory Updating Strategy (TSMem) modules. Specifically, we
design ECUL to naturally integrates intra-modality clustering, inter-modality
clustering and inter-modality instance selection, establishing compact and
accurate cross-modality associations while reducing the introduction of noisy
labels. Moreover, EMCC captures and filters the neighborhood relationships by
extending the encoding vector, which further promotes the learning of
modality-invariant and camera-invariant knowledge in terms of clustering
algorithm. Finally, TSMem provides accurate and generalized proxy points for
contrastive learning by updating the memory in stages. Extensive experiments
results on SYSU-MM01 and RegDB datasets demonstrate that the proposed ECUL
shows promising performance and even outperforms certain supervised methods.",2024-12-26T09:30:26Z,http://arxiv.org/abs/2412.19134v1,"Ruixing Wu, Yiming Yang, Jiakai He, Haifeng Hu"
"Unveiling the Chiral States in Multi-Weyl Semimetals through
  Magneto-Optical Spectroscopy","This study investigates the transport parameters in multi-Weyl semimetals,
focusing on their magneto-optical properties and the role of chiral states. The
tilting parameter is identified as a key factor in higher-order Weyl nodes,
significantly influencing the magneto-optical response. We obtain a generic
Landau-level expression for multi-Weyl semimetals, establishing a robust
framework for analyzing their quantum transport properties. A comprehensive
expression for the conductivity tensor components is presented, uncovering
distinctive low-frequency peaks and other features shaped by the tilting
parameter. Our findings reveal that the signatures of chiral states in the
conductivity tensors become increasingly pronounced with the Weyl node order.
Particularly, the tilting parameter is shown to impact Faraday rotation, at
energies near the tilted Dirac cone energies. These results provide critical
insights into the magneto-optical behavior of multi-Weyl semimetals and their
potential for exploring topological phenomena.",2024-12-26T09:29:59Z,http://arxiv.org/abs/2412.19132v1,"Sushmita Saha, Deepannita Das, Alestin Mawrie"
"A Rhetorical Relations-Based Framework for Tailored Multimedia Document
  Summarization","In the rapidly evolving landscape of digital content, the task of summarizing
multimedia documents, which encompass textual, visual, and auditory elements,
presents intricate challenges. These challenges include extracting pertinent
information from diverse formats, maintaining the structural integrity and
semantic coherence of the original content, and generating concise yet
informative summaries. This paper introduces a novel framework for multimedia
document summarization that capitalizes on the inherent structure of the
document to craft coherent and succinct summaries. Central to this framework is
the incorporation of a rhetorical structure for structural analysis, augmented
by a graph-based representation to facilitate the extraction of pivotal
information. Weighting algorithms are employed to assign significance values to
document units, thereby enabling effective ranking and selection of relevant
content. Furthermore, the framework is designed to accommodate user preferences
and time constraints, ensuring the production of personalized and contextually
relevant summaries. The summarization process is elaborately delineated,
encompassing document specification, graph construction, unit weighting, and
summary extraction, supported by illustrative examples and algorithmic
elucidation. This proposed framework represents a significant advancement in
automatic summarization, with broad potential applications across multimedia
document processing, promising transformative impacts in the field.",2024-12-26T09:29:59Z,http://arxiv.org/abs/2412.19133v1,"Azze-Eddine Maredj, Madjid Sadallah"
Semantic Residual for Multimodal Unified Discrete Representation,"Recent research in the domain of multimodal unified representations
predominantly employs codebook as representation forms, utilizing Vector
Quantization(VQ) for quantization, yet there has been insufficient exploration
of other quantization representation forms. Our work explores more precise
quantization methods and introduces a new framework, Semantic Residual
Cross-modal Information Disentanglement (SRCID), inspired by the numerical
residual concept inherent to Residual Vector Quantization (RVQ). SRCID employs
semantic residual-based information disentanglement for multimodal data to
better handle the inherent discrepancies between different modalities. Our
method enhances the capabilities of unified multimodal representations and
demonstrates exceptional performance in cross-modal generalization and
cross-modal zero-shot retrieval. Its average results significantly surpass
existing state-of-the-art models, as well as previous attempts with RVQ and
Finite Scalar Quantization (FSQ) based on these modals.",2024-12-26T09:08:52Z,http://arxiv.org/abs/2412.19128v1,"Hai Huang, Shulei Wang, Yan Xia"
"Advanced Knowledge Transfer: Refined Feature Distillation for Zero-Shot
  Quantization in Edge Computing","We introduce AKT (Advanced Knowledge Transfer), a novel method to enhance the
training ability of low-bit quantized (Q) models in the field of zero-shot
quantization (ZSQ). Existing research in ZSQ has focused on generating
high-quality data from full-precision (FP) models. However, these approaches
struggle with reduced learning ability in low-bit quantization due to its
limited information capacity. To overcome this limitation, we propose effective
training strategy compared to data generation. Particularly, we analyzed that
refining feature maps in the feature distillation process is an effective way
to transfer knowledge to the Q model. Based on this analysis, AKT efficiently
transfer core information from the FP model to the Q model. AKT is the first
approach to utilize both spatial and channel attention information in feature
distillation in ZSQ. Our method addresses the fundamental gradient exploding
problem in low-bit Q models. Experiments on CIFAR-10 and CIFAR-100 datasets
demonstrated the effectiveness of the AKT. Our method led to significant
performance enhancement in existing generative models. Notably, AKT achieved
significant accuracy improvements in low-bit Q models, achieving
state-of-the-art in the 3,5bit scenarios on CIFAR-10. The code is available at
https://github.com/Inpyo-Hong/AKT-Advanced-knowledge-Transfer.",2024-12-26T08:52:27Z,http://arxiv.org/abs/2412.19125v1,"Inpyo Hong, Youngwan Jo, Hyojeong Lee, Sunghyun Ahn, Sanghyun Park"
"Evaluating Self-Supervised Learning in Medical Imaging: A Benchmark for
  Robustness, Generalizability, and Multi-Domain Impact","Self-supervised learning (SSL) has emerged as a promising paradigm in medical
imaging, addressing the chronic challenge of limited labeled data in healthcare
settings. While SSL has shown impressive results, existing studies in the
medical domain are often limited in scope, focusing on specific datasets or
modalities, or evaluating only isolated aspects of model performance. This
fragmented evaluation approach poses a significant challenge, as models
deployed in critical medical settings must not only achieve high accuracy but
also demonstrate robust performance and generalizability across diverse
datasets and varying conditions. To address this gap, we present a
comprehensive evaluation of SSL methods within the medical domain, with a
particular focus on robustness and generalizability. Using the MedMNIST dataset
collection as a standardized benchmark, we evaluate 8 major SSL methods across
11 different medical datasets. Our study provides an in-depth analysis of model
performance in both in-domain scenarios and the detection of
out-of-distribution (OOD) samples, while exploring the effect of various
initialization strategies, model architectures, and multi-domain pre-training.
We further assess the generalizability of SSL methods through cross-dataset
evaluations and the in-domain performance with varying label proportions (1%,
10%, and 100%) to simulate real-world scenarios with limited supervision. We
hope this comprehensive benchmark helps practitioners and researchers make more
informed decisions when applying SSL methods to medical applications.",2024-12-26T08:51:56Z,http://arxiv.org/abs/2412.19124v1,"Valay Bundele, Oğuz Ata Çal, Bora Kargi, Karahan Sarıtaş, Kıvanç Tezören, Zohreh Ghaderi, Hendrik Lensch"
"CoheDancers: Enhancing Interactive Group Dance Generation through
  Music-Driven Coherence Decomposition","Dance generation is crucial and challenging, particularly in domains like
dance performance and virtual gaming. In the current body of literature, most
methodologies focus on Solo Music2Dance. While there are efforts directed
towards Group Music2Dance, these often suffer from a lack of coherence,
resulting in aesthetically poor dance performances. Thus, we introduce
CoheDancers, a novel framework for Music-Driven Interactive Group Dance
Generation. CoheDancers aims to enhance group dance generation coherence by
decomposing it into three key aspects: synchronization, naturalness, and
fluidity. Correspondingly, we develop a Cycle Consistency based Dance
Synchronization strategy to foster music-dance correspondences, an
Auto-Regressive-based Exposure Bias Correction strategy to enhance the fluidity
of the generated dances, and an Adversarial Training Strategy to augment the
naturalness of the group dance output. Collectively, these strategies enable
CohdeDancers to produce highly coherent group dances with superior quality.
Furthermore, to establish better benchmarks for Group Music2Dance, we construct
the most diverse and comprehensive open-source dataset to date, I-Dancers,
featuring rich dancer interactions, and create comprehensive evaluation
metrics. Experimental evaluations on I-Dancers and other extant datasets
substantiate that CoheDancers achieves unprecedented state-of-the-art
performance. Code will be released.",2024-12-26T08:47:13Z,http://arxiv.org/abs/2412.19123v1,"Kaixing Yang, Xulong Tang, Haoyu Wu, Qinliang Xue, Biao Qin, Hongyan Liu, Zhaoxin Fan"
Discrete vs. Continuous Trade-offs for Generative Models,"This work explores the theoretical and practical foundations of denoising
diffusion probabilistic models (DDPMs) and score-based generative models, which
leverage stochastic processes and Brownian motion to model complex data
distributions. These models employ forward and reverse diffusion processes
defined through stochastic differential equations (SDEs) to iteratively add and
remove noise, enabling high-quality data generation. By analyzing the
performance bounds of these models, we demonstrate how score estimation errors
propagate through the reverse process and bound the total variation distance
using discrete Girsanov transformations, Pinsker's inequality, and the data
processing inequality (DPI) for an information theoretic lens.",2024-12-26T08:14:27Z,http://arxiv.org/abs/2412.19114v1,"Jathin Korrapati, Tanish Baranwal, Rahul Shah"
"SketchFill: Sketch-Guided Code Generation for Imputing Derived Missing
  Values","Missing value is a critical issue in data science, significantly impacting
the reliability of analyses and predictions. Missing value imputation (MVI) is
a longstanding problem because it highly relies on domain knowledge. Large
language models (LLMs) have emerged as a promising tool for data cleaning,
including MVI for tabular data, offering advanced capabilities for
understanding and generating content. However, despite their promise, existing
LLM techniques such as in-context learning and Chain-of-Thought (CoT) often
fall short in guiding LLMs to perform complex reasoning for MVI, particularly
when imputing derived missing values, which require mathematical formulas and
data relationships across rows and columns. This gap underscores the need for
further advancements in LLM methodologies to enhance their reasoning
capabilities for more reliable imputation outcomes. To fill this gap, we
propose SketchFill, a novel sketch-based method to guide LLMs in generating
accurate formulas to impute missing numerical values. Our experimental results
demonstrate that SketchFill significantly outperforms state-of-the-art methods,
achieving 56.2% higher accuracy than CoT-based methods and 78.8% higher
accuracy than MetaGPT. This sets a new standard for automated data cleaning and
advances the field of MVI for numerical values.",2024-12-26T08:13:34Z,http://arxiv.org/abs/2412.19113v1,"Yunfan Zhang, Changlun Li, Yuyu Luo, Nan Tang"
"Spectral Enhancement and Pseudo-Anchor Guidance for Infrared-Visible
  Person Re-Identification","The development of deep learning has facilitated the application of person
re-identification (ReID) technology in intelligent security. Visible-infrared
person re-identification (VI-ReID) aims to match pedestrians across infrared
and visible modality images enabling 24-hour surveillance. Current studies
relying on unsupervised modality transformations as well as inefficient
embedding constraints to bridge the spectral differences between infrared and
visible images, however, limit their potential performance. To tackle the
limitations of the above approaches, this paper introduces a simple yet
effective Spectral Enhancement and Pseudo-anchor Guidance Network, named
SEPG-Net. Specifically, we propose a more homogeneous spectral enhancement
scheme based on frequency domain information and greyscale space, which avoids
the information loss typically caused by inefficient modality transformations.
Further, a Pseudo Anchor-guided Bidirectional Aggregation (PABA) loss is
introduced to bridge local modality discrepancies while better preserving
discriminative identity embeddings. Experimental results on two public
benchmark datasets demonstrate the superior performance of SEPG-Net against
other state-of-the-art methods. The code is available at
https://github.com/1024AILab/ReID-SEPG.",2024-12-26T08:03:53Z,http://arxiv.org/abs/2412.19111v1,"Yiyuan Ge, Zhihao Chen, Ziyang Wang, Jiaju Kang, Mingya Zhang"
Stochastic normalizing flows for Effective String Theory,"Effective String Theory (EST) is a powerful tool used to study confinement in
pure gauge theories by modeling the confining flux tube connecting a static
quark-anti-quark pair as a thin vibrating string. Recently, flow-based samplers
have been applied as an efficient numerical method to study EST regularized on
the lattice, opening the route to study observables previously inaccessible to
standard analytical methods. Flow-based samplers are a class of algorithms
based on Normalizing Flows (NFs), deep generative models recently proposed as a
promising alternative to traditional Markov Chain Monte Carlo methods in
lattice field theory calculations. By combining NF layers with
out-of-equilibrium stochastic updates, we obtain Stochastic Normalizing Flows
(SNFs), a scalable class of machine learning algorithms that can be explained
in terms of stochastic thermodynamics. In this contribution, we outline EST and
SNFs, and report some numerical results for the shape of the flux tube.",2024-12-26T07:58:09Z,http://arxiv.org/abs/2412.19109v1,"Michele Caselle, Elia Cellini, Alessandro Nada"
"Graph Mixture of Experts and Memory-augmented Routers for Multivariate
  Time Series Anomaly Detection","Multivariate time series (MTS) anomaly detection is a critical task that
involves identifying abnormal patterns or events in data that consist of
multiple interrelated time series. In order to better model the complex
interdependence between entities and the various inherent characteristics of
each entity, the GNN based methods are widely adopted by existing methods. In
each layer of GNN, node features aggregate information from their neighboring
nodes to update their information. In doing so, from shallow layer to deep
layer in GNN, original individual node features continue to be weakened and
more structural information,i.e., from short-distance neighborhood to
long-distance neighborhood, continues to be enhanced. However, research to date
has largely ignored the understanding of how hierarchical graph information is
represented and their characteristics that can benefit anomaly detection.
Existing methods simply leverage the output from the last layer of GNN for
anomaly estimation while neglecting the essential information contained in the
intermediate GNN layers. To address such limitations, in this paper, we propose
a Graph Mixture of Experts (Graph-MoE) network for multivariate time series
anomaly detection, which incorporates the mixture of experts (MoE) module to
adaptively represent and integrate hierarchical multi-layer graph information
into entity representations. It is worth noting that our Graph-MoE can be
integrated into any GNN-based MTS anomaly detection method in a plug-and-play
manner. In addition, the memory-augmented routers are proposed in this paper to
capture the correlation temporal information in terms of the global historical
features of MTS to adaptively weigh the obtained entity representations to
achieve successful anomaly estimation. Extensive experiments on five
challenging datasets prove the superiority of our approach and each proposed
module.",2024-12-26T07:49:51Z,http://arxiv.org/abs/2412.19108v1,"Xiaoyu Huang, Weidong Chen, Bo Hu, Zhendong Mao"
"ERGNN: Spectral Graph Neural Network with Explicitly-optimized Rational
  Graph Filters","Approximation-based spectral graph neural networks, which construct graph
filters with function approximation, have shown substantial performance in
graph learning tasks. Despite their great success, existing works primarily
employ polynomial approximation to construct the filters, whereas another
superior option, namely ration approximation, remains underexplored. Although a
handful of prior works have attempted to deploy the rational approximation,
their implementations often involve intensive computational demands or still
resort to polynomial approximations, hindering full potential of the rational
graph filters. To address the issues, this paper introduces ERGNN, a novel
spectral GNN with explicitly-optimized rational filter. ERGNN adopts a unique
two-step framework that sequentially applies the numerator filter and the
denominator filter to the input signals, thus streamlining the model paradigm
while enabling explicit optimization of both numerator and denominator of the
rational filter. Extensive experiments validate the superiority of ERGNN over
state-of-the-art methods, establishing it as a practical solution for deploying
rational-based GNNs.",2024-12-26T07:48:47Z,http://arxiv.org/abs/2412.19106v1,"Guoming Li, Jian Yang, Shangsong Liang"
"Improving Generative Pre-Training: An In-depth Study of Masked Image
  Modeling and Denoising Models","In this work, we dive deep into the impact of additive noise in pre-training
deep networks. While various methods have attempted to use additive noise
inspired by the success of latent denoising diffusion models, when used in
combination with masked image modeling, their gains have been marginal when it
comes to recognition tasks. We thus investigate why this would be the case, in
an attempt to find effective ways to combine the two ideas. Specifically, we
find three critical conditions: corruption and restoration must be applied
within the encoder, noise must be introduced in the feature space, and an
explicit disentanglement between noised and masked tokens is necessary. By
implementing these findings, we demonstrate improved pre-training performance
for a wide range of recognition tasks, including those that require
fine-grained, high-frequency information to solve.",2024-12-26T07:47:20Z,http://arxiv.org/abs/2412.19104v1,"Hyesong Choi, Daeun Kim, Sungmin Cha, Kwang Moo Yi, Dongbo Min"
"Reconstruction Target Matters in Masked Image Modeling for Cross-Domain
  Few-Shot Learning","Cross-Domain Few-Shot Learning (CDFSL) requires the model to transfer
knowledge from the data-abundant source domain to data-scarce target domains
for fast adaptation, where the large domain gap makes CDFSL a challenging
problem. Masked Autoencoder (MAE) excels in effectively using unlabeled data
and learning image's global structures, enhancing model generalization and
robustness. However, in the CDFSL task with significant domain shifts, we find
MAE even shows lower performance than the baseline supervised models. In this
paper, we first delve into this phenomenon for an interpretation. We find that
MAE tends to focus on low-level domain information during reconstructing pixels
while changing the reconstruction target to token features could mitigate this
problem. However, not all features are beneficial, as we then find
reconstructing high-level features can hardly improve the model's
transferability, indicating a trade-off between filtering domain information
and preserving the image's global structure. In all, the reconstruction target
matters for the CDFSL task. Based on the above findings and interpretations, we
further propose Domain-Agnostic Masked Image Modeling (DAMIM) for the CDFSL
task. DAMIM includes an Aggregated Feature Reconstruction module to
automatically aggregate features for reconstruction, with balanced learning of
domain-agnostic information and images' global structure, and a Lightweight
Decoder module to further benefit the encoder's generalizability. Experiments
on four CDFSL datasets demonstrate that our method achieves state-of-the-art
performance.",2024-12-26T07:43:01Z,http://arxiv.org/abs/2412.19101v1,"Ran Ma, Yixiong Zou, Yuhua Li, Ruixuan Li"
"BSDB-Net: Band-Split Dual-Branch Network with Selective State Spaces
  Mechanism for Monaural Speech Enhancement","Although the complex spectrum-based speech enhancement(SE) methods have
achieved significant performance, coupling amplitude and phase can lead to a
compensation effect, where amplitude information is sacrificed to compensate
for the phase that is harmful to SE. In addition, to further improve the
performance of SE, many modules are stacked onto SE, resulting in increased
model complexity that limits the application of SE. To address these problems,
we proposed a dual-path network based on compressed frequency using Mamba.
First, we extract amplitude and phase information through parallel dual
branches. This approach leverages structured complex spectra to implicitly
capture phase information and solves the compensation effect by decoupling
amplitude and phase, and the network incorporates an interaction module to
suppress unnecessary parts and recover missing components from the other
branch. Second, to reduce network complexity, the network introduces a
band-split strategy to compress the frequency dimension. To further reduce
complexity while maintaining good performance, we designed a Mamba-based module
that models the time and frequency dimensions under linear complexity. Finally,
compared to baselines, our model achieves an average 8.3 times reduction in
computational complexity while maintaining superior performance. Furthermore,
it achieves a 25 times reduction in complexity compared to transformer-based
models.",2024-12-26T07:42:07Z,http://arxiv.org/abs/2412.19099v1,"Cunhang Fan, Enrui Liu, Andong Li, Jianhua Tao, Jian Zhou, Jiahao Li, Chengshi Zheng, Zhao Lv"
Tint Your Models Task-wise for Improved Multi-task Model Merging,"Traditional model merging methods for multi-task learning (MTL) address task
conflicts with straightforward strategies such as weight averaging, sign
consensus, or minimal test-time adjustments. This presumably counts on the
assumption that a merged encoder still retains abundant task knowledge from
individual encoders, implying that its shared representation is sufficiently
general across tasks. However, our insight is that adding just a single
trainable task-specific layer further can bring striking performance gains, as
demonstrated by our pilot study. Motivated by this finding, we propose Model
Tinting, a new test-time approach that introduces a single task-specific layer
for each task as trainable adjustments. Our method jointly trains merging
coefficients and task-specific layers, which effectively reduces task conflicts
with minimal additional costs. Additionally, we propose a sampling method that
utilizes the difference in confidence levels of both merged and individual
encoders. Extensive experiments demonstrate our method's effectiveness, which
achieves state-of-the-art performance across both computer vision and natural
language processing tasks and significantly surpasses prior works. Our code is
available at https://github.com/AIM-SKKU/ModelTinting.",2024-12-26T07:42:06Z,http://arxiv.org/abs/2412.19098v1,"Aecheon Jung, Seunghwan Lee, Dongyoon Han, Sungeun Hong"
"Anisotropic and tunable vortex topology in multiband iron-based
  superconductors","Building on the multiband nature of iron-based superconductors (FeSCs), we
have uncovered pronounced anisotropy in Majorana vortex topology arising from
the interaction between vortex orientation and multiple electronic topologies.
This anisotropy manifests in two distinct vortex configurations: the z-vortex
and x-vortex, oriented perpendicular and parallel to the Dirac axis (z-axis for
FeSCs), respectively. The x-vortex exhibits a unique duality, displaying two
distinct topological phase diagrams. One is strikingly simple, comprising only
trivial and topological superconducting phases, and remains resilient to
multiband entanglement. The other mirrors the z-vortex's complex diagram,
featuring alternating trivial, topological crystalline and topological
superconducting phases. Crucially, the former is exclusive to the x-vortex and
supports unpaired Majorana vortices across a wide parameter range, even with
Dirac nodes in electronic bands. Notably, uniaxial strain can modulate these
x-vortex phases, enabling the x-vortex to support both stable Majorana vortices
and rich exotic physics in a controllable manner. Moreover, we propose that the
x-vortex offers promising advantages for developing iron-based superconducting
quantum devices. Our findings introduce a novel paradigm in vortex topology
within multiband superconducting systems, highlighting the x-vortex as a
promising platform for exploring Majorana physics and advancing iron-based
superconducting quantum technology.",2024-12-26T07:31:11Z,http://arxiv.org/abs/2412.19096v1,"Si-Qi Yu, Wei Cheng, Chuang Li, Xiao-Hong Pan, Gang Xu, Fu-Chun Zhang, Xin Liu"
"TrajGEOS: Trajectory Graph Enhanced Orientation-based Sequential Network
  for Mobility Prediction","Human mobility studies how people move to access their needed resources and
plays a significant role in urban planning and location-based services. As a
paramount task of human mobility modeling, next location prediction is
challenging because of the diversity of users' historical trajectories that
gives rise to complex mobility patterns and various contexts. Deep sequential
models have been widely used to predict the next location by leveraging the
inherent sequentiality of trajectory data. However, they do not fully leverage
the relationship between locations and fail to capture users' multi-level
preferences. This work constructs a trajectory graph from users' historical
traces and proposes a \textbf{Traj}ectory \textbf{G}raph \textbf{E}nhanced
\textbf{O}rientation-based \textbf{S}equential network (TrajGEOS) for
next-location prediction tasks. TrajGEOS introduces hierarchical graph
convolution to capture location and user embeddings. Such embeddings consider
not only the contextual feature of locations but also the relation between
them, and serve as additional features in downstream modules. In addition, we
design an orientation-based module to learn users' mid-term preferences from
sequential modeling modules and their recent trajectories. Extensive
experiments on three real-world LBSN datasets corroborate the value of graph
and orientation-based modules and demonstrate that TrajGEOS outperforms the
state-of-the-art methods on the next location prediction task.",2024-12-26T07:18:38Z,http://arxiv.org/abs/2412.19092v1,"Zhaoping Hu, Zongyuan Huang, Jinming Yang, Tao Yang, Yaohui Jin, Yanyan Xu"
From Coin to Data: The Impact of Object Detection on Digital Numismatics,"In this work we investigate the application of advanced object detection
techniques to digital numismatics, focussing on the analysis of historical
coins. Leveraging models such as Contrastive Language-Image Pre-training
(CLIP), we develop a flexible framework for identifying and classifying
specific coin features using both image and textual descriptions. By examining
two distinct datasets, modern Russian coins featuring intricate ""Saint George
and the Dragon"" designs and degraded 1st millennium AD Southeast Asian coins
bearing Hindu-Buddhist symbols, we evaluate the efficacy of different detection
algorithms in search and classification tasks. Our results demonstrate the
superior performance of larger CLIP models in detecting complex imagery, while
traditional methods excel in identifying simple geometric patterns.
Additionally, we propose a statistical calibration mechanism to enhance the
reliability of similarity scores in low-quality datasets. This work highlights
the transformative potential of integrating state-of-the-art object detection
into digital numismatics, enabling more scalable, precise, and efficient
analysis of historical artifacts. These advancements pave the way for new
methodologies in cultural heritage research, artefact provenance studies, and
the detection of forgeries.",2024-12-26T07:05:53Z,http://arxiv.org/abs/2412.19091v1,"Rafael Cabral, Maria De Iorio, Andrew Harris"
"Humans as a Calibration Pattern: Dynamic 3D Scene Reconstruction from
  Unsynchronized and Uncalibrated Videos","Recent works on dynamic neural field reconstruction assume input from
synchronized multi-view videos with known poses. These input constraints are
often unmet in real-world setups, making the approach impractical. We
demonstrate that unsynchronized videos with unknown poses can generate dynamic
neural fields if the videos capture human motion. Humans are one of the most
common dynamic subjects whose poses can be estimated using state-of-the-art
methods. While noisy, the estimated human shape and pose parameters provide a
decent initialization for the highly non-convex and under-constrained problem
of training a consistent dynamic neural representation. Given the sequences of
pose and shape of humans, we estimate the time offsets between videos, followed
by camera pose estimations by analyzing 3D joint locations. Then, we train
dynamic NeRF employing multiresolution rids while simultaneously refining both
time offsets and camera poses. The setup still involves optimizing many
parameters, therefore, we introduce a robust progressive learning strategy to
stabilize the process. Experiments show that our approach achieves accurate
spatiotemporal calibration and high-quality scene reconstruction in challenging
conditions.",2024-12-26T07:04:20Z,http://arxiv.org/abs/2412.19089v1,"Changwoon Choi, Jeongjun Kim, Geonho Cha, Minkwan Kim, Dongyoon Wee, Young Min Kim"
MoPD: Mixture-of-Prompts Distillation for Vision-Language Models,"Soft prompt learning methods are effective for adapting vision-language
models (VLMs) to downstream tasks. Nevertheless, empirical evidence reveals a
tendency of existing methods that they overfit seen classes and exhibit
degraded performance on unseen classes. This limitation is due to the inherent
bias in the training data towards the seen classes. To address this issue, we
propose a novel soft prompt learning method, named Mixture-of-Prompts
Distillation (MoPD), which can effectively transfer useful knowledge from hard
prompts manually hand-crafted (a.k.a. teacher prompts) to the learnable soft
prompt (a.k.a. student prompt), thereby enhancing the generalization ability of
soft prompts on unseen classes. Moreover, the proposed MoPD method utilizes a
gating network that learns to select hard prompts used for prompt distillation.
Extensive experiments demonstrate that the proposed MoPD method outperforms
state-of-the-art baselines especially on on unseen classes.",2024-12-26T06:57:04Z,http://arxiv.org/abs/2412.19087v1,"Yang Chen, Shuai Fu, Yu Zhang"
"Assessing Pre-trained Models for Transfer Learning through Distribution
  of Spectral Components","Pre-trained model assessment for transfer learning aims to identify the
optimal candidate for the downstream tasks from a model hub, without the need
of time-consuming fine-tuning. Existing advanced works mainly focus on
analyzing the intrinsic characteristics of the entire features extracted by
each pre-trained model or how well such features fit the target labels. This
paper proposes a novel perspective for pre-trained model assessment through the
Distribution of Spectral Components (DISCO). Through singular value
decomposition of features extracted from pre-trained models, we investigate
different spectral components and observe that they possess distinct
transferability, contributing diversely to the fine-tuning performance.
Inspired by this, we propose an assessment method based on the distribution of
spectral components which measures the proportions of their corresponding
singular values. Pre-trained models with features concentrating on more
transferable components are regarded as better choices for transfer learning.
We further leverage the labels of downstream data to better estimate the
transferability of each spectral component and derive the final assessment
criterion. Our proposed method is flexible and can be applied to both
classification and regression tasks. We conducted comprehensive experiments
across three benchmarks and two tasks including image classification and object
detection, demonstrating that our method achieves state-of-the-art performance
in choosing proper pre-trained models from the model hub for transfer learning.",2024-12-26T06:54:22Z,http://arxiv.org/abs/2412.19085v1,"Tengxue Zhang, Yang Shu, Xinyang Chen, Yifei Long, Chenjuan Guo, Bin Yang"
A Microservice Graph Generator with Production Characteristics,"A production microservice application may provide multiple services, queries
of a service may have different call graphs, and a microservice may be shared
across call graphs. It is challenging to improve the resource efficiency of
such complex applications without proper benchmarks, while production traces
are too large to be used in experiments. To this end, we propose a Service
Dependency Graph Generator (DGG) that comprises a Data Handler and a Graph
Generator, for generating the service dependency graphs of benchmarks that
incorporate production-level characteristics from traces. The data handler
first constructs fine-grained call graphs with dynamic interface and repeated
calling features from the trace and merges them into dependency graphs, and
then clusters them into different categories based on the topological and
invocation types. Taking the organized data and the selected category, the
graph generator simulates the process of real microservices invoking downstream
microservices using a random graph model, generates multiple call graphs, and
merges the call graphs to form the small-scale service dependency graph with
production-level characteristics. Case studies show that DGG's generated graphs
are similar to real traces in terms of topologies. Moreover, the resource
scaling based on DGG's fine-grained call graph constructing increases the
resource efficiency by up to 44.8% while ensuring the required QoS.",2024-12-26T06:51:35Z,http://arxiv.org/abs/2412.19083v1,"Fanrong Du, Jiuchen Shi, Quan Chen, Li Li, Minyi Guo"
"Social Optima in Linear Quadratic Graphon Field Control: Analysis via
  Infinite Dimensional Approach","This paper is concerned with linear quadratic graphon field social control
problem where the noises of individual agents are correlated. Compared with the
well-studied mean field system, the graphon field system consists of a large
number of agents coupled weakly via a weighted undirected graph where each node
represents an individual agent. Another notable feature of this paper is that
the dynamics of states of agents are driven by Brownian motions with a
correlation matrix. The infinite dimensional approach is adopted to design the
centralized and decentralized controls for our large population system. By
graphon theory, we prove that the linear quadratic (LQ) social optimum control
problem under the centralized information pattern is equivalent to an LQ
optimal control problem concerned with a stochastic evolution equation, and the
feedback-type optimal centralized control is obtained. Then, by designing an
auxiliary infinite dimensional optimal control problem through agent number
$N\rightarrow\infty$, a set of decentralized strategies are constructed, which
are further shown to be asymptotically social optimal.",2024-12-26T06:46:19Z,http://arxiv.org/abs/2412.19082v1,"De-xuan Xu, Zhun Gou, Nan-jing Huang"
"Graph-Enhanced Dual-Stream Feature Fusion with Pre-Trained Model for
  Acoustic Traffic Monitoring","Microphone array techniques are widely used in sound source localization and
smart city acoustic-based traffic monitoring, but these applications face
significant challenges due to the scarcity of labeled real-world traffic audio
data and the complexity and diversity of application scenarios. The DCASE
Challenge's Task 10 focuses on using multi-channel audio signals to count
vehicles (cars or commercial vehicles) and identify their directions
(left-to-right or vice versa). In this paper, we propose a graph-enhanced
dual-stream feature fusion network (GEDF-Net) for acoustic traffic monitoring,
which simultaneously considers vehicle type and direction to improve detection.
We propose a graph-enhanced dual-stream feature fusion strategy which consists
of a vehicle type feature extraction (VTFE) branch, a vehicle direction feature
extraction (VDFE) branch, and a frame-level feature fusion module to combine
the type and direction feature for enhanced performance. A pre-trained model
(PANNs) is used in the VTFE branch to mitigate data scarcity and enhance the
type features, followed by a graph attention mechanism to exploit temporal
relationships and highlight important audio events within these features. The
frame-level fusion of direction and type features enables fine-grained feature
representation, resulting in better detection performance. Experiments
demonstrate the effectiveness of our proposed method. GEDF-Net is our
submission that achieved 1st place in the DCASE 2024 Challenge Task 10.",2024-12-26T06:28:42Z,http://arxiv.org/abs/2412.19078v1,"Shitong Fan, Feiyang Xiao, Wenbo Wang, Shuhan Qi, Qiaoxi Zhu, Wenwu Wang, Jian Guan"
"Robust Speech and Natural Language Processing Models for Depression
  Screening","Depression is a global health concern with a critical need for increased
patient screening. Speech technology offers advantages for remote screening but
must perform robustly across patients. We have described two deep learning
models developed for this purpose. One model is based on acoustics; the other
is based on natural language processing. Both models employ transfer learning.
Data from a depression-labeled corpus in which 11,000 unique users interacted
with a human-machine application using conversational speech is used. Results
on binary depression classification have shown that both models perform at or
above AUC=0.80 on unseen data with no speaker overlap. Performance is further
analyzed as a function of test subset characteristics, finding that the models
are generally robust over speaker and session variables. We conclude that
models based on these approaches offer promise for generalized automated
depression screening.",2024-12-26T06:05:52Z,http://arxiv.org/abs/2412.19072v1,"Y. Lu, A. Harati, T. Rutowski, R. Oliveira, P. Chlebek, E. Shriberg"
Cross-Demographic Portability of Deep NLP-Based Depression Models,"Deep learning models are rapidly gaining interest for real-world applications
in behavioral health. An important gap in current literature is how well such
models generalize over different populations. We study Natural Language
Processing (NLP) based models to explore portability over two different corpora
highly mismatched in age. The first and larger corpus contains younger
speakers. It is used to train an NLP model to predict depression. When testing
on unseen speakers from the same age distribution, this model performs at
AUC=0.82. We then test this model on the second corpus, which comprises seniors
from a retirement community. Despite the large demographic differences in the
two corpora, we saw only modest degradation in performance for the
senior-corpus data, achieving AUC=0.76. Interestingly, in the senior
population, we find AUC=0.81 for the subset of patients whose health state is
consistent over time. Implications for demographic portability of speech-based
applications are discussed.",2024-12-26T05:54:24Z,http://arxiv.org/abs/2412.19070v1,"Tomek Rutowski, Elizabeth Shriberg, Amir Harati, Yang Lu, Ricardo Oliveira, Piotr Chlebek"
Effective and secure federated online learning to rank,"Online Learning to Rank (OLTR) optimises ranking models using implicit user
feedback, such as clicks. Unlike traditional Learning to Rank (LTR) methods
that rely on a static set of training data with relevance judgements to learn a
ranking model, OLTR methods update the model continually as new data arrives.
Thus, it addresses several drawbacks such as the high cost of human
annotations, potential misalignment between user preferences and human
judgments, and the rapid changes in user query intents. However, OLTR methods
typically require the collection of searchable data, user queries, and clicks,
which poses privacy concerns for users.
  Federated Online Learning to Rank (FOLTR) integrates OLTR within a Federated
Learning (FL) framework to enhance privacy by not sharing raw data. While
promising, FOLTR methods currently lag behind traditional centralised OLTR due
to challenges in ranking effectiveness, robustness with respect to data
distribution across clients, susceptibility to attacks, and the ability to
unlearn client interactions and data. This thesis presents a comprehensive
study on Federated Online Learning to Rank, addressing its effectiveness,
robustness, security, and unlearning capabilities, thereby expanding the
landscape of FOLTR.",2024-12-26T05:53:10Z,http://arxiv.org/abs/2412.19069v1,Shuyi Wang
"Attacking Voice Anonymization Systems with Augmented Feature and Speaker
  Identity Difference","This study focuses on the First VoicePrivacy Attacker Challenge within the
ICASSP 2025 Signal Processing Grand Challenge, which aims to develop speaker
verification systems capable of determining whether two anonymized speech
signals are from the same speaker. However, differences between feature
distributions of original and anonymized speech complicate this task. To
address this challenge, we propose an attacker system that combines Data
Augmentation enhanced feature representation and Speaker Identity Difference
enhanced classifier to improve verification performance, termed DA-SID.
Specifically, data augmentation strategies (i.e., data fusion and SpecAugment)
are utilized to mitigate feature distribution gaps, while probabilistic linear
discriminant analysis (PLDA) is employed to further enhance speaker identity
difference. Our system significantly outperforms the baseline, demonstrating
exceptional effectiveness and robustness against various voice anonymization
systems, ultimately securing a top-5 ranking in the challenge.",2024-12-26T05:52:44Z,http://arxiv.org/abs/2412.19068v1,"Yanzhe Zhang, Zhonghao Bi, Feiyang Xiao, Xuefeng Yang, Qiaoxi Zhu, Jian Guan"
Learning Monocular Depth from Events via Egomotion Compensation,"Event cameras are neuromorphically inspired sensors that sparsely and
asynchronously report brightness changes. Their unique characteristics of high
temporal resolution, high dynamic range, and low power consumption make them
well-suited for addressing challenges in monocular depth estimation (e.g.,
high-speed or low-lighting conditions). However, current existing methods
primarily treat event streams as black-box learning systems without
incorporating prior physical principles, thus becoming over-parameterized and
failing to fully exploit the rich temporal information inherent in event camera
data. To address this limitation, we incorporate physical motion principles to
propose an interpretable monocular depth estimation framework, where the
likelihood of various depth hypotheses is explicitly determined by the effect
of motion compensation. To achieve this, we propose a Focus Cost Discrimination
(FCD) module that measures the clarity of edges as an essential indicator of
focus level and integrates spatial surroundings to facilitate cost estimation.
Furthermore, we analyze the noise patterns within our framework and improve it
with the newly introduced Inter-Hypotheses Cost Aggregation (IHCA) module,
where the cost volume is refined through cost trend prediction and multi-scale
cost consistency constraints. Extensive experiments on real-world and synthetic
datasets demonstrate that our proposed framework outperforms cutting-edge
methods by up to 10\% in terms of the absolute relative error metric, revealing
superior performance in predicting accuracy.",2024-12-26T05:41:18Z,http://arxiv.org/abs/2412.19067v1,"Haitao Meng, Chonghao Zhong, Sheng Tang, Lian JunJia, Wenwei Lin, Zhenshan Bing, Yi Chang, Gang Chen, Alois Knoll"
"FFCG: Effective and Fast Family Column Generation for Solving
  Large-Scale Linear Program","Column Generation (CG) is an effective and iterative algorithm to solve
large-scale linear programs (LP). During each CG iteration, new columns are
added to improve the solution of the LP. Typically, CG greedily selects one
column with the most negative reduced cost, which can be improved by adding
more columns at once. However, selecting all columns with negative reduced
costs would lead to the addition of redundant columns that do not improve the
objective value. Therefore, selecting the appropriate columns to add is still
an open problem and previous machine-learning-based approaches for CG only add
a constant quantity of columns per iteration due to the state-space explosion
problem. To address this, we propose Fast Family Column Generation (FFCG) -- a
novel reinforcement-learning-based CG that selects a variable number of columns
as needed in an iteration. Specifically, we formulate the column selection
problem in CG as an MDP and design a reward metric that balances both the
convergence speed and the number of redundant columns. In our experiments, FFCG
converges faster on the common benchmarks and reduces the number of CG
iterations by 77.1% for Cutting Stock Problem (CSP) and 84.8% for Vehicle
Routing Problem with Time Windows (VRPTW), and a 71.4% reduction in computing
time for CSP and 84.0% for VRPTW on average compared to several
state-of-the-art baselines.",2024-12-26T05:35:48Z,http://arxiv.org/abs/2412.19066v1,"Yi-Xiang Hu, Feng Wu, Shaoang Li, Yifang Zhao, Xiang-Yang Li"
"Predicting Accurate X-ray Absorption Spectra for CN$^+$, CN$^\bullet$,
  and CN$^-$: Insights from First-Principles Simulations","High-resolution X-ray spectroscopy is an essential tool in X-ray astronomy,
enabling detailed studies of celestial objects and their physical and chemical
properties. However, comprehensive mapping of high-resolution X-ray spectra for
even simple interstellar and circumstellar molecules is still lacking. In this
study, we conducted systematic quantum chemical simulations to predict the C1s
X-ray absorption spectra of CN$^+$, CN, and CN$^-$. Our findings provide
valuable references for both X-ray astronomy and laboratory studies. We
assigned the first electronic peak of CN$^+$ and CN to C1s $\rightarrow
\sigma^*$ transitions, while the peak for CN$^-$ corresponds to a C1s
$\rightarrow \pi^*$ transition. We further calculated the vibronic fine
structures for these transitions using the quantum wavepacket method based on
multiconfigurational-level, anharmonic potential energy curves, revealing
distinct energy positions for the 0-0 absorptions at 280.7 eV, 279.6 eV, and
285.8 eV. Each vibronic profile features a prominent 0-0 peak, showing overall
similarity but differing intensity ratios of the 0-0 and 0-1 peaks. Notably,
introducing a C1s core hole leads to shortened C-N bond lengths and increased
vibrational frequencies across all species. These findings enhance our
understanding of the electronic structures and X-ray spectra of carbon-nitrogen
species, emphasizing the influence of charge state on X-ray absorptions.",2024-12-26T05:27:06Z,http://arxiv.org/abs/2412.19065v1,"Jinyu Li, Sheng-Yu Wang, Lu Zhang, Guoyan Ge, Minrui Wei, Junxiang Zuo, Weijie Hua"
"Hierarchical Multi-agent Meta-Reinforcement Learning for Cross-channel
  Bidding","Real-time bidding (RTB) plays a pivotal role in online advertising
ecosystems. Advertisers employ strategic bidding to optimize their advertising
impact while adhering to various financial constraints, such as the
return-on-investment (ROI) and cost-per-click (CPC). Primarily focusing on
bidding with fixed budget constraints, traditional approaches cannot
effectively manage the dynamic budget allocation problem where the goal is to
achieve global optimization of bidding performance across multiple channels
with a shared budget. In this paper, we propose a hierarchical multi-agent
reinforcement learning framework for multi-channel bidding optimization. In
this framework, the top-level strategy applies a CPC constrained diffusion
model to dynamically allocate budgets among the channels according to their
distinct features and complex interdependencies, while the bottom-level
strategy adopts a state-action decoupled actor-critic method to address the
problem of extrapolation errors in offline learning caused by
out-of-distribution actions and a context-based meta-channel knowledge learning
method to improve the state representation capability of the policy based on
the shared knowledge among different channels. Comprehensive experiments
conducted on a large scale real-world industrial dataset from the Meituan ad
bidding platform demonstrate that our method achieves a state-of-the-art
performance.",2024-12-26T05:26:30Z,http://arxiv.org/abs/2412.19064v1,"Shenghong He, Chao Yu"
DAPoinTr: Domain Adaptive Point Transformer for Point Cloud Completion,"Point Transformers (PoinTr) have shown great potential in point cloud
completion recently. Nevertheless, effective domain adaptation that improves
transferability toward target domains remains unexplored. In this paper, we
delve into this topic and empirically discover that direct feature alignment on
point Transformer's CNN backbone only brings limited improvements since it
cannot guarantee sequence-wise domain-invariant features in the Transformer. To
this end, we propose a pioneering Domain Adaptive Point Transformer (DAPoinTr)
framework for point cloud completion. DAPoinTr consists of three key
components: Domain Query-based Feature Alignment (DQFA), Point Token-wise
Feature alignment (PTFA), and Voted Prediction Consistency (VPC). In
particular, DQFA is presented to narrow the global domain gaps from the
sequence via the presented domain proxy and domain query at the Transformer
encoder and decoder, respectively. PTFA is proposed to close the local domain
shifts by aligning the tokens, \emph{i.e.,} point proxy and dynamic query, at
the Transformer encoder and decoder, respectively. VPC is designed to consider
different Transformer decoders as multiple of experts (MoE) for ensembled
prediction voting and pseudo-label generation. Extensive experiments with
visualization on several domain adaptation benchmarks demonstrate the
effectiveness and superiority of our DAPoinTr compared with state-of-the-art
methods. Code will be publicly available at:
https://github.com/Yinghui-Li-New/DAPoinTr",2024-12-26T05:16:54Z,http://arxiv.org/abs/2412.19062v1,"Yinghui Li, Qianyu Zhou, Jingyu Gong, Ye Zhu, Richard Dazeley, Xinkui Zhao, Xuequan Lu"
"An active hydroelastic liquid crystal phase of a fluttering
  ferroelectric nematic","Polarization flutter, produced by an applied AC electric field drives an
equilibrium ferroelectric nematic ($\mathrm{N_F}$) liquid crystal (LC) through
a transition into a dissipative active ferroelectric nematic state exhibiting
strong elasto-hydrodynamic intermolecular interaction. In such a fluttering
ferroelectric, the typical equilibrium $\mathrm{N_F}$ textural features adopted
to reduce electrostatic energy, such as preferences for director bend, and
alignment of polarization parallel to LC/air interfaces, are overcome, giving
way to nonequilibrium conjugate structures in which director splay, and
alignment of polarization normal to $\mathrm{N_F}$/air interfaces are
preferred. Viewing the latter textures as those of an active nematic phase
reveals that self-organization to reduce effective viscosity and resulting
dissipation generates a flow-driven apparent nematic elasticity and interface
structuring that dominates equilibrium LC elastic and surface forces.",2024-12-26T05:13:55Z,http://arxiv.org/abs/2412.19061v1,"Xi Chen, Cory Pecinovsky, Eva Korblova, Matthew A. Glaser, Leo Radzihovsky, Joseph E. Maclennan, David M. Walba, Noel A. Clark"
Coarse-grained binning in Drell-Yan transverse momentum spectra,"We report a study of the determination of the intrinsic transverse momentum
of partons, the intrinsic $k_T$, from the dilepton transverse momentum $p_T$ in
Drell-Yan (DY) production at hadron colliders. The result shows that a good
sensitivity to the intrinsic $k_T$ distribution is achieved by measuring
relative ratios between the cross sections of suitably defined low-$p_T$ and
high-$p_T$ regions. The study is performed through both a pseudo-data test and
an extraction from measurements of the DY process by the CMS collaboration.
Since the methodology does not rely on any dedicated partition of bins, this
$p_T$-ratio observable requires less special treatment in very low $p_T$
regions, and propagates lower systematic uncertainties induced from unfolding
or momentum migration, in contrast with previous proposals of using a
fine-binning measurement of the differential cross section.",2024-12-26T05:13:39Z,http://arxiv.org/abs/2412.19060v1,"Wenxiao Zhan, Siqi Yang, Minghui Liu, Francesco Hautmann, Liang Han"
"The System of BSDEs with Singular Terminal Values Arising in Optimal
  Liquidation with Regime Switching","We study a stochastic control problem with regime switching arising in an
optimal liquidation problem with dark pools and multiple regimes. The new
feature of this model is that it introduces a system of BSDEs with jumps and
with singular terminal values, which appears in literature for the first time.
The existence result for this system is obtained. As a result, we solve the
stochastic control problem with regime switching. More importantly, the
uniqueness result of this system is also obtained, in contrast to merely
minimal solutions established in most related literature.",2024-12-26T05:03:16Z,http://arxiv.org/abs/2412.19058v1,"Guanxing Fu, Xiaomin Shi, Zuoquan Xu"
"SpectralKD: Understanding and Optimizing Vision Transformer Distillation
  through Spectral Analysis","Knowledge distillation effectively reduces model complexity while improving
performance, yet the underlying knowledge transfer mechanisms remain poorly
understood. We propose novel spectral analysis methods and guidelines to
optimize distillation, making the knowledge transfer process more
interpretable. Our analysis reveals that CaiT models concentrate information in
their first and last few layers, informing optimal layer selection for feature
map distillation. Surprisingly, we discover that Swin Transformer and CaiT
exhibit similar spectral encoding patterns despite their architectural
differences, enhancing our understanding of transformer architectures and
leading to improved feature map alignment strategies. Based on these insights,
we introduce a simple yet effective spectral alignment method named SpectralKD.
Experimental results demonstrate that following our guidelines enables
SpectralKD to achieve state-of-the-art performance (DeiT-Tiny: $+5.2\%$,
Swin-Tiny: $+1.4\%$ in ImageNet-1k Top-1 accuracy). Furthermore, through
spectral analysis of student models trained with and without distillation, we
show that distilled models mirror spectral patterns of their teachers,
providing a new lens for interpreting knowledge distillation dynamics. Our
code, pre-trained models, and experimental logs will be made publicly
available.",2024-12-26T04:45:05Z,http://arxiv.org/abs/2412.19055v1,"Huiyuan Tian, Bonan Xu, Shijian Li, Gang Pan"
"Performance Characterization and Optimizations of Traditional ML
  Applications","Even in the era of Deep Learning based methods, traditional machine learning
methods with large data sets continue to attract significant attention.
However, we find an apparent lack of a detailed performance characterization of
these methods in the context of large training datasets. In this work, we study
the system's behavior of a number of traditional ML methods as implemented in
popular free software libraries/modules to identify critical performance
bottlenecks experienced by these applications. The performance characterization
study reveals several interesting insights on the performance of these
applications. Then we evaluate the performance benefits of applying some
well-known optimizations at the levels of caches and the main memory. More
specifically, we test the usefulness of optimizations such as (i) software
prefetching to improve cache performance and (ii) data layout and computation
reordering optimizations to improve locality in DRAM accesses. These
optimizations are implemented as modifications to the well-known scikit-learn
library, and hence can be easily leveraged by application programmers. We
evaluate the impact of the proposed optimizations using a combination of
simulation and execution on a real system. The software prefetching
optimization results in performance benefits varying from 5.2%-27.1% on
different ML applications while the data layout and computation reordering
approaches yield 6.16%-28.0% performance improvement.",2024-12-26T04:13:52Z,http://arxiv.org/abs/2412.19051v1,"Harsh Kumar, R. Govindarajan"
Jasper and Stella: distillation of SOTA embedding models,"A crucial component of many deep learning applications (such as FAQ and RAG)
is dense retrieval, in which embedding models are used to convert raw text to
numerical vectors and then get the most similar text by MIPS (Maximum Inner
Product Search). Some text embedding benchmarks (e.g. MTEB, BEIR, and
AIR-Bench) have been established to evaluate embedding models accurately.
Thanks to these benchmarks, we can use SOTA models; however, the deployment and
application of these models in industry were hampered by their large vector
dimensions and numerous parameters. To alleviate this problem, 1) we present a
distillation technique that can enable a smaller student model to achieve good
performance. 2) Inspired by MRL we present a training approach of reducing the
vector dimensions based on its own vectors or its teacher vectors. 3) We do
simple yet effective alignment training between images and text to make our
model a multimodal encoder. We trained Stella and Jasper models using the
technologies above and achieved high scores on the MTEB leaderboard. We release
the model and data at Hugging Face Hub
(https://huggingface.co/infgrad/jasper_en_vision_language_v1) and the training
logs are at https://api.wandb.ai/links/dunnzhang0/z8jqoqpb.",2024-12-26T04:05:28Z,http://arxiv.org/abs/2412.19048v1,"Dun Zhang, FulongWang"
Revealing the Self: Brainwave-Based Human Trait Identification,"People exhibit unique emotional responses. In the same scenario, the
emotional reactions of two individuals can be either similar or vastly
different. For instance, consider one person's reaction to an invitation to
smoke versus another person's response to a query about their sleep quality.
The identification of these individual traits through the observation of common
physical parameters opens the door to a wide range of applications, including
psychological analysis, criminology, disease prediction, addiction control, and
more. While there has been previous research in the fields of psychometrics,
inertial sensors, computer vision, and audio analysis, this paper introduces a
novel technique for identifying human traits in real time using brainwave data.
To achieve this, we begin with an extensive study of brainwave data collected
from 80 participants using a portable EEG headset. We also conduct a
statistical analysis of the collected data utilizing box plots. Our analysis
uncovers several new insights, leading us to a groundbreaking unified approach
for identifying diverse human traits by leveraging machine learning techniques
on EEG data. Our analysis demonstrates that this proposed solution achieves
high accuracy. Moreover, we explore two deep-learning models to compare the
performance of our solution. Consequently, we have developed an integrated,
real-time trait identification solution using EEG data, based on the insights
from our analysis. To validate our approach, we conducted a rigorous user
evaluation with an additional 20 participants. The outcomes of this evaluation
illustrate both high accuracy and favorable user ratings, emphasizing the
robust potential of our proposed method to serve as a versatile solution for
human trait identification.",2024-12-26T03:27:34Z,http://arxiv.org/abs/2412.19041v1,"Md Mirajul Islam, Md Nahiyan Uddin, Maoyejatun Hasana, Debojit Pandit, Nafis Mahmud Rahman, Sriram Chellappan, Sami Azam, A. B. M. Alim Al Islam"
Neural Networks Perform Sufficient Dimension Reduction,"This paper investigates the connection between neural networks and sufficient
dimension reduction (SDR), demonstrating that neural networks inherently
perform SDR in regression tasks under appropriate rank regularizations.
Specifically, the weights in the first layer span the central mean subspace. We
establish the statistical consistency of the neural network-based estimator for
the central mean subspace, underscoring the suitability of neural networks in
addressing SDR-related challenges. Numerical experiments further validate our
theoretical findings, and highlight the underlying capability of neural
networks to facilitate SDR compared to the existing methods. Additionally, we
discuss an extension to unravel the central subspace, broadening the scope of
our investigation.",2024-12-26T03:05:43Z,http://arxiv.org/abs/2412.19033v1,"Shuntuo Xu, Zhou Yu"
"Modality-Projection Universal Model for Comprehensive Full-Body Medical
  Imaging Segmentation","The integration of deep learning in medical imaging has shown great promise
for enhancing diagnostic, therapeutic, and research outcomes. However, applying
universal models across multiple modalities remains challenging due to the
inherent variability in data characteristics. This study aims to introduce and
evaluate a Modality Projection Universal Model (MPUM). MPUM employs a novel
modality-projection strategy, which allows the model to dynamically adjust its
parameters to optimize performance across different imaging modalities. The
MPUM demonstrated superior accuracy in identifying anatomical structures,
enabling precise quantification for improved clinical decision-making. It also
identifies metabolic associations within the brain-body axis, advancing
research on brain-body physiological correlations. Furthermore, MPUM's unique
controller-based convolution layer enables visualization of saliency maps
across all network layers, significantly enhancing the model's
interpretability.",2024-12-26T02:23:27Z,http://arxiv.org/abs/2412.19026v1,"Yixin Chen, Lin Gao, Yajuan Gao, Rui Wang, Jingge Lian, Xiangxi Meng, Yanhua Duan, Leiying Chai, Hongbin Han, Zhaoping Cheng, Zhaoheng Xie"
"Channel-Aware Optimal Transport: A Theoretical Framework for Generative
  Communication","Optimal transport has numerous applications, particularly in machine learning
tasks involving generative models. In practice, the transportation process
often encounters an information bottleneck, typically arising from the
conversion of a communication channel into a rate-limited bit pipeline using
error correction codes. While this conversion enables a channel-oblivious
approach to optimal transport, it fails to fully exploit the available degrees
of freedom. Motivated by the emerging paradigm of generative communication,
this paper examines the problem of channel-aware optimal transport, where a
block of i.i.d. random variables is transmitted through a memoryless channel to
generate another block of i.i.d. random variables with a prescribed marginal
distribution such that the end-to-end distortion is minimized. With unlimited
common randomness available to the encoder and decoder, the source-channel
separation architecture is shown to be asymptotically optimal as the
blocklength approaches infinity. On the other hand, in the absence of common
randomness, the source-channel separation architecture is generally suboptimal.
For this scenario, a hybrid coding scheme is proposed, which partially retains
the generative capabilities of the given channel while enabling reliable
transmission of digital information. It is demonstrated that the proposed
hybrid coding scheme can outperform both separation-based and uncoded schemes.",2024-12-26T02:23:08Z,http://arxiv.org/abs/2412.19025v1,"Xiqiang Qu, Ruibin Li, Jun Chen, Lei Yu, Xinbing Wang"
Adaptivity can help exponentially for shadow tomography,"In recent years there has been significant interest in understanding the
statistical complexity of learning from quantum data under the constraint that
one can only make unentangled measurements. While a key challenge in
establishing tight lower bounds in this setting is to deal with the fact that
the measurements can be chosen in an adaptive fashion, a recurring theme has
been that adaptivity offers little advantage over more straightforward,
nonadaptive protocols.
  In this note, we offer a counterpoint to this. We show that for the basic
task of shadow tomography, protocols that use adaptively chosen two-copy
measurements can be exponentially more sample-efficient than any protocol that
uses nonadaptive two-copy measurements.",2024-12-26T02:13:04Z,http://arxiv.org/abs/2412.19022v1,"Sitan Chen, Weiyuan Gong, Zhihan Zhang"
"Let the Rule Speak: Enhancing In-context Learning Debiasing with
  Interpretability","In-context learning, which allows large language models to perform diverse
tasks with a few demonstrations, is found to have imbalanced per-class
prediction accuracy on multi-class text classification. Although notable output
correction methods have been developed to tackle the issue and simultaneously
improve downstream prediction accuracy, they may fail to answer the core
interpretability challenges: why and which certain classes need corrections,
and more importantly, a tailored correction for per-sample, per-class's
probability. To address such interpretability gaps, we first find that the
imbalance arises from certain classes consistently receiving high ICL output
probabilities, whereas others receiving lower or mixed ranges, so the former is
more frequently chosen, resulting in higher accuracy; more crucially, we find
that these ranges have significantly varying degrees of influence on the
accuracy bias, highlighting the need for precise, interpretable probability
corrections by range. Motivated by this, we propose FuRud, a Fuzzy Rule
Optimization based Debiasing method, that (1) detects which classes need
corrections, and (2) for each correction-needed class, detects its probability
ranges and applies asymmetric amplifications or reductions to correct them
interpretably. Notably, across seven benchmark datasets, FuRud reduces the
pairwise class accuracy bias (COBias) by more than half (56%), while achieving
a relative increase of 21% in accuracy, outperforming state-of-the-art
debiasing methods. Moreover, FuRud can optimize downstream tasks with as few as
10 optimization examples. Furthermore, FuRud can work for prompt formats that
lead to highly skewed predictions. For example, FuRud greatly improves ICL
outputs which use letter options, with 44% relative accuracy increase and 54%
relative COBias reduction.",2024-12-26T01:56:42Z,http://arxiv.org/abs/2412.19018v1,"Ruixi Lin, Yang You"
"Brain Ageing Prediction using Isolation Forest Technique and Residual
  Neural Network (ResNet)","Brain aging is a complex and dynamic process, leading to functional and
structural changes in the brain. These changes could lead to the increased risk
of neurodegenerative diseases and cognitive decline. Accurate brain-age
estimation utilizing neuroimaging data has become necessary for detecting
initial signs of neurodegeneration. Here, we propose a novel deep learning
approach using the Residual Neural Network 101 Version 2 (ResNet101V2) model to
predict brain age from MRI scans. To train, validate and test our proposed
model, we used a large dataset of 2102 images which were selected randomly from
the International Consortium for Brain Mapping (ICBM). Next, we applied data
preprocessing techniques, including normalizing the images and using outlier
detection via Isolation Forest method. Then, we evaluated various pre-trained
approaches (namely: MobileNetV2, ResNet50V2, ResNet101V2, Xception). The
results demonstrated that the ResNet101V2 model has higher performance compared
with the other models, attaining MAEs of 0.9136 and 0.8242 years for before and
after using Isolation Forest process. Our method achieved a high accuracy in
brain age estimation in ICBM dataset and it provides a reliable brain age
prediction.",2024-12-26T01:49:21Z,http://arxiv.org/abs/2412.19017v1,"Saadat Behzadi, Danial Sharifrazi, Roohallah Alizadehsani, Mojtaba Lotfaliany, Mohammadreza Mohebbi"
"Imperceptible Adversarial Attacks on Point Clouds Guided by
  Point-to-Surface Field","Adversarial attacks on point clouds are crucial for assessing and improving
the adversarial robustness of 3D deep learning models. Traditional solutions
strictly limit point displacement during attacks, making it challenging to
balance imperceptibility with adversarial effectiveness. In this paper, we
attribute the inadequate imperceptibility of adversarial attacks on point
clouds to deviations from the underlying surface. To address this, we introduce
a novel point-to-surface (P2S) field that adjusts adversarial perturbation
directions by dragging points back to their original underlying surface.
Specifically, we use a denoising network to learn the gradient field of the
logarithmic density function encoding the shape's surface, and apply a
distance-aware adjustment to perturbation directions during attacks, thereby
enhancing imperceptibility. Extensive experiments show that adversarial attacks
guided by our P2S field are more imperceptible, outperforming state-of-the-art
methods.",2024-12-26T01:36:35Z,http://arxiv.org/abs/2412.19015v1,"Keke Tang, Weiyao Ke, Weilong Peng, Xiaofei Wang, Ziyong Du, Zhize Wu, Peican Zhu, Zhihong Tian"
Dynamic networks clustering via mirror distance,"The classification of different patterns of network evolution, for example in
brain connectomes or social networks, is a key problem in network inference and
modern data science. Building on the notion of a network's Euclidean mirror,
which captures its evolution as a curve in Euclidean space, we develop the
Dynamic Network Clustering through Mirror Distance (DNCMD), an algorithm for
clustering dynamic networks based on a distance measure between their
associated mirrors. We provide theoretical guarantees for DNCMD to achieve
exact recovery of distinct evolutionary patterns for latent position random
networks both when underlying vertex features change deterministically and when
they follow a stochastic process. We validate our theoretical results through
numerical simulations and demonstrate the application of DNCMD to understand
edge functions in Drosophila larval connectome data, as well as to analyze
temporal patterns in dynamic trade networks.",2024-12-26T01:14:21Z,http://arxiv.org/abs/2412.19012v1,"Runbing Zheng, Avanti Athreya, Marta Zlatic, Michael Clayton, Carey E. Priebe"
"FACEMUG: A Multimodal Generative and Fusion Framework for Local Facial
  Editing","Existing facial editing methods have achieved remarkable results, yet they
often fall short in supporting multimodal conditional local facial editing. One
of the significant evidences is that their output image quality degrades
dramatically after several iterations of incremental editing, as they do not
support local editing. In this paper, we present a novel multimodal generative
and fusion framework for globally-consistent local facial editing (FACEMUG)
that can handle a wide range of input modalities and enable fine-grained and
semantic manipulation while remaining unedited parts unchanged. Different
modalities, including sketches, semantic maps, color maps, exemplar images,
text, and attribute labels, are adept at conveying diverse conditioning
details, and their combined synergy can provide more explicit guidance for the
editing process. We thus integrate all modalities into a unified generative
latent space to enable multimodal local facial edits. Specifically, a novel
multimodal feature fusion mechanism is proposed by utilizing multimodal
aggregation and style fusion blocks to fuse facial priors and multimodalities
in both latent and feature spaces. We further introduce a novel self-supervised
latent warping algorithm to rectify misaligned facial features, efficiently
transferring the pose of the edited image to the given latent codes. We
evaluate our FACEMUG through extensive experiments and comparisons to
state-of-the-art (SOTA) methods. The results demonstrate the superiority of
FACEMUG in terms of editing quality, flexibility, and semantic control, making
it a promising solution for a wide range of local facial editing tasks.",2024-12-26T00:53:54Z,http://arxiv.org/abs/2412.19009v1,"Wanglong Lu, Jikai Wang, Xiaogang Jin, Xianta Jiang, Hanli Zhao"
"Enhancing Audiovisual Speech Recognition through Bifocal Preference
  Optimization","Audiovisual Automatic Speech Recognition (AV-ASR) aims to improve speech
recognition accuracy by leveraging visual signals. It is particularly
challenging in unconstrained real-world scenarios across various domains due to
noisy acoustic environments, spontaneous speech, and the uncertain use of
visual information. Most previous works fine-tune audio-only ASR models on
audiovisual datasets, optimizing them for conventional ASR objectives. However,
they often neglect visual features and common errors in unconstrained video
scenarios. In this paper, we propose using a preference optimization strategy
to improve speech recognition accuracy for real-world videos. First, we create
preference data via simulating common errors that occurred in AV-ASR from two
focals: manipulating the audio or vision input and rewriting the output
transcript. Second, we propose BPO-AVASR, a Bifocal Preference Optimization
method to improve AV-ASR models by leveraging both input-side and output-side
preference. Extensive experiments demonstrate that our approach significantly
improves speech recognition accuracy across various domains, outperforming
previous state-of-the-art models on real-world video speech recognition.",2024-12-26T00:26:45Z,http://arxiv.org/abs/2412.19005v1,"Yihan Wu, Yichen Lu, Yifan Peng, Xihua Wang, Ruihua Song, Shinji Watanabe"
"New Theorem on Chaos Transitions in Second-Order Dynamical Systems with
  Tikhonov Regularization","This study examines second-order dynamical systems incorporating Tikhonov
regularization. It focuses on how nonlinearities induce bifurcations and
chaotic dynamics. By using Lyapunov functions, bifurcation theory, and
numerical simulations, we identify critical transitions that lead to complex
behaviors like strange attractors and chaos. The findings provide a theoretical
framework for applications in optimization, machine learning, and biological
modeling. Key contributions include stability conditions, characterization of
chaotic regimes, and methods for managing nonlinear instabilities in
interdisciplinary systems.",2024-12-25T23:54:16Z,http://arxiv.org/abs/2412.19003v1,Illych Alvarez
"Tempus Core: Area-Power Efficient Temporal-Unary Convolution Core for
  Low-Precision Edge DLAs","The increasing complexity of deep neural networks (DNNs) poses significant
challenges for edge inference deployment due to resource and power constraints
of edge devices. Recent works on unary-based matrix multiplication hardware aim
to leverage data sparsity and low-precision values to enhance hardware
efficiency. However, the adoption and integration of such unary hardware into
commercial deep learning accelerators (DLA) remain limited due to processing
element (PE) array dataflow differences. This work presents Tempus Core, a
convolution core with highly scalable unary-based PE array comprising of tub
(temporal-unary-binary) multipliers that seamlessly integrates with the NVDLA
(NVIDIA's open-source DLA for accelerating CNNs) while maintaining dataflow
compliance and boosting hardware efficiency. Analysis across various datapath
granularities shows that for INT8 precision in 45nm CMOS, Tempus Core's PE cell
unit (PCU) yields 59.3% and 15.3% reductions in area and power consumption,
respectively, over NVDLA's CMAC unit. Considering a 16x16 PE array in Tempus
Core, area and power improves by 75% and 62%, respectively, while delivering 5x
and 4x iso-area throughput improvements for INT8 and INT4 precisions.
Post-place and route analysis of Tempus Core's PCU shows that the 16x4 PE array
for INT4 precision in 45nm CMOS requires only 0.017 mm^2 die area and consumes
only 6.2mW of total power. We demonstrate that area-power efficient unary-based
hardware can be seamlessly integrated into conventional DLAs, paving the path
for efficient unary hardware for edge AI inference.",2024-12-25T23:20:02Z,http://arxiv.org/abs/2412.19002v1,"Prabhu Vellaisamy, Harideep Nair, Thomas Kang, Yichen Ni, Haoyang Fan, Bin Qi, Jeff Chen, Shawn Blanton, John Paul Shen"
"Impact of resummation on the production and experimental bounds of
  scalar high-electric-charge objects","A one-loop Dyson-Schwinger-like resummation scheme is applied to scalar
High-Electric-Charge compact Objects (HECOs), extending previous work on
spin-1/2 case. The electromagnetic interactions of HECOs are considered within
the framework of strongly coupled scalar Quantun Electrodynamics. The
resummation amounts to determining non-trivial ultraviolet (UV) fixed points,
at which the effective Lagrangian, which will lead to the pertinent predictions
on the cross sections, is computed. In contrast to the fermionic HECO case, in
which the fixed point structure was determined solely by the interactions of
the HECOs with the photon field, in the scalar case the existence of
non-trivial UV fixed points requires the presence of additional strong self
interactions among the HECOs. Our resummation scheme, which is notably
different from a lattice strong-coupling approach, makes the computation of the
pertinent scalar-HECO-production cross sections reliable, thus allowing
revisiting the mass bounds obtained from searches for such objects in current
or future colliders. Our MadGraph implementation of the results leads to
enhanced (up to ~30%) lower bounds on the mass of scalar HECOs, as compared to
those extracted from the tree-level processes typically used in LHC collider
searches by ATLAS and MoEDAL experiments.",2024-12-25T23:13:36Z,http://arxiv.org/abs/2412.19001v1,"Jean Alexandre, Nick E. Mavromatos, Vasiliki A. Mitsou, Emanuela Musumeci"
"MGAN-CRCM: A Novel Multiple Generative Adversarial Network and
  Coarse-Refinement Based Cognizant Method for Image Inpainting","Image inpainting is a widely used technique in computer vision for
reconstructing missing or damaged pixels in images. Recent advancements with
Generative Adversarial Networks (GANs) have demonstrated superior performance
over traditional methods due to their deep learning capabilities and
adaptability across diverse image domains. Residual Networks (ResNet) have also
gained prominence for their ability to enhance feature representation and
compatibility with other architectures. This paper introduces a novel
architecture combining GAN and ResNet models to improve image inpainting
outcomes. Our framework integrates three components: Transpose
Convolution-based GAN for guided and blind inpainting, Fast
ResNet-Convolutional Neural Network (FR-CNN) for object removal, and
Co-Modulation GAN (Co-Mod GAN) for refinement. The model's performance was
evaluated on benchmark datasets, achieving accuracies of 96.59% on Image-Net,
96.70% on Places2, and 96.16% on CelebA. Comparative analyses demonstrate that
the proposed architecture outperforms existing methods, highlighting its
effectiveness in both qualitative and quantitative evaluations.",2024-12-25T22:54:28Z,http://arxiv.org/abs/2412.19000v1,"Nafiz Al Asad, Md. Appel Mahmud Pranto, Shbiruzzaman Shiam, Musaddeq Mahmud Akand, Mohammad Abu Yousuf, Khondokar Fida Hasan, Mohammad Ali Moni"
"GeoMatch++: Morphology Conditioned Geometry Matching for
  Multi-Embodiment Grasping","Despite recent progress on multi-finger dexterous grasping, current methods
focus on single grippers and unseen objects, and even the ones that explore
cross-embodiment, often fail to generalize well to unseen end-effectors. This
work addresses the problem of dexterous grasping generalization to unseen
end-effectors via a unified policy that learns correlation between gripper
morphology and object geometry. Robot morphology contains rich information
representing how joints and links connect and move with respect to each other
and thus, we leverage it through attention to learn better end-effector
geometry features. Our experiments show an average of 9.64% increase in grasp
success rate across 3 out-of-domain end-effectors compared to previous methods.",2024-12-25T22:36:57Z,http://arxiv.org/abs/2412.18998v1,"Yunze Wei, Maria Attarian, Igor Gilitschenski"
"WaveDiffUR: A diffusion SDE-based solver for ultra magnification
  super-resolution in remote sensing images","Deep neural networks have recently achieved significant advancements in
remote sensing superresolu-tion (SR). However, most existing methods are
limited to low magnification rates (e.g., 2 or 4) due to the escalating
ill-posedness at higher magnification scales. To tackle this challenge, we
redefine high-magnification SR as the ultra-resolution (UR) problem, reframing
it as solving a conditional diffusion stochastic differential equation (SDE).
In this context, we propose WaveDiffUR, a novel wavelet-domain diffusion UR
solver that decomposes the UR process into sequential sub-processes addressing
conditional wavelet components. WaveDiffUR iteratively reconstructs
low-frequency wavelet details (ensuring global consistency) and high-frequency
components (enhancing local fidelity) by incorporating pre-trained SR models as
plug-and-play modules. This modularity mitigates the ill-posedness of the SDE
and ensures scalability across diverse applications. To address limitations in
fixed boundary conditions at extreme magnifications, we introduce the
cross-scale pyramid (CSP) constraint, a dynamic and adaptive framework that
guides WaveDiffUR in generating fine-grained wavelet details, ensuring
consistent and high-fidelity outputs even at extreme magnification rates.",2024-12-25T22:26:39Z,http://arxiv.org/abs/2412.18996v1,"Yue Shi, Liangxiu Han, Darren Dancy, Lianghao Han"
"MiTREE: Multi-input Transformer Ecoregion Encoder for Species
  Distribution Modelling","Climate change poses an extreme threat to biodiversity, making it imperative
to efficiently model the geographical range of different species. The
availability of large-scale remote sensing images and environmental data has
facilitated the use of machine learning in Species Distribution Models (SDMs),
which aim to predict the presence of a species at any given location.
Traditional SDMs, reliant on expert observation, are labor-intensive, but
advancements in remote sensing and citizen science data have facilitated
machine learning approaches to SDM development. However, these models often
struggle with leveraging spatial relationships between different inputs -- for
instance, learning how climate data should inform the data present in satellite
imagery -- without upsampling or distorting the original inputs. Additionally,
location information and ecological characteristics at a location play a
crucial role in predicting species distribution models, but these aspects have
not yet been incorporated into state-of-the-art approaches. In this work, we
introduce MiTREE: a multi-input Vision-Transformer-based model with an
ecoregion encoder. MiTREE computes spatial cross-modal relationships without
upsampling as well as integrates location and ecological context. We evaluate
our model on the SatBird Summer and Winter datasets, the goal of which is to
predict bird species encounter rates, and we find that our approach improves
upon state-of-the-art baselines.",2024-12-25T22:20:47Z,http://arxiv.org/abs/2412.18995v1,"Theresa Chen, Yao-Yi Chiang"
"Geospatial Data Fusion: Combining Lidar, SAR, and Optical Imagery with
  AI for Enhanced Urban Mapping","This study explores the integration of Lidar, Synthetic Aperture Radar (SAR),
and optical imagery through advanced artificial intelligence techniques for
enhanced urban mapping. By fusing these diverse geospatial datasets, we aim to
overcome the limitations associated with single-sensor data, achieving a more
comprehensive representation of urban environments. The research employs Fully
Convolutional Networks (FCNs) as the primary deep learning model for urban
feature extraction, enabling precise pixel-wise classification of essential
urban elements, including buildings, roads, and vegetation. To optimize the
performance of the FCN model, we utilize Particle Swarm Optimization (PSO) for
hyperparameter tuning, significantly enhancing model accuracy. Key findings
indicate that the FCN-PSO model achieved a pixel accuracy of 92.3% and a mean
Intersection over Union (IoU) of 87.6%, surpassing traditional single-sensor
approaches. These results underscore the potential of fused geospatial data and
AI-driven methodologies in urban mapping, providing valuable insights for urban
planning and management. The implications of this research pave the way for
future developments in real-time mapping and adaptive urban infrastructure
planning.",2024-12-25T22:17:31Z,http://arxiv.org/abs/2412.18994v1,"Sajjad Afroosheh, Mohammadreza Askari"
"On the architecture of the Symplectic $(A_\infty,2)$-Category","This note relates to the author's construction of the Symplectic
$(A_\infty,2)$-Category, $\mathsf{Symp}$. Here we explain two ways of encoding
the information in $\mathsf{Symp}$, one topological, one algebraic. The
topological encoding is as an $(A_\infty,2)$-flow category, which we define
here. The algebraic encoding is as a linear $(A_\infty,2)$-category, which we
extract from the topological encoding. In upcoming work, the author and
Wehrheim plan to use the adiabatic Fredholm theory recently developed by
Bottman-Wehrheim to construct $\mathsf{Symp}$ as an $(A_\infty,2)$-flow
category.
  The definition of linear $(A_\infty,2)$-category that we give in this note is
different than the one proposed by Bottman-Carmeli. The recursive structure of
the 2-associahedra identifies faces with fiber products of 2-associahedra over
associahedra, and these fiber products led Bottman-Carmeli to associate
operations to singular chains on 2-associahedra. The innovation in our new
definition of linear $(A_\infty,2)$-category is to extend the family of
2-associahedra to include all fiber products of 2-associahedra over
associahedra. This allows us to associate operations to cellular chains, which
in particular enables us to produce a definition that involves only one
operation in each arity, governed by a collection of $(A_\infty,2)$-equations.",2024-12-25T22:11:18Z,http://arxiv.org/abs/2412.18993v1,Nathaniel Bottman
"Optimal Federated Learning for Functional Mean Estimation under
  Heterogeneous Privacy Constraints","Federated learning (FL) is a distributed machine learning technique designed
to preserve data privacy and security, and it has gained significant importance
due to its broad range of applications. This paper addresses the problem of
optimal functional mean estimation from discretely sampled data in a federated
setting.
  We consider a heterogeneous framework where the number of individuals,
measurements per individual, and privacy parameters vary across one or more
servers, under both common and independent design settings. In the common
design setting, the same design points are measured for each individual,
whereas in the independent design, each individual has their own random
collection of design points. Within this framework, we establish minimax upper
and lower bounds for the estimation error of the underlying mean function,
highlighting the nuanced differences between common and independent designs
under distributed privacy constraints.
  We propose algorithms that achieve the optimal trade-off between privacy and
accuracy and provide optimality results that quantify the fundamental limits of
private functional mean estimation across diverse distributed settings. These
results characterize the cost of privacy and offer practical insights into the
potential for privacy-preserving statistical analysis in federated
environments.",2024-12-25T22:06:12Z,http://arxiv.org/abs/2412.18992v1,"Tony Cai, Abhinav Chakraborty, Lasse Vuursteen"
"Detection and classification of DDoS flooding attacks by machine
  learning method","This study focuses on a method for detecting and classifying distributed
denial of service (DDoS) attacks, such as SYN Flooding, ACK Flooding, HTTP
Flooding, and UDP Flooding, using neural networks. Machine learning,
particularly neural networks, is highly effective in detecting malicious
traffic. A dataset containing normal traffic and various DDoS attacks was used
to train a neural network model with a 24-106-5 architecture. The model
achieved high Accuracy (99.35%), Precision (99.32%), Recall (99.54%), and
F-score (0.99) in the classification task. All major attack types were
correctly identified. The model was also further tested in the lab using
virtual infrastructures to generate normal and DDoS traffic. The results showed
that the model can accurately classify attacks under near-real-world
conditions, demonstrating 95.05% accuracy and balanced F-score scores for all
attack types. This confirms that neural networks are an effective tool for
detecting DDoS attacks in modern information security systems.",2024-12-25T21:58:52Z,http://arxiv.org/abs/2412.18990v1,"Dmytro Tymoshchuk, Oleh Yasniy, Mykola Mytnyk, Nataliya Zagorodna, Vitaliy Tymoshchuk"
"MTCAE-DFER: Multi-Task Cascaded Autoencoder for Dynamic Facial
  Expression Recognition","This paper expands the cascaded network branch of the autoencoder-based
multi-task learning (MTL) framework for dynamic facial expression recognition,
namely Multi-Task Cascaded Autoencoder for Dynamic Facial Expression
Recognition (MTCAE-DFER). MTCAE-DFER builds a plug-and-play cascaded decoder
module, which is based on the Vision Transformer (ViT) architecture and employs
the decoder concept of Transformer to reconstruct the multi-head attention
module. The decoder output from the previous task serves as the query (Q),
representing local dynamic features, while the Video Masked Autoencoder
(VideoMAE) shared encoder output acts as both the key (K) and value (V),
representing global dynamic features. This setup facilitates interaction
between global and local dynamic features across related tasks. Additionally,
this proposal aims to alleviate overfitting of complex large model. We utilize
autoencoder-based multi-task cascaded learning approach to explore the impact
of dynamic face detection and dynamic face landmark on dynamic facial
expression recognition, which enhances the model's generalization ability.
After we conduct extensive ablation experiments and comparison with
state-of-the-art (SOTA) methods on various public datasets for dynamic facial
expression recognition, the robustness of the MTCAE-DFER model and the
effectiveness of global-local dynamic feature interaction among related tasks
have been proven.",2024-12-25T21:52:31Z,http://arxiv.org/abs/2412.18988v1,"Peihao Xiang, Kaida Wu, Chaohao Lin, Ou Bai"
"Deep Learning-Based Traffic-Aware Base Station Sleep Mode and Cell
  Zooming Strategy in RIS-Aided Multi-Cell Networks","Advances in wireless technology have significantly increased the number of
wireless connections, leading to higher energy consumption in networks. Among
these, base stations (BSs) in radio access networks (RANs) account for over
half of the total energy usage. To address this, we propose a multi-cell sleep
strategy combined with adaptive cell zooming, user association, and
reconfigurable intelligent surface (RIS) to minimize BS energy consumption.
This approach allows BSs to enter sleep during low traffic, while adaptive cell
zooming and user association dynamically adjust coverage to balance traffic
load and enhance data rates through RIS, minimizing the number of active BSs.
However, it is important to note that the proposed method may achieve
energy-savings at the cost of increased delay, requiring a trade-off between
these two factors. Moreover, minimizing BS energy consumption under the delay
constraint is a complicated non-convex problem. To address this issue, we model
the RIS-aided multi-cell network as a Markov decision process (MDP) and use the
proximal policy optimization (PPO) algorithm to optimize sleep mode (SM), cell
zooming, and user association. Besides, we utilize a double cascade correlation
network (DCCN) algorithm to optimize the RIS reflection coefficients.
Simulation results demonstrate that PPO balances energy-savings and delay,
while DCCN-optimized RIS enhances BS energy-savings. Compared to systems
optimised by the benchmark DQN algorithm, energy consumption is reduced by
49.61%",2024-12-25T21:06:40Z,http://arxiv.org/abs/2412.18983v1,"Shuo Sun, Chong Huang, Gaojie Chen, Pei Xiao, Rahim Tafazolli"
"HAND: Hierarchical Attention Network for Multi-Scale Handwritten
  Document Recognition and Layout Analysis","Handwritten document recognition (HDR) is one of the most challenging tasks
in the field of computer vision, due to the various writing styles and complex
layouts inherent in handwritten texts. Traditionally, this problem has been
approached as two separate tasks, handwritten text recognition and layout
analysis, and struggled to integrate the two processes effectively. This paper
introduces HAND (Hierarchical Attention Network for Multi-Scale Document), a
novel end-to-end and segmentation-free architecture for simultaneous text
recognition and layout analysis tasks. Our model's key components include an
advanced convolutional encoder integrating Gated Depth-wise Separable and
Octave Convolutions for robust feature extraction, a Multi-Scale Adaptive
Processing (MSAP) framework that dynamically adjusts to document complexity and
a hierarchical attention decoder with memory-augmented and sparse attention
mechanisms. These components enable our model to scale effectively from
single-line to triple-column pages while maintaining computational efficiency.
Additionally, HAND adopts curriculum learning across five complexity levels. To
improve the recognition accuracy of complex ancient manuscripts, we fine-tune
and integrate a Domain-Adaptive Pre-trained mT5 model for post-processing
refinement. Extensive evaluations on the READ 2016 dataset demonstrate the
superior performance of HAND, achieving up to 59.8% reduction in CER for
line-level recognition and 31.2% for page-level recognition compared to
state-of-the-art methods. The model also maintains a compact size of 5.60M
parameters while establishing new benchmarks in both text recognition and
layout analysis. Source code and pre-trained models are available at :
https://github.com/MHHamdan/HAND.",2024-12-25T20:36:29Z,http://arxiv.org/abs/2412.18981v1,"Mohammed Hamdan, Abderrahmane Rahiche, Mohamed Cheriet"
"Evaluating deep learning models for fault diagnosis of a rotating
  machinery with epistemic and aleatoric uncertainty","Uncertainty-aware deep learning (DL) models recently gained attention in
fault diagnosis as a way to promote the reliable detection of faults when
out-of-distribution (OOD) data arise from unseen faults (epistemic uncertainty)
or the presence of noise (aleatoric uncertainty). In this paper, we present the
first comprehensive comparative study of state-of-the-art uncertainty-aware DL
architectures for fault diagnosis in rotating machinery, where different
scenarios affected by epistemic uncertainty and different types of aleatoric
uncertainty are investigated. The selected architectures include sampling by
dropout, Bayesian neural networks, and deep ensembles. Moreover, to distinguish
between in-distribution and OOD data in the different scenarios two uncertainty
thresholds, one of which is introduced in this paper, are alternatively
applied. Our empirical findings offer guidance to practitioners and researchers
who have to deploy real-world uncertainty-aware fault diagnosis systems. In
particular, they reveal that, in the presence of epistemic uncertainty, all DL
models are capable of effectively detecting, on average, a substantial portion
of OOD data across all the scenarios. However, deep ensemble models show
superior performance, independently of the uncertainty threshold used for
discrimination. In the presence of aleatoric uncertainty, the noise level plays
an important role. Specifically, low noise levels hinder the models' ability to
effectively detect OOD data. Even in this case, however, deep ensemble models
exhibit a milder degradation in performance, dominating the others. These
achievements, combined with their shorter inference time, make deep ensemble
architectures the preferred choice.",2024-12-25T20:22:59Z,http://arxiv.org/abs/2412.18980v1,"Reza Jalayer, Masoud Jalayer, Andrea Mor, Carlotta Orsenigo, Carlo Vercellis"
Quantum memristors for neuromorphic quantum machine learning,"Quantum machine learning may permit to realize more efficient machine
learning calculations with near-term quantum devices. Among the diverse quantum
machine learning paradigms which are currently being considered, quantum
memristors are promising as a way of combining, in the same quantum hardware, a
unitary evolution with the nonlinearity provided by the measurement and
feedforward. Thus, an efficient way of deploying neuromorphic quantum computing
for quantum machine learning may be enabled.",2024-12-25T20:21:24Z,http://arxiv.org/abs/2412.18979v1,Lucas Lamata
CGCOD: Class-Guided Camouflaged Object Detection,"Camouflaged Object Detection (COD) is designed to identify objects that blend
seamlessly with their surroundings. Due to the complexity of camouflaged
objects (such as shape, color, and texture), their semantic cues are often
blurred or completely lost, posing a significant challenge for COD. Existing
COD methods often rely on visual features, which are not stable enough in
changeable camouflage environments. This instability leads to false positives
and false negatives, resulting in incomplete or inaccurate segmentation
results. In this paper, to solve this problem, we propose a new task,
Class-Guided Camouflaged Object Detection (CG-COD), which extends the
traditional COD task by introducing object class knowledge, significantly
improving the robustness and segmentation accuracy of the model in complex
environments. Toward this end, we construct a dataset, CamoClass, containing
the camouflaged objects in the real scenes and their corresponding class
annotation. Based on this, we propose a multi-stage framework CGNet which
consists of a plug-and-play class prompt generator and a class-guided detector.
Under the guidance of textual information, CGNet enables efficient
segmentation. It is worth emphasizing that for the first time, we extend the
object class annotations on existing COD benchmark datasets, and introduce a
flexible framework to improve the performance of the existing COD model under
text guidance.",2024-12-25T19:38:32Z,http://arxiv.org/abs/2412.18977v1,"Chenxi Zhang, Qing Zhang, Jiayun Wu, Youwei Pang"
Injecting Bias into Text Classification Models using Backdoor Attacks,"The rapid growth of natural language processing (NLP) and pre-trained
language models have enabled accurate text classification in a variety of
settings. However, text classification models are susceptible to backdoor
attacks, where an attacker embeds a trigger into the victim model to make the
model predict attacker-desired labels in targeted scenarios. In this paper, we
propose to utilize backdoor attacks for a new purpose: bias injection. We
develop a backdoor attack in which a subset of the training dataset is poisoned
to associate strong male actors with negative sentiment. We execute our attack
on two popular text classification datasets (IMDb and SST) and seven different
models ranging from traditional Doc2Vec-based models to LSTM networks and
modern transformer-based BERT and RoBERTa models. Our results show that the
reduction in backdoored models' benign classification accuracy is limited,
implying that our attacks remain stealthy, whereas the models successfully
learn to associate strong male actors with negative sentiment (100% attack
success rate with &gt;= 3% poison rate). Attacks on BERT and RoBERTa are
particularly more stealthy and effective, demonstrating an increased risk of
using modern and larger models. We also measure the generalizability of our
bias injection by proposing two metrics: (i) U-BBSR which uses previously
unseen words when measuring attack success, and (ii) P-BBSR which measures
attack success using paraphrased test samples. U-BBSR and P-BBSR results show
that the bias injected by our attack can go beyond memorizing a trigger phrase.",2024-12-25T19:32:02Z,http://arxiv.org/abs/2412.18975v1,"A. Dilara Yavuz, M. Emre Gursoy"
"Derandomized shallow shadows: Efficient Pauli learning with
  bounded-depth circuits","Efficiently estimating large numbers of non-commuting observables is an
important subroutine of many quantum science tasks. We present the derandomized
shallow shadows (DSS) algorithm for efficiently learning a large set of
non-commuting observables, using shallow circuits to rotate into measurement
bases. Exploiting tensor network techniques to ensure polynomial scaling of
classical resources, our algorithm outputs a set of shallow measurement
circuits that approximately minimizes the sample complexity of estimating a
given set of Pauli strings. We numerically demonstrate systematic improvement,
in comparison with state-of-the-art techniques, for energy estimation of
quantum chemistry benchmarks and verification of quantum many-body systems, and
we observe DSS's performance consistently improves as one allows deeper
measurement circuits. These results indicate that in addition to being an
efficient, low-depth, stand-alone algorithm, DSS can also benefit many larger
quantum algorithms requiring estimation of multiple non-commuting observables.",2024-12-25T19:23:29Z,http://arxiv.org/abs/2412.18973v1,"Katherine Van Kirk, Christian Kokail, Jonathan Kunjummen, Hong-Ye Hu, Yanting Teng, Madelyn Cain, Jacob Taylor, Susanne F. Yelin, Hannes Pichler, Mikhail Lukin"
Recommending Pre-Trained Models for IoT Devices,"The availability of pre-trained models (PTMs) has enabled faster deployment
of machine learning across applications by reducing the need for extensive
training. Techniques like quantization and distillation have further expanded
PTM applicability to resource-constrained IoT hardware. Given the many PTM
options for any given task, engineers often find it too costly to evaluate each
model's suitability. Approaches such as LogME, LEEP, and ModelSpider help
streamline model selection by estimating task relevance without exhaustive
tuning. However, these methods largely leave hardware constraints as future
work-a significant limitation in IoT settings. In this paper, we identify the
limitations of current model recommendation approaches regarding hardware
constraints and introduce a novel, hardware-aware method for PTM selection. We
also propose a research agenda to guide the development of effective,
hardware-conscious model recommendation systems for IoT applications.",2024-12-25T19:19:55Z,http://arxiv.org/abs/2412.18972v1,"Parth V. Patil, Wenxin Jiang, Huiyun Peng, Daniel Lugo, Kelechi G. Kalu, Josh LeBlanc, Lawrence Smith, Hyeonwoo Heo, Nathanael Aou, James C. Davis"
"Adopting Trustworthy AI for Sleep Disorder Prediction: Deep Time Series
  Analysis with Temporal Attention Mechanism and Counterfactual Explanations","Sleep disorders have a major impact on both lifestyle and health. Effective
sleep disorder prediction from lifestyle and physiological data can provide
essential details for early intervention. This research utilizes three deep
time series models and facilitates them with explainability approaches for
sleep disorder prediction. Specifically, our approach adopts Temporal
Convolutional Networks (TCN), Long Short-Term Memory (LSTM) for time series
data analysis, and Temporal Fusion Transformer model (TFT). Meanwhile, the
temporal attention mechanism and counterfactual explanation with SHapley
Additive exPlanations (SHAP) approach are employed to ensure dependable,
accurate, and interpretable predictions. Finally, using a large dataset of
sleep health measures, our evaluation demonstrates the effect of our method in
predicting sleep disorders.",2024-12-25T19:19:45Z,http://arxiv.org/abs/2412.18971v1,"Pegah Ahadian, Wei Xu, Sherry Wang, Qiang Guan"
"ModelGrow: Continual Text-to-Video Pre-training with Model Expansion and
  Language Understanding Enhancement","Text-to-video (T2V) generation has gained significant attention recently.
However, the costs of training a T2V model from scratch remain persistently
high, and there is considerable room for improving the generation performance,
especially under limited computation resources. This work explores the
continual general pre-training of text-to-video models, enabling the model to
""grow"" its abilities based on a pre-trained foundation, analogous to how humans
acquire new knowledge based on past experiences. There is a lack of extensive
study of the continual pre-training techniques in T2V generation. In this work,
we take the initial step toward exploring this task systematically and propose
ModelGrow. Specifically, we break this task into two key aspects: increasing
model capacity and improving semantic understanding. For model capacity, we
introduce several novel techniques to expand the model size, enabling it to
store new knowledge and improve generation performance. For semantic
understanding, we propose a method that leverages large language models as
advanced text encoders, integrating them into T2V models to enhance language
comprehension and guide generation results according to detailed prompts. This
approach enables the model to achieve better semantic alignment, particularly
in response to complex user prompts. Extensive experiments demonstrate the
effectiveness of our method across various metrics. The source code and the
model of ModelGrow will be publicly available.",2024-12-25T18:58:07Z,http://arxiv.org/abs/2412.18966v1,"Zhefan Rao, Liya Ji, Yazhou Xing, Runtao Liu, Zhaoyang Liu, Jiaxin Xie, Ziqiao Peng, Yingqing He, Qifeng Chen"
"Don't Lose Yourself: Boosting Multimodal Recommendation via Reducing
  Node-neighbor Discrepancy in Graph Convolutional Network","The rapid expansion of multimedia contents has led to the emergence of
multimodal recommendation systems. It has attracted increasing attention in
recommendation systems because its full utilization of data from different
modalities alleviates the persistent data sparsity problem. As such, multimodal
recommendation models can learn personalized information about nodes in terms
of visual and textual. To further alleviate the data sparsity problem, some
previous works have introduced graph convolutional networks (GCNs) for
multimodal recommendation systems, to enhance the semantic representation of
users and items by capturing the potential relationships between them. However,
adopting GCNs inevitably introduces the over-smoothing problem, which make
nodes to be too similar. Unfortunately, incorporating multimodal information
will exacerbate this challenge because nodes that are too similar will lose the
personalized information learned through multimodal information. To address
this problem, we propose a novel model that retains the personalized
information of ego nodes during feature aggregation by Reducing Node-neighbor
Discrepancy (RedN^nD). Extensive experiments on three public datasets show that
RedN^nD achieves state-of-the-art performance on accuracy and robustness, with
significant improvements over existing GCN-based multimodal frameworks.",2024-12-25T18:41:36Z,http://arxiv.org/abs/2412.18962v1,"Zheyu Chen, Jinfeng Xu, Haibo Hu"
"RIS-Assisted Aerial Non-Terrestrial Networks: An Intelligent Synergy
  with Deep Reinforcement Learning","Reconfigurable intelligent surface (RIS)-assisted aerial non-terrestrial
networks (NTNs) offer a promising paradigm for enhancing wireless
communications in the era of 6G and beyond. By integrating RIS with aerial
platforms such as unmanned aerial vehicles (UAVs) and high-altitude platforms
(HAPs), these networks can intelligently control signal propagation, extending
coverage, improving capacity, and enhancing link reliability. This article
explores the application of deep reinforcement learning (DRL) as a powerful
tool for optimizing RIS-assisted aerial NTNs. We focus on hybrid proximal
policy optimization (H-PPO), a robust DRL algorithm well-suited for handling
the complex, hybrid action spaces inherent in these networks. Through a case
study of an aerial RIS (ARIS)-aided coordinated multi-point non-orthogonal
multiple access (CoMP-NOMA) network, we demonstrate how H-PPO can effectively
optimize the system and maximize the sum rate while adhering to system
constraints. Finally, we discuss key challenges and promising research
directions for DRL-powered RIS-assisted aerial NTNs, highlighting their
potential to transform next-generation wireless networks.",2024-12-25T18:11:34Z,http://arxiv.org/abs/2412.18957v1,"Muhammad Umer, Muhammad Ahmed Mohsin, Aryan Kaushik, Qurrat-ul-Ain Nadeem, Ali Arshad Nasir, Syed Ali Hassan"
"Leave-One-EquiVariant: Alleviating invariance-related information loss
  in contrastive music representations","Contrastive learning has proven effective in self-supervised musical
representation learning, particularly for Music Information Retrieval (MIR)
tasks. However, reliance on augmentation chains for contrastive view generation
and the resulting learnt invariances pose challenges when different downstream
tasks require sensitivity to certain musical attributes. To address this, we
propose the Leave One EquiVariant (LOEV) framework, which introduces a
flexible, task-adaptive approach compared to previous work by selectively
preserving information about specific augmentations, allowing the model to
maintain task-relevant equivariances. We demonstrate that LOEV alleviates
information loss related to learned invariances, improving performance on
augmentation related tasks and retrieval without sacrificing general
representation quality. Furthermore, we introduce a variant of LOEV, LOEV++,
which builds a disentangled latent space by design in a self-supervised manner,
and enables targeted retrieval based on augmentation related attributes.",2024-12-25T18:06:44Z,http://arxiv.org/abs/2412.18955v1,"Julien Guinot, Elio Quinton, György Fazekas"
"Engineering whispering gallery modes in MoSe$_2$/WS$_2$ double
  heterostructure nanocavities: Towards developing all-TMDC light sources","Transition metal dichalcogenides (TMDCs) have emerged as highly promising
materials for nanophotonics and optoelectronics due to their exceptionally high
refractive indices, strong excitonic photoluminescence (PL) in monolayer
configurations, and the versatility to engineer van der Waals (vdW)
heterostructures. In this work, we exploit the intense excitonic PL of a
MoSe$_2$ monolayer combined with the high refractive index of bulk WS$_2$ to
fabricate microdisk cavities with tunable light emission characteristics. These
microdisks are created from a 50-nm-thick WS$_2$/MoSe$_2$/WS$_2$ double
heterostructure using frictional mechanical scanning probe lithography. The
resulting cavities achieve a 4-10-fold enhancement in excitonic PL from the
MoSe$_2$ monolayer at wavelengths near 800 nm. The excitonic PL peak is
modulated by sharp spectral features, which correspond to whispering gallery
modes (WGMs) supported by the cavity. A microdisk with a diameter of 2.35
$\mu$m demonstrates WGMs with a quality factor of up to 700, significantly
surpassing theoretical predictions and suggesting strong potential for lasing
applications. The spectral positions of the WGMs can be finely tuned by
adjusting the microdisk's diameter and thickness, as confirmed by theoretical
calculations. This approach offers a novel route for developing ultra-compact,
all-TMDC double heterostructure light sources with record-small size.",2024-12-25T17:36:35Z,http://arxiv.org/abs/2412.18953v1,"P. A. Alekseev, I. A. Milekhin, K. A. Gasnikova, I. A. Eliseyev, V. Yu. Davydov, A. A. Bogdanov, V. Kravtsov, A. O. Mikhin, B. R. Borodin, A. G. Milekhin"
"Bridging Interpretability and Robustness Using LIME-Guided Model
  Refinement","This paper explores the intricate relationship between interpretability and
robustness in deep learning models. Despite their remarkable performance across
various tasks, deep learning models often exhibit critical vulnerabilities,
including susceptibility to adversarial attacks, over-reliance on spurious
correlations, and a lack of transparency in their decision-making processes. To
address these limitations, we propose a novel framework that leverages Local
Interpretable Model-Agnostic Explanations (LIME) to systematically enhance
model robustness. By identifying and mitigating the influence of irrelevant or
misleading features, our approach iteratively refines the model, penalizing
reliance on these features during training. Empirical evaluations on multiple
benchmark datasets demonstrate that LIME-guided refinement not only improves
interpretability but also significantly enhances resistance to adversarial
perturbations and generalization to out-of-distribution data.",2024-12-25T17:32:45Z,http://arxiv.org/abs/2412.18952v1,"Navid Nayyem, Abdullah Rakin, Longwei Wang"
"TopoBDA: Towards Bezier Deformable Attention for Road Topology
  Understanding","Understanding road topology is crucial for autonomous driving. This paper
introduces TopoBDA (Topology with Bezier Deformable Attention), a novel
approach that enhances road topology understanding by leveraging Bezier
Deformable Attention (BDA). BDA utilizes Bezier control points to drive the
deformable attention mechanism, significantly improving the detection and
representation of elongated and thin polyline structures, such as lane
centerlines. TopoBDA processes multi-camera 360-degree imagery to generate
Bird's Eye View (BEV) features, which are refined through a transformer decoder
employing BDA. This method enhances computational efficiency while maintaining
high accuracy in centerline prediction. Additionally, TopoBDA incorporates an
instance mask formulation and an auxiliary one-to-many set prediction loss
strategy to further refine centerline detection and improve road topology
understanding. Experimental evaluations on the OpenLane-V2 dataset demonstrate
that TopoBDA outperforms existing methods, achieving state-of-the-art results
in centerline detection and topology reasoning. The integration of multi-modal
data, including lidar and radar, specifically for road topology understanding,
further enhances the model's performance, underscoring its importance in
autonomous driving applications.",2024-12-25T17:31:54Z,http://arxiv.org/abs/2412.18951v1,"Muhammet Esat Kalfaoglu, Halil Ibrahim Ozturk, Ozsel Kilinc, Alptekin Temizel"
"MedHallBench: A New Benchmark for Assessing Hallucination in Medical
  Large Language Models","Medical Large Language Models (MLLMs) have demonstrated potential in
healthcare applications, yet their propensity for hallucinations -- generating
medically implausible or inaccurate information -- presents substantial risks
to patient care. This paper introduces MedHallBench, a comprehensive benchmark
framework for evaluating and mitigating hallucinations in MLLMs. Our
methodology integrates expert-validated medical case scenarios with established
medical databases to create a robust evaluation dataset. The framework employs
a sophisticated measurement system that combines automated ACHMI (Automatic
Caption Hallucination Measurement in Medical Imaging) scoring with rigorous
clinical expert evaluations and utilizes reinforcement learning methods to
achieve automatic annotation. Through an optimized reinforcement learning from
human feedback (RLHF) training pipeline specifically designed for medical
applications, MedHallBench enables thorough evaluation of MLLMs across diverse
clinical contexts while maintaining stringent accuracy standards. We conducted
comparative experiments involving various models, utilizing the benchmark to
establish a baseline for widely adopted large language models (LLMs). Our
findings indicate that ACHMI provides a more nuanced understanding of the
effects of hallucinations compared to traditional metrics, thereby highlighting
its advantages in hallucination assessment. This research establishes a
foundational framework for enhancing MLLMs' reliability in healthcare settings
and presents actionable strategies for addressing the critical challenge of AI
hallucinations in medical applications.",2024-12-25T16:51:29Z,http://arxiv.org/abs/2412.18947v1,"Kaiwen Zuo, Yirui Jiang"
"Constraint-Adaptive Policy Switching for Offline Safe Reinforcement
  Learning","Offline safe reinforcement learning (OSRL) involves learning a
decision-making policy to maximize rewards from a fixed batch of training data
to satisfy pre-defined safety constraints. However, adapting to varying safety
constraints during deployment without retraining remains an under-explored
challenge. To address this challenge, we introduce constraint-adaptive policy
switching (CAPS), a wrapper framework around existing offline RL algorithms.
During training, CAPS uses offline data to learn multiple policies with a
shared representation that optimize different reward and cost trade-offs.
During testing, CAPS switches between those policies by selecting at each state
the policy that maximizes future rewards among those that satisfy the current
cost constraint. Our experiments on 38 tasks from the DSRL benchmark
demonstrate that CAPS consistently outperforms existing methods, establishing a
strong wrapper-based baseline for OSRL. The code is publicly available at
https://github.com/yassineCh/CAPS.",2024-12-25T16:42:27Z,http://arxiv.org/abs/2412.18946v1,"Yassine Chemingui, Aryan Deshwal, Honghao Wei, Alan Fern, Janardhan Rao Doppa"
Amuse: Human-AI Collaborative Songwriting with Multimodal Inspirations,"Songwriting is often driven by multimodal inspirations, such as imagery,
narratives, or existing music, yet songwriters remain unsupported by current
music AI systems in incorporating these multimodal inputs into their creative
processes. We introduce Amuse, a songwriting assistant that transforms
multimodal (image, text, or audio) inputs into chord progressions that can be
seamlessly incorporated into songwriters' creative processes. A key feature of
Amuse is its novel method for generating coherent chords that are relevant to
music keywords in the absence of datasets with paired examples of multimodal
inputs and chords. Specifically, we propose a method that leverages multimodal
large language models (LLMs) to convert multimodal inputs into noisy chord
suggestions and uses a unimodal chord model to filter the suggestions. A user
study with songwriters shows that Amuse effectively supports transforming
multimodal ideas into coherent musical suggestions, enhancing users' agency and
creativity throughout the songwriting process.",2024-12-25T16:23:32Z,http://arxiv.org/abs/2412.18940v1,"Yewon Kim, Sung-Ju Lee, Chris Donahue"
"Label-free SERS Discrimination of Proline from Hydroxylated Proline at
  Single-molecule Level Assisted by a Deep Learning Model","Discriminating the low-abundance hydroxylated proline from hydroxylated
proline is crucial for monitoring diseases and eval-uating therapeutic outcomes
that require single-molecule sensors. While the plasmonic nanopore sensor can
detect the hydrox-ylation with single-molecule sensitivity by surface enhanced
Raman spectroscopy (SERS), it suffers from intrinsic fluctuations of
single-molecule signals as well as strong interference from citrates. Here, we
used the occurrence frequency histogram of the single-molecule SERS peaks to
extract overall dataset spectral features, overcome the signal fluctuations and
investigate the citrate-replaced plasmonic nanopore sensors for clean and
distinguishable signals of proline and hydroxylated proline. By ligand exchange
of the citrates by analyte molecules, the representative peaks of citrates
decreased with incubation time, prov-ing occupation of the plasmonic hot spot
by the analytes. As a result, the discrimination of the single-molecule SERS
signals of proline and hydroxylated proline was possible with the convolutional
neural network model with 96.6% accuracy.",2024-12-25T15:46:52Z,http://arxiv.org/abs/2412.18935v1,"Yingqi Zhao, Kuo Zhan, Pei-Lin Xin, Zuyan Chen, Shuai Li, Francesco De Angelis, Jianan Huang"
Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference,"Due to the high resource demands of Large Language Models (LLMs), achieving
widespread deployment on consumer-grade devices presents significant
challenges. Typically, personal or consumer-grade devices, including servers
configured prior to the era of large-scale models, generally have relatively
weak GPUs and relatively strong CPUs. However, most current methods primarily
depend on GPUs for computation. Therefore, we propose Dovetail, an approach
that deploys the draft model on the GPU to generate draft tokens while allowing
the target model to perform parallel verification on the CPU, thereby improving
the utilization of all available hardware resources and occupying less
inter-device communication bandwidth. Accordingly, we have redesigned the draft
model to better align with heterogeneous hardware characteristics. To this end,
we implemented several optimizations: reducing the number of draft tokens to
mitigate latency in parallel verification, increasing the depth of the draft
model to enhance its predictive capacity, and introducing DGF (Dynamic Gating
Fusion) to improve the integration of features and token embeddings. In the
HumanEval benchmark, Dovetail achieved an inference speed of 5.86 tokens per
second for LLaMA2-Chat-7B using 3GB of VRAM, representing an approximately
2.77x improvement over CPU-only inference. Furthermore, the inference speed was
increased to 8 tokens per second when utilizing 7GB of VRAM.",2024-12-25T15:45:18Z,http://arxiv.org/abs/2412.18934v1,"Libo Zhang, Zhaoning Zhang, Baizhou Xu, Songzhu Mei, Dongsheng Li"
TINQ: Temporal Inconsistency Guided Blind Video Quality Assessment,"Blind video quality assessment (BVQA) has been actively researched for
user-generated content (UGC) videos. Recently, super-resolution (SR) techniques
have been widely applied in UGC. Therefore, an effective BVQA method for both
UGC and SR scenarios is essential. Temporal inconsistency, referring to
irregularities between consecutive frames, is relevant to video quality.
Current BVQA approaches typically model temporal relationships in UGC videos
using statistics of motion information, but inconsistencies remain unexplored.
Additionally, different from temporal inconsistency in UGC videos, such
inconsistency in SR videos is amplified due to upscaling algorithms. In this
paper, we introduce the Temporal Inconsistency Guided Blind Video Quality
Assessment (TINQ) metric, demonstrating that exploring temporal inconsistency
is crucial for effective BVQA. Since temporal inconsistencies vary between UGC
and SR videos, they are calculated in different ways. Based on this, a spatial
module highlights inconsistent areas across consecutive frames at coarse and
fine granularities. In addition, a temporal module aggregates features over
time in two stages. The first stage employs a visual memory capacity block to
adaptively segment the time dimension based on estimated complexity, while the
second stage focuses on selecting key features. The stages work together
through Consistency-aware Fusion Units to regress cross-time-scale video
quality. Extensive experiments on UGC and SR video quality datasets show that
our method outperforms existing state-of-the-art BVQA methods. Code is
available at https://github.com/Lighting-YXLI/TINQ.",2024-12-25T15:43:41Z,http://arxiv.org/abs/2412.18933v1,"Yixiao Li, Xiaoyuan Yang, Weide Liu, Xin Jin, Xu Jia, Yukun Lai, Haotao Liu, Paul L Rosin, Wei Zhou"
"Malware Classification using a Hybrid Hidden Markov Model-Convolutional
  Neural Network","The proliferation of malware variants poses a significant challenges to
traditional malware detection approaches, such as signature-based methods,
necessitating the development of advanced machine learning techniques. In this
research, we present a novel approach based on a hybrid architecture combining
features extracted using a Hidden Markov Model (HMM), with a Convolutional
Neural Network (CNN) then used for malware classification. Inspired by the
strong results in previous work using an HMM-Random Forest model, we propose
integrating HMMs, which serve to capture sequential patterns in opcode
sequences, with CNNs, which are adept at extracting hierarchical features. We
demonstrate the effectiveness of our approach on the popular Malicia dataset,
and we obtain superior performance, as compared to other machine learning
methods -- our results surpass the aforementioned HMM-Random Forest model. Our
findings underscore the potential of hybrid HMM-CNN architectures in bolstering
malware classification capabilities, offering several promising avenues for
further research in the field of cybersecurity.",2024-12-25T15:34:57Z,http://arxiv.org/abs/2412.18932v1,"Ritik Mehta, Olha Jureckova, Mark Stamp"
"Graph Cut-guided Maximal Coding Rate Reduction for Learning Image
  Embedding and Clustering","In the era of pre-trained models, image clustering task is usually addressed
by two relevant stages: a) to produce features from pre-trained vision models;
and b) to find clusters from the pre-trained features. However, these two
stages are often considered separately or learned by different paradigms,
leading to suboptimal clustering performance. In this paper, we propose a
unified framework, termed graph Cut-guided Maximal Coding Rate Reduction
(CgMCR$^2$), for jointly learning the structured embeddings and the clustering.
To be specific, we attempt to integrate an efficient clustering module into the
principled framework for learning structured representation, in which the
clustering module is used to provide partition information to guide the
cluster-wise compression and the learned embeddings is aligned to desired
geometric structures in turn to help for yielding more accurate partitions. We
conduct extensive experiments on both standard and out-of-domain image datasets
and experimental results validate the effectiveness of our approach.",2024-12-25T15:20:54Z,http://arxiv.org/abs/2412.18930v1,"W. He, Z. Huang, X. Meng, X. Qi, R. Xiao, C. -G. Li"
"UNIC-Adapter: Unified Image-instruction Adapter with Multi-modal
  Transformer for Image Generation","Recently, text-to-image generation models have achieved remarkable
advancements, particularly with diffusion models facilitating high-quality
image synthesis from textual descriptions. However, these models often struggle
with achieving precise control over pixel-level layouts, object appearances,
and global styles when using text prompts alone. To mitigate this issue,
previous works introduce conditional images as auxiliary inputs for image
generation, enhancing control but typically necessitating specialized models
tailored to different types of reference inputs. In this paper, we explore a
new approach to unify controllable generation within a single framework.
Specifically, we propose the unified image-instruction adapter (UNIC-Adapter)
built on the Multi-Modal-Diffusion Transformer architecture, to enable flexible
and controllable generation across diverse conditions without the need for
multiple specialized models. Our UNIC-Adapter effectively extracts multi-modal
instruction information by incorporating both conditional images and task
instructions, injecting this information into the image generation process
through a cross-attention mechanism enhanced by Rotary Position Embedding.
Experimental results across a variety of tasks, including pixel-level spatial
control, subject-driven image generation, and style-image-based image
synthesis, demonstrate the effectiveness of our UNIC-Adapter in unified
controllable image generation.",2024-12-25T15:19:02Z,http://arxiv.org/abs/2412.18928v1,"Lunhao Duan, Shanshan Zhao, Wenjun Yan, Yinglun Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Mingming Gong, Gui-Song Xia"
Exemplar-condensed Federated Class-incremental Learning,"We propose Exemplar-Condensed federated class-incremental learning (ECoral)
to distil the training characteristics of real images from streaming data into
informative rehearsal exemplars. The proposed method eliminates the limitations
of exemplar selection in replay-based approaches for mitigating catastrophic
forgetting in federated continual learning (FCL). The limitations particularly
related to the heterogeneity of information density of each summarized data.
Our approach maintains the consistency of training gradients and the
relationship to past tasks for the summarized exemplars to represent the
streaming data compared to the original images effectively. Additionally, our
approach reduces the information-level heterogeneity of the summarized data by
inter-client sharing of the disentanglement generative model. Extensive
experiments show that our ECoral outperforms several state-of-the-art methods
and can be seamlessly integrated with many existing approaches to enhance
performance.",2024-12-25T15:13:40Z,http://arxiv.org/abs/2412.18926v1,"Rui Sun, Yumin Zhang, Varun Ojha, Tejal Shah, Haoran Duan, Bo Wei, Rajiv Ranjan"
"HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs","The breakthrough of OpenAI o1 highlights the potential of enhancing reasoning
to improve LLM. Yet, most research in reasoning has focused on mathematical
tasks, leaving domains like medicine underexplored. The medical domain, though
distinct from mathematics, also demands robust reasoning to provide reliable
answers, given the high standards of healthcare. However, verifying medical
reasoning is challenging, unlike those in mathematics. To address this, we
propose verifiable medical problems with a medical verifier to check the
correctness of model outputs. This verifiable nature enables advancements in
medical reasoning through a two-stage approach: (1) using the verifier to guide
the search for a complex reasoning trajectory for fine-tuning LLMs, (2)
applying reinforcement learning (RL) with verifier-based rewards to enhance
complex reasoning further. Finally, we introduce HuatuoGPT-o1, a medical LLM
capable of complex reasoning, which outperforms general and medical-specific
baselines using only 40K verifiable problems. Experiments show complex
reasoning improves medical problem-solving and benefits more from RL. We hope
our approach inspires advancements in reasoning across medical and other
specialized domains.",2024-12-25T15:12:34Z,http://arxiv.org/abs/2412.18925v1,"Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, Benyou Wang"
"Generative Face Parsing Map Guided 3D Face Reconstruction Under Occluded
  Scenes","Over the past few years, single-view 3D face reconstruction methods can
produce beautiful 3D models. Nevertheless,the input of these works is
unobstructed faces.We describe a system designed to reconstruct convincing face
texture in the case of occlusion.Motivated by parsing facial features,we
propose a complete face parsing map generation method guided by landmarks.We
estimate the 2D face structure of the reasonable position of the occlusion
area,which is used for the construction of 3D texture.An excellent
anti-occlusion face reconstruction method should ensure the authenticity of the
output,including the topological structure between the eyes,nose, and mouth. We
extensively tested our method and its components, qualitatively demonstrating
the rationality of our estimated facial structure. We conduct extensive
experiments on general 3D face reconstruction tasks as concrete examples to
demonstrate the method's superior regulation ability over existing methods
often break down.We further provide numerous quantitative examples showing that
our method advances both the quality and the robustness of 3D face
reconstruction under occlusion scenes.",2024-12-25T14:49:41Z,http://arxiv.org/abs/2412.18920v1,"Dapeng Zhao, Yue Qi"
"An Attentive Dual-Encoder Framework Leveraging Multimodal Visual and
  Semantic Information for Automatic OSAHS Diagnosis","Obstructive sleep apnea-hypopnea syndrome (OSAHS) is a common sleep disorder
caused by upper airway blockage, leading to oxygen deprivation and disrupted
sleep. Traditional diagnosis using polysomnography (PSG) is expensive,
time-consuming, and uncomfortable. Existing deep learning methods using facial
image analysis lack accuracy due to poor facial feature capture and limited
sample sizes. To address this, we propose a multimodal dual encoder model that
integrates visual and language inputs for automated OSAHS diagnosis. The model
balances data using randomOverSampler, extracts key facial features with
attention grids, and converts physiological data into meaningful text.
Cross-attention combines image and text data for better feature extraction, and
ordered regression loss ensures stable learning. Our approach improves
diagnostic efficiency and accuracy, achieving 91.3% top-1 accuracy in a
four-class severity classification task, demonstrating state-of-the-art
performance. Code will be released upon acceptance.",2024-12-25T14:42:17Z,http://arxiv.org/abs/2412.18919v1,"Yingchen Wei, Xihe Qiu, Xiaoyu Tan, Jingjing Huang, Wei Chu, Yinghui Xu, Yuan Qi"
"BCR-Net: Boundary-Category Refinement Network for Weakly Semi-Supervised
  X-Ray Prohibited Item Detection with Points","Automatic prohibited item detection in X-ray images is crucial for public
safety. However, most existing detection methods either rely on expensive box
annotations to achieve high performance or use weak annotations but suffer from
limited accuracy. To balance annotation cost and detection performance, we
study Weakly Semi-Supervised X-ray Prohibited Item Detection with Points
(WSSPID-P) and propose a novel \textbf{B}oundary-\textbf{C}ategory
\textbf{R}efinement \textbf{Net}work (\textbf{BCR-Net}) that requires only a
few box annotations and a large number of point annotations. BCR-Net is built
based on Group R-CNN and introduces a new Boundary Refinement (BR) module and a
new Category Refinement (CR) module. The BR module develops a dual attention
mechanism to focus on both the boundaries and salient features of prohibited
items. Meanwhile, the CR module incorporates contrastive branches into the
heads of RPN and ROI by introducing a scale- and rotation-aware contrastive
loss, enhancing intra-class consistency and inter-class separability in the
feature space. Based on the above designs, BCR-Net effectively addresses the
closely related problems of imprecise localization and inaccurate
classification. Experimental results on public X-ray datasets show the
effectiveness of BCR-Net, achieving significant performance improvements to
state-of-the-art methods under limited annotations.",2024-12-25T14:37:05Z,http://arxiv.org/abs/2412.18918v1,Sanjoeng Wong
"Open-Vocabulary Panoptic Segmentation Using BERT Pre-Training of
  Vision-Language Multiway Transformer Model","Open-vocabulary panoptic segmentation remains a challenging problem. One of
the biggest difficulties lies in training models to generalize to an unlimited
number of classes using limited categorized training data. Recent popular
methods involve large-scale vision-language pre-trained foundation models, such
as CLIP. In this paper, we propose OMTSeg for open-vocabulary segmentation
using another large-scale vision-language pre-trained model called BEiT-3 and
leveraging the cross-modal attention between visual and linguistic features in
BEiT-3 to achieve better performance. Experiments result demonstrates that
OMTSeg performs favorably against state-of-the-art models.",2024-12-25T14:31:00Z,http://arxiv.org/abs/2412.18917v1,"Yi-Chia Chen, Wei-Hua Li, Chu-Song Chen"
"Optimization-based model order reduction of fluid-structure interaction
  problems","We introduce optimization-based full-order and reduced-order formulations of
fluid structure interaction problems. We study the flow of an incompressible
Newtonian fluid which interacts with an elastic body: we consider an arbitrary
Lagrangian Eulerian formulation of the fluid problem and a fully Lagrangian
formulation of the solid problem; we rely on a finite element discretization of
both fluid and solid equations. The distinctive feature of our approach is an
implicit coupling of fluid and structural problems that relies on the solution
to a constrained optimization problem with equality constraints. We discuss the
application of projection-based model reduction to both fluid and solid
subproblems: we rely on Galerkin projection for the solid equations and on
least-square Petrov-Galerkin projection for the fluid equations. Numerical
results for three model problems illustrate the many features of the
formulation.",2024-12-25T14:26:54Z,http://arxiv.org/abs/2412.18916v1,"Tommaso Taddei, Xuejun Xu, Lei Zhang"
Robust Target Speaker Direction of Arrival Estimation,"In multi-speaker environments the direction of arrival (DOA) of a target
speaker is key for improving speech clarity and extracting target speaker's
voice. However, traditional DOA estimation methods often struggle in the
presence of noise, reverberation, and particularly when competing speakers are
present. To address these challenges, we propose RTS-DOA, a robust real-time
DOA estimation system. This system innovatively uses the registered speech of
the target speaker as a reference and leverages full-band and sub-band spectral
information from a microphone array to estimate the DOA of the target speaker's
voice. Specifically, the system comprises a speech enhancement module for
initially improving speech quality, a spatial module for learning spatial
information, and a speaker module for extracting voiceprint features.
Experimental results on the LibriSpeech dataset demonstrate that our RTS-DOA
system effectively tackles multi-speaker scenarios and established new optimal
benchmarks.",2024-12-25T14:04:21Z,http://arxiv.org/abs/2412.18913v1,"Zixuan Li, Shulin He, Xueliang Zhang"
Accelerating Diffusion Transformers with Dual Feature Caching,"Diffusion Transformers (DiT) have become the dominant methods in image and
video generation yet still suffer substantial computational costs. As an
effective approach for DiT acceleration, feature caching methods are designed
to cache the features of DiT in previous timesteps and reuse them in the next
timesteps, allowing us to skip the computation in the next timesteps. However,
on the one hand, aggressively reusing all the features cached in previous
timesteps leads to a severe drop in generation quality. On the other hand,
conservatively caching only the features in the redundant layers or tokens but
still computing the important ones successfully preserves the generation
quality but results in reductions in acceleration ratios. Observing such a
tradeoff between generation quality and acceleration performance, this paper
begins by quantitatively studying the accumulated error from cached features.
Surprisingly, we find that aggressive caching does not introduce significantly
more caching errors in the caching step, and the conservative feature caching
can fix the error introduced by aggressive caching. Thereby, we propose a dual
caching strategy that adopts aggressive and conservative caching iteratively,
leading to significant acceleration and high generation quality at the same
time. Besides, we further introduce a V-caching strategy for token-wise
conservative caching, which is compatible with flash attention and requires no
training and calibration data.
  Our codes have been released in Github: \textbf{Code:
\href{https://github.com/Shenyi-Z/DuCa}{\texttt{\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",2024-12-25T14:00:14Z,http://arxiv.org/abs/2412.18911v1,"Chang Zou, Evelyn Zhang, Runlin Guo, Haohang Xu, Conghui He, Xuming Hu, Linfeng Zhang"
"Research Experiment on Multi-Model Comparison for Chinese Text
  Classification Tasks","With the explosive growth of Chinese text data and advancements in natural
language processing technologies, Chinese text classification has become one of
the key techniques in fields such as information retrieval and sentiment
analysis, attracting increasing attention. This paper conducts a comparative
study on three deep learning models:TextCNN, TextRNN, and FastText.specifically
for Chinese text classification tasks. By conducting experiments on the
THUCNews dataset, the performance of these models is evaluated, and their
applicability in different scenarios is discussed.",2024-12-25T13:54:40Z,http://arxiv.org/abs/2412.18908v1,JiaCheng Li
"EC-Diffuser: Multi-Object Manipulation via Entity-Centric Behavior
  Generation","Object manipulation is a common component of everyday tasks, but learning to
manipulate objects from high-dimensional observations presents significant
challenges. These challenges are heightened in multi-object environments due to
the combinatorial complexity of the state space as well as of the desired
behaviors. While recent approaches have utilized large-scale offline data to
train models from pixel observations, achieving performance gains through
scaling, these methods struggle with compositional generalization in unseen
object configurations with constrained network and dataset sizes. To address
these issues, we propose a novel behavioral cloning (BC) approach that
leverages object-centric representations and an entity-centric Transformer with
diffusion-based optimization, enabling efficient learning from offline image
data. Our method first decomposes observations into an object-centric
representation, which is then processed by our entity-centric Transformer that
computes attention at the object level, simultaneously predicting object
dynamics and the agent's actions. Combined with the ability of diffusion models
to capture multi-modal behavior distributions, this results in substantial
performance improvements in multi-object tasks and, more importantly, enables
compositional generalization. We present BC agents capable of zero-shot
generalization to tasks with novel compositions of objects and goals, including
larger numbers of objects than seen during training. We provide video rollouts
on our webpage: https://sites.google.com/view/ec-diffuser.",2024-12-25T13:50:15Z,http://arxiv.org/abs/2412.18907v1,"Carl Qi, Dan Haramati, Tal Daniel, Aviv Tamar, Amy Zhang"
"FedCFA: Alleviating Simpson's Paradox in Model Aggregation with
  Counterfactual Federated Learning","Federated learning (FL) is a promising technology for data privacy and
distributed optimization, but it suffers from data imbalance and heterogeneity
among clients. Existing FL methods try to solve the problems by aligning client
with server model or by correcting client model with control variables. These
methods excel on IID and general Non-IID data but perform mediocrely in
Simpson's Paradox scenarios. Simpson's Paradox refers to the phenomenon that
the trend observed on the global dataset disappears or reverses on a subset,
which may lead to the fact that global model obtained through aggregation in FL
does not accurately reflect the distribution of global data. Thus, we propose
FedCFA, a novel FL framework employing counterfactual learning to generate
counterfactual samples by replacing local data critical factors with global
average data, aligning local data distributions with the global and mitigating
Simpson's Paradox effects. In addition, to improve the quality of
counterfactual samples, we introduce factor decorrelation (FDC) loss to reduce
the correlation among features and thus improve the independence of extracted
factors. We conduct extensive experiments on six datasets and verify that our
method outperforms other FL methods in terms of efficiency and global model
accuracy under limited communication rounds.",2024-12-25T13:35:54Z,http://arxiv.org/abs/2412.18904v1,"Zhonghua Jiang, Jimin Xu, Shengyu Zhang, Tao Shen, Jiwei Li, Kun Kuang, Haibin Cai, Fei Wu"
"Stationary Processes, Wiener-Granger Causality, and Matrix Spectral
  Factorization","Granger causality has become an indispensable tool for analyzing causal
relationships between time series. In this paper, we provide a detailed
overview of its mathematical foundations, trace its historical development, and
explore how recent computational advancements can enhance its application in
various fields. We will not hesitate to present the proofs in full if they are
simple and transparent. For more complex theorems on which we rely, we will
provide supporting citations. We also discuss potential future directions for
the method, particularly in the context of largescale data analysis.",2024-12-25T13:29:39Z,http://arxiv.org/abs/2412.18901v1,Lasha Ephremidze
"Observation of on- and off-resonant interaction between a solid-state
  spin qubit and a superconducting resonator","Hybrid systems consisting of multiple materials with distinct physical
properties and tunable interactions provide a promising route for fulfilling
transformative quantum innovations. Solid-state spin qubits and superconducting
circuits stand out as leading candidates in this context due to their
complementary device performance and quantum mechanical properties. Here, we
report experimental integration of a single nitrogen-vacancy (NV) spin qubit
and an on-chip superconducting resonator for realizing multimodal quantum
applications. Specifically, we have observed superconductivity enhanced NV spin
relaxation, which shows a similar Hebel-Slichter peak feature around the phase
transition point. In the coherent interaction regime, we show that the
superconducting resonator mode is capable of exciting NV Rabi oscillations.
Taking advantage of scanning NV magnetometry, we further visualized microscopic
electromagnetic behaviors of the superconducting resonator, revealing the
formation and evolution of superconducting vortices at the nanoscale. Our
results highlight the potential of harnessing NV centers and superconducting
circuits for designing hybrid systems to advance the burgeoning quantum
revolution. The current study will also open a new pathway to test and evaluate
miniaturized superconducting electronics for their future design and
performance improvements.",2024-12-25T13:06:25Z,http://arxiv.org/abs/2412.18896v1,"Senlei Li, Shane P. Kelly, Jingcheng Zhou, Hanyi Lu, Yaroslav Tserkovnyak, Hailong Wang, Chunhui Rita Du"
"Effects of chiral symmetry restoration on dilepton production in heavy
  ion collisions","Because of their weak interactions with the strongly interacting matter
produced in relativistic heavy-ion collisions, dileptons provide an ideal probe
of the early dynamics of these collisions. Here, we study dilepton production
using a partonic transport model that is based on an extended
Nambu-Jona-Lasinio (NJL) model. In this model, the in-medium quark masses
decrease with increasing temperature as a result of the restoration of chiral
symmetry. We find that the extracted temperature from dileptons of intermediate
masses agrees well with the temperature of the partonic matter, suggesting that
dilepton production can be used as a thermometer for the produced partonic
matter. Our results also indicate that the extracted in-medium quark masses
decrease with increasing dilepton temperature, implying that dilepton
production can further serve as a probe of chiral symmetry restoration in high
energy heavy-ion collisions.",2024-12-25T12:57:25Z,http://arxiv.org/abs/2412.18895v1,"Wen-Hao Zhou, Che Ming Ko, Kai-Jia Sun"
"Comprehensive Study on Lumbar Disc Segmentation Techniques Using MRI
  Data","Lumbar disk segmentation is essential for diagnosing and curing spinal
disorders by enabling precise detection of disk boundaries in medical imaging.
The advent of deep learning has resulted in the development of many
segmentation methods, offering differing levels of accuracy and effectiveness.
This study assesses the effectiveness of several sophisticated deep learning
architectures, including ResUnext, Ef3 Net, UNet, and TransUNet, for lumbar
disk segmentation, highlighting key metrics like as Pixel Accuracy, Mean
Intersection over Union (Mean IoU), and Dice Coefficient. The findings indicate
that ResUnext achieved the highest segmentation accuracy, with a Pixel Accuracy
of 0.9492 and a Dice Coefficient of 0.8425, with TransUNet following closely
after. Filtering techniques somewhat enhanced the performance of most models,
particularly Dense UNet, improving stability and segmentation quality. The
findings underscore the efficacy of these models in lumbar disk segmentation
and highlight potential areas for improvement.",2024-12-25T12:54:52Z,http://arxiv.org/abs/2412.18894v1,"Serkan Salturk, Irem Sayin, Ibrahim Cem Balci, Taha Emre Pamukcu, Zafer Soydan, Huseyin Uvet"
"Emergent Intermediate Phase in the $J_1$-$J_2$ XY model from Tensor
  Network Approaches","We investigate the finite-temperature phase diagram of the classical
$J_1$-$J_2$ XY model on a square lattice using a tensor network approach
designed for frustrated spin systems. This model, characterized by competing
nearest-neighbor and next-to-nearest-neighbor interactions, exhibits a complex
interplay between $U(1)$ and $Z_2$ symmetries. Our study reveals an emergent
intermediate phase around $J_2/J_1 \sim 0.505$, which is characterized by a
$Z_2$ long-range stripe order without phase coherence in the XY spins. The
intermediate phase features two well-separated phase transitions: a
higher-temperature Ising transition and a lower-temperature
Berezinskii-Kosterlitz-Thouless transition. The relative separation between
these transitions is significantly larger than previously reported, enabling a
clearer investigation of their distinct thermodynamic properties. For
$0.5&lt;J_2/J_1 &lt; 0.501$, two transitions merge into a single first-order phase
transition, a phenomenon that cannot be explained solely by mapping to the
Ising-XY model. As $J_2/J_1 \to \infty$, the transition evolves continuously
into the BKT universality class. These findings advance the understanding of
the mechanisms driving phase transitions in frustrated spin systems and suggest
potential experimental realizations in platforms such as ultracold atoms,
Josephson junction arrays, and optical lattices.",2024-12-25T12:51:07Z,http://arxiv.org/abs/2412.18892v1,"Feng-Feng Song, Hanggai Nuomin, Naoki Kawashima"
"CoEvo: Continual Evolution of Symbolic Solutions Using Large Language
  Models","Large Language Models (LLMs) have emerged as transformative tools in
artificial intelligence, capable of processing and understanding extensive
human knowledge to enhance problem-solving across various domains. This paper
explores the potential of LLMs to drive the discovery of symbolic solutions
within scientific and engineering disciplines, where such solutions are crucial
for advancing theoretical and practical applications. We propose a novel
framework that utilizes LLMs in an evolutionary search methodology, augmented
by a dynamic knowledge library that integrates and refines insights in an
\textit{open-ended manner}. This approach aims to tackle the dual challenges of
efficiently navigating complex symbolic representation spaces and leveraging
both existing and newly generated knowledge to foster open-ended innovation. By
enabling LLMs to interact with and expand upon a knowledge library, we
facilitate the continuous generation of novel solutions in diverse forms such
as language, code, and mathematical expressions. Our experimental results
demonstrate that this method not only enhances the efficiency of searching for
symbolic solutions but also supports the ongoing discovery process, akin to
human scientific endeavors. This study represents a first effort in
conceptualizing the search for symbolic solutions as a lifelong, iterative
process, marking a significant step towards harnessing AI in the perpetual
pursuit of scientific and engineering breakthroughs. We have open-sourced our
code and data, please visit \url{https://github.com/pgg3/CoEvo} for more
information.",2024-12-25T12:27:27Z,http://arxiv.org/abs/2412.18890v1,"Ping Guo, Qingfu Zhang, Xi Lin"
"Adversarial Training for Graph Neural Networks via Graph Subspace Energy
  Optimization","Despite impressive capability in learning over graph-structured data, graph
neural networks (GNN) suffer from adversarial topology perturbation in both
training and inference phases. While adversarial training has demonstrated
remarkable effectiveness in image classification tasks, its suitability for GNN
models has been doubted until a recent advance that shifts the focus from
transductive to inductive learning. Still, GNN robustness in the inductive
setting is under-explored, and it calls for deeper understanding of GNN
adversarial training. To this end, we propose a new concept of graph subspace
energy (GSE) -- a generalization of graph energy that measures graph stability
-- of the adjacency matrix, as an indicator of GNN robustness against topology
perturbations. To further demonstrate the effectiveness of such concept, we
propose an adversarial training method with the perturbed graphs generated by
maximizing the GSE regularization term, referred to as AT-GSE. To deal with the
local and global topology perturbations raised respectively by LRBCD and PRBCD,
we employ randomized SVD (RndSVD) and Nystrom low-rank approximation to favor
the different aspects of the GSE terms. An extensive set of experiments shows
that AT-GSE outperforms consistently the state-of-the-art GNN adversarial
training methods over different homophily and heterophily datasets in terms of
adversarial accuracy, whilst more surprisingly achieving a superior clean
accuracy on non-perturbed graphs.",2024-12-25T12:04:18Z,http://arxiv.org/abs/2412.18886v1,"Ganlin Liu, Ziling Liang, Xiaowei Huang, Xinping Yi, Shi Jin"
"HV-BEV: Decoupling Horizontal and Vertical Feature Sampling for
  Multi-View 3D Object Detection","The application of vision-based multi-view environmental perception system
has been increasingly recognized in autonomous driving technology, especially
the BEV-based models. Current state-of-the-art solutions primarily encode image
features from each camera view into the BEV space through explicit or implicit
depth prediction. However, these methods often focus on improving the accuracy
of projecting 2D features into corresponding depth regions, while overlooking
the highly structured information of real-world objects and the varying height
distributions of objects across different scenes. In this work, we propose
HV-BEV, a novel approach that decouples feature sampling in the BEV grid
queries paradigm into horizontal feature aggregation and vertical adaptive
height-aware reference point sampling, aiming to improve both the aggregation
of objects' complete information and generalization to diverse road
environments. Specifically, we construct a learnable graph structure in the
horizontal plane aligned with the ground for 3D reference points, reinforcing
the association of the same instance across different BEV grids, especially
when the instance spans multiple image views around the vehicle. Additionally,
instead of relying on uniform sampling within a fixed height range, we
introduce a height-aware module that incorporates historical information,
enabling the reference points to adaptively focus on the varying heights at
which objects appear in different scenes. Extensive experiments validate the
effectiveness of our proposed method, demonstrating its superior performance
over the baseline across the nuScenes dataset. Moreover, our best-performing
model achieves a remarkable 50.5% mAP and 59.8% NDS on the nuScenes testing
set.",2024-12-25T11:49:14Z,http://arxiv.org/abs/2412.18884v1,"Di Wu, Feng Yang, Benlian Xu, Pan Liao, Wenhui Zhao, Dingwen Zhang"
MotionMap: Representing Multimodality in Human Pose Forecasting,"Human pose forecasting is inherently multimodal since multiple futures exist
for an observed pose sequence. However, evaluating multimodality is challenging
since the task is ill-posed. Therefore, we first propose an alternative
paradigm to make the task well-posed. Next, while state-of-the-art methods
predict multimodality, this requires oversampling a large volume of
predictions. This raises key questions: (1) Can we capture multimodality by
efficiently sampling a smaller number of predictions? (2) Subsequently, which
of the predicted futures is more likely for an observed pose sequence? We
address these questions with MotionMap, a simple yet effective heatmap based
representation for multimodality. We extend heatmaps to represent a spatial
distribution over the space of all possible motions, where different local
maxima correspond to different forecasts for a given observation. MotionMap can
capture a variable number of modes per observation and provide confidence
measures for different modes. Further, MotionMap allows us to introduce the
notion of uncertainty and controllability over the forecasted pose sequence.
Finally, MotionMap captures rare modes that are non-trivial to evaluate yet
critical for safety. We support our claims through multiple qualitative and
quantitative experiments using popular 3D human pose datasets: Human3.6M and
AMASS, highlighting the strengths and limitations of our proposed method.
Project Page: https://www.epfl.ch/labs/vita/research/prediction/motionmap/",2024-12-25T11:47:26Z,http://arxiv.org/abs/2412.18883v1,"Reyhaneh Hosseininejad, Megh Shukla, Saeed Saadatnejad, Mathieu Salzmann, Alexandre Alahi"
"Towards Compatible Semantic Communication: A Perspective on Digital
  Coding and Modulation","Semantic communication (SC) is emerging as a pivotal innovation within the 6G
framework, aimed at enabling more intelligent transmission. This development
has led to numerous studies focused on designing advanced systems through
powerful deep learning techniques. Nevertheless, many of these approaches
envision an analog transmission manner by formulating the transmitted signals
as continuous-valued semantic representation vectors, limiting their
compatibility with existing digital systems. To enhance compatibility, it is
essential to explore digitized SC systems. This article systematically
identifies two promising paradigms for designing digital SC: probabilistic and
deterministic approaches, according to the modulation strategies. For both, we
first provide a comprehensive analysis of the methodologies. Then, we put
forward the principles of designing digital SC systems with a specific focus on
informativeness and robustness of semantic representations to enhance
performance, along with constellation design. Additionally, we present a case
study to demonstrate the effectiveness of these methods. Moreover, this article
also explores the intrinsic advantages and opportunities provided by digital SC
systems, and then outlines several potential research directions for future
investigation.",2024-12-25T11:19:40Z,http://arxiv.org/abs/2412.18876v1,"Guangyi Zhang, Kequan Zhou, Yunlong Cai, Qiyu Hu, Guanding Yu"
IUST_PersonReId: A New Domain in Person Re-Identification Datasets,"Person re-identification (ReID) models often struggle to generalize across
diverse cultural contexts, particularly in Islamic regions like Iran, where
modest clothing styles are prevalent. Existing datasets predominantly feature
Western and East Asian fashion, limiting their applicability in these settings.
To address this gap, we introduce IUST_PersonReId, a dataset designed to
reflect the unique challenges of ReID in new cultural environments, emphasizing
modest attire and diverse scenarios from Iran, including markets, campuses, and
mosques. Experiments on IUST_PersonReId with state-of-the-art models, such as
Solider and CLIP-ReID, reveal significant performance drops compared to
benchmarks like Market1501 and MSMT17, highlighting the challenges posed by
occlusion and limited distinctive features. Sequence-based evaluations show
improvements by leveraging temporal context, emphasizing the dataset's
potential for advancing culturally sensitive and robust ReID systems.
IUST_PersonReId offers a critical resource for addressing fairness and bias in
ReID research globally. The dataset is publicly available at
https://computervisioniust.github.io/IUST_PersonReId/.",2024-12-25T11:17:43Z,http://arxiv.org/abs/2412.18874v1,"Alireza Sedighi Moghaddam, Fatemeh Anvari, Mohammadjavad Mirshekari Haghighi, Mohammadali Fakhari, Mohammad Reza Mohammadi"
Cross-PCR: A Robust Cross-Source Point Cloud Registration Framework,"Due to the density inconsistency and distribution difference between
cross-source point clouds, previous methods fail in cross-source point cloud
registration. We propose a density-robust feature extraction and matching
scheme to achieve robust and accurate cross-source registration. To address the
density inconsistency between cross-source data, we introduce a density-robust
encoder for extracting density-robust features. To tackle the issue of
challenging feature matching and few correct correspondences, we adopt a
loose-to-strict matching pipeline with a ``loose generation, strict selection''
idea. Under it, we employ a one-to-many strategy to loosely generate initial
correspondences. Subsequently, high-quality correspondences are strictly
selected to achieve robust registration through sparse matching and dense
matching. On the challenging Kinect-LiDAR scene in the cross-source 3DCSR
dataset, our method improves feature matching recall by 63.5 percentage points
(pp) and registration recall by 57.6 pp. It also achieves the best performance
on 3DMatch, while maintaining robustness under diverse downsampling densities.",2024-12-25T11:14:59Z,http://arxiv.org/abs/2412.18873v1,"Guiyu Zhao, Zhentao Guo, Zewen Du, Hongbin Ma"
Proton Flux Measurement from Neutron Monitor Data Using Neural Networks,"Accurate measurements of cosmic ray proton flux are crucial for studying the
modulation processes of cosmic rays during the solar activity cycle. We present
a proton flux measurement method based on ground-based neutron monitor (NM)
data and machine learning techniques. After preprocessing the NM data, we use a
convolutional neural network (CNN) model to simulate the relationship between
the NM observations and proton flux measured by the Alpha Magnetic Spectrometer
(AMS-02). We obtain daily proton flux data ranging from 1GV to 100GV for the
period from 2011 to 2024, showing that the measured values are in good
agreement with the observed ones. In particular, we provide daily proton flux
measurements for periods when AMS-02 data are unavailable due to operational
reasons. We also perform wavelet analyses on the continuous proton flux data to
investigate the relationship between proton flux and solar activity variations,
particularly during late 2014 when AMS-02 measurements were missing.",2024-12-25T11:14:13Z,http://arxiv.org/abs/2412.18872v1,"Pengwei Zhao, Jie Feng"
"TSceneJAL: Joint Active Learning of Traffic Scenes for 3D Object
  Detection","Most autonomous driving (AD) datasets incur substantial costs for collection
and labeling, inevitably yielding a plethora of low-quality and redundant data
instances, thereby compromising performance and efficiency. Many applications
in AD systems necessitate high-quality training datasets using both existing
datasets and newly collected data. In this paper, we propose a traffic scene
joint active learning (TSceneJAL) framework that can efficiently sample the
balanced, diverse, and complex traffic scenes from both labeled and unlabeled
data. The novelty of this framework is threefold: 1) a scene sampling scheme
based on a category entropy, to identify scenes containing multiple object
classes, thus mitigating class imbalance for the active learner; 2) a
similarity sampling scheme, estimated through the directed graph representation
and a marginalize kernel algorithm, to pick sparse and diverse scenes; 3) an
uncertainty sampling scheme, predicted by a mixture density network, to select
instances with the most unclear or complex regression outcomes for the learner.
Finally, the integration of these three schemes in a joint selection strategy
yields an optimal and valuable subdataset. Experiments on the KITTI, Lyft,
nuScenes and SUScape datasets demonstrate that our approach outperforms
existing state-of-the-art methods on 3D object detection tasks with up to 12%
improvements.",2024-12-25T11:07:04Z,http://arxiv.org/abs/2412.18870v1,"Chenyang Lei, Meiying Zhang, Weiyuan Peng, Qi Hao, Chengzhong Xu, Chunlin Ji, Guang Zhou"
"Autonomous Navigation of 4WIS4WID Agricultural Field Mobile Robot using
  Deep Reinforcement Learning","In the futuristic agricultural fields compatible with Agriculture 4.0, robots
are envisaged to navigate through crops to perform functions like pesticide
spraying and fruit harvesting, which are complex tasks due to factors such as
non-geometric internal obstacles, space constraints, and outdoor conditions. In
this paper, we attempt to employ Deep Reinforcement Learning (DRL) to solve the
problem of 4WIS4WID mobile robot navigation in a structured, automated
agricultural field. This paper consists of three sections: parameterization of
four-wheel steering configurations, crop row tracking using DRL, and autonomous
navigation of 4WIS4WID mobile robot using DRL through multiple crop rows. We
show how to parametrize various configurations of four-wheel steering to two
variables. This includes symmetric four-wheel steering, zero-turn, and an
additional steering configuration that allows the 4WIS4WID mobile robot to move
laterally. Using DRL, we also followed an irregularly shaped crop row with
symmetric four-wheel steering. In the multiple crop row simulation environment,
with the help of waypoints, we effectively performed point-to-point navigation.
Finally, a comparative analysis of various DRL algorithms that use continuous
actions was carried out.",2024-12-25T10:33:33Z,http://arxiv.org/abs/2412.18865v1,"Tom Baby, Mahendra Kumar Gohil, Bishakh Bhattacharya"
"WeatherGS: 3D Scene Reconstruction in Adverse Weather Conditions via
  Gaussian Splatting","3D Gaussian Splatting (3DGS) has gained significant attention for 3D scene
reconstruction, but still suffers from complex outdoor environments, especially
under adverse weather. This is because 3DGS treats the artifacts caused by
adverse weather as part of the scene and will directly reconstruct them,
largely reducing the clarity of the reconstructed scene. To address this
challenge, we propose WeatherGS, a 3DGS-based framework for reconstructing
clear scenes from multi-view images under different weather conditions.
Specifically, we explicitly categorize the multi-weather artifacts into the
dense particles and lens occlusions that have very different characters, in
which the former are caused by snowflakes and raindrops in the air, and the
latter are raised by the precipitation on the camera lens. In light of this, we
propose a dense-to-sparse preprocess strategy, which sequentially removes the
dense particles by an Atmospheric Effect Filter (AEF) and then extracts the
relatively sparse occlusion masks with a Lens Effect Detector (LED). Finally,
we train a set of 3D Gaussians by the processed images and generated masks for
excluding occluded areas, and accurately recover the underlying clear scene by
Gaussian splatting. We conduct a diverse and challenging benchmark to
facilitate the evaluation of 3D reconstruction under complex weather scenarios.
Extensive experiments on this benchmark demonstrate that our WeatherGS
consistently produces high-quality, clean scenes across various weather
scenarios, outperforming existing state-of-the-art methods. See project
page:https://jumponthemoon.github.io/weather-gs.",2024-12-25T10:16:57Z,http://arxiv.org/abs/2412.18862v1,"Chenghao Qian, Yuhu Guo, Wenjing Li, Gustav Markkula"
"Few-shot Metric Domain Adaptation: Practical Learning Strategies for an
  Automated Plant Disease Diagnosis","Numerous studies have explored image-based automated systems for plant
disease diagnosis, demonstrating impressive diagnostic capabilities. However,
recent large-scale analyses have revealed a critical limitation: that the
diagnostic capability suffers significantly when validated on images captured
in environments (domains) differing from those used during training. This
shortfall stems from the inherently limited dataset size and the diverse
manifestation of disease symptoms, combined with substantial variations in
cultivation environments and imaging conditions, such as equipment and
composition. These factors lead to insufficient variety in training data,
ultimately constraining the system's robustness and generalization. To address
these challenges, we propose Few-shot Metric Domain Adaptation (FMDA), a
flexible and effective approach for enhancing diagnostic accuracy in practical
systems, even when only limited target data is available. FMDA reduces domain
discrepancies by introducing a constraint to the diagnostic model that
minimizes the ""distance"" between feature spaces of source (training) data and
target data with limited samples. FMDA is computationally efficient, requiring
only basic feature distance calculations and backpropagation, and can be
seamlessly integrated into any machine learning (ML) pipeline. In large-scale
experiments, involving 223,015 leaf images across 20 fields and 3 crop species,
FMDA achieved F1 score improvements of 11.1 to 29.3 points compared to cases
without target data, using only 10 images per disease from the target domain.
Moreover, FMDA consistently outperformed fine-tuning methods utilizing the same
data, with an average improvement of 8.5 points.",2024-12-25T10:01:30Z,http://arxiv.org/abs/2412.18859v1,"Shoma Kudo, Satoshi Kagiwada, Hitoshi Iyatomi"
Computing Approximate Graph Edit Distance via Optimal Transport,"Given a graph pair $(G^1, G^2)$, graph edit distance (GED) is defined as the
minimum number of edit operations converting $G^1$ to $G^2$. GED is a
fundamental operation widely used in many applications, but its exact
computation is NP-hard, so the approximation of GED has gained a lot of
attention. Data-driven learning-based methods have been found to provide
superior results compared to classical approximate algorithms, but they
directly fit the coupling relationship between a pair of vertices from their
vertex features. We argue that while pairwise vertex features can capture the
coupling cost (discrepancy) of a pair of vertices, the vertex coupling matrix
should be derived from the vertex-pair cost matrix through a more
well-established method that is aware of the global context of the graph pair,
such as optimal transport. In this paper, we propose an ensemble approach that
integrates a supervised learning-based method and an unsupervised method, both
based on optimal transport. Our learning method, GEDIOT, is based on inverse
optimal transport that leverages a learnable Sinkhorn algorithm to generate the
coupling matrix. Our unsupervised method, GEDGW, models GED computation as a
linear combination of optimal transport and its variant, Gromov-Wasserstein
discrepancy, for node and edge operations, respectively, which can be solved
efficiently without needing the ground truth. Our ensemble method, GEDHOT,
combines GEDIOT and GEDGW to further boost the performance. Extensive
experiments demonstrate that our methods significantly outperform the existing
methods in terms of the performance of GED computation, edit path generation,
and model generalizability.",2024-12-25T09:55:14Z,http://arxiv.org/abs/2412.18857v1,"Qihao Cheng, Da Yan, Tianhao Wu, Zhongyi Huang, Qin Zhang"
"Digital Twin Enhanced Deep Reinforcement Learning for Intelligent
  Omni-Surface Configurations in MU-MIMO Systems","Intelligent omni-surface (IOS) is a promising technique to enhance the
capacity of wireless networks, by reflecting and refracting the incident signal
simultaneously. Traditional IOS configuration schemes, relying on all
sub-channels' channel state information and user equipments' mobility, are
difficult to implement in complex realistic systems. Existing works attempt to
address this issue employing deep reinforcement learning (DRL), but this method
requires a lot of trial-and-error interactions with the external environment
for efficient results and thus cannot satisfy the real-time decision-making. To
enable model-free and real-time IOS control, this paper puts forth a new
framework that integrates DRL and digital twins. DeepIOS, a DRL based IOS
configuration scheme with the goal of maximizing the sum data rate, is first
developed to jointly optimize the phase-shift and amplitude of IOS in
multi-user multiple-input-multiple-output systems. Thereafter, to further
reduce the computational complexity, DeepIOS introduces an action branch
architecture, which separately decides two optimization variables in parallel.
Finally, a digital twin module is constructed through supervised learning as a
pre-verification platform for DeepIOS, such that the decision-making's
real-time can be guaranteed. The formulated framework is a closed-loop system,
in which the physical space provides data to establish and calibrate the
digital space, while the digital space generates experience samples for DeepIOS
training and sends the trained parameters to the IOS controller for
configurations. Numerical results show that compared with random and MAB
schemes, the proposed framework attains a higher data rate and is more robust
to different settings. Furthermore, the action branch architecture reduces
DeepIOS's computational complexity, and the digital twin module improves the
convergence speed and run-time.",2024-12-25T09:53:07Z,http://arxiv.org/abs/2412.18856v1,"Xiaowen Ye, Xianghao Yu, Liqun Fu"
"Optimistic Critic Reconstruction and Constrained Fine-Tuning for General
  Offline-to-Online RL","Offline-to-online (O2O) reinforcement learning (RL) provides an effective
means of leveraging an offline pre-trained policy as initialization to improve
performance rapidly with limited online interactions. Recent studies often
design fine-tuning strategies for a specific offline RL method and cannot
perform general O2O learning from any offline method. To deal with this
problem, we disclose that there are evaluation and improvement mismatches
between the offline dataset and the online environment, which hinders the
direct application of pre-trained policies to online fine-tuning. In this
paper, we propose to handle these two mismatches simultaneously, which aims to
achieve general O2O learning from any offline method to any online method.
Before online fine-tuning, we re-evaluate the pessimistic critic trained on the
offline dataset in an optimistic way and then calibrate the misaligned critic
with the reliable offline actor to avoid erroneous update. After obtaining an
optimistic and and aligned critic, we perform constrained fine-tuning to combat
distribution shift during online learning. We show empirically that the
proposed method can achieve stable and efficient performance improvement on
multiple simulated tasks when compared to the state-of-the-art methods.",2024-12-25T09:52:22Z,http://arxiv.org/abs/2412.18855v1,"Qin-Wen Luo, Ming-Kun Xie, Ye-Wen Wang, Sheng-Jun Huang"
"Gravitational waves from equatorially eccentric extreme mass ratio
  inspirals around swirling Kerr black holes","We have studied the gravitational wave generated by extreme mass ratio
inspirals (EMRIs) along eccentric orbits on equatorial plane within the frame
of the swirling-Kerr black hole spacetime. The swirling-Kerr black hole has an
extra swirling parameter, which characterizes the rotation of universe
background. Our findings indicate that this swirling parameter leads to a delay
phase shift in the gravitational waveforms. The impact of the swirling
parameter on EMRI gravitational waves is suppressed by the black hole's spin
parameter. As a result, extracting information about the swirling parameter
from gravitational waves in a static black hole spacetime is much easier than
in the case of a rapidly rotating black hole. Our analysis also shows that a
high black hole spin leads to a greater overlap of gravitational waveforms for
different swirling parameters. We further investigate the potential issue of
waveform confusion caused by the orbital eccentricity and semi-latus rectum
parameters. As the swirling parameter increases, the relative variation in
eccentricity also increases, while the variation in the semi-latus rectum
decreases rapidly. The trends in these changes with the swirling parameter
resemble those observed with the MOG (Modified Gravity) parameter, though with
different rates of change. These results provide deeper insights into the
properties of EMRI gravitational waves and the swirling of the universe
background.",2024-12-25T09:51:58Z,http://arxiv.org/abs/2412.18854v1,"Yuhang Gu, Songbai Chen, Jiliang Jing"
Cross-View Image Set Geo-Localization,"Cross-view geo-localization (CVGL) has been widely applied in fields such as
robotic navigation and augmented reality. Existing approaches primarily use
single images or fixed-view image sequences as queries, which limits
perspective diversity. In contrast, when humans determine their location
visually, they typically move around to gather multiple perspectives. This
behavior suggests that integrating diverse visual cues can improve
geo-localization reliability. Therefore, we propose a novel task: Cross-View
Image Set Geo-Localization (Set-CVGL), which gathers multiple images with
diverse perspectives as a query set for localization. To support this task, we
introduce SetVL-480K, a benchmark comprising 480,000 ground images captured
worldwide and their corresponding satellite images, with each satellite image
corresponds to an average of 40 ground images from varied perspectives and
locations. Furthermore, we propose FlexGeo, a flexible method designed for
Set-CVGL that can also adapt to single-image and image-sequence inputs. FlexGeo
includes two key modules: the Similarity-guided Feature Fuser (SFF), which
adaptively fuses image features without prior content dependency, and the
Individual-level Attributes Learner (IAL), leveraging geo-attributes of each
image for comprehensive scene perception. FlexGeo consistently outperforms
existing methods on SetVL-480K and two public datasets, SeqGeo and KITTI-CVL,
achieving a localization accuracy improvement of over 22% on SetVL-480K.",2024-12-25T09:46:14Z,http://arxiv.org/abs/2412.18852v1,"Qiong Wu, Panwang Xia, Lei Yu, Yi Liu, Mingtao Xiong, Liheng Zhong, Jingdong Chen, Ming Yang, Yongjun Zhang, Yi Wan"
"Attention-Enhanced Short-Time Wiener Solution for Acoustic Echo
  Cancellation","Acoustic Echo Cancellation (AEC) is an essential speech signal processing
technology that removes echoes from microphone inputs to facilitate
natural-sounding full-duplex communication. Currently, deep learning-based AEC
methods primarily focus on refining model architectures, frequently neglecting
the incorporation of knowledge from traditional filter theory. This paper
presents an innovative approach to AEC by introducing an attention-enhanced
short-time Wiener solution. Our method strategically harnesses attention
mechanisms to mitigate the impact of double-talk interference, thereby
optimizing the efficiency of knowledge utilization. The derivation of the
short-term Wiener solution, which adapts classical Wiener solutions to finite
input causality, integrates established insights from filter theory into this
method. The experimental outcomes corroborate the effectiveness of our proposed
approach, surpassing other baseline models in performance and generalization.
The official code is available at https://github.com/ZhaoF-i/ASTWS-AEC",2024-12-25T09:42:41Z,http://arxiv.org/abs/2412.18851v1,"Fei Zhao, Xueliang Zhang"
"SWAG: Long-term Surgical Workflow Prediction with Generative-based
  Anticipation","While existing recognition approaches excel at identifying current surgical
phases, they provide limited foresight into future procedural steps,
restricting their intraoperative utility. Similarly, current anticipation
methods are constrained to predicting short-term events or singular future
occurrences, neglecting the dynamic and sequential nature of surgical
workflows. To address these limitations, we propose SWAG (Surgical Workflow
Anticipative Generation), a unified framework for phase recognition and
long-term anticipation of surgical workflows. SWAG employs two generative
decoding methods -- single-pass (SP) and auto-regressive (AR) -- to predict
sequences of future surgical phases. A novel prior knowledge embedding
mechanism enhances the accuracy of anticipatory predictions. The framework
addresses future phase classification and remaining time regression tasks.
Additionally, a regression-to-classification (R2C) method is introduced to map
continuous predictions to discrete temporal segments. SWAG's performance was
evaluated on the Cholec80 and AutoLaparo21 datasets. The single-pass
classification model with prior knowledge embeddings (SWAG-SP\*) achieved
53.5\% accuracy in 15-minute anticipation on AutoLaparo21, while the R2C model
reached 60.8\% accuracy on Cholec80. SWAG's single-pass regression approach
outperformed existing methods for remaining time prediction, achieving weighted
mean absolute errors of 0.32 and 0.48 minutes for 2- and 3-minute horizons,
respectively. SWAG demonstrates versatility across classification and
regression tasks, offering robust tools for real-time surgical workflow
anticipation. By unifying recognition and anticipatory capabilities, SWAG
provides actionable predictions to enhance intraoperative decision-making.",2024-12-25T09:29:57Z,http://arxiv.org/abs/2412.18849v1,"Maxence Boels, Yang Liu, Prokar Dasgupta, Alejandro Granados, Sebastien Ourselin"
Machine Learning-Based Detection of Pump-and-Dump Schemes in Real-Time,"Cryptocurrency markets often face manipulation through prevalent
pump-and-dump (P&amp;D) schemes, where self-organized Telegram groups, some
exceeding two million members, artificially inflate target cryptocurrency
prices. These groups sell premium access to inside information, worsening
information asymmetry and financial risks for subscribers and all investors.
This paper presents a real-time prediction pipeline to forecast target coins
and alert investors to possible P&amp;D schemes. In a Poloniex case study, the
model accurately identified the target coin among the top five from 50 random
coins in 24 out of 43 (55.81%) P&amp;D events. The pipeline uses advanced natural
language processing (NLP) to classify Telegram messages, identifying 2,079 past
pump events and detecting new ones in real-time. Our analysis also evaluates
the susceptibility of token standards - ERC-20, ERC-721, BRC-20, Inscriptions,
and Runes - to manipulation and identifies exchanges commonly involved in P&amp;D
schemes.",2024-12-25T09:23:36Z,http://arxiv.org/abs/2412.18848v1,"Manuel Bolz, Kevin Bründler, Liam Kane, Panagiotis Patsias, Liam Tessendorf, Krzysztof Gogol, Taehoon Kim, Claudio Tessone"
"TPCH: Tensor-interacted Projection and Cooperative Hashing for
  Multi-view Clustering","In recent years, anchor and hash-based multi-view clustering methods have
gained attention for their efficiency and simplicity in handling large-scale
data. However, existing methods often overlook the interactions among
multi-view data and higher-order cooperative relationships during projection,
negatively impacting the quality of hash representation in low-dimensional
spaces, clustering performance, and sensitivity to noise. To address this
issue, we propose a novel approach named Tensor-Interacted Projection and
Cooperative Hashing for Multi-View Clustering(TPCH). TPCH stacks multiple
projection matrices into a tensor, taking into account the synergies and
communications during the projection process. By capturing higher-order
multi-view information through dual projection and Hamming space, TPCH employs
an enhanced tensor nuclear norm to learn more compact and distinguishable hash
representations, promoting communication within and between views. Experimental
results demonstrate that this refined method significantly outperforms
state-of-the-art methods in clustering on five large-scale multi-view datasets.
Moreover, in terms of CPU time, TPCH achieves substantial acceleration compared
to the most advanced current methods. The code is available at
\textcolor{red}{\url{https://github.com/jankin-wang/TPCH}}.",2024-12-25T09:22:11Z,http://arxiv.org/abs/2412.18847v1,"Zhongwen Wang, Xingfeng Li, Yinghui Sun, Quansen Sun, Yuan Sun, Han Ling, Jian Dai, Zhenwen Ren"
"Enhancing Federated Graph Learning via Adaptive Fusion of Structural and
  Node Characteristics","Federated Graph Learning (FGL) has demonstrated the advantage of training a
global Graph Neural Network (GNN) model across distributed clients using their
local graph data. Unlike Euclidean data (\eg, images), graph data is composed
of nodes and edges, where the overall node-edge connections determine the
topological structure, and individual nodes along with their neighbors capture
local node features. However, existing studies tend to prioritize one aspect
over the other, leading to an incomplete understanding of the data and the
potential misidentification of key characteristics across varying graph
scenarios. Additionally, the non-independent and identically distributed
(non-IID) nature of graph data makes the extraction of these two data
characteristics even more challenging. To address the above issues, we propose
a novel FGL framework, named FedGCF, which aims to simultaneously extract and
fuse structural properties and node features to effectively handle diverse
graph scenarios. FedGCF first clusters clients by structural similarity,
performing model aggregation within each cluster to form the shared structural
model. Next, FedGCF selects the clients with common node features and
aggregates their models to generate a common node model. This model is then
propagated to all clients, allowing common node features to be shared. By
combining these two models with a proper ratio, FedGCF can achieve a
comprehensive understanding of the graph data and deliver better performance,
even under non-IID distributions. Experimental results show that FedGCF
improves accuracy by 4.94%-7.24% under different data distributions and reduces
communication cost by 64.18%-81.25% to reach the same accuracy compared to
baselines.",2024-12-25T09:20:06Z,http://arxiv.org/abs/2412.18845v1,"Xianjun Gao, Jianchun Liu, Hongli Xu, Shilong Wang, Liusheng Huang"
"Improving Integrated Gradient-based Transferable Adversarial Examples by
  Refining the Integration Path","Transferable adversarial examples are known to cause threats in practical,
black-box attack scenarios. A notable approach to improving transferability is
using integrated gradients (IG), originally developed for model
interpretability. In this paper, we find that existing IG-based attacks have
limited transferability due to their naive adoption of IG in model
interpretability. To address this limitation, we focus on the IG integration
path and refine it in three aspects: multiplicity, monotonicity, and diversity,
supported by theoretical analyses. We propose the Multiple Monotonic
Diversified Integrated Gradients (MuMoDIG) attack, which can generate highly
transferable adversarial examples on different CNN and ViT models and defenses.
Experiments validate that MuMoDIG outperforms the latest IG-based attack by up
to 37.3\% and other state-of-the-art attacks by 8.4\%. In general, our study
reveals that migrating established techniques to improve transferability may
require non-trivial efforts. Code is available at
\url{https://github.com/RYC-98/MuMoDIG}.",2024-12-25T09:15:39Z,http://arxiv.org/abs/2412.18844v1,"Yuchen Ren, Zhengyu Zhao, Chenhao Lin, Bo Yang, Lu Zhou, Zhe Liu, Chao Shen"
"Context-Based Semantic-Aware Alignment for Semi-Supervised Multi-Label
  Learning","Due to the lack of extensive precisely-annotated multi-label data in real
word, semi-supervised multi-label learning (SSMLL) has gradually gained
attention. Abundant knowledge embedded in vision-language models (VLMs)
pre-trained on large-scale image-text pairs could alleviate the challenge of
limited labeled data under SSMLL setting.Despite existing methods based on
fine-tuning VLMs have achieved advances in weakly-supervised multi-label
learning, they failed to fully leverage the information from labeled data to
enhance the learning of unlabeled data. In this paper, we propose a
context-based semantic-aware alignment method to solve the SSMLL problem by
leveraging the knowledge of VLMs. To address the challenge of handling multiple
semantics within an image, we introduce a novel framework design to extract
label-specific image features. This design allows us to achieve a more compact
alignment between text features and label-specific image features, leading the
model to generate high-quality pseudo-labels. To incorporate the model with
comprehensive understanding of image, we design a semi-supervised context
identification auxiliary task to enhance the feature representation by
capturing co-occurrence information. Extensive experiments on multiple
benchmark datasets demonstrate the effectiveness of our proposed method.",2024-12-25T09:06:54Z,http://arxiv.org/abs/2412.18842v1,"Heng-Bo Fan, Ming-Kun Xie, Jia-Hao Xiao, Sheng-Jun Huang"
"Advancing NAM-to-Speech Conversion with Novel Methods and the MultiNAM
  Dataset","Current Non-Audible Murmur (NAM)-to-speech techniques rely on voice cloning
to simulate ground-truth speech from paired whispers. However, the simulated
speech often lacks intelligibility and fails to generalize well across
different speakers. To address this issue, we focus on learning phoneme-level
alignments from paired whispers and text and employ a Text-to-Speech (TTS)
system to simulate the ground-truth. To reduce dependence on whispers, we learn
phoneme alignments directly from NAMs, though the quality is constrained by the
available training data. To further mitigate reliance on NAM/whisper data for
ground-truth simulation, we propose incorporating the lip modality to infer
speech and introduce a novel diffusion-based method that leverages recent
advancements in lip-to-speech technology. Additionally, we release the MultiNAM
dataset with over $7.96$ hours of paired NAM, whisper, video, and text data
from two speakers and benchmark all methods on this dataset. Speech samples and
the dataset are available at \url{https://diff-nam.github.io/DiffNAM/}",2024-12-25T08:57:24Z,http://arxiv.org/abs/2412.18839v1,"Neil Shah, Shirish Karande, Vineet Gandhi"
DiFiC: Your Diffusion Model Holds the Secret to Fine-Grained Clustering,"Fine-grained clustering is a practical yet challenging task, whose essence
lies in capturing the subtle differences between instances of different
classes. Such subtle differences can be easily disrupted by data augmentation
or be overwhelmed by redundant information in data, leading to significant
performance degradation for existing clustering methods. In this work, we
introduce DiFiC a fine-grained clustering method building upon the conditional
diffusion model. Distinct from existing works that focus on extracting
discriminative features from images, DiFiC resorts to deducing the textual
conditions used for image generation. To distill more precise and
clustering-favorable object semantics, DiFiC further regularizes the diffusion
target and guides the distillation process utilizing neighborhood similarity.
Extensive experiments demonstrate that DiFiC outperforms both state-of-the-art
discriminative and generative clustering methods on four fine-grained image
clustering benchmarks. We hope the success of DiFiC will inspire future
research to unlock the potential of diffusion models in tasks beyond
generation. The code will be released.",2024-12-25T08:55:48Z,http://arxiv.org/abs/2412.18838v1,"Ruohong Yang, Peng Hu, Xi Peng, Xiting Liu, Yunfan Li"
"MRI2Speech: Speech Synthesis from Articulatory Movements Recorded by
  Real-time MRI","Previous real-time MRI (rtMRI)-based speech synthesis models depend heavily
on noisy ground-truth speech. Applying loss directly over ground truth
mel-spectrograms entangles speech content with MRI noise, resulting in poor
intelligibility. We introduce a novel approach that adapts the multi-modal
self-supervised AV-HuBERT model for text prediction from rtMRI and incorporates
a new flow-based duration predictor for speaker-specific alignment. The
predicted text and durations are then used by a speech decoder to synthesize
aligned speech in any novel voice. We conduct thorough experiments on two
datasets and demonstrate our method's generalization ability to unseen
speakers. We assess our framework's performance by masking parts of the rtMRI
video to evaluate the impact of different articulators on text prediction. Our
method achieves a $15.18\%$ Word Error Rate (WER) on the USC-TIMIT MRI corpus,
marking a huge improvement over the current state-of-the-art. Speech samples
are available at \url{https://mri2speech.github.io/MRI2Speech/}",2024-12-25T08:49:43Z,http://arxiv.org/abs/2412.18836v1,"Neil Shah, Ayan Kashyap, Shirish Karande, Vineet Gandhi"
"Adaptive Rate Control for Deep Video Compression with Rate-Distortion
  Prediction","Deep video compression has made significant progress in recent years,
achieving rate-distortion performance that surpasses that of traditional video
compression methods. However, rate control schemes tailored for deep video
compression have not been well studied. In this paper, we propose a neural
network-based $\lambda$-domain rate control scheme for deep video compression,
which determines the coding parameter $\lambda$ for each to-be-coded frame
based on the rate-distortion-$\lambda$ (R-D-$\lambda$) relationships directly
learned from uncompressed frames, achieving high rate control accuracy
efficiently without the need for pre-encoding. Moreover, this content-aware
scheme is able to mitigate inter-frame quality fluctuations and adapt to abrupt
changes in video content. Specifically, we introduce two neural network-based
predictors to estimate the relationship between bitrate and $\lambda$, as well
as the relationship between distortion and $\lambda$ for each frame. Then we
determine the coding parameter $\lambda$ for each frame to achieve the target
bitrate. Experimental results demonstrate that our approach achieves high rate
control accuracy at the mini-GOP level with low time overhead and mitigates
inter-frame quality fluctuations across video content of varying resolutions.",2024-12-25T08:42:23Z,http://arxiv.org/abs/2412.18834v1,"Bowen Gu, Hao Chen, Ming Lu, Jie Yao, Zhan Ma"
"Federated Learning with Partially Labeled Data: A Conditional
  Distillation Approach","In medical imaging, developing generalized segmentation models that can
handle multiple organs and lesions is crucial. However, the scarcity of fully
annotated datasets and strict privacy regulations present significant barriers
to data sharing. Federated Learning (FL) allows decentralized model training,
but existing FL methods often struggle with partial labeling, leading to model
divergence and catastrophic forgetting. We propose ConDistFL, a novel FL
framework incorporating conditional distillation to address these challenges.
ConDistFL enables effective learning from partially labeled datasets,
significantly improving segmentation accuracy across distributed and
non-uniform datasets. In addition to its superior segmentation performance,
ConDistFL maintains computational and communication efficiency, ensuring its
scalability for real-world applications. Furthermore, ConDistFL demonstrates
remarkable generalizability, significantly outperforming existing FL methods in
out-of-federation tests, even adapting to unseen contrast phases (e.g.,
non-contrast CT images) in our experiments. Extensive evaluations on 3D CT and
2D chest X-ray datasets show that ConDistFL is an efficient, adaptable solution
for collaborative medical image segmentation in privacy-constrained settings.",2024-12-25T08:40:03Z,http://arxiv.org/abs/2412.18833v1,"Pochuan Wang, Chen Shen, Masahiro Oda, Chiou-Shann Fuh, Kensaku Mori, Weichung Wang, Holger R. Roth"
"Structured Speaker-Deficiency Adaptation of Foundation Models for
  Dysarthric and Elderly Speech Recognition","Data-intensive fine-tuning of speech foundation models (SFMs) to scarce and
diverse dysarthric and elderly speech leads to data bias and poor
generalization to unseen speakers. This paper proposes novel structured
speaker-deficiency adaptation approaches for SSL pre-trained SFMs on such data.
Speaker and speech deficiency invariant SFMs were constructed in their
supervised adaptive fine-tuning stage to reduce undue bias to training data
speakers, and serves as a more neutral and robust starting point for test time
unsupervised adaptation. Speech variability attributed to speaker identity and
speech impairment severity, or aging induced neurocognitive decline, are
modelled using separate adapters that can be combined together to model any
seen or unseen speaker. Experiments on the UASpeech dysarthric and DementiaBank
Pitt elderly speech corpora suggest structured speaker-deficiency adaptation of
HuBERT and Wav2vec2-conformer models consistently outperforms baseline SFMs
using either: a) no adapters; b) global adapters shared among all speakers; or
c) single attribute adapters modelling speaker or deficiency labels alone by
statistically significant WER reductions up to 3.01% and 1.50% absolute (10.86%
and 6.94% relative) on the two tasks respectively. The lowest published WER of
19.45% (49.34% on very low intelligibility, 33.17% on unseen words) is obtained
on the UASpeech test set of 16 dysarthric speakers.",2024-12-25T08:39:02Z,http://arxiv.org/abs/2412.18832v1,"Shujie Hu, Xurong Xie, Mengzhe Geng, Jiajun Deng, Zengrui Jin, Tianzi Wang, Mingyu Cui, Guinan Li, Zhaoqing Li, Helen Meng, Xunying Liu"
"PhyloGen: Language Model-Enhanced Phylogenetic Inference via Graph
  Structure Generation","Phylogenetic trees elucidate evolutionary relationships among species, but
phylogenetic inference remains challenging due to the complexity of combining
continuous (branch lengths) and discrete parameters (tree topology).
Traditional Markov Chain Monte Carlo methods face slow convergence and
computational burdens. Existing Variational Inference methods, which require
pre-generated topologies and typically treat tree structures and branch lengths
independently, may overlook critical sequence features, limiting their accuracy
and flexibility. We propose PhyloGen, a novel method leveraging a pre-trained
genomic language model to generate and optimize phylogenetic trees without
dependence on evolutionary models or aligned sequence constraints. PhyloGen
views phylogenetic inference as a conditionally constrained tree structure
generation problem, jointly optimizing tree topology and branch lengths through
three core modules: (i) Feature Extraction, (ii) PhyloTree Construction, and
(iii) PhyloTree Structure Modeling. Meanwhile, we introduce a Scoring Function
to guide the model towards a more stable gradient descent. We demonstrate the
effectiveness and robustness of PhyloGen on eight real-world benchmark
datasets. Visualization results confirm PhyloGen provides deeper insights into
phylogenetic relationships.",2024-12-25T08:33:05Z,http://arxiv.org/abs/2412.18827v1,"ChenRui Duan, Zelin Zang, Siyuan Li, Yongjie Xu, Stan Z. Li"
"CausalTAD: Causal Implicit Generative Model for Debiased Online
  Trajectory Anomaly Detection","Trajectory anomaly detection, aiming to estimate the anomaly risk of
trajectories given the Source-Destination (SD) pairs, has become a critical
problem for many real-world applications. Existing solutions directly train a
generative model for observed trajectories and calculate the conditional
generative probability $P({T}|{C})$ as the anomaly risk, where ${T}$ and ${C}$
represent the trajectory and SD pair respectively. However, we argue that the
observed trajectories are confounded by road network preference which is a
common cause of both SD distribution and trajectories. Existing methods ignore
this issue limiting their generalization ability on out-of-distribution
trajectories. In this paper, we define the debiased trajectory anomaly
detection problem and propose a causal implicit generative model, namely
CausalTAD, to solve it. CausalTAD adopts do-calculus to eliminate the
confounding bias of road network preference and estimates $P({T}|do({C}))$ as
the anomaly criterion. Extensive experiments show that CausalTAD can not only
achieve superior performance on trained trajectories but also generally improve
the performance of out-of-distribution data, with improvements of $2.1\% \sim
5.7\%$ and $10.6\% \sim 32.7\%$ respectively.",2024-12-25T08:20:52Z,http://arxiv.org/abs/2412.18820v1,"Wenbin Li, Di Yao, Chang Gong, Xiaokai Chu, Quanliang Jing, Xiaolei Zhou, Yuxuan Zhang, Yunxia Fan, Jingping Bi"
LLM-assisted vector similarity search,"As data retrieval demands become increasingly complex, traditional search
methods often fall short in addressing nuanced and conceptual queries. Vector
similarity search has emerged as a promising technique for finding semantically
similar information efficiently. However, its effectiveness diminishes when
handling intricate queries with contextual nuances. This paper explores a
hybrid approach combining vector similarity search with Large Language Models
(LLMs) to enhance search accuracy and relevance. The proposed two-step solution
first employs vector similarity search to shortlist potential matches, followed
by an LLM for context-aware ranking of the results. Experiments on structured
datasets demonstrate that while vector similarity search alone performs well
for straightforward queries, the LLM-assisted approach excels in processing
complex queries involving constraints, negations, or conceptual requirements.
By leveraging the natural language understanding capabilities of LLMs, this
method improves the accuracy of search results for complex tasks without
sacrificing efficiency. We also discuss real-world applications and propose
directions for future research to refine and scale this technique for diverse
datasets and use cases.
  Original article:
https://engineering.grab.com/llm-assisted-vector-similarity-search",2024-12-25T08:17:37Z,http://arxiv.org/abs/2412.18819v1,"Md Riyadh, Muqi Li, Felix Haryanto Lie, Jia Long Loh, Haotian Mi, Sayam Bohra"
GSAVS: Gaussian Splatting-based Autonomous Vehicle Simulator,"Modern autonomous vehicle simulators feature an ever-growing library of
assets, including vehicles, buildings, roads, pedestrians, and more. While this
level of customization proves beneficial when creating virtual urban
environments, this process becomes cumbersome when intending to train within a
digital twin or a duplicate of a real scene. Gaussian splatting emerged as a
powerful technique in scene reconstruction and novel view synthesis, boasting
high fidelity and rendering speeds. In this paper, we introduce GSAVS, an
autonomous vehicle simulator that supports the creation and development of
autonomous vehicle models. Every asset within the simulator is a 3D Gaussian
splat, including the vehicles and the environment. However, the simulator runs
within a classical 3D engine, rendering 3D Gaussian splats in real-time. This
allows the simulator to utilize the photorealism that 3D Gaussian splatting
boasts while providing the customization and ease of use of a classical 3D
engine.",2024-12-25T07:52:09Z,http://arxiv.org/abs/2412.18816v1,Rami Wilson
"Distortion-Aware Adversarial Attacks on Bounding Boxes of Object
  Detectors","Deep learning-based object detection has become ubiquitous in the last decade
due to its high accuracy in many real-world applications. With this growing
trend, these models are interested in being attacked by adversaries, with most
of the results being on classifiers, which do not match the context of
practical object detection. In this work, we propose a novel method to fool
object detectors, expose the vulnerability of state-of-the-art detectors, and
promote later works to build more robust detectors to adversarial examples. Our
method aims to generate adversarial images by perturbing object confidence
scores during training, which is crucial in predicting confidence for each
class in the testing phase. Herein, we provide a more intuitive technique to
embed additive noises based on detected objects' masks and the training loss
with distortion control over the original image by leveraging the gradient of
iterative images. To verify the proposed method, we perform adversarial attacks
against different object detectors, including the most recent state-of-the-art
models like YOLOv8, Faster R-CNN, RetinaNet, and Swin Transformer. We also
evaluate our technique on MS COCO 2017 and PASCAL VOC 2012 datasets and analyze
the trade-off between success attack rate and image distortion. Our experiments
show that the achievable success attack rate is up to $100$\% and up to $98$\%
when performing white-box and black-box attacks, respectively. The source code
and relevant documentation for this work are available at the following link:
https://github.com/anonymous20210106/attack_detector",2024-12-25T07:51:57Z,http://arxiv.org/abs/2412.18815v1,"Pham Phuc, Son Vuong, Khang Nguyen, Tuan Dang"
"DebiasDiff: Debiasing Text-to-image Diffusion Models with
  Self-discovering Latent Attribute Directions","While Diffusion Models (DM) exhibit remarkable performance across various
image generative tasks, they nonetheless reflect the inherent bias presented in
the training set. As DMs are now widely used in real-world applications, these
biases could perpetuate a distorted worldview and hinder opportunities for
minority groups. Existing methods on debiasing DMs usually requires model
re-training with a human-crafted reference dataset or additional classifiers,
which suffer from two major limitations: (1) collecting reference datasets
causes expensive annotation cost; (2) the debiasing performance is heavily
constrained by the quality of the reference dataset or the additional
classifier. To address the above limitations, we propose DebiasDiff, a
plug-and-play method that learns attribute latent directions in a
self-discovering manner, thus eliminating the reliance on such reference
dataset. Specifically, DebiasDiff consists of two parts: a set of attribute
adapters and a distribution indicator. Each adapter in the set aims to learn an
attribute latent direction, and is optimized via noise composition through a
self-discovering process. Then, the distribution indicator is multiplied by the
set of adapters to guide the generation process towards the prescribed
distribution. Our method enables debiasing multiple attributes in DMs
simultaneously, while remaining lightweight and easily integrable with other
DMs, eliminating the need for re-training. Extensive experiments on debiasing
gender, racial, and their intersectional biases show that our method
outperforms previous SOTA by a large margin.",2024-12-25T07:30:20Z,http://arxiv.org/abs/2412.18810v1,"Yilei Jiang, Weihong Li, Yiyuan Zhang, Minghong Cai, Xiangyu Yue"
Provable Uncertainty Decomposition via Higher-Order Calibration,"We give a principled method for decomposing the predictive uncertainty of a
model into aleatoric and epistemic components with explicit semantics relating
them to the real-world data distribution. While many works in the literature
have proposed such decompositions, they lack the type of formal guarantees we
provide. Our method is based on the new notion of higher-order calibration,
which generalizes ordinary calibration to the setting of higher-order
predictors that predict mixtures over label distributions at every point. We
show how to measure as well as achieve higher-order calibration using access to
$k$-snapshots, namely examples where each point has $k$ independent conditional
labels. Under higher-order calibration, the estimated aleatoric uncertainty at
a point is guaranteed to match the real-world aleatoric uncertainty averaged
over all points where the prediction is made. To our knowledge, this is the
first formal guarantee of this type that places no assumptions whatsoever on
the real-world data distribution. Importantly, higher-order calibration is also
applicable to existing higher-order predictors such as Bayesian and ensemble
models and provides a natural evaluation metric for such models. We demonstrate
through experiments that our method produces meaningful uncertainty
decompositions for image classification.",2024-12-25T07:26:36Z,http://arxiv.org/abs/2412.18808v1,"Gustaf Ahdritz, Aravind Gollakota, Parikshit Gopalan, Charlotte Peale, Udi Wieder"
FOR: Finetuning for Object Level Open Vocabulary Image Retrieval,"As working with large datasets becomes standard, the task of accurately
retrieving images containing objects of interest by an open set textual query
gains practical importance. The current leading approach utilizes a pre-trained
CLIP model without any adaptation to the target domain, balancing accuracy and
efficiency through additional post-processing. In this work, we propose FOR:
Finetuning for Object-centric Open-vocabulary Image Retrieval, which allows
finetuning on a target dataset using closed-set labels while keeping the
visual-language association crucial for open vocabulary retrieval. FOR is based
on two design elements: a specialized decoder variant of the CLIP head
customized for the intended task, and its coupling within a multi-objective
training framework. Together, these design choices result in a significant
increase in accuracy, showcasing improvements of up to 8 mAP@50 points over
SoTA across three datasets. Additionally, we demonstrate that FOR is also
effective in a semi-supervised setting, achieving impressive results even when
only a small portion of the dataset is labeled.",2024-12-25T07:08:51Z,http://arxiv.org/abs/2412.18806v1,"Hila Levi, Guy Heller, Dan Levi"
Time-Periodic Solutions for Hyperbolic-Parabolic Systems,"Time-periodic weak solutions for a coupled hyperbolic-parabolic system are
obtained. A linear heat and wave equation are considered on two respective
$d$-dimensional spatial domains that share a common $(d-1)$-dimensional
interface $\Gamma$. The system is only partially damped, leading to an
indeterminate case for existing theory (Galdi et al., 2014). We construct
periodic solutions by obtaining novel a priori estimates for the coupled
system, reconstructing the total energy via the interface $\Gamma$. As a
byproduct, geometric constraints manifest on the wave domain which are
reminiscent of classical boundary control conditions for wave stabilizability.
We note a ``loss"" of regularity between the forcing and solution which is
greater than that associated with the heat-wave Cauchy problem. However, we
consider a broader class of spatial domains and mitigate this regularity loss
by trading time and space differentiations, a feature unique to the periodic
setting. This seems to be the first constructive result addressing existence
and uniqueness of periodic solutions in the heat-wave context, where no
dissipation is present in the wave interior. Our results speak to the open
problem of the (non-)emergence of resonance in complex systems, and are readily
generalizable to related systems and certain nonlinear cases.",2024-12-25T06:42:30Z,http://arxiv.org/abs/2412.18801v1,"Stanislav Mosny, Boris Muha, Sebastian Schwarzacher, Justin T. Webster"
"Improving Generated and Retrieved Knowledge Combination Through
  Zero-shot Generation","Open-domain Question Answering (QA) has garnered substantial interest by
combining the advantages of faithfully retrieved passages and relevant passages
generated through Large Language Models (LLMs). However, there is a lack of
definitive labels available to pair these sources of knowledge. In order to
address this issue, we propose an unsupervised and simple framework called
Bi-Reranking for Merging Generated and Retrieved Knowledge (BRMGR), which
utilizes re-ranking methods for both retrieved passages and LLM-generated
passages. We pair the two types of passages using two separate re-ranking
methods and then combine them through greedy matching. We demonstrate that
BRMGR is equivalent to employing a bipartite matching loss when assigning each
retrieved passage with a corresponding LLM-generated passage. The application
of our model yielded experimental results from three datasets, improving their
performance by +1.7 and +1.6 on NQ and WebQ datasets, respectively, and
obtaining comparable result on TriviaQA dataset when compared to competitive
baselines.",2024-12-25T06:40:36Z,http://arxiv.org/abs/2412.18800v1,"Xinkai Du, Quanjie Han, Chao Lv, Yan Liu, Yalin Sun, Hao Shu, Hongbo Shan, Maosong Sun"
Quantifying the Risk of Pastoral Conflict in 4 Central African Countries,"Climate change is becoming a widely recognized risk factor of farmer-herder
conflict in Africa. Using an 8 year dataset (Jan 2015 to Sep 2022) of detailed
weather and terrain data across four African nations, we apply statistical and
machine learning methods to analyze pastoral conflict. We test hypotheses
linking these variables with pastoral conflict within each country using
geospatial and statistical analysis. Complementing this analysis are risk maps
automatically updated for decision-makers. Our models estimate which cells have
a high likelihood of experiencing pastoral conflict with high predictive
accuracy and study the variation of this accuracy with the granularity of the
cells.",2024-12-25T06:37:29Z,http://arxiv.org/abs/2412.18799v1,"Lirika Solaa, Youdinghuan Chen, Samantha K. Murphy, V. S. Subrahmanian"
"Ister: Inverted Seasonal-Trend Decomposition Transformer for Explainable
  Multivariate Time Series Forecasting","In long-term time series forecasting, Transformer-based models have achieved
great success, due to its ability to capture long-range dependencies. However,
existing transformer-based methods face challenges in accurately identifying
which variables play a pivotal role in the prediction process and tend to
overemphasize noisy channels, thereby limiting the interpretability and
practical effectiveness of the models. Besides, it faces scalability issues due
to quadratic computational complexity of self-attention. In this paper, we
propose a new model named Inverted Seasonal-Trend Decomposition Transformer
(Ister), which addresses these challenges in long-term multivariate time series
forecasting by designing an improved Transformer-based structure. Ister firstly
decomposes original time series into seasonal and trend components. Then we
propose a new Dot-attention mechanism to process the seasonal component, which
improves both accuracy, computation complexity and interpretability. Upon
completion of the training phase, it allows users to intuitively visualize the
significance of each feature in the overall prediction. We conduct
comprehensive experiments, and the results show that Ister achieves
state-of-the-art (SOTA) performance on multiple datasets, surpassing existing
models in long-term prediction tasks.",2024-12-25T06:37:19Z,http://arxiv.org/abs/2412.18798v1,"Fanpu Cao, Shu Yang, Zhengjian Chen, Ye Liu, Laizhong Cui"
"DRDM: A Disentangled Representations Diffusion Model for Synthesizing
  Realistic Person Images","Person image synthesis with controllable body poses and appearances is an
essential task owing to the practical needs in the context of virtual try-on,
image editing and video production. However, existing methods face significant
challenges with details missing, limbs distortion and the garment style
deviation. To address these issues, we propose a Disentangled Representations
Diffusion Model (DRDM) to generate photo-realistic images from source portraits
in specific desired poses and appearances. First, a pose encoder is responsible
for encoding pose features into a high-dimensional space to guide the
generation of person images. Second, a body-part subspace decoupling block
(BSDB) disentangles features from the different body parts of a source figure
and feeds them to the various layers of the noise prediction block, thereby
supplying the network with rich disentangled features for generating a
realistic target image. Moreover, during inference, we develop a parsing
map-based disentangled classifier-free guided sampling method, which amplifies
the conditional signals of texture and pose. Extensive experimental results on
the Deepfashion dataset demonstrate the effectiveness of our approach in
achieving pose transfer and appearance control.",2024-12-25T06:36:24Z,http://arxiv.org/abs/2412.18797v1,"Enbo Huang, Yuan Zhang, Faliang Huang, Guangyu Zhang, Yang Liu"
"Unveiling the local elemental arrangements across the interfaces inside
  CdSe/Cd1-xZnxS core-shell and CdSe/CdS/ Cd1-xZnxS core-crown-shell quantum
  wells","We report on a systematic study of the Cd, Zn, Se, and S elemental
distributions across the interfaces in CdSe/Cd1-xZnxS core-shell and
CdSe/CdS/Cd1-xZnxS core-crown-shell quantum wells with the CdSe core thickness
ranging from 3.5 to 5.5 ML. By processing the XAS data, we observe that the
Cd-Se bonds dominate at the CdSe/Cd1-xZnxS core-shell interface of structures
with the 3.5 ML cores, while the Cd-Se bonds were more abundant in the cases of
the 4.5 and 5.5 ML cores. The complementary information about prevailing bonds
were extracted for other constituting elements, thus, describing the
distribution of the elements at the core-shell interface of CdSe-based NPLs.
The naked CdSe cores are covered with an organic shell via bridging oxygen
atoms. Also, we address the issue of stability of such core-shell systems over
the time. We demonstrate that after a half year of aging of the
commercial-ready 4.5 ML CdSe/CdxZn1-xS NPLs, the Cd-Se bonds become more
evident due to the partial degradation of the Cd-S bonds. This is the first
experimental assessment of prevailing interatomic bonds at the core-shell
interface in the CdSe-based NPLs of incremental structural heterogeneity,
providing factual evidences about the elemental arrangement inside the
core-crown-shell NPLs and the growth path of crowns and shells.",2024-12-25T06:28:08Z,http://arxiv.org/abs/2412.18795v1,"Tatiana Lastovina, Oleg Usoltsev, Furkan Isik, Andriy Budnyk, Messaoud Harfouche, Betul Canimkurbey, Hilmi Volkan Demir"
Torque-Aware Momentum,"Efficiently exploring complex loss landscapes is key to the performance of
deep neural networks. While momentum-based optimizers are widely used in
state-of-the-art setups, classical momentum can still struggle with large,
misaligned gradients, leading to oscillations. To address this, we propose
Torque-Aware Momentum (TAM), which introduces a damping factor based on the
angle between the new gradients and previous momentum, stabilizing the update
direction during training. Empirical results show that TAM, which can be
combined with both SGD and Adam, enhances exploration, handles distribution
shifts more effectively, and improves generalization performance across various
tasks, including image classification and large language model fine-tuning,
when compared to classical momentum-based optimizers.",2024-12-25T05:58:07Z,http://arxiv.org/abs/2412.18790v1,"Pranshu Malviya, Goncalo Mordido, Aristide Baratin, Reza Babanezhad Harikandeh, Gintare Karolina Dziugaite, Razvan Pascanu, Sarath Chandar"
On Improved Regret Bounds In Bayesian Optimization with Gaussian Noise,"Bayesian optimization (BO) with Gaussian process (GP) surrogate models is a
powerful black-box optimization method. Acquisition functions are a critical
part of a BO algorithm as they determine how the new samples are selected. Some
of the most widely used acquisition functions include upper confidence bound
(UCB) and Thompson sampling (TS). The convergence analysis of BO algorithms has
focused on the cumulative regret under both the Bayesian and frequentist
settings for the objective. In this paper, we establish new pointwise bounds on
the prediction error of GP under the frequentist setting with Gaussian noise.
Consequently, we prove improved convergence rates of cumulative regret bound
for both GP-UCB and GP-TS. Of note, the new prediction error bound under
Gaussian noise can be applied to general BO algorithms and convergence
analysis, e.g., the asymptotic convergence of expected improvement (EI) with
noise.",2024-12-25T05:57:27Z,http://arxiv.org/abs/2412.18789v1,"Jingyi Wang, Haowei Wang, Cosmin G. Petra, Nai-Yuan Chiang"
"Computational Analysis of Yaredawi YeZema Silt in Ethiopian Orthodox
  Tewahedo Church Chants","Despite its musicological, cultural, and religious significance, the
Ethiopian Orthodox Tewahedo Church (EOTC) chant is relatively underrepresented
in music research. Historical records, including manuscripts, research papers,
and oral traditions, confirm Saint Yared's establishment of three canonical
EOTC chanting modes during the 6th century. This paper attempts to investigate
the EOTC chants using music information retrieval (MIR) techniques. Among the
research questions regarding the analysis and understanding of EOTC chants,
Yaredawi YeZema Silt, namely the mode of chanting adhering to Saint Yared's
standards, is of primary importance. Therefore, we consider the task of
Yaredawi YeZema Silt classification in EOTC chants by introducing a new dataset
and showcasing a series of classification experiments for this task. Results
show that using the distribution of stabilized pitch contours as the feature
representation on a simple neural network-based classifier becomes an effective
solution. The musicological implications and insights of such results are
further discussed through a comparative study with the previous ethnomusicology
literature on EOTC chants. By making this dataset publicly accessible, we aim
to promote future exploration and analysis of EOTC chants and highlight
potential directions for further research, thereby fostering a deeper
understanding and preservation of this unique spiritual and cultural heritage.",2024-12-25T05:42:56Z,http://arxiv.org/abs/2412.18788v1,"Mequanent Argaw Muluneh, Yan-Tsung Peng, Li Su"
"Thermal-Mechanical Physics Informed Deep Learning For Fast Prediction of
  Thermal Stress Evolution in Laser Metal Deposition","Understanding thermal stress evolution in metal additive manufacturing (AM)
is crucial for producing high-quality components. Recent advancements in
machine learning (ML) have shown great potential for modeling complex
multiphysics problems in metal AM. While physics-based simulations face the
challenge of high computational costs, conventional data-driven ML models
require large, labeled training datasets to achieve accurate predictions.
Unfortunately, generating large datasets for ML model training through
time-consuming experiments or high-fidelity simulations is highly expensive in
metal AM. To address these challenges, this study introduces a physics-informed
neural network (PINN) framework that incorporates governing physical laws into
deep neural networks (NNs) to predict temperature and thermal stress evolution
during the laser metal deposition (LMD) process. The study also discusses the
enhanced accuracy and efficiency of the PINN model when supplemented with small
simulation data. Furthermore, it highlights the PINN transferability, enabling
fast predictions with a set of new process parameters using a pre-trained PINN
model as an online soft sensor, significantly reducing computation time
compared to physics-based numerical models while maintaining accuracy.",2024-12-25T05:37:48Z,http://arxiv.org/abs/2412.18786v1,"R. Sharma, Y. B. Guo"
"Simultaneously Recovering Multi-Person Meshes and Multi-View Cameras
  with Human Semantics","Dynamic multi-person mesh recovery has broad applications in sports
broadcasting, virtual reality, and video games. However, current multi-view
frameworks rely on a time-consuming camera calibration procedure. In this work,
we focus on multi-person motion capture with uncalibrated cameras, which mainly
faces two challenges: one is that inter-person interactions and occlusions
introduce inherent ambiguities for both camera calibration and motion capture;
the other is that a lack of dense correspondences can be used to constrain
sparse camera geometries in a dynamic multi-person scene. Our key idea is to
incorporate motion prior knowledge to simultaneously estimate camera parameters
and human meshes from noisy human semantics. We first utilize human information
from 2D images to initialize intrinsic and extrinsic parameters. Thus, the
approach does not rely on any other calibration tools or background features.
Then, a pose-geometry consistency is introduced to associate the detected
humans from different views. Finally, a latent motion prior is proposed to
refine the camera parameters and human motions. Experimental results show that
accurate camera parameters and human motions can be obtained through a one-step
reconstruction. The code are publicly available
at~\url{https://github.com/boycehbz/DMMR}.",2024-12-25T05:35:30Z,http://arxiv.org/abs/2412.18785v1,"Buzhen Huang, Jingyi Ju, Yuan Shu, Yangang Wang"
"Robustness Evaluation of Offline Reinforcement Learning for Robot
  Control Against Action Perturbations","Offline reinforcement learning, which learns solely from datasets without
environmental interaction, has gained attention. This approach, similar to
traditional online deep reinforcement learning, is particularly promising for
robot control applications. Nevertheless, its robustness against real-world
challenges, such as joint actuator faults in robots, remains a critical
concern. This study evaluates the robustness of existing offline reinforcement
learning methods using legged robots from OpenAI Gym based on average episodic
rewards. For robustness evaluation, we simulate failures by incorporating both
random and adversarial perturbations, representing worst-case scenarios, into
the joint torque signals. Our experiments show that existing offline
reinforcement learning methods exhibit significant vulnerabilities to these
action perturbations and are more vulnerable than online reinforcement learning
methods, highlighting the need for more robust approaches in this field.",2024-12-25T05:02:22Z,http://arxiv.org/abs/2412.18781v1,"Shingo Ayabe, Takuto Otomo, Hiroshi Kera, Kazuhiko Kawamoto"
"Skeleton-based Action Recognition with Non-linear Dependency Modeling
  and Hilbert-Schmidt Independence Criterion","Human skeleton-based action recognition has long been an indispensable aspect
of artificial intelligence. Current state-of-the-art methods tend to consider
only the dependencies between connected skeletal joints, limiting their ability
to capture non-linear dependencies between physically distant joints. Moreover,
most existing approaches distinguish action classes by estimating the
probability density of motion representations, yet the high-dimensional nature
of human motions invokes inherent difficulties in accomplishing such
measurements. In this paper, we seek to tackle these challenges from two
directions: (1) We propose a novel dependency refinement approach that
explicitly models dependencies between any pair of joints, effectively
transcending the limitations imposed by joint distance. (2) We further propose
a framework that utilizes the Hilbert-Schmidt Independence Criterion to
differentiate action classes without being affected by data dimensionality, and
mathematically derive learning objectives guaranteeing precise recognition.
Empirically, our approach sets the state-of-the-art performance on NTU RGB+D,
NTU RGB+D 120, and Northwestern-UCLA datasets.",2024-12-25T05:02:11Z,http://arxiv.org/abs/2412.18780v1,Yuheng Yang
"Integrating Zero-Shot Classification to Advance Long COVID Literature: A
  Systematic Social Media-Centered Review","Long COVID continues to challenge public health by affecting a significant
segment of individuals who have recovered from acute SARS-CoV-2 infection yet
endure prolonged and often debilitating symptoms. Social media has emerged as a
vital resource for those seeking real-time information, peer support, and
validating their health concerns related to Long COVID. This paper examines
recent works focusing on mining, analyzing, and interpreting user-generated
content on social media platforms such as Twitter, Reddit, Facebook, and
YouTube to capture the broader discourse on persistent post-COVID conditions. A
novel transformer-based zero-shot learning approach serves as the foundation
for classifying research papers in this area into four primary categories:
Clinical or Symptom Characterization, Advanced NLP or Computational Methods,
Policy, Advocacy, or Public Health Communication, and Online Communities and
Social Support. This methodology showcases the adaptability of advanced
language models in categorizing research papers without predefined training
labels, thus enabling a more rapid and scalable assessment of existing
literature. This review highlights the multifaceted nature of Long COVID
research, where computational techniques applied to social media data reveal
insights into narratives of individuals suffering from Long COVID. This review
also demonstrates the capacity of social media analytics to inform clinical
practice and contribute to policy-making related to Long COVID.",2024-12-25T05:01:17Z,http://arxiv.org/abs/2412.18779v1,Nirmalya Thakur
"Unified Local and Global Attention Interaction Modeling for Vision
  Transformers","We present a novel method that extends the self-attention mechanism of a
vision transformer (ViT) for more accurate object detection across diverse
datasets. ViTs show strong capability for image understanding tasks such as
object detection, segmentation, and classification. This is due in part to
their ability to leverage global information from interactions among visual
tokens. However, the self-attention mechanism in ViTs are limited because they
do not allow visual tokens to exchange local or global information with
neighboring features before computing global attention. This is problematic
because tokens are treated in isolation when attending (matching) to other
tokens, and valuable spatial relationships are overlooked. This isolation is
further compounded by dot-product similarity operations that make tokens from
different semantic classes appear visually similar. To address these
limitations, we introduce two modifications to the traditional self-attention
framework; a novel aggressive convolution pooling strategy for local feature
mixing, and a new conceptual attention transformation to facilitate interaction
and feature exchange between semantic concepts. Experimental results
demonstrate that local and global information exchange among visual features
before self-attention significantly improves performance on challenging object
detection tasks and generalizes across multiple benchmark datasets and
challenging medical datasets. We publish source code and a novel dataset of
cancerous tumors (chimeric cell clusters).",2024-12-25T04:53:19Z,http://arxiv.org/abs/2412.18778v1,"Tan Nguyen, Coy D. Heldermon, Corey Toler-Franklin"
ObitoNet: Multimodal High-Resolution Point Cloud Reconstruction,"ObitoNet employs a Cross Attention mechanism to integrate multimodal inputs,
where Vision Transformers (ViT) extract semantic features from images and a
point cloud tokenizer processes geometric information using Farthest Point
Sampling (FPS) and K Nearest Neighbors (KNN) for spatial structure capture. The
learned multimodal features are fed into a transformer-based decoder for
high-resolution point cloud reconstruction. This approach leverages the
complementary strengths of both modalities rich image features and precise
geometric details ensuring robust point cloud generation even in challenging
conditions such as sparse or noisy data.",2024-12-25T04:34:22Z,http://arxiv.org/abs/2412.18775v1,"Apoorv Thapliyal, Vinay Lanka, Swathi Baskaran"
Learning Broken Symmetries with Approximate Invariance,"Recognizing symmetries in data allows for significant boosts in neural
network training, which is especially important where training data are
limited. In many cases, however, the exact underlying symmetry is present only
in an idealized dataset, and is broken in actual data, due to asymmetries in
the detector, or varying response resolution as a function of particle
momentum. Standard approaches, such as data augmentation or equivariant
networks fail to represent the nature of the full, broken symmetry, effectively
overconstraining the response of the neural network. We propose a learning
model which balances the generality and asymptotic performance of unconstrained
networks with the rapid learning of constrained networks. This is achieved
through a dual-subnet structure, where one network is constrained by the
symmetry and the other is not, along with a learned symmetry factor. In a
simplified toy example that demonstrates violation of Lorentz invariance, our
model learns as rapidly as symmetry-constrained networks but escapes its
performance limitations.",2024-12-25T04:29:04Z,http://arxiv.org/abs/2412.18773v1,"Seth Nabat, Aishik Ghosh, Edmund Witkowski, Gregor Kasieczka, Daniel Whiteson"
Hierarchical Multi-Graphs Learning for Robust Group Re-Identification,"Group Re-identification (G-ReID) faces greater complexity than individual
Re-identification (ReID) due to challenges like mutual occlusion, dynamic
member interactions, and evolving group structures. Prior graph-based
approaches have aimed to capture these dynamics by modeling the group as a
single topological structure. However, these methods struggle to generalize
across diverse group compositions, as they fail to fully represent the
multifaceted relationships within the group.
  In this study, we introduce a Hierarchical Multi-Graphs Learning (HMGL)
framework to address these challenges. Our approach models the group as a
collection of multi-relational graphs, leveraging both explicit features (such
as occlusion, appearance, and foreground information) and implicit dependencies
between members. This hierarchical representation, encoded via a Multi-Graphs
Neural Network (MGNN), allows us to resolve ambiguities in member
relationships, particularly in complex, densely populated scenes. To further
enhance matching accuracy, we propose a Multi-Scale Matching (MSM) algorithm,
which mitigates issues of member information ambiguity and sensitivity to hard
samples, improving robustness in challenging scenarios.
  Our method achieves state-of-the-art performance on two standard benchmarks,
CSG and RoadGroup, with Rank-1/mAP scores of 95.3%/94.4% and 93.9%/95.4%,
respectively. These results mark notable improvements of 1.7% and 2.5% in
Rank-1 accuracy over existing approaches.",2024-12-25T03:33:43Z,http://arxiv.org/abs/2412.18766v1,"Ruiqi Liu, Xingyu Liu, Xiaohao Xu, Yixuan Zhang, Yongxin Ge, Lubin Weng"
"Evaluating authorship disambiguation quality through anomaly analysis on
  researchers' career transition","Authorship disambiguation is crucial for advancing studies in science of
science. However, assessing the quality of authorship disambiguation in
large-scale databases remains challenging since it is difficult to manually
curate a gold-standard dataset that contains disambiguated authors. Through
estimating the timing of when 5.8 million biomedical researchers became
independent Principal Investigators (PIs) with authorship metadata extracted
from the OpenAlex -- the largest open-source bibliometric database -- we
unexpectedly discovered an anomaly: over 60% of researchers appeared as the
last authors in their first career year. We hypothesized that this improbable
finding results from poor name disambiguation, suggesting that such an anomaly
may serve as an indicator of low-quality authorship disambiguation. Our
findings indicated that authors who lack affiliation information, which makes
it more difficult to disambiguate, were far more likely to exhibit this anomaly
compared to those who included their affiliation information. In contrast,
authors with Open Researcher and Contributor ID (ORCID) -- expected to have
higher quality disambiguation -- showed significantly lower anomaly rates. We
further applied this approach to examine the authorship disambiguation quality
by gender over time, and we found that the quality of disambiguation for female
authors was lower than that for male authors before 2010, suggesting that
gender disparity findings based on pre-2010 data may require careful
reexamination. Our results provide a framework for systematically evaluating
authorship disambiguation quality in various contexts, facilitating future
improvements in efforts to authorship disambiguation.",2024-12-25T03:04:46Z,http://arxiv.org/abs/2412.18757v1,"Huaxia Zhou, Mengyi Sun"
"Towards a Statistical Understanding of Neural Networks: Beyond the
  Neural Tangent Kernel Theories","A primary advantage of neural networks lies in their feature learning
characteristics, which is challenging to theoretically analyze due to the
complexity of their training dynamics. We propose a new paradigm for studying
feature learning and the resulting benefits in generalizability. After
reviewing the neural tangent kernel (NTK) theory and recent results in kernel
regression, which address the generalization issue of sufficiently wide neural
networks, we examine limitations and implications of the fixed kernel theory
(as the NTK theory) and review recent theoretical advancements in feature
learning. Moving beyond the fixed kernel/feature theory, we consider neural
networks as adaptive feature models. Finally, we propose an over-parameterized
Gaussian sequence model as a prototype model to study the feature learning
characteristics of neural networks.",2024-12-25T03:03:58Z,http://arxiv.org/abs/2412.18756v1,"Haobo Zhang, Jianfa Lai, Yicheng Li, Qian Lin, Jun S. Liu"
Concentration of ergotropy in many-body systems,"Ergotropy -- the maximal amount of unitarily extractable work -- measures the
``charge level'' of quantum batteries. We prove that in large many-body
batteries ergotropy exhibits a concentration of measure phenomenon. Namely, the
ergotropy of such systems is almost constant for almost all states sampled from
the Hilbert--Schmidt measure. We establish this by first proving that
ergotropy, as a function of the state, is Lipschitz-continuous with respect to
the Bures distance, and then applying Levy's measure concentration lemma. In
parallel, we showcase the analogous properties of von Neumann entropy,
compiling and adapting known results about its continuity and concentration
properties. Furthermore, we consider the situation with the least amount of
prior information about the state. This corresponds to the quantum version of
the Jeffreys prior distribution -- the Bures measure. In this case, there exist
no analytical bounds guaranteeing exponential concentration of measure.
Nonetheless, we provide numerical evidence that ergotropy, as well as von
Neumann entropy, concentrate also in this case.",2024-12-27T18:58:43Z,http://arxiv.org/abs/2412.19801v1,"Karen V. Hovhannisyan, Rick P. A. Simon, Janet Anders"
Entangled dual-comb spectroscopy,"Optical frequency combs have emerged as a cornerstone for a wide range of
areas, including spectroscopy, ranging, optical clocks, time and frequency
transfer, waveform synthesis, and communications. However, quantum mechanical
fluctuations of the optical carrier impose fundamental performance limits on
the precision of traditional classical laser frequency combs, particularly in
their use for interferometry and spectroscopy. Entanglement, as a
quintessential quantum resource, allows for surpassing the fundamental limits
of classical systems. Here, we introduce and experimentally demonstrate
entangled dual-comb spectroscopy (EDCS) that surmounts the fundamental limits
of classical DCS. EDCS builds on tailored entangled spectral structures of the
frequency combs, enabling simultaneous detection of all comb lines below the
standard quantum limit of classical DCS. Applying EDCS in gas detection, we
achieve a 2.6 dB enhancement in signal-to-noise ratio and a 1.7-fold reduction
in integration time over classical DCS, rendering EDCS particularly suited for
dynamic chemical and biological sensing, where fast, precise measurements
subject to power constraints are required. EDCS represents a new paradigm for
quantum frequency combs, underscoring their prospects in a plethora of
applications in precision metrology, spectroscopy, and timekeeping.",2024-12-27T18:57:59Z,http://arxiv.org/abs/2412.19800v1,"Abdulkarim Hariri, Shuai Liu, Haowei Shi, Quntao Zhuang, Xudong Fan, Zheshen Zhang"
Computing Direct Sum Decompositions,"The problems of finding isomorphism classes of indecomposable modules with
certain properties, or determining the indecomposable summands of a module, are
ubiquitous in commutative algebra, group theory, representation theory, and
other fields. The purpose of this work is to describe and prove correctness of
a practical algorithm for computing indecomposable summands of finitely
generated modules over a finitely generated k-algebra, for k a field of
positive characteristic. Our algorithm works over multigraded rings, which
enables the computation of indecomposable summands of coherent sheaves on
subvarieties of toric varieties (in particular, for varieties embedded in
projective space). We also present multiple examples, including some which
present previously unknown phenomena regarding the behavior of summands of
Frobenius pushforwards and syzygies over Artinian rings.",2024-12-27T18:57:35Z,http://arxiv.org/abs/2412.19799v1,"Devlin Mallory, Mahrud Sayrafi"
Non-Scaling Topological Defects and Gravitational Waves in Higgs Portal,"One of the simplest extensions of the Standard Model is the Higgs portal
extension involving a dark Higgs. Dark sectors that include dark matter
candidates, WIMPs, axions, and dark photons, can naturally have this type of
interaction, where the dark Higgs is charged under some symmetry, which may or
may not be spontaneous broken by the vacuum expectation value. In this paper,
using lattice simulations, I show that if the reheating temperature of the
Universe is sufficiently high, topological defects such as domain walls and
cosmic strings associated with these symmetries are naturally formed even if
the symmetries are never restored due to negative thermal mass squareds. This
occurs due to the early Universe's non-adiabatic oscillation of the Higgs
around the onset of oscillation, which overshoots the origin, and tachyonic
instability that enhances fluctuations. The gravitational waves generated by
these topological defects may be very significant due to the energetic
processes induced by matter effects in the hot and dense Universe irrelevant to
the typical energy scale of the dark sector in the vacuum or whether the
symmetry is broken in the vacuum. Alongside earlier studies such as usual phase
transition, melting domain walls and melting cosmic strings scenarios that
assume a symmetric phase in the early Universe, the Higgs portal models
naturally predict local overdensities from topological defects, which may
induce miniclusters and primordial black holes, as well as the gravitational
waves.These phenomena provide novel opportunities to search for such scenarios.
I also perform various numerical simulations for the relevant topic including
melting domain walls and cosmic strings with inflationary and Gaussian
fluctuations, for comparison--which have not been performed previously.",2024-12-27T18:57:22Z,http://arxiv.org/abs/2412.19798v1,Wen Yin
"Streamlined Krylov construction and classification of ergodic Floquet
  systems","We generalize Krylov construction to periodically driven (Floquet) quantum
systems using the theory of orthogonal polynomials on the unit circle. Compared
to other approaches, our method works faster and maps any quantum dynamics to a
one-dimensional tight-binding Krylov chain. We also suggest a classification of
chaotic and integrable Floquet systems based on the asymptotic behavior of
Krylov chain hopping parameters (Verblunsky coefficients). We illustrate this
classification with random matrix ensembles, kicked top, and kicked Ising
chain.",2024-12-27T18:56:27Z,http://arxiv.org/abs/2412.19797v1,"Nikita Kolganov, Dmitrii A. Trunin"
"Generalized Grade-of-Membership Estimation for High-dimensional Locally
  Dependent Data","This work focuses on the mixed membership models for multivariate categorical
data widely used for analyzing survey responses and population genetics data.
These grade of membership (GoM) models offer rich modeling power but present
significant estimation challenges for high-dimensional polytomous data. Popular
existing approaches, such as Bayesian MCMC inference, are not scalable and lack
theoretical guarantees in high-dimensional settings. To address this, we first
observe that data from this model can be reformulated as a three-way
(quasi-)tensor, with many subjects responding to many items with varying
numbers of categories. We introduce a novel and simple approach that flattens
the three-way quasi-tensor into a ""fat"" matrix, and then perform a singular
value decomposition of it to estimate parameters by exploiting the singular
subspace geometry. Our fast spectral method can accommodate a broad range of
data distributions with arbitrarily locally dependent noise, which we formalize
as the generalized-GoM models. We establish finite-sample entrywise error
bounds for the generalized-GoM model parameters. This is supported by a new
sharp two-to-infinity singular subspace perturbation theory for locally
dependent and flexibly distributed noise, a contribution of independent
interest. Simulations and applications to data in political surveys, population
genetics, and single-cell sequencing demonstrate our method's superior
performance.",2024-12-27T18:51:15Z,http://arxiv.org/abs/2412.19796v1,"Ling Chen, Chengzhu Huang, Yuqi Gu"
"g-factor theory of Si/SiGe quantum dots: spin-valley and giant
  renormalization effects","Understanding the $g$-factor physics of Si/SiGe quantum dots is crucial for
realizing high-quality spin qubits. While previous work has explained some
aspects of $g$-factor physics in idealized geometries, the results do not
extend to general cases and they miss several important features. Here, we
construct a theory that gives $g$ in terms of readily computable matrix
elements, and can be applied to all Si/SiGe heterostructures of current
interest. As a concrete example, which currently has no $g$-factor
understanding, we study the so-called Wiggle Well structure, containing Ge
concentration oscillations inside the quantum well. Here we find a significant
renormalization of the $g$-factor compared to conventional Si/SiGe quantum
wells. We also uncover a giant $g$-factor suppression of order
$\mathcal{O}(1)$, which arises due to spin-valley coupling, and occurs at
locations of low valley splitting. Our work therefore opens up new avenues for
$g$-factor engineering in Si/SiGe quantum dots.",2024-12-27T18:50:38Z,http://arxiv.org/abs/2412.19795v1,"Benjamin D. Woods, Merritt P. Losert, Robert Joynt, Mark Friesen"
InfAlign: Inference-aware language model alignment,"Language model alignment has become a critical step in training modern
generative language models. The goal of alignment is to finetune a reference
model such that the win rate of a sample from the aligned model over a sample
from the reference model is high, subject to a KL divergence constraint. Today,
we are increasingly using inference-time algorithms (e.g., Best-of-N,
controlled decoding, tree search) to decode from language models rather than
standard sampling. However, the alignment objective does not capture such
inference-time decoding procedures. We show that the existing alignment
framework is sub-optimal in view of such inference-time methods. We then modify
the alignment objective and propose a framework for inference-aware alignment
(IAPO). We prove that for any inference-time decoding algorithm, the optimal
solution that optimizes the inference-time win rate of the aligned policy
against the reference policy is the solution to the typical RLHF problem with a
transformation of the reward. This motivates us to provide the KL-regularized
calibrate-and-transform RL (CTRL) algorithm to solve this problem, which
involves a reward calibration step and a KL-regularized reward maximization
step with a transformation of the calibrated reward. We particularize our study
to two important inference-time strategies: best-of-N sampling and best-of-N
jailbreaking, where N responses are sampled from the model and the one with the
highest or lowest reward is selected. We propose specific transformations for
these strategies and demonstrate that our framework offers significant
improvements over existing state-of-the-art methods for language model
alignment. Empirically, we outperform baselines that are designed without
taking inference-time decoding into consideration by 8-12% and 4-9% on
inference-time win rates over the Anthropic helpfulness and harmlessness dialog
benchmark datasets.",2024-12-27T18:45:36Z,http://arxiv.org/abs/2412.19792v1,"Ananth Balashankar, Ziteng Sun, Jonathan Berant, Jacob Eisenstein, Michael Collins, Adrian Hutter, Jong Lee, Chirag Nagpal, Flavien Prost, Aradhana Sinha, and Ananda Theertha Suresh, Ahmad Beirami"
"Local Characteristic Decomposition of Equilibrium Variables for
  Hyperbolic Systems of Balance Laws","This paper is concerned with high-order numerical methods for hyperbolic
systems of balance laws. Such methods are typically based on high-order
piecewise polynomial reconstructions (interpolations) of the computed discrete
quantities. However, such reconstructions (interpolations) may be oscillatory
unless the reconstruction (interpolation) procedure is applied to the local
characteristic variables via the local characteristic decomposition (LCD).
Another challenge in designing accurate and stable high-order schemes is
related to enforcing a delicate balance between the fluxes, sources, and
nonconservative product terms: a good scheme should be well-balanced (WB) in
the sense that it should be capable of exactly preserving certain (physically
relevant) steady states. One of the ways to ensure that the reconstruction
(interpolation) preserves these steady states is to apply the reconstruction
(interpolation) to the equilibrium variables, which are supposed to be constant
at the steady states. To achieve this goal and to keep the reconstruction
(interpolation) non-oscillatory, we introduce a new LCD of equilibrium
variables. We apply the developed technique to the fifth-order Ai-WENO-Z
interpolation implemented within the WB A-WENO framework recently introduced in
[S. Chu, A. Kurganov, and R. Xin, Beijing J. of Pure and Appl. Math., to
appear], and illustrate its performance on a variety of numerical examples.",2024-12-27T18:45:23Z,http://arxiv.org/abs/2412.19791v1,"Shaoshuai Chu, Alexander Kurganov, Mingye Na, Ruixiao Xin"
"Bottom-up robust modeling for the foraging behavior of Physarum
  polycephalum","The true slime mold \textit{Physarum polycephalum} has the remarkable
capability to perform self-organized activities such as network formation among
food sources. Despite well reproducing the emergence of slime networks,
existing models are limited in the investigation of the minimal mechanisms, at
the microscopic scale, that ensure robust problem-solving capabilities at the
macroscopic scale. To this end, we develop three progressively more complex
multi-agent models to provide a flexible framework to understand the
self-organized foraging and network formation behaviors of \textit{Physarum}.
The hierarchy of models allows for a stepwise investigation of the minimal set
of rules that allow bio-inspired computing agents to achieve the desired
behaviors on nutrient-poor substrates. By introducing a quantitative measure of
connectedness among food sources, we assess the sensitivity of the model to
user-defined and bio-inspired parameters, as well as the robustness of the
model to parameter heterogeneity across agents. We ultimately observe the
robust emergence of pattern formation, in line with experimental evidence.
Overall, our study sheds light onto the basic mechanisms of self-organization
and paves the way towards the development of decentralized strategies for
network formation in engineered systems, focusing on trade-offs between
biological fidelity and computational efficiency.",2024-12-27T18:44:14Z,http://arxiv.org/abs/2412.19790v1,"Damiano Reginato, Daniele Proverbio, Giulia Giordano"
Transmon qutrit-based simulation of spin-1 AKLT systems,"Qutrit-based quantum circuits could help reduce the overall circuit depths,
and hence the noise, when the system of interest has local dimension of three.
Accessing second excited states in superconducting transmons provides a
straightforward hardware realization of three-level systems, or qutrits, useful
for such ternary encoding. In this work, we successfully calibrate pulse gates
to a low error rate to obtain transmon qutrits. We use these qutrits to
simulate one-dimensional spin-1 AKLT states (Affleck, Lieb, Kennedy, and
Tasaki) which exhibit a multitude of interesting phenomena such as
topologically protected ground states, string order and the existence of a
robust Berry phase. We demonstrate the efficacy of qutrit-based simulation by
preparing high-fidelity ground states of the AKLT Hamiltonian with open
boundaries for various chain lengths. We then use ground state preparations of
the perturbed AKLT Hamiltonian with periodic boundaries to calculate the Berry
phase and illustrate non-trivial ground state topology. We further present
scalable methods for preparing the AKLT state and computing its Berry phase,
and use tensor network simulations to establish the advantage of qutrit-based
implementation over qubit-based ones in the presence of noise. Our work
provides a pathway toward more general spin-1 physics simulations using
transmon qutrits, with applications in chemistry, magnetism, and topological
phases of matter.",2024-12-27T18:32:39Z,http://arxiv.org/abs/2412.19786v1,"Keerthi Kumaran, Faisal Alam, Norhan Eassa, Kaelyn Ferris, Xiao Xiao, Lukasz Cincio, Nicholas Bronn, Arnab Banerjee"
"Enhancing Whisper's Accuracy and Speed for Indian Languages through
  Prompt-Tuning and Tokenization","Automatic speech recognition has recently seen a significant advancement with
large foundational models such as Whisper. However, these models often struggle
to perform well in low-resource languages, such as Indian languages. This paper
explores two novel approaches to enhance Whisper's multilingual speech
recognition performance in Indian languages. First, we propose prompt-tuning
with language family information, which enhances Whisper's accuracy in
linguistically similar languages. Second, we introduce a novel tokenizer that
reduces the number of generated tokens, thereby accelerating Whisper's
inference speed. Our extensive experiments demonstrate that the tokenizer
significantly reduces inference time, while prompt-tuning enhances accuracy
across various Whisper model sizes, including Small, Medium, and Large.
Together, these techniques achieve a balance between optimal WER and inference
speed.",2024-12-27T18:32:24Z,http://arxiv.org/abs/2412.19785v1,"Kumud Tripathi, Raj Gothi, Pankaj Wasnik"
Can AI Help with Your Personal Finances?,"In recent years, Large Language Models (LLMs) have emerged as a
transformative development in artificial intelligence (AI), drawing significant
attention from industry and academia. Trained on vast datasets, these
sophisticated AI systems exhibit impressive natural language processing and
content generation capabilities. This paper explores the potential of LLMs to
address key challenges in personal finance, focusing on the United States. We
evaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini,
Anthropic's Claude, and Meta's Llama, to assess their effectiveness in
providing accurate financial advice on topics such as mortgages, taxes, loans,
and investments. Our findings show that while these models achieve an average
accuracy rate of approximately 70%, they also display notable limitations in
certain areas. Specifically, LLMs struggle to provide accurate responses for
complex financial queries, with performance varying significantly across
different topics. Despite these limitations, the analysis reveals notable
improvements in newer versions of these models, highlighting their growing
utility for individuals and financial advisors. As these AI systems continue to
evolve, their potential for advancing AI-driven applications in personal
finance becomes increasingly promising.",2024-12-27T18:25:27Z,http://arxiv.org/abs/2412.19784v1,"Oudom Hean, Utsha Saha, Binita Saha"
A note on the log-concavity of parking functions,"We settle a conjecture of B\'ona regarding the log-concavity of a certain
statistic on parking functions by utilizing recent log-concavity results on
matroids. This result allows us to also prove that connected, labeled graphs
graded by their number of edges are log-concave. Furthermore, we generalize
these results to $G$-parking functions.",2024-12-27T18:25:24Z,http://arxiv.org/abs/2412.19783v1,Joseph Pappe
Cosmological perturbations meet Wheeler DeWitt,"We study solutions of the Wheeler DeWitt (WdW) equation in order to recover
standard results of cosmological perturbation theory. In mini-superspace, we
introduce a dimensionless gravitational coupling $\alpha$ that is typically
very small and functions like $\hbar$ in a WKB expansion. We seek solutions of
the form $\Psi = e^{iS/\alpha} \psi$ that are the closest quantum analog of a
given classical background spacetime. The function $S$ satisfies the
Hamilton-Jacobi equation, while $\psi$ obeys a Schr\""odinger-like equation and
has a clear probabilistic interpretation. By using the semiclassical limit we
express the relation between $\psi$ and the wavefunction of the universe in
perturbation theory, $\psi_P$. We apply our formalism to two main examples. The
first is a scalar field with a purely exponential potential, of which
particularly simple, scaling solutions are known. The other is a slow-roll
scenario expanded in the vicinity of the origin in field space. We discuss
possible deviations from the classical background trajectory as well as the
higher ``time"" derivative terms that are present in the WdW equation but not in
the perturbative approach. We clarify the conditional probability content of
the wavefunctions and how this is related with the standard gauge fixing
procedure in perturbation theory.",2024-12-27T18:25:17Z,http://arxiv.org/abs/2412.19782v1,"Federico Piazza, Siméon Vareilles"
"Machine Learning for Sentiment Analysis of Imported Food in Trinidad and
  Tobago","This research investigates the performance of various machine learning
algorithms (CNN, LSTM, VADER, and RoBERTa) for sentiment analysis of Twitter
data related to imported food items in Trinidad and Tobago. The study addresses
three primary research questions: the comparative accuracy and efficiency of
the algorithms, the optimal configurations for each model, and the potential
applications of the optimized models in a live system for monitoring public
sentiment and its impact on the import bill. The dataset comprises tweets from
2018 to 2024, divided into imbalanced, balanced, and temporal subsets to assess
the impact of data balancing and the COVID-19 pandemic on sentiment trends. Ten
experiments were conducted to evaluate the models under various configurations.
Results indicated that VADER outperformed the other models in both multi-class
and binary sentiment classifications. The study highlights significant changes
in sentiment trends pre- and post-COVID-19, with implications for import
policies.",2024-12-27T18:25:08Z,http://arxiv.org/abs/2412.19781v1,"Cassandra Daniels, Koffka Khan"
Tensor Network Estimation of Distribution Algorithms,"Tensor networks are a tool first employed in the context of many-body quantum
physics that now have a wide range of uses across the computational sciences,
from numerical methods to machine learning. Methods integrating tensor networks
into evolutionary optimization algorithms have appeared in the recent
literature. In essence, these methods can be understood as replacing the
traditional crossover operation of a genetic algorithm with a tensor
network-based generative model. We investigate these methods from the point of
view that they are Estimation of Distribution Algorithms (EDAs). We find that
optimization performance of these methods is not related to the power of the
generative model in a straightforward way. Generative models that are better
(in the sense that they better model the distribution from which their training
data is drawn) do not necessarily result in better performance of the
optimization algorithm they form a part of. This raises the question of how
best to incorporate powerful generative models into optimization routines. In
light of this we find that adding an explicit mutation operator to the output
of the generative model often improves optimization performance.",2024-12-27T18:22:47Z,http://arxiv.org/abs/2412.19780v1,"John Gardiner, Javier Lopez-Piqueres"
Extended Set Difference : Inverse Operation of Minkowski Summation,"This paper introduces the extended set difference, a generalization of the
Hukuhara and generalized Hukuhara differences, defined for compact convex sets
in $R^d$. The proposed difference guarantees the existence and uniqueness for
any pair of such sets, offering a broader framework for set arithmetic. The
paper also explores the properties of this new difference.",2024-12-27T18:19:27Z,http://arxiv.org/abs/2412.19779v1,"Arie Beresteanu, Behrooz Moosavi Ramezanzadeh"
"Application of normalizing flows to nuclear many-body perturbation
  theory","Many-body perturbation theory provides a powerful framework to study the
ground state and thermodynamic properties of nuclear matter as well as
associated single-particle potentials and response functions within a
systematic order-by-order expansion. However, computational challenges can
emerge beyond the lowest orders of perturbation theory, especially when
computing both single-particle potentials and response functions, which in
general are complex-valued and require Cauchy principal value calculations of
high-dimensional integrals. We demonstrate that normalizing flows are suitable
for Monte Carlo importance sampling of both regular and irregular functions
appearing in nuclear many-body calculations. Normalizing flows are a class of
machine learning models that can be used to build and sample from complicated
distributions through a bijective mapping from a simple base distribution.
Furthermore, a well-trained model for a certain target integrand can be
efficiently transferred to calculate related integrals with varying physical
conditions. These features can enable more efficient tabulations of nuclear
physics inputs to numerical simulations of supernovae and neutron star mergers
across varying physical conditions and nuclear force models.",2024-12-27T18:18:20Z,http://arxiv.org/abs/2412.19777v1,"Pengsheng Wen, Jeremy W. Holt, Albany Blackburn"
"On the numerical solution of Lasserre relaxations of unconstrained
  binary quadratic optimization problem","The aim of this paper is to solve linear semidefinite programs arising from
higher-order Lasserre relaxations of unconstrained binary quadratic
optimization problems. For this we use an interior point method with a
preconditioned conjugate gradient method solving the linear systems. The
preconditioner utilizes the low-rank structure of the solution of the
relaxations. In order to fully exploit this, we need to re-write the moment
relaxations. To treat the arising linear equality constraints we use an
$\ell_1$-penalty approach within the interior-point solver. The efficiency of
this approach is demonstrated by numerical experiments with the MAXCUT and
other randomly generated problems and a comparison with a state-of-the-art
semidefinite solver and the ADMM method. We further propose a hybrid
ADMM-interior-point method that proves to be efficient for certain problem
classes. As a by-product, we observe that the second-order relaxation is often
high enough to deliver a globally optimal solution of the original problem.",2024-12-27T18:17:45Z,http://arxiv.org/abs/2412.19776v1,"Soodeh Habibi, Michal Kocvara, Michael Stingl"
"On the uplift of 4D wormholes in Braneworld models and their 5D
  structure","Recent developments in the consistent embedding of general 4D static and
spherically-symmetric spacetimes in arbitrary single-brane braneworld models
[Phys.Rev.D 109 (2024) 4, L041501] initiated the program of studying the bulk
structure of braneworld wormholes. In this article, adopting a completely
generic approach, we derive the general conditions that the metric functions of
any braneworld spacetime must satisfy to describe a wormhole structure in the
bulk. Particular emphasis is placed on clarifying the proper uplift of 4D
wormholes, expressed in terms of various radial coordinates on the brane, and
we demonstrate the important role of the circumferential radius metric function
for the embedding. Additionally, the flare-out conditions for braneworld
wormholes are presented for the first time and are found to differ from the
case of flat extra dimensions. To illustrate the method, we first perform the
uplift into the Randall-Sundrum II braneworld model for three well-known 4D
wormhole spacetimes; the effective braneworld wormhole solutions of
Casadio-Fabbri-Mazzacurati and Bronnikov-Kim, and the Simpson-Visser spacetime.
Subsequently, we study their bulk features by means of curvature invariants,
flare-out conditions, energy conditions and embedding diagrams. Our analysis
reveals that the assumption of a warped extra dimension has non-trivial
implications for the structure of 5D wormholes.",2024-12-27T18:12:03Z,http://arxiv.org/abs/2412.19773v1,"Thomas Pappas, Theodoros Nakas"
Direct estimates of irreversibility from time series,"The arrow of time can be quantified through the Kullback-Leibler divergence
($D_{KL}$) between the distributions of forward and reverse trajectories in a
system. Many approaches to estimate this rely on specific models, but the use
of incorrect models can introduce uncontrolled errors. Here, we describe a
model-free method that uses trajectory data directly to estimate the evidence
for irreversibility over finite windows of time. To do this we build on
previous work to identify and correct for errors that arise from limited sample
size. Importantly, our approach accurately recovers $D_{KL} = 0$ in systems
that adhere to detailed balance, and the correct nonzero $D_{KL}$ for data
generated by well understood models of nonequilibrium systems. We apply our
method to trajectories of neural activity in the retina as it responds to
naturalistic inputs, and find evidence of irreversibility in single neurons,
emphasizing the non-Markovian character of these data. These results open new
avenues for investigating how the brain represents the arrow of time.",2024-12-27T18:10:53Z,http://arxiv.org/abs/2412.19772v1,"Trevor GrandPre, Gianluca Teza, William Bialek"
"Fortran2CPP: Automating Fortran-to-C++ Migration using LLMs via
  Multi-Turn Dialogue and Dual-Agent Integration","Migrating Fortran code to C++ is a common task for many scientific computing
teams, driven by the need to leverage modern programming paradigms, enhance
cross-platform compatibility, and improve maintainability. Automating this
translation process using large language models (LLMs) has shown promise, but
the lack of high-quality, specialized datasets has hindered their
effectiveness. In this paper, we address this challenge by introducing a novel
multi-turn dialogue dataset, Fortran2CPP, specifically designed for
Fortran-to-C++ code migration. Our dataset, significantly larger than existing
alternatives, is generated using a unique LLM-driven, dual-agent pipeline
incorporating iterative compilation, execution, and code repair to ensure high
quality and functional correctness. To demonstrate the effectiveness of our
dataset, we fine-tuned several open-weight LLMs on Fortran2CPP and evaluated
their performance on two independent benchmarks. Fine-tuning on our dataset led
to remarkable gains, with models achieving up to a 3.31x increase in CodeBLEU
score and a 92\% improvement in compilation success rate. This highlights the
dataset's ability to enhance both the syntactic accuracy and compilability of
the translated C++ code. Our dataset and model have been open-sourced and are
available on our public GitHub
repository\footnote{\url{https://github.com/HPC-Fortran2CPP/Fortran2Cpp}}.",2024-12-27T18:06:25Z,http://arxiv.org/abs/2412.19770v1,"Le Chen, Bin Lei, Dunzhi Zhou, Pei-Hung Lin, Chunhua Liao, Caiwen Ding, Ali Jannesari"
"Teaching materials aligned or unaligned with the principles of the
  Cognitive Theory of Multimedia Learning: the choices made by Physics teachers
  and students","In a recent study [Rev. Bras. Ens. F\'is. vol. 45, 2023], the absence of the
Cognitive Theory of Multimedia Learning (CTML) in the curricula of Physics
teacher education programs at Brazilian public universities was highlighted.
Considering this gap, the present study investigates whether, even without any
formal prior knowledge of CTML principles (Coherence, Signaling, Spatial
Contiguity, Segmentation, Multimedia, and Personalization), Physics teacher
trainees and educators tend to choose, among two formats of multimedia
materials - one aligned with a given CTML principle and the other not - the
materials aligned with these principles. The findings of this case study
revealed that, although most participants generally selected materials aligned
with the mentioned principles, a significant portion did not. These results
underscore the importance of Brazilian universities considering the inclusion
of CTML in Physics teacher education curricula.",2024-12-27T18:02:16Z,http://arxiv.org/abs/2412.19768v1,"Aline N. Braga, Antonio A. M. Neto, Alessandra N. Braga, Silvio C. F. Pereira Filho, Nelson P. C. de Souza, Danilo T. Alves"
"On Universally Free First-Order Extensions of Belnap-Dunn's Four-Valued
  Logic and Nelson's Paraconsistent Logic N4","The aim of this paper is to introduce the logics FFDE and FN4, which are
universally free versions of Belnap-Dunn's four-valued logic, also known as the
logic of first-degree entailment (FDE), and Nelson's paraconsistent logic QN4
(N-). Both FDE and QN4 are suitable to be interpreted as information-based
logics, that is, logics that are capable of representing the deductive behavior
of possibly inconsistent and incomplete information in a database. Like QN4 and
some non-free first-order extensions of FDE, FFDE and FN4 are endowed with
Kripke-style variable domain semantics, which allows representing the dynamic
aspect of information processing, that is, how a database receives new
information over time, including information about new individuals. We argue,
however, that FFDE and FN4 can better represent the development of inconsistent
and incomplete information states (i.e., configurations of a database) over
time than their non-free versions. First, because they allow for empty domains,
which corresponds to the idea that a database may acknowledge no individual at
all at an early stage of its development. Second, because they allow for empty
names, which get interpreted as information about new individuals is inserted
into the database. Also, both systems include an identity predicate that is
interpreted along the same lines of the other logical operators, viz., in terms
of independent positive and negative rules.",2024-12-27T17:57:18Z,http://arxiv.org/abs/2412.19767v1,"Henrique Antunes, Abilio Rodrigues"
Generative Video Propagation,"Large-scale video generation models have the inherent ability to
realistically model natural scenes. In this paper, we demonstrate that through
a careful design of a generative video propagation framework, various video
tasks can be addressed in a unified way by leveraging the generative power of
such models. Specifically, our framework, GenProp, encodes the original video
with a selective content encoder and propagates the changes made to the first
frame using an image-to-video generation model. We propose a data generation
scheme to cover multiple video tasks based on instance-level video segmentation
datasets. Our model is trained by incorporating a mask prediction decoder head
and optimizing a region-aware loss to aid the encoder to preserve the original
content while the generation model propagates the modified region. This novel
design opens up new possibilities: In editing scenarios, GenProp allows
substantial changes to an object's shape; for insertion, the inserted objects
can exhibit independent motion; for removal, GenProp effectively removes
effects like shadows and reflections from the whole video; for tracking,
GenProp is capable of tracking objects and their associated effects together.
Experiment results demonstrate the leading performance of our model in various
video tasks, and we further provide in-depth analyses of the proposed
framework.",2024-12-27T17:42:29Z,http://arxiv.org/abs/2412.19761v1,"Shaoteng Liu, Tianyu Wang, Jui-Hsien Wang, Qing Liu, Zhifei Zhang, Joon-Young Lee, Yijun Li, Bei Yu, Zhe Lin, Soo Ye Kim, Jiaya Jia"
The Hole Argument for Reference Frames,"We exploit the results of Bamonti and Gomes (2024) concerning the dynamical
(un)coupling of reference frames to gravity to analyse the role of reference
frames in the Hole Argument. We introduce a new possible threat to determinism,
which we call Arbitrariness Problem (ARB), resulting from the inherent freedom
in selecting a reference frame.",2024-12-27T17:41:52Z,http://arxiv.org/abs/2412.19760v1,"Nicola Bamonti, Henrique Gomes"
Nonlinear trident using WKB and worldline instantons,"We consider nonlinear trident, $e^{\scriptscriptstyle -}\to
e^{\scriptscriptstyle -} e^{\scriptscriptstyle -} e^{\scriptscriptstyle +}$, in
various electric background fields. This process has so far been studied for
plane-wave backgrounds, using Volkov solutions. Here we first use WKB for
trident in time-dependent electric fields, and then for fields which vary
slowly in space. Then we show how to use worldline instantons for more general
fields which depend on both time and space.",2024-12-27T17:39:55Z,http://arxiv.org/abs/2412.19758v1,"Gianluca Degli Esposti, Greger Torgrimsson"
"""Did my figure do justice to the answer?"" : Towards Multimodal Short
  Answer Grading with Feedback (MMSAF)","Personalized feedback plays a vital role in a student's learning process.
While existing systems are adept at providing feedback over MCQ-based
evaluation, this work focuses more on subjective and open-ended questions,
which is similar to the problem of Automatic Short Answer Grading (ASAG) with
feedback. Additionally, we introduce the Multimodal Short Answer grading with
Feedback (MMSAF) problem over the traditional ASAG feedback problem to address
the scenario where the student answer and reference answer might contain
images. Moreover, we introduce the MMSAF dataset with 2197 data points along
with an automated framework for generating such data sets. Our evaluations on
existing LLMs over this dataset achieved an overall accuracy of 55\% on Level
of Correctness labels, 75\% on Image Relevance labels and a score of 4.27 out
of 5 in correctness level of LLM generated feedback as rated by experts. As per
experts, Pixtral achieved a rating of above 4 out of all metrics, indicating
that it is more aligned to human judgement, and that it is the best solution
for assisting students.",2024-12-27T17:33:39Z,http://arxiv.org/abs/2412.19755v1,"Pritam Sil, Bhaskaran Raman, Pushpak Bhattacharyya"
Complement or substitute? How AI increases the demand for human skills,"The question of whether AI substitutes or complements human work is central
to debates on the future of work. This paper examines the impact of AI on skill
demand and compensation in the U.S. economy, analysing 12 million online job
vacancies from 2018 to 2023. It investigates internal effects (within-job
substitution and complementation) and external effects (across occupations,
industries, and regions). Our findings reveal a significant increase in demand
for AI-complementary skills, such as digital literacy, teamwork, and
resilience, alongside rising wage premiums for these skills in AI roles like
Data Scientist. Conversely, substitute skills, including customer service and
text review, have declined in both demand and value within AI-related
positions. Examining external effects, we find a notable rise in demand for
complementary skills in non-AI roles linked to the growth of AI-related jobs in
specific industries or regions. At the same time, there is a moderate decline
in non-AI roles requiring substitute skills. Overall, AI's complementary effect
is up to 50% larger than its substitution effect, resulting in net positive
demand for skills. These results, replicated for the UK and Australia,
highlight AI's transformative impact on workforce skill requirements. They
suggest reskilling efforts should prioritise not only technical AI skills but
also complementary skills like ethics and digital literacy.",2024-12-27T17:26:30Z,http://arxiv.org/abs/2412.19754v1,"Elina Mäkelä, Fabian Stephany"
Right-handed neutrino production through first-generation leptoquarks,"The collider phenomenology of leptoquarks (LQs) and right-handed neutrinos
(RHNs) has been studied extensively in the literature. Because of the gauge
singlet nature, the production of RHNs at the LHC is typically suppressed by
the tiny light-heavy neutrino mixing angles. In this study, we explore a
promising scenario where the presence of an LQ mediator significantly enhances
RHN production. We focus on first-generation scalar and vector LQs interacting
with the first-generation RHN. The prospects are better for the
first-generation scenario than the other generations because of the enhanced
parton distribution functions (PDFs) of first-generation quarks. The enhanced
PDFs boost the production cross sections of LQs, particularly their single and
indirect productions. Incorporating all production modes of LQs that result in
a pair of RHNs, we estimate the discovery prospects by analysing the
monoelectron and dielectron channels arising from the decay of the RHN pair. We
find that the indirect production of LQs is crucial in determining the
discovery reach at the HL-LHC for the first-generation scenario.",2024-12-27T17:20:55Z,http://arxiv.org/abs/2412.19751v1,"Gokul Duraikandan, Rishabh Khanna, Tanumoy Mandal, Subhadip Mitra, Rachit Sharma"
"IMAGINE: An 8-to-1b 22nm FD-SOI Compute-In-Memory CNN Accelerator With
  an End-to-End Analog Charge-Based 0.15-8POPS/W Macro Featuring
  Distribution-Aware Data Reshaping","Charge-domain compute-in-memory (CIM) SRAMs have recently become an enticing
compromise between computing efficiency and accuracy to process sub-8b
convolutional neural networks (CNNs) at the edge. Yet, they commonly make use
of a fixed dot-product (DP) voltage swing, which leads to a loss in effective
ADC bits due to data-dependent clipping or truncation effects that waste
precious conversion energy and computing accuracy. To overcome this, we present
IMAGINE, a workload-adaptive 1-to-8b CIM-CNN accelerator in 22nm FD-SOI. It
introduces a 1152x256 end-to-end charge-based macro with a multi-bit DP based
on an input-serial, weight-parallel accumulation that avoids power-hungry DACs.
An adaptive swing is achieved by combining a channel-wise DP array split with a
linear in-ADC implementation of analog batch-normalization (ABN), obtaining a
distribution-aware data reshaping. Critical design constraints are relaxed by
including the post-silicon equivalent noise within a CIM-aware CNN training
framework. Measurement results showcase an 8b system-level energy efficiency of
40TOPS/W at 0.3/0.6V, with competitive accuracies on MNIST and CIFAR-10.
Moreover, the peak energy and area efficiencies of the 187kB/mm2 macro
respectively reach up to 0.15-8POPS/W and 2.6-154TOPS/mm2, scaling with the
8-to-1b computing precision. These results exceed previous charge-based designs
by 3-to-5x while being the first work to provide linear in-memory rescaling.",2024-12-27T17:18:15Z,http://arxiv.org/abs/2412.19750v1,"Adrian Kneip, Martin Lefebvre, Pol Maistriaux, David Bol"
"UAV-Enabled Secure ISAC Against Dual Eavesdropping Threats: Joint
  Beamforming and Trajectory Design","In this work, we study an unmanned aerial vehicle (UAV)-enabled secure
integrated sensing and communication (ISAC) system, where a UAV serves as an
aerial base station (BS) to simultaneously perform communication with a user
and detect a target on the ground, while a dual-functional eavesdropper
attempts to intercept the signals for both sensing and communication. Facing
the dual eavesdropping threats, we aim to enhance the average achievable
secrecy rate for the communication user by jointly designing the UAV trajectory
together with the transmit information and sensing beamforming, while
satisfying the requirements on sensing performance and sensing security, as
well as the UAV power and flight constraints. To address the non-convex nature
of the optimization problem, we employ the alternating optimization (AO)
strategy, jointly with the successive convex approximation (SCA) and
semidefinite relaxation (SDR) methods. Numerical results validate the proposed
approach, demonstrating its ability to achieve a high secrecy rate while
meeting the required sensing and security constraints.",2024-12-27T17:15:20Z,http://arxiv.org/abs/2412.19748v1,"Jianping Yao, Zeyu Yang, Zai Yang, Jie Xu, Tony Q. S. Quek"
"AAM-SEALS: Developing Aerial-Aquatic Manipulators in SEa, Air, and Land
  Simulator","Current simulators lack the ability to accurately model integrated
environments that encompass sea, air, and land. To address this gap, we
introduce Aerial-Aquatic Manipulators (AAMs) in SEa, Air, and Land Simulator
(SEALS), a comprehensive and photorealistic simulator designed for AAMs to
operate and learn in these diverse environments. The development of AAM-SEALS
tackles several significant challenges, including the creation of integrated
controllers for flying, swimming, and manipulation, and the high-fidelity
simulation of aerial dynamics and hydrodynamics leveraging particle physics.
Our evaluation demonstrates smooth operation and photorealistic transitions
across air, water, and their interfaces. We quantitatively validate the
fidelity of particle-based hydrodynamics by comparing position-tracking errors
across real-world and simulated systems. AAM-SEALS promises to benefit a broad
range of robotics communities, including robot learning, aerial robotics,
underwater robotics, mobile manipulation, and robotic simulators. We will
open-source our code and data to foster the advancement of research in these
fields. Please access our project website at: https:
//aam-seals.github.io/aam-seals-v1/",2024-12-27T17:13:14Z,http://arxiv.org/abs/2412.19744v1,"William Wang Yang, Karthikeya Kona, Yashveer Jain, Abhinav Bhamidipati, Tomer Atzili, Xiaomin Lin, Yantian Zha"
Flavor Physics at CEPC: a General Perspective,"We discuss the landscape of flavor physics at the Circular Electron-Positron
Collider (CEPC), based on the nominal luminosity outlined in its Technical
Design Report. The CEPC is designed to operate in multiple modes to address a
variety of tasks. At the $Z$ pole, the expected production of 4 Tera $Z$ bosons
will provide unique and highly precise measurements of $Z$ boson couplings,
while the substantial number of boosted heavy-flavored quarks and leptons
produced in clean $Z$ decays will facilitate investigations into their flavor
physics with unprecedented precision. We investigate the prospects of measuring
various physics benchmarks and discuss their implications for particle theories
and phenomenological models. Our studies indicate that, with its highlighted
advantages and anticipated excellent detector performance, the CEPC can explore
beauty and $\tau$ physics in ways that are superior to or complementary with
the Belle II and Large-Hadron-Collider-beauty experiments, potentially enabling
the detection of new physics at energy scales of 10 TeV and above. This
potential also extends to the observation of yet-to-be-discovered rare and
exotic processes, as well as testing fundamental principles such as lepton
flavor universality, lepton and baryon number conservation, etc., making the
CEPC a vibrant platform for flavor physics research. The $WW$ threshold scan,
Higgs-factory operation and top-pair productions of the CEPC further enhance
its merits in this regard, especially for measuring the
Cabibbo-Kobayashi-Maskawa matrix elements, and Flavor-Changing-Neutral-Current
physics of Higgs boson and top quarks. We outline the requirements for detector
performance and considerations for future development to achieve the
anticipated scientific goals.",2024-12-27T17:08:42Z,http://arxiv.org/abs/2412.19743v1,"Xiaocong Ai, Wolfgang Altmannshofer, Peter Athron, Xiaozhi Bai, Lorenzo Calibbi, Lu Cao, Yuzhi Che, Chunhui Chen, Ji-Yuan Chen, Long Chen, Mingshui Chen, Shanzhen Chen, Xuan Chen, Shan Cheng, Cheng-Wei Chiang, Andreas Crivellin, Hanhua Cui, Olivier Deschamps, Sébastien Descotes-Genon, Xiaokang Du, Shuangshi Fang, Yu Gao, Li-Sheng Geng, Pablo Goldenzweig, Jiayin Gu, Feng-Kun Guo, Yuchen Guo, Zhi-Hui Guo, Tao Han, Hong-Jian He, Jibo He, Miao He, Yanping Huang, Gino Isidori, Quan Ji, Jianfeng Jiang, Xu-Hui Jiang, Jernej F. Kamenik, Tsz Hong Kwok, Gang Li, Geng Li, Haibo Li, Haitao Li, Hengne Li, Honglei Li, Liang Li, Lingfeng Li, Qiang Li, Shu Li, Xiaomei Li, Xin-Qiang Li, Yiming Li, Yubo Li, Yuji Li, Zhao Li, Hao Liang, Zhijun Liang, Libo Liao, Zoltan Ligeti, Jia Liu, Jianbei Liu, Tao Liu, Yi Liu, Yong Liu, Zhen Liu, Xinchou Lou, Peng-Cheng Lu, Alberto Lusiani, Hong-Hao Ma, Kai Ma, Yaxian Mao, David Marzocca, Juan-Juan Niu, Soeren Prell, Huirong Qi, Sen Qian, Wenbin Qian, Zhuoni Qian, Qin Qin, Ariel Rock, Jonathan L. Rosner, Manqi Ruan, Dingyu Shao, Chengping Shen, Xiaoyan Shen, Haoyu Shi, Liaoshan Shi, Zong-Guo Si, Cristian Sierra, Huayang Song, Shufang Su, Wei Su, Michele Tammaro, En Wang, Fei Wang, Hengyu Wang, Jian Wang, Jianchun Wang, Kun Wang, Lian-Tao Wang, Wei Wang, Xiaolong Wang, Xiaoping Wang, Yadi Wang, Yifang Wang, Yuexin Wang, Xing-Gang Wu, Yongcheng Wu, Rui-Qing Xiao, Ke-Pan Xie, Yuehong Xie, Zijun Xu, Haijun Yang, Hongtao Yang, Lin Yang, Shuo Yang, Zhongbao Yin, Fusheng Yu, Changzheng Yuan, Xing-Bo Yuan, Xuhao Yuan, Chongxing Yue, Xi-Jie Zhan, Kaili Zhang, Liming Zhang, Xiaoming Zhang, Yang Zhang, Yanxi Zhang, Yongchao Zhang, Yu Zhang, Zhen-Hua Zhang, Zhong Zhang, Mingrui Zhao, Qiang Zhao, Xu-Chang Zheng, Yangheng Zheng, Chen Zhou, Pengxuan Zhu, Yongfeng Zhu, Xunwu Zuo, Jure Zupan"
"Weak lumping of left-invariant random walks on left cosets of finite
  groups","Let $G$ be a finite group and let $H$ be a subgroup of $G$. The
left-invariant random walk driven by a probability measure $w$ on $G$ is the
Markov chain in which from any state $x \in G$, the probability of stepping to
$xg \in G$ is $w(g)$. The initial state is chosen randomly according to a given
distribution. The walk is said to lump weakly on left cosets if the induced
process on $G/H$ is a time-homogeneous Markov chain. We characterise all the
initial distributions and weights $w$ such that the walk is irreducible and
lumps weakly on left cosets, and determine all the possible transition matrices
of the induced Markov chain. In the case where $H$ is abelian we refine our
main results to give a necessary and sufficient condition for weak lumping by
an explicit system of linear equations on $w$, organized by the double cosets
$HxH$. As an application we consider shuffles of a deck of $n$ cards such that
repeated observations of the top card form a Markov chain. Such shuffles
include the random-to-top shuffle, and also, when the deck is started in a
uniform random order, the top-to-random shuffle. We give a further family of
examples in which our full theory of weak lumping is needed to verify that the
top card sequence is Markov.",2024-12-27T17:08:17Z,http://arxiv.org/abs/2412.19742v1,"Edward Crane, Álvaro Gutiérrez, Erin Russell, Mark Wildon"
"Physics of 2D magnets and magnetic thin films: Surface structure and
  surface phase transition, criticality and skyrmions","Recently, there is an increasing renewed interest in 2D magnetism such as Van
der Waals magnets. The physics of 2D magnetism and ultra-thin magnetic films
has a long history. This chapter is a review devoted to some fundamental
theoretical properties of 2D magnets and and magnetic thin films including
frustrated systems and topological spin textures. These properties allow to
understand macroscopic behaviors experimentally observed in thin films and
superlattices where the surface and the interface play a crucial role. The
chapter begins with a review on 2D magnets, their spin structures and phase
transitions. Next, the case of thin films is considered. The theory of surface
spin waves is discussed in various situations with and without surface
reconstruction of spin ordering. Various interactions are taken into account:
surface interaction different from the bulk one, competing interactions,
Dzyaloshinskii-Moriya interaction. Surface phase transitions are shown in some
particularly striking cases. Finally, some cases of topological spin textures
called ""skyrmions"" are reviewed. All the results shown in this chapter have
been published in various research papers cited in the text. Therefore, we will
discuss some important results but avoid to enter complicated methods. Instead,
the reader is referred to original papers for detailed demonstrations.",2024-12-27T17:06:31Z,http://arxiv.org/abs/2412.19741v1,Hung T. Diep
"On dual-projectively equivalent connections associated to second order
  superintegrable systems","In projective differential geometry, pre-geodesics of an affine connection
are curves that are geodesics after a reparametrization (the analogous concept
in K\""ahler geometry is known as $J$-planar curves). Similarly, dual-geodesics
on a Riemannian manifold are curves along which the $1$-forms associated to the
velocity are preserved after a reparametrization. Superintegrable systems are
Hamiltonian systems with a large number of independent constants of the motion.
They are said to be second order if the constants of the motion can be chosen
to be quadratic polynomials in the momenta. Famous examples include the
Kepler-Coulomb and the harmonic oscillator systems, which are foundational
models in the Sciences. We show that certain torsion-free affine connections
which are naturally associated to certain second order superintegrable systems
share the same dual-geodesics.",2024-12-27T17:01:46Z,http://arxiv.org/abs/2412.19739v1,Andreas Vollmer
"Periodically and aperiodically Thue-Morse driven long-range systems:
  from dynamical localization to slow dynamics","We investigate the electric-field driven power-law random banded
matrix(PLRBM) model where a variation in the power-law exponent $\alpha$ yields
a delocalization-to-localization phase transition. We examine the periodically
driven PLRBM model with the help of the Floquet operator. The level spacing
ratio and the generalized participation ratio of the Floquet Hamiltonian reveal
a drive-induced fractal phase accompanied by diffusive transport on the
delocalized side of the undriven PLRBM model. On the localized side, the
time-periodic model remains localized - the average spacing ratio corresponds
to Poisson statistics and logarithmic transport is observed in the dynamics.
Extending our analysis to the aperiodic Thue-Morse (TM) driven system, we find
that the aperiodically driven clean long-range hopping model (clean counterpart
of the PLRBM model) exhibits the phenomenon of \textit{exact dynamical
localization} (EDL) on tuning the drive-parameters at special points. The
disordered time-aperiodic system shows diffusive transport followed by
relaxation to the infinite-temperature state on the delocalized side, and a
prethermal plateau with subdiffusion on the localized side. Additionally, we
compare this with a quasi-periodically driven AAH model that also undergoes a
localization-delocalization transition. Unlike the disordered long-range model,
it features a prolonged prethermal plateau followed by subdiffusion to the
infinite temperature state, even on the delocalized side.",2024-12-27T16:55:47Z,http://arxiv.org/abs/2412.19736v1,"Vatsana Tiwari, Devendra Singh Bhakuni, Auditya Sharma"
"A General Framework of Brain Region Detection And Genetic Variants
  Selection in Imaging Genetics","Imaging genetics is a growing field that employs structural or functional
neuroimaging techniques to study individuals with genetic risk variants
potentially linked to specific illnesses. This area presents considerable
challenges to statisticians due to the heterogeneous information and different
data forms it involves. In addition, both imaging and genetic data are
typically high-dimensional, creating a ""big data squared"" problem. Moreover,
brain imaging data contains extensive spatial information. Simply vectorizing
tensor images and treating voxels as independent features can lead to
computational issues and disregard spatial structure. This paper presents a
novel statistical method for imaging genetics modeling while addressing all
these challenges. We explore a Canonical Correlation Analysis based linear
model for the joint modeling of brain imaging, genetic information, and
clinical phenotype, enabling the simultaneous detection of significant brain
regions and selection of important genetic variants associated with the
phenotype outcome. Scalable algorithms are developed to tackle the ""big data
squared"" issue. We apply the proposed method to explore the reaction speed, an
indicator of cognitive functions, and its associations with brain MRI and
genetic factors using the UK Biobank database. Our study reveals a notable
connection between the caudate nucleus region of brain and specific significant
SNPs, along with their respective regulated genes, and the reaction speed.",2024-12-27T16:54:11Z,http://arxiv.org/abs/2412.19735v1,"Siqiang Su, Zhenghao Li, Long Feng, Ting Li"
"Dynamics, data and reconstruction","Data-driven learning is prevalent in many fields of science, mathematics and
engineering. The goal of data-driven learning of dynamical systems is to
interpret timeseries as a continuous observation of an underlying dynamical
system. This task is not well-posed for a variety of reasons. A dynamical
system may have multiple sub-systems co-existing within it. The nature of the
dataset depends on the portion of the phase space being viewed, and may thus my
confined to a sub-system. Secondly these sub-systems may be topologically
inter-weaved, so may be inseparable computationally. Thirdly, two timeseries
sampled separately from different dynamical systems may be close or even
indistinguishable. So there is no unqiue source for the timeseries. We show how
these ambiguities are circumvented if one considers dynamical systems and
measurement maps collectively. This is made possible in a category theoretical
framework, in which reconstruction is unique up to equivalences. We introduce
two categories of observed dynamical systems and timeseries-data. These are
related to the well known category of dynamical systems via functors. This
enables a functorial interpretation of the task of reconstruction as well.",2024-12-27T16:49:52Z,http://arxiv.org/abs/2412.19734v1,"Suddhasattwa Das, Tomoharu Suda"
"Generative Pretrained Embedding and Hierarchical Irregular Time Series
  Representation for Daily Living Activity Recognition","Within the evolving landscape of smart homes, the precise recognition of
daily living activities using ambient sensor data stands paramount. This paper
not only aims to bolster existing algorithms by evaluating two distinct
pretrained embeddings suited for ambient sensor activations but also introduces
a novel hierarchical architecture. We delve into an architecture anchored on
Transformer Decoder-based pre-trained embeddings, reminiscent of the GPT
design, and contrast it with the previously established state-of-the-art (SOTA)
ELMo embeddings for ambient sensors. Our proposed hierarchical structure
leverages the strengths of each pre-trained embedding, enabling the discernment
of activity dependencies and sequence order, thereby enhancing classification
precision. To further refine recognition, we incorporate into our proposed
architecture an hour-of-the-day embedding. Empirical evaluations underscore the
preeminence of the Transformer Decoder embedding in classification endeavors.
Additionally, our innovative hierarchical design significantly bolsters the
efficacy of both pre-trained embeddings, notably in capturing inter-activity
nuances. The integration of temporal aspects subtly but distinctively augments
classification, especially for time-sensitive activities. In conclusion, our
GPT-inspired hierarchical approach, infused with temporal insights, outshines
the SOTA ELMo benchmark.",2024-12-27T16:43:52Z,http://arxiv.org/abs/2412.19732v1,"Damien Bouchabou, Sao Mai Nguyen"
"Fully-relativistic evolution of vacuum tensor inhomogeneities during
  inflation","We present a complete method for the initialisation and extraction of
first-order inflationary tensor perturbations for fully relativistic
simulations which incorporate gravitational back-reaction. We outline a
correspondence between the Cosmological Perturbation Theory (CPT) framework and
the numerical relativity BSSN variables in the appropriate limit. We describe a
generation method for stochastic tensoral initial conditions, inspired by the
standard scalar initial condition used from inflation and implemented in
lattice cosmology. We discuss the implementation of this procedure in the
GRChombo/GRTeclyn code, and demonstrate the detailed quantitative
correspondence between the linearised and fully-nonlinear solutions in the
perturbative limit, through the evolution of the background and the tensor
power spectrum. We also validate the methodology by showing that energy and
momentum constraints are introduced and preserved to second-order or better. We
provide some preliminary indicative results probing tensoral non-Gaussianity
using the skewness and kurtosis. The computational pipeline presented here will
be used to study the emergence of a primordial tensor bispectra and
cross-spectra that incorporate the effect of nonlinear gravitational couplings
with the metric, which has potential applications for the analysis of
next-generation CMB surveys.",2024-12-27T16:42:59Z,http://arxiv.org/abs/2412.19731v1,"Ericka Florio, E. Paul S. Shellard"
High-dimensional permutons: theory and applications,"Permutons, which are probability measures on the unit square $[0, 1]^2$ with
uniform marginals, are the natural scaling limits for sequences of (random)
permutations.
  We introduce a $d$-dimensional generalization of these measures for all $d
\ge 2$, which we call $d$-dimensional permutons, and extend -- from the
two-dimensional setting -- the theory to prove convergence of sequences of
(random) $d$-dimensional permutations to (random) $d$-dimensional permutons.
  Building on this new theory, we determine the random high-dimensional
permuton limits for two natural families of high-dimensional permutations.
First, we determine the $3$-dimensional permuton limit for Schnyder wood
permutations, which bijectively encode planar triangulations decorated by
triples of spanning trees known as Schnyder woods. Second, we identify the
$d$-dimensional permuton limit for $d$-separable permutations, a
pattern-avoiding class of $d$-dimensional permutations generalizing ordinary
separable permutations.
  Both high-dimensional permuton limits are random and connected to previously
studied universal 2-dimensional permutons, such as the Brownian separable
permutons and the skew Brownian permutons, and share interesting connections
with objects arising from random geometry, including the continuum random tree,
Schramm--Loewner evolutions, and Liouville quantum gravity surfaces.",2024-12-27T16:40:41Z,http://arxiv.org/abs/2412.19730v1,"Jacopo Borga, Andrew Lin"
Can Large Language Models Adapt to Other Agents In-Context?,"As the research community aims to build better AI assistants that are more
dynamic and personalized to the diversity of humans that they interact with,
there is increased interest in evaluating the theory of mind capabilities of
large language models (LLMs). Indeed, several recent studies suggest that LLM
theory of mind capabilities are quite impressive, approximating human-level
performance. Our paper aims to rebuke this narrative and argues instead that
past studies were not directly measuring agent performance, potentially leading
to findings that are illusory in nature as a result. We draw a strong
distinction between what we call literal theory of mind i.e. measuring the
agent's ability to predict the behavior of others and functional theory of mind
i.e. adapting to agents in-context based on a rational response to predictions
of their behavior. We find that top performing open source LLMs may display
strong capabilities in literal theory of mind, depending on how they are
prompted, but seem to struggle with functional theory of mind -- even when
partner policies are exceedingly simple. Our work serves to highlight the
double sided nature of inductive bias in LLMs when adapting to new situations.
While this bias can lead to strong performance over limited horizons, it often
hinders convergence to optimal long-term behavior.",2024-12-27T16:30:12Z,http://arxiv.org/abs/2412.19726v1,"Matthew Riemer, Zahra Ashktorab, Djallel Bouneffouf, Payel Das, Miao Liu, Justin D. Weisz, Murray Campbell"
"Exploring low-rank structure for an inverse scattering problem with
  far-field data","The inverse scattering problem exhibits an inherent low-rank structure due to
its ill-posed nature; however developing low-rank structures for the inverse
scattering problem remains challenging. In this work, we introduce a novel
low-rank structure tailored for solving the inverse scattering problem. The
particular low-rank structure is given by the generalized prolate spheroidal
wave functions, computed stably and accurately via a Sturm-Liouville problem.
We first process the far-field data to obtain a post-processed data set within
a disk domain. Subsequently, the post-processed data are projected onto a
low-rank space given by the low-rank structure. The unknown is approximately
solved in this low-rank space, by dropping higher-order terms. The low-rank
structure leads to a H\""{o}lder-logarithmic type stability estimate for
arbitrary unknown functions, and a Lipschitz stability estimate for unknowns
belonging to a finite dimensional low-rank space. Various numerical experiments
are conducted to validate its performance, encompassing assessments of
resolution capability, robustness against randomly added noise and modeling
errors, and demonstration of increasing stability.",2024-12-27T16:24:20Z,http://arxiv.org/abs/2412.19724v1,"Yuyuan Zhou, Lorenzo Audibert, Shixu Meng, Bo Zhang"
"OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse
  Task Synthesis","Graphical User Interface (GUI) agents powered by Vision-Language Models
(VLMs) have demonstrated human-like computer control capability. Despite their
utility in advancing digital automation, a critical bottleneck persists:
collecting high-quality trajectory data for training. Common practices for
collecting such data rely on human supervision or synthetic data generation
through executing pre-defined tasks, which are either resource-intensive or
unable to guarantee data quality. Moreover, these methods suffer from limited
data diversity and significant gaps between synthetic data and real-world
environments. To address these challenges, we propose OS-Genesis, a novel GUI
data synthesis pipeline that reverses the conventional trajectory collection
process. Instead of relying on pre-defined tasks, OS-Genesis enables agents
first to perceive environments and perform step-wise interactions, then
retrospectively derive high-quality tasks to enable trajectory-level
exploration. A trajectory reward model is then employed to ensure the quality
of the generated trajectories. We demonstrate that training GUI agents with
OS-Genesis significantly improves their performance on highly challenging
online benchmarks. In-depth analysis further validates OS-Genesis's efficiency
and its superior data quality and diversity compared to existing synthesis
methods. Our codes, data, and checkpoints are available at
\href{https://qiushisun.github.io/OS-Genesis-Home/}{OS-Genesis Homepage}.",2024-12-27T16:21:58Z,http://arxiv.org/abs/2412.19723v1,"Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, Ben Kao, Guohao Li, Junxian He, Yu Qiao, Zhiyong Wu"
"Quantum correlations in a gravitational collapse simulation with
  SpheriCo.jl","We report on work using a newly developed code, SpheriCo.jl, that computes
the gravitational collapse of a spherical scalar field, where the scalar can be
either a classical field, or a quantum field operator. By utilising
summation-by-parts methods for the numerical derivatives we are able to
simulate the collapse longer than was possible previously due to enhanced
numerical stability. We present a suite of tests for the code that tests its
accuracy and stability, both for the classical and quantum fields. We are able
to observe critical behavior of gravitational collapse for the classical setup,
in agreement with expected results. The code is also used to compute two-point
correlation functions, with results that hint at a non-trivial correlation
across the horizon of Hawking quanta.",2024-12-27T16:20:27Z,http://arxiv.org/abs/2412.19722v1,"Benjamin Berczi, Magdalena Eriksson, Thanasis Giannakopoulos, Paul M. Saffin"
Sharpening Neural Implicit Functions with Frequency Consolidation Priors,"Signed Distance Functions (SDFs) are vital implicit representations to
represent high fidelity 3D surfaces. Current methods mainly leverage a neural
network to learn an SDF from various supervisions including signed distances,
3D point clouds, or multi-view images. However, due to various reasons
including the bias of neural network on low frequency content, 3D unaware
sampling, sparsity in point clouds, or low resolutions of images, neural
implicit representations still struggle to represent geometries with high
frequency components like sharp structures, especially for the ones learned
from images or point clouds. To overcome this challenge, we introduce a method
to sharpen a low frequency SDF observation by recovering its high frequency
components, pursuing a sharper and more complete surface. Our key idea is to
learn a mapping from a low frequency observation to a full frequency coverage
in a data-driven manner, leading to a prior knowledge of shape consolidation in
the frequency domain, dubbed frequency consolidation priors. To better
generalize a learned prior to unseen shapes, we introduce to represent
frequency components as embeddings and disentangle the embedding of the low
frequency component from the embedding of the full frequency component. This
disentanglement allows the prior to generalize on an unseen low frequency
observation by simply recovering its full frequency embedding through a
test-time self-reconstruction. Our evaluations under widely used benchmarks or
real scenes show that our method can recover high frequency component and
produce more accurate surfaces than the latest methods. The code, data, and
pre-trained models are available at \url{https://github.com/chenchao15/FCP}.",2024-12-27T16:18:46Z,http://arxiv.org/abs/2412.19720v1,"Chao Chen, Yu-Shen Liu, Zhizhong Han"
"Trading Off Energy Storage and Payload -- An Analytical Model for
  Freight Train Configuration","To support planning of alternative fuel technology (e.g., battery-electric
locomotives) deployment for decarbonizing non-electrified freight rail, we
develop a convex optimization formulation with a closed-form solution to
determine the optimal number of energy storage tender cars in a train. The
formulation shares a similar structure to an Economic Order Quantity (EOQ)
model. For given market characteristics, cost forecasts, and technology
parameters, our model captures the trade-offs between inventory carrying costs
associated with trip times (including delays due to charging/refueling) and
ordering costs associated with train dispatch and operation (energy, amortized
equipment, and labor costs). To illustrate the framework, we find the optimal
number of battery-electric energy tender cars in 22,501 freight markets
(origin-destination pairs and commodities) for U.S. Class I railroads. The
results display heterogeneity in optimal configurations with lighter, yet more
time-sensitive shipments (e.g., intermodal) utilizing more battery tender cars.
For heavier commodities (e.g., coal) with lower holding costs, single battery
tender car configurations are generally optimal. The results also show that the
optimal train configurations are sensitive to delays associated with recharging
or swapping tender cars.",2024-12-27T16:18:35Z,http://arxiv.org/abs/2412.19719v1,"Max T. M. Ng, Adrian Hernandez, Pablo L. Durango-Cohen, Hani S. Mahmassani"
"Text2Insight: Transform natural language text into insights seamlessly
  using multi-model architecture","The growing demand for dynamic, user-centric data analysis and visualization
is evident across domains like healthcare, finance, and research. Traditional
visualization tools often fail to meet individual user needs due to their
static and predefined nature. To address this gap, Text2Insight is introduced
as an innovative solution that delivers customized data analysis and
visualizations based on user-defined natural language requirements. Leveraging
a multi-model architecture, Text2Insight transforms user inputs into actionable
insights and dynamic visualizations.
  The methodology begins with analyzing the input dataset to extract structural
details such as columns and values. A pre-trained Llama3 model converts the
user's natural language query into an SQL query, which is further refined using
a Named Entity Recognition (NER) model for accuracy. A chart predictor
determines the most suitable visualization type, while the Llama3 model
generates insights based on the SQL query's results. The output is a
user-friendly and visually informative chart. To enhance analysis capabilities,
the system integrates a question-answering model and a predictive model using
the BERT framework. These models provide insights into historical data and
predict future trends.
  Performance evaluation of Text2Insight demonstrates its effectiveness,
achieving high accuracy (99%), precision (100%), recall (99%), and F1-score
(99%), with a BLEU score of 0.5. The question-answering model attained an
accuracy of 89% and the predictive model achieved 70% accuracy. These results
validate Text2Insight as a robust and viable solution for transforming natural
language text into dynamic, user-specific data analysis and visualizations.",2024-12-27T16:17:22Z,http://arxiv.org/abs/2412.19718v1,Pradeep Sain
"The non-standad logic of physics: the case of the Boltzmann-Sinai
  hard-sphere system","One of the most challenging and fascinating issues in mathematical and
theoretical physics concerns identifying the common logic, if any, which
underlies the physical world. More precisely, this involves the search of the
possibly-unique axiomatic logical proposition calculus to apply simultaneously
both to classical and quantum realms of physics and to be consistent with the
corresponding mathematical and filosophysical setups. Based on the recent
establishment of quantum logic, which has been shown to apply both to Quantum
Mechanics and Quantum Gravity, the crucial remaining step involves the
identification of the appropriate axiomatic logical proposition calculus to be
associated with Classical Mechanics. In this paper the issue is posed for a
fundamental example of Classical Mechanics, which is represented by the
so-called Boltzmann-Sinai dynamical system. This is realized by the ensemble of
classical smooth hard-spheres, which is set at the basis of Classical
Statistical Mechanics and is also commonly regarded as a possible realization
of Classical Newtonian Cosmology. Depending on the initial conditions which are
prescribed for such a system, its classical state is shown to obey the
propositional calculus of non-classical logic. In particular, the latter is
expressed by the 3-way Principle of Non-Contradiction, namely the same logical
principle that holds for quantum logic. The result therefore permits to
question on a mathematical basis the principles of deterministic classical
logic and the validity of their character within the domain of Classical
Physics. Such a conclusion represents a potential notable innovation in the
logical dicotomy true/false, a crucial topic which has crossed millennia
through philosophy, logic, mathematics and physics.",2024-12-27T16:15:24Z,http://arxiv.org/abs/2412.19716v1,"Massimo Tessarotto, Claudio Cremaschini, Claudio Asci, Alessandro Soranzo, Marco Tessarotto, Gino Tironi"
Entanglement-Driven Energy Exchange in a Two-Qubit Quantum Battery,"This study investigates the dynamics of quantum batteries (QBs), focusing on
the pivotal role of quantum entanglement in mediating inter-cellular energy
transfer within a two-cell configuration (two-qubit), wherein one cell is
directly coupled to the charging source. Employing the Lindblad master equation
to model the system's evolution, the influence of coherent state amplitudes,
detuning, inter-cellular coupling strength, and dissipation rates on stored
energy, energy fluctuations, concurrence-quantified entanglement, and their
parametric interrelations is scrutinized. Our results indicate a direct
correlation between the degree of entanglement and energy transfer efficiency
between the qubits. Specifically, the stronger the entanglement between primary
cell, which is connected to the charger, and secondary cell, the more
effectively energy is transferred. This demonstrates that enhanced entanglement
significantly facilitates energy transfer between the two qubits.",2024-12-27T16:15:18Z,http://arxiv.org/abs/2412.19715v1,"Ahmed A. Zahia, M. Y. Abd-Rabbou, Ahmed M. Megahed"
"From Elements to Design: A Layered Approach for Automatic Graphic Design
  Composition","In this work, we investigate automatic design composition from multimodal
graphic elements. Although recent studies have developed various generative
models for graphic design, they usually face the following limitations: they
only focus on certain subtasks and are far from achieving the design
composition task; they do not consider the hierarchical information of graphic
designs during the generation process. To tackle these issues, we introduce the
layered design principle into Large Multimodal Models (LMMs) and propose a
novel approach, called LaDeCo, to accomplish this challenging task.
Specifically, LaDeCo first performs layer planning for a given element set,
dividing the input elements into different semantic layers according to their
contents. Based on the planning results, it subsequently predicts element
attributes that control the design composition in a layer-wise manner, and
includes the rendered image of previously generated layers into the context.
With this insightful design, LaDeCo decomposes the difficult task into smaller
manageable steps, making the generation process smoother and clearer. The
experimental results demonstrate the effectiveness of LaDeCo in design
composition. Furthermore, we show that LaDeCo enables some interesting
applications in graphic design, such as resolution adjustment, element filling,
design variation, etc. In addition, it even outperforms the specialized models
in some design subtasks without any task-specific training.",2024-12-27T16:13:08Z,http://arxiv.org/abs/2412.19712v1,"Jiawei Lin, Shizhao Sun, Danqing Huang, Ting Liu, Ji Li, Jiang Bian"
"All Finite (Anti)Hermitian Irreducible Representations of the de Sitter
  and Anti-de Sitter Lie Algebras and Their Lorentz Structure","Because of the importance of unitarity in quantum physics, work on the
representations of the de Sitter group has focussed on the unitary case, which
necessarily means infinite dimensional matrices for this non-compact group.
Here we address the finite dimensional representations resulting from the
requirement that the Lie algebra generators are either Hermitian or
anti-Hermitian. The complete classification of all such irreducible
representations is found and their matrix elements specified. These irreducible
representations (irreps) are based on backbones defined as the homogeneous
Lorentz sub-algebra and consisting of direct sums of the finite irreps of the
homogeneous Lorentz algebra (HLA). Only two types of such backbones arise (see
5.1a,b herein). Consequently, only certain dimensions of representation are
possible, namely 4, 5, 10, 14, 20, 30, 35, 55, 56, 91, etc or generally either
1/6 N(N+1)(N+2) or 1/6 N(N+1)(2N+1) where N=2,3,4,etc is the number of HLA
irreps in the backbone (minimum 2). The two Casimir invariants can be specified
in terms of a single integral or half-integral parameter, p. For irreps based
on (5.1a), -C1=p(p+1)-2 and C2=0 with p taking values 2,3,4,etc. For irreps
based on (5.1b), -C1=2(p^2-1) and -C2= p^2 (p^2-1) with p taking values
3/2,2,5/2,3,etc. These correspond to the same expressions found for the unitary
representations, -C1=p(p+1)+(q+1)(q-2) and -C2=p(p+1)q(q-1) with q=0 and q=p
respectively for the two types of irrep. There is thus a far more restricted
set of finite irreps with Hermitian or anti-Hermitian generators than for the
discrete infinite dimensional unitary irreps. The corresponding irreps of the
anti-de Sitter group follow immediately from the replacement of the 4-momentum
operators from V to iV.",2024-12-27T16:02:41Z,http://arxiv.org/abs/2412.19708v1,Richard A. W. Bradford
"Noise Sensitivity of the Semidefinite Programs for Direct Data-Driven
  LQR","In this paper, we study the noise sensitivity of the semidefinite program
(SDP) proposed for direct data-driven infinite-horizon linear quadratic
regulator (LQR) problem for discrete-time linear time-invariant systems. While
this SDP is shown to find the true LQR controller in the noise-free setting, we
show that it leads to a trivial solution with zero gain matrices when data is
corrupted by noise, even when the noise is arbitrarily small. We then study a
variant of the SDP that includes a robustness promoting regularization term and
prove that regularization does not fully eliminate the sensitivity issue. In
particular, the solution of the regularized SDP converges in probability also
to a trivial solution.",2024-12-27T15:59:42Z,http://arxiv.org/abs/2412.19705v1,"Xiong Zeng, Laurent Bako, Necmiye Ozay"
"Reparameterization Invariance of FRW Model: Supervariable and BRST
  Approaches","We perform the Becchi-Rouet-Stora-Tyutin (BRST) quantization of a (0 +
1)-dimensional non-interacting cosmological Friedmann-Robertson-Walker (FRW)
model. This quantization leverages the classical infinitesimal and continuous
reparameterization symmetry transformations of the system. To derive the
nilpotent reparameterization invariant (anti-)BRST symmetry transformations for
the scale factor and corresponding momentum variables of the FRW model, we
employ the modified Bonora-Tonin supervariable approach (MBTSA) to BRST
formalism. Through this approach, we also establish the (anti-)BRST invariant
Curci-Ferrari (CF)-type restriction for this cosmological reparameterization
invariant model. Further, we obtain the nilpotent (anti-)BRST symmetry
transformations for other variables within the model using the (anti-)chiral
supervariable approach (ACSA) to BRST formalism. Within the framework of ACSA,
the CF-type restriction is demonstrated through two key aspects: (i) the
invariance of the coupled Lagrangians under symmetry transformations, and (ii)
the absolute anticommutativity of the conserved (anti-)BRST charges. Notably,
applying the MBTSA to a physical cosmological system, specifically a
one-dimensional one, constitutes a novel contribution to this work.
Additionally, in the application of ACSA, we restrict our analysis to
(anti-)chiral super expansions of supervariables, leading to the unique
observation of the absolute anticommutativity of the (anti-)BRST charges.
Moreover, we highlight that the CF-type restriction demonstrates a universal
nature, remaining consistent across any reparameterization invariant models in
D-dimensional spacetime.",2024-12-27T15:58:29Z,http://arxiv.org/abs/2412.19704v1,"B. Chauhan, R. Tripathi"
"Numerical inverse scattering transform for the defocusing nonlinear
  Schrödinger equation with box-type initial conditions on a nonzero
  background","We present a method to solve numerically the Cauchy problem for the
defocusing nonlinear Schr\""{o}dinger (NLS) equation with a box-type initial
condition (IC) having a nontrivial background of amplitude $q_o&gt;0$ as $x\to \pm
\infty$ by implementing numerically the associated Inverse Scattering Transform
(IST). The Riemann--Hilbert problem associated to the inverse transform is
solved numerically by means of appropriate contour deformations in the complex
plane following the numerical implementation of the Deift-Zhou nonlinear
steepest descent method. In this work, the box parameters are chosen so that
there is no discrete spectrum (i.e., no solitons). In particular, the numerical
method is demonstrated to be accurate within the two asymptotic regimes
corresponding to two different regions of the $(x,t)$-plane depending on
whether $|x/(2t)| &lt; q_o$ or $|x/(2t)| &gt; q_o$, as $t \to \infty$.",2024-12-27T15:57:44Z,http://arxiv.org/abs/2412.19703v1,"Aikaterini Gkogkou, Barbara Prinari, Thomas Trogdon"
"Search for the double Dalitz decays $η/η' \to e^+e^-μ^+μ^-$
  and $η' \to μ^+μ^-μ^+μ^-$","Using a data sample of $(10087 \pm 44) \times {10^{6}}$ $J/{\psi}$ events
collected with the BESIII detector, we search for the decays $\eta/\eta'\to
e^+e^-\mu^+\mu^-$ and $\eta' \to \mu^+\mu^-\mu^+\mu^-$ via the radiative decays
$J/{\psi}\to\gamma\eta$/$\gamma\eta'$. No excess of events over expected
background is observed for any of the decays of interest. At 90% confidence
level, we report the first upper limits on the branching fractions of $\eta'
\to e^{+}e^{-}\mu^{+}\mu^{-}$ and $\eta' \to \mu^{+}\mu^{-}\mu^{+}\mu^{-}$ to
be $ 1.75 \times {10^{-6}}$ and $5.28 \times {10^{-7}}$, respectively. In
addition, we set an upper limit on the branching fraction of $\eta \to
e^{+}e^{-}\mu^{+}\mu^{-}$ to be $6.88 \times {10^{-6}}$, which improves the
previous result by about two orders of magnitude.",2024-12-27T15:55:02Z,http://arxiv.org/abs/2412.19702v1,"BESIII Collaboration, M. Ablikim, M. N. Achasov, P. Adlarson, O. Afedulidis, X. C. Ai, R. Aliberti, A. Amoroso, Y. Bai, O. Bakina, I. Balossino, Y. Ban, H. -R. Bao, V. Batozskaya, K. Begzsuren, N. Berger, M. Berlowski, M. Bertani, D. Bettoni, F. Bianchi, E. Bianco, A. Bortone, I. Boyko, R. A. Briere, A. Brueggemann, H. Cai, X. Cai, A. Calcaterra, G. F. Cao, N. Cao, S. A. Cetin, X. Y. Chai, J. F. Chang, G. R. Che, Y. Z. Che, G. Chelkov, C. Chen, C. H. Chen, Chao Chen, G. Chen, H. S. Chen, H. Y. Chen, M. L. Chen, S. J. Chen, S. L. Chen, S. M. Chen, T. Chen, X. R. Chen, X. T. Chen, Y. B. Chen, Y. Q. Chen, Z. J. Chen, Z. Y. Chen, S. K. Choi, G. Cibinetto, F. Cossio, J. J. Cui, H. L. Dai, J. P. Dai, A. Dbeyssi, R. E. de Boer, D. Dedovich, C. Q. Deng, Z. Y. Deng, A. Denig, I. Denysenko, M. Destefanis, F. De Mori, B. Ding, X. X. Ding, Y. Ding, Y. Ding, J. Dong, L. Y. Dong, M. Y. Dong, X. Dong, M. C. Du, S. X. Du, Y. Y. Duan, Z. H. Duan, P. Egorov, Y. H. Fan, J. Fang, J. Fang, S. S. Fang, W. X. Fang, Y. Fang, Y. Q. Fang, R. Farinelli, L. Fava, F. Feldbauer, G. Felici, C. Q. Feng, J. H. Feng, Y. T. Feng, M. Fritsch, C. D. Fu, J. L. Fu, Y. W. Fu, H. Gao, X. B. Gao, Y. N. Gao, Yang Gao, S. Garbolino, I. Garzia, L. Ge, P. T. Ge, Z. W. Ge, C. Geng, E. M. Gersabeck, A. Gilman, K. Goetzen, L. Gong, W. X. Gong, W. Gradl, S. Gramigna, M. Greco, M. H. Gu, Y. T. Gu, C. Y. Guan, A. Q. Guo, L. B. Guo, M. J. Guo, R. P. Guo, Y. P. Guo, A. Guskov, J. Gutierrez, K. L. Han, T. T. Han, F. Hanisch, X. Q. Hao, F. A. Harris, K. K. He, K. L. He, F. H. Heinsius, C. H. Heinz, Y. K. Heng, C. Herold, T. Holtmann, P. C. Hong, G. Y. Hou, X. T. Hou, Y. R. Hou, Z. L. Hou, B. Y. Hu, H. M. Hu, J. F. Hu, Q. P. Hu, S. L. Hu, T. Hu, Y. Hu, G. S. Huang, K. X. Huang, L. Q. Huang, X. T. Huang, Y. P. Huang, Y. S. Huang, T. Hussain, F. Hölzken, N. Hüsken, N. in der Wiesche, J. Jackson, S. Janchiv, J. H. Jeong, Q. Ji, Q. P. Ji, W. Ji, X. B. Ji, X. L. Ji, Y. Y. Ji, X. Q. Jia, Z. K. Jia, D. Jiang, H. B. Jiang, P. C. Jiang, S. S. Jiang, T. J. Jiang, X. S. Jiang, Y. Jiang, J. B. Jiao, J. K. Jiao, Z. Jiao, S. Jin, Y. Jin, M. Q. Jing, X. M. Jing, T. Johansson, S. Kabana, N. Kalantar-Nayestanaki, X. L. Kang, X. S. Kang, M. Kavatsyuk, B. C. Ke, V. Khachatryan, A. Khoukaz, R. Kiuchi, O. B. Kolcu, B. Kopf, M. Kuessner, X. Kui, N. Kumar, A. Kupsc, W. Kühn, L. Lavezzi, T. T. Lei, Z. H. Lei, M. Lellmann, T. Lenz, C. Li, C. Li, C. H. Li, Cheng Li, D. M. Li, F. Li, G. Li, H. B. Li, H. J. Li, H. N. Li, Hui Li, J. R. Li, J. S. Li, K. Li, K. L. Li, L. J. Li, L. K. Li, Lei Li, M. H. Li, P. R. Li, Q. M. Li, Q. X. Li, R. Li, S. X. Li, T. Li, W. D. Li, W. G. Li, X. Li, X. H. Li, X. L. Li, X. Y. Li, X. Z. Li, Y. G. Li, Z. J. Li, Z. Y. Li, C. Liang, H. Liang, H. Liang, Y. F. Liang, Y. T. Liang, G. R. Liao, Y. P. Liao, J. Libby, A. Limphirat, C. C. Lin, C. X. Lin, D. X. Lin, T. Lin, B. J. Liu, B. X. Liu, C. Liu, C. X. Liu, F. Liu, F. H. Liu, Feng Liu, G. M. Liu, H. Liu, H. B. Liu, H. H. Liu, H. M. Liu, Huihui Liu, J. B. Liu, J. Y. Liu, K. Liu, K. Y. Liu, Ke Liu, L. Liu, L. C. Liu, Lu Liu, M. H. Liu, P. L. Liu, Q. Liu, S. B. Liu, T. Liu, W. K. Liu, W. M. Liu, X. Liu, X. Liu, Y. Liu, Y. Liu, Y. B. Liu, Z. A. Liu, Z. D. Liu, Z. Q. Liu, X. C. Lou, F. X. Lu, H. J. Lu, J. G. Lu, X. L. Lu, Y. Lu, Y. P. Lu, Z. H. Lu, C. L. Luo, J. R. Luo, M. X. Luo, T. Luo, X. L. Luo, X. R. Lyu, Y. F. Lyu, F. C. Ma, H. Ma, H. L. Ma, J. L. Ma, L. L. Ma, L. R. Ma, M. M. Ma, Q. M. Ma, R. Q. Ma, T. Ma, X. T. Ma, X. Y. Ma, Y. M. Ma, F. E. Maas, I. MacKay, M. Maggiora, S. Malde, Y. J. Mao, Z. P. Mao, S. Marcello, Z. X. Meng, J. G. Messchendorp, G. Mezzadri, H. Miao, T. J. Min, R. E. Mitchell, X. H. Mo, B. Moses, N. Yu. Muchnoi, J. Muskalla, Y. Nefedov, F. Nerling, L. S. Nie, I. B. Nikolaev, Z. Ning, S. Nisar, Q. L. Niu, W. D. Niu, Y. Niu, S. L. Olsen, S. L. Olsen, Q. Ouyang, S. Pacetti, X. Pan, Y. Pan, A. Pathak, Y. P. Pei, M. Pelizaeus, H. P. Peng, Y. Y. Peng, K. Peters, J. L. Ping, R. G. Ping, S. Plura, V. Prasad, F. Z. Qi, H. Qi, H. R. Qi, M. Qi, T. Y. Qi, S. Qian, W. B. Qian, C. F. Qiao, X. K. Qiao, J. J. Qin, L. Q. Qin, L. Y. Qin, X. P. Qin, X. S. Qin, Z. H. Qin, J. F. Qiu, Z. H. Qu, C. F. Redmer, K. J. Ren, A. Rivetti, M. Rolo, G. Rong, Ch. Rosner, M. Q. Ruan, S. N. Ruan, N. Salone, A. Sarantsev, Y. Schelhaas, K. Schoenning, M. Scodeggio, K. Y. Shan, W. Shan, X. Y. Shan, Z. J. Shang, J. F. Shangguan, L. G. Shao, M. Shao, C. P. Shen, H. F. Shen, W. H. Shen, X. Y. Shen, B. A. Shi, H. Shi, H. C. Shi, J. L. Shi, J. Y. Shi, Q. Q. Shi, S. Y. Shi, X. Shi, J. J. Song, T. Z. Song, W. M. Song, Y. J. Song, Y. X. Song, S. Sosio, S. Spataro, F. Stieler, S. S Su, Y. J. Su, G. B. Sun, G. X. Sun, H. Sun, H. K. Sun, J. F. Sun, K. Sun, L. Sun, S. S. Sun, T. Sun, W. Y. Sun, Y. Sun, Y. J. Sun, Y. Z. Sun, Z. Q. Sun, Z. T. Sun, C. J. Tang, G. Y. Tang, J. Tang, M. Tang, Y. A. Tang, L. Y. Tao, Q. T. Tao, M. Tat, J. X. Teng, V. Thoren, W. H. Tian, Y. Tian, Z. F. Tian, I. Uman, Y. Wan, S. J. Wang, B. Wang, B. L. Wang, Bo Wang, D. Y. Wang, F. Wang, H. J. Wang, J. J. Wang, J. P. Wang, K. Wang, L. L. Wang, M. Wang, N. Y. Wang, S. Wang, S. Wang, T. Wang, T. J. Wang, W. Wang, W. Wang, W. P. Wang, X. Wang, X. F. Wang, X. J. Wang, X. L. Wang, X. N. Wang, Y. Wang, Y. D. Wang, Y. F. Wang, Y. H. Wang, Y. L. Wang, Y. N. Wang, Y. Q. Wang, Yaqian Wang, Yi Wang, Z. Wang, Z. L. Wang, Z. Y. Wang, Ziyi Wang, D. H. Wei, F. Weidner, S. P. Wen, Y. R. Wen, U. Wiedner, G. Wilkinson, M. Wolke, L. Wollenberg, C. Wu, J. F. Wu, L. H. Wu, L. J. Wu, X. Wu, X. H. Wu, Y. Wu, Y. H. Wu, Y. J. Wu, Z. Wu, L. Xia, X. M. Xian, B. H. Xiang, T. Xiang, D. Xiao, G. Y. Xiao, S. Y. Xiao, Y. L. Xiao, Z. J. Xiao, C. Xie, X. H. Xie, Y. Xie, Y. G. Xie, Y. H. Xie, Z. P. Xie, T. Y. Xing, C. F. Xu, C. J. Xu, G. F. Xu, H. Y. Xu, M. Xu, Q. J. Xu, Q. N. Xu, W. Xu, W. L. Xu, X. P. Xu, Y. Xu, Y. C. Xu, Z. S. Xu, F. Yan, L. Yan, W. B. Yan, W. C. Yan, X. Q. Yan, H. J. Yang, H. L. Yang, H. X. Yang, J. H. Yang, T. Yang, Y. Yang, Y. F. Yang, Y. F. Yang, Y. X. Yang, Z. W. Yang, Z. P. Yao, M. Ye, M. H. Ye, J. H. Yin, Junhao Yin, Z. Y. You, B. X. Yu, C. X. Yu, G. Yu, J. S. Yu, M. C. Yu, T. Yu, X. D. Yu, Y. C. Yu, C. Z. Yuan, J. Yuan, J. Yuan, L. Yuan, S. C. Yuan, Y. Yuan, Z. Y. Yuan, C. X. Yue, A. A. Zafar, F. R. Zeng, S. H. Zeng, X. Zeng, Y. Zeng, Y. J. Zeng, Y. J. Zeng, X. Y. Zhai, Y. C. Zhai, Y. H. Zhan, A. Q. Zhang, B. L. Zhang, B. X. Zhang, D. H. Zhang, G. Y. Zhang, H. Zhang, H. Zhang, H. C. Zhang, H. H. Zhang, H. H. Zhang, H. Q. Zhang, H. R. Zhang, H. Y. Zhang, J. Zhang, J. Zhang, J. J. Zhang, J. L. Zhang, J. Q. Zhang, J. S. Zhang, J. W. Zhang, J. X. Zhang, J. Y. Zhang, J. Z. Zhang, Jianyu Zhang, L. M. Zhang, Lei Zhang, P. Zhang, Q. Y. Zhang, R. Y. Zhang, S. H. Zhang, Shulei Zhang, X. M. Zhang, X. Y Zhang, X. Y. Zhang, Y. Zhang, Y. Zhang, Y. T. Zhang, Y. H. Zhang, Y. M. Zhang, Yan Zhang, Z. D. Zhang, Z. H. Zhang, Z. L. Zhang, Z. Y. Zhang, Z. Y. Zhang, Z. Z. Zhang, G. Zhao, J. Y. Zhao, J. Z. Zhao, L. Zhao, Lei Zhao, M. G. Zhao, N. Zhao, R. P. Zhao, S. J. Zhao, Y. B. Zhao, Y. X. Zhao, Z. G. Zhao, A. Zhemchugov, B. Zheng, B. M. Zheng, J. P. Zheng, W. J. Zheng, Y. H. Zheng, B. Zhong, X. Zhong, H. Zhou, J. Y. Zhou, L. P. Zhou, S. Zhou, X. Zhou, X. K. Zhou, X. R. Zhou, X. Y. Zhou, Y. Z. Zhou, Z. C. Zhou, A. N. Zhu, J. Zhu, K. Zhu, K. J. Zhu, K. S. Zhu, L. Zhu, L. X. Zhu, S. H. Zhu, T. J. Zhu, W. D. Zhu, Y. C. Zhu, Z. A. Zhu, J. H. Zou, J. Zu"
Love Numbers for Extremal Kerr Black Hole,"We perform a detailed study of the gravitational tidal Love numbers of
extremal zero-temperature Kerr black holes. These coefficients are finite and
exhibit the dissipative nature of these maximally spinning black holes. Upon
considering the dynamical behavior of the tidal deformations of the extremal
Kerr black holes, we provide explicit expressions of the Love numbers at low
frequencies. Their calculation is simplified to specific formulas, which are
directly derived using the Leaver-MST methods.",2024-12-27T15:51:51Z,http://arxiv.org/abs/2412.19699v1,"Malcolm Perry, Maria J. Rodriguez"
Continuous majorization in quantum phase space with Wigner negativity,"Different variants of partial orders among quantum states arise naturally in
the context of various quantum resources. For example, in discrete variable
quantum computation, stabilizer operations naturally produce an order between
input and output states; in technical terms this order is vector majorization
of discrete Wigner functions in discrete phase space. The order results in
inequalities for magic monotones. In the continuous variable case, a natural
counterpart would be continuous majorization of Wigner functions in quantum
phase space. Indeed, this concept was recently proposed and explored (mostly
restricting to the single-mode case) in Van Herstraeten, Jabbour, Cerf, Quantum
7, 1021 (2023). In this work, we develop the theory of continuous majorization
in the general $N$-mode case. In particular, we propose extensions to include
states with finite Wigner negativity. Among our results, we prove a conjecture
made by Van Herstraeten, Jabbour and Cerf for the convex hull of $N$-mode
Gaussian states, and a phase space counterpart of Uhlmann's theorem of
majorization.",2024-12-27T15:51:15Z,http://arxiv.org/abs/2412.19698v1,"Jan de Boer, Giuseppe Di Giulio, Esko Keski-Vakkuri, Erik Tonni"
Differentiable groupoid objects and their abstract Lie algebroids,"The infinitesimal counterpart of a Lie groupoid is its Lie algebroid. As a
vector bundle, it is given by the source vertical tangent bundle restricted to
the identity bisection. Its sections can be identified with the invariant
vector fields on the groupoid, which are closed under the Lie bracket. We
generalize this differentiation procedure to groupoid objects in any category
with an abstract tangent structure in the sense of Rosick\'{y} and a scalar
multiplication by a ring object that plays the role of the real numbers. We
identify the categorical conditions that the groupoid object must satisfy to
admit a natural notion of invariant vector fields. Then we show that invariant
vector fields are closed under the Lie bracket defined by Rosick\'{y} and
satisfy the Leibniz rule with respect to ring-valued morphisms on the base of
the groupoid. The result is what we define axiomatically as an abstract Lie
algebroid, by generalizing the underlying vector bundle to a module object in
the slice category over its base. Examples include diffeomorphism groups,
bisection groups of Lie groupoids, the diffeological symmetry groupoids of
general relativity (Blohmann/Fernandes/Weinstein), symmetry groupoids in
Lagrangian Field Theory, holonomy groupoids of singular foliations, elastic
diffeological groupoids, and groupoid objects in differentiable stacks.",2024-12-27T15:50:02Z,http://arxiv.org/abs/2412.19697v1,"Lory Aintablian, Christian Blohmann"
"An Integrated Optimization and Deep Learning Pipeline for Predicting
  Live Birth Success in IVF Using Feature Optimization and Transformer-Based
  Models","In vitro fertilization (IVF) is a widely utilized assisted reproductive
technology, yet predicting its success remains challenging due to the
multifaceted interplay of clinical, demographic, and procedural factors. This
study develops a robust artificial intelligence (AI) pipeline aimed at
predicting live birth outcomes in IVF treatments. The pipeline uses anonymized
data from 2010 to 2018, obtained from the Human Fertilization and Embryology
Authority (HFEA). We evaluated the prediction performance of live birth success
as a binary outcome (success/failure) by integrating different feature
selection methods, such as principal component analysis (PCA) and particle
swarm optimization (PSO), with different traditional machine learning-based
classifiers including random forest (RF) and decision tree, as well as deep
learning-based classifiers including custom transformer-based model and a tab
transformer model with an attention mechanism. Our research demonstrated that
the best performance was achieved by combining PSO for feature selection with
the TabTransformer-based deep learning model, yielding an accuracy of 99.50%
and an AUC of 99.96%, highlighting its significant performance to predict live
births. This study establishes a highly accurate AI pipeline for predicting
live birth outcomes in IVF, demonstrating its potential to enhance personalized
fertility treatments.",2024-12-27T15:46:59Z,http://arxiv.org/abs/2412.19696v1,"Arezoo Borji, Hossam Haick, Birgit Pohn, Antonia Graf, Jana Zakall, S M Ragib Shahriar Islam, Gernot Kronreif, Daniel Kovatchki, Heinz Strohmer, Sepideh Hatamikia"
"Nonperturbative effects in triple-differential dijet and Z+jet
  production at the LHC","In comparisons of precision collider data to the most accurate highest-order
calculations in perturbative quantum chromodynamics (QCD), it is required to
correct for nonperturbative effects. Such effects are typically studied using
Monte Carlo event generators that complement fixed-order predictions with
perturbative parton showers and models for the nonperturbative effects of the
Underlying Event and hadronisation. Thereby, the final state of collision
events can be predicted at the level of stable particles, which serve as input
for full detector simulations.
  This article investigates the impact of nonperturbative effects on two
processes that may be used for precision determinations of the strong coupling
constant and the proton structure: the triple-differential dijet and Z+jet
production. While nonperturbative effects impact both processes, significant
differences among them are observed and further investigated. Indications are
found that the Underlying Event and hadronisation cannot fully explain these
differences and the perturbative modelling may play a significant role as well.",2024-12-27T15:42:07Z,http://arxiv.org/abs/2412.19694v1,"Stefan Gieseke, Maximilian Horzela, Manjit Kaur, Dari Leonardi, Klaus Rabbertz, Aayushi Singla, Cedric Verstege"
"From prediction to explanation: managing influential negative reviews
  through explainable AI","The profound impact of online reviews on consumer decision-making has made it
crucial for businesses to manage negative reviews. Recent advancements in
artificial intelligence (AI) technology have offered businesses novel and
effective ways to manage and analyze substantial consumer feedback. In response
to the growing demand for explainablility and transparency in AI applications,
this study proposes a novel explainable AI (XAI) algorithm aimed at identifying
influential negative reviews. The experiments conducted on 101,338 restaurant
reviews validate the algorithm's effectiveness and provides understandable
explanations from both the feature-level and word-level perspectives. By
leveraging this algorithm, businesses can gain actionable insights for
predicting, perceiving, and strategically responding to online negative
feedback, fostering improved customer service and mitigating the potential
damage caused by negative reviews.",2024-12-27T15:37:19Z,http://arxiv.org/abs/2412.19692v1,Rongping Shen
"Quantum Many-Body Lattice C-R-T Symmetry: Fractionalization, Anomaly,
  and Symmetric Mass Generation","Charge conjugation (C), mirror reflection (R), and time reversal (T)
symmetries, along with internal symmetries, are essential for massless Majorana
and Dirac fermions. These symmetries are sufficient to rule out potential
fermion bilinear mass terms, thereby establishing a gapless free fermion fixed
point phase, pivotal for symmetric mass generation (SMG) transition. In this
work, we systematically study the anomaly of C-R-T-internal symmetry in all
spacetime dimensions by analyzing the projective representation (i.e. the
fractionalization) of the C-R-T-internal symmetry group in the quantum
many-body Hilbert space on the lattice. By discovering the
fermion-flavor-number-dependent C-R-T-internal symmetry's anomaly structure, we
demonstrate an alternative way to derive the minimal flavor number for SMG,
which shows consistency with known results from K\""ahler-Dirac fermion or
cobordism classification. Our findings reveal that, in general spatial
dimensions, either 8 copies of staggered Majorana fermions or 4 copies of
staggered Dirac fermions admit SMG. By directly searching for 4-fermion
interactions that form commuting stabilizers respecting all symmetry
constraints, we can prove the explicit SMG gapping retained a unique ground
state in the codespace. Furthermore, we establish the correspondence between
the symmetry operators of staggered fermions and free fermions, which is
instrumental in facilitating the analysis of symmetry fractionalization at the
field theory level.",2024-12-27T15:36:31Z,http://arxiv.org/abs/2412.19691v1,"Yang-Yang Li, Juven Wang, Yi-Zhuang You"
"Terms that define nuclei on residuated lattices: a case study of
  BL-algebras","A nucleus $\gamma$ on a (bounded commutative integral) residuated lattice
$\mathbf{A}$ is a closure operator that satisfies the inequality $\gamma(a)
\cdot \gamma(b) \leq \gamma(a \cdot b)$ for all $a,b \in A$. In this article,
among several results, a description of an arbitrary nucleus on a residuated
lattice is given. Special attention is given to terms that define a nucleus on
every structure of a variety, as a means of generalizing the double negation
operation. Some general results about these terms are presented, together with
examples. The main result of this article consists of the description of all
terms of this kind for every given subvariety of BL-algebras. We exhibit
interesting nontrivial examples.",2024-12-27T15:34:51Z,http://arxiv.org/abs/2412.19690v1,"Sebastián Buss, Diego Castaño, José Patricio Díaz Varela"
"A Review on the Integration of Artificial Intelligence and Medical
  Imaging in IVF Ovarian Stimulation","Artificial intelligence (AI) has emerged as a powerful tool to enhance
decision-making and optimize treatment protocols in in vitro fertilization
(IVF). In particular, AI shows significant promise in supporting
decision-making during the ovarian stimulation phase of the IVF process. This
review evaluates studies focused on the applications of AI combined with
medical imaging in ovarian stimulation, examining methodologies, outcomes, and
current limitations. Our analysis of 13 studies on this topic reveals that,
reveal that while AI algorithms demonstrated notable potential in predicting
optimal hormonal dosages, trigger timing, and oocyte retrieval outcomes, the
medical imaging data utilized predominantly came from two-dimensional (2D)
ultrasound which mainly involved basic quantifications, such as follicle size
and number, with limited use of direct feature extraction or advanced image
analysis techniques. This points to an underexplored opportunity where advanced
image analysis approaches, such as deep learning, and more diverse imaging
modalities, like three-dimensional (3D) ultrasound, could unlock deeper
insights. Additionally, the lack of explainable AI (XAI) in most studies raises
concerns about the transparency and traceability of AI-driven decisions - key
factors for clinical adoption and trust. Furthermore, many studies relied on
single-center designs and small datasets, which limit the generalizability of
their findings. This review highlights the need for integrating advanced
imaging analysis techniques with explainable AI methodologies, as well as the
importance of leveraging multicenter collaborations and larger datasets.
Addressing these gaps has the potential to enhance ovarian stimulation
management, paving the way for efficient, personalized, and data-driven
treatment pathways that improve IVF outcomes.",2024-12-27T15:29:08Z,http://arxiv.org/abs/2412.19688v1,"Jana Zakall, Birgit Pohn, Antonia Graf, Daniel Kovatchki, Arezoo Borji, Ragib Shahriar Islam, Hossam Haick, Heinz Strohmer, Sepideh Hatamikia"
"A Large-scale Interpretable Multi-modality Benchmark for Facial Image
  Forgery Localization","Image forgery localization, which centers on identifying tampered pixels
within an image, has seen significant advancements. Traditional approaches
often model this challenge as a variant of image segmentation, treating the
binary segmentation of forged areas as the end product. We argue that the basic
binary forgery mask is inadequate for explaining model predictions. It doesn't
clarify why the model pinpoints certain areas and treats all forged pixels the
same, making it hard to spot the most fake-looking parts. In this study, we
mitigate the aforementioned limitations by generating salient region-focused
interpretation for the forgery images. To support this, we craft a Multi-Modal
Tramper Tracing (MMTT) dataset, comprising facial images manipulated using
deepfake techniques and paired with manual, interpretable textual annotations.
To harvest high-quality annotation, annotators are instructed to meticulously
observe the manipulated images and articulate the typical characteristics of
the forgery regions. Subsequently, we collect a dataset of 128,303 image-text
pairs. Leveraging the MMTT dataset, we develop ForgeryTalker, an architecture
designed for concurrent forgery localization and interpretation. ForgeryTalker
first trains a forgery prompter network to identify the pivotal clues within
the explanatory text. Subsequently, the region prompter is incorporated into
multimodal large language model for finetuning to achieve the dual goals of
localization and interpretation. Extensive experiments conducted on the MMTT
dataset verify the superior performance of our proposed model. The dataset,
code as well as pretrained checkpoints will be made publicly available to
facilitate further research and ensure the reproducibility of our results.",2024-12-27T15:23:39Z,http://arxiv.org/abs/2412.19685v1,"Jingchun Lian, Lingyu Liu, Yaxiong Wang, Yujiao Wu, Li Zhu, Zhedong Zheng"
"Boosting Private Domain Understanding of Efficient MLLMs: A Tuning-free,
  Adaptive, Universal Prompt Optimization Framework","Efficient multimodal large language models (EMLLMs), in contrast to
multimodal large language models (MLLMs), reduce model size and computational
costs and are often deployed on resource-constrained devices. However, due to
data privacy concerns, existing open-source EMLLMs rarely have access to
private domain-specific data during the pre-training process, making them
difficult to directly apply in device-specific domains, such as certain
business scenarios. To address this weakness, this paper focuses on the
efficient adaptation of EMLLMs to private domains, specifically in two areas:
1) how to reduce data requirements, and 2) how to avoid parameter fine-tuning.
Specifically, we propose a tun\textbf{\underline{I}}ng-free,
a\textbf{\underline{D}}aptiv\textbf{\underline{E}},
univers\textbf{\underline{AL}} \textbf{\underline{Prompt}} Optimization
Framework, abbreviated as \textit{\textbf{\ourmethod{}}} which consists of two
stages: 1) Predefined Prompt, based on the reinforcement searching strategy,
generate a prompt optimization strategy tree to acquire optimization priors; 2)
Prompt Reflection initializes the prompt based on optimization priors, followed
by self-reflection to further search and refine the prompt. By doing so,
\ourmethod{} elegantly generates the ``ideal prompts'' for processing private
domain-specific data. Note that our method requires no parameter fine-tuning
and only a small amount of data to quickly adapt to the data distribution of
private data. Extensive experiments across multiple tasks demonstrate that our
proposed \ourmethod{} significantly improves both efficiency and performance
compared to baselines.",2024-12-27T15:21:17Z,http://arxiv.org/abs/2412.19684v1,"Jiang Liu, Bolin Li, Haoyuan Li, Tianwei Lin, Wenqiao Zhang, Tao Zhong, Zhelun Yu, Jinghao Wei, Hao Cheng, Hao Jiang, Zheqi Lv, Juncheng Li, Siliang Tang, Yueting Zhuang"
"Combining Machine Learning with Recurrence Analysis for resonance
  detection","The width of a resonance in a nearly integrable system, i.e. in a
non-integrable system where chaotic motion is still not prominent, can tell us
how a perturbation parameter is driving the system away from integrability.
Although the tool that we are presenting here can be used is quite generic and
can be used in a variety of systems, our particular interest lies in binary
compact object systems known as extreme mass ratio inspirals (EMRIs). In an
EMRI a lighter compact object, like a black hole or a neutron star, inspirals
into a supermassive black hole due to gravitational radiation reaction. During
this inspiral the lighter object crosses resonances, which are still not very
well modeled. Measuring the width of resonances in EMRI models allows us to
estimate the importance of each perturbation parameter able to drive the system
away from resonances and decide whether its impact should be included in EMRI
waveform modeling or not. To tackle this issue in our study we show first that
recurrence quantifiers of orbits carry imprints of resonant behavior,
regardless of the system's dimensionality. As a next step, we apply a long
short-term memory machine learning architecture to automate the resonance
detection procedure. Our analysis is developed on a simple standard map and
gradually we extend it to more complicated systems until finally we employ it
in a generic deformed Kerr spacetime known in the literature as the
Johannsen-Psaltis spacetime.",2024-12-27T15:20:57Z,http://arxiv.org/abs/2412.19683v1,"Ondřej Zelenka, Ondřej Kopáček, Georgios Lukes-Gerakopoulos"
"A Hybrid Technique for Plant Disease Identification and Localisation in
  Real-time","Over the past decade, several image-processing methods and algorithms have
been proposed for identifying plant diseases based on visual data. DNN (Deep
Neural Networks) have recently become popular for this task. Both traditional
image processing and DNN-based methods encounter significant performance issues
in real-time detection owing to computational limitations and a broad spectrum
of plant disease features. This article proposes a novel technique for
identifying and localising plant disease based on the Quad-Tree decomposition
of an image and feature learning simultaneously. The proposed algorithm
significantly improves accuracy and faster convergence in high-resolution
images with relatively low computational load. Hence it is ideal for deploying
the algorithm in a standalone processor in a remotely operated image
acquisition and disease detection system, ideally mounted on drones and robots
working on large agricultural fields. The technique proposed in this article is
hybrid as it exploits the advantages of traditional image processing methods
and DNN-based models at different scales, resulting in faster inference. The F1
score is approximately 0.80 for four disease classes corresponding to potato
and tomato crops.",2024-12-27T15:20:45Z,http://arxiv.org/abs/2412.19682v1,"Mahendra Kumar Gohil, Anirudha Bhattacharjee, Rwik Rana, Kishan Lal, Samir Kumar Biswas, Nachiketa Tiwari, Bishakh Bhattacharya"
"An optimal uniqueness result for Riccati equations arising in abstract
  parabolic control problems","An abstract nonautonomous parabolic linear-quadratic regulator problem with
very general final cost operator P_T is considered, subject to the same
assumptions under which a classical solution of the associated differential
Riccati equation was shown to exist, in two papers appeared in 1999 and 2000,
by Terreni and the first named author. We prove an optimal uniqueness result
for the integral Riccati equation in a wide and natural class, filling a gap
existing in the autonomous case, too. In addition, we give a regularity result
for the optimal state.",2024-12-27T15:03:13Z,http://arxiv.org/abs/2412.19678v1,"Paolo Acquistapace, Francesco Bartaloni"
Deep ReLU networks -- injectivity capacity upper bounds,"We study deep ReLU feed forward neural networks (NN) and their injectivity
abilities. The main focus is on \emph{precisely} determining the so-called
injectivity capacity. For any given hidden layers architecture, it is defined
as the minimal ratio between number of network's outputs and inputs which
ensures unique recoverability of the input from a realizable output. A strong
recent progress in precisely studying single ReLU layer injectivity properties
is here moved to a deep network level. In particular, we develop a program that
connects deep $l$-layer net injectivity to an $l$-extension of the $\ell_0$
spherical perceptrons, thereby massively generalizing an isomorphism between
studying single layer injectivity and the capacity of the so-called
(1-extension) $\ell_0$ spherical perceptrons discussed in [82]. \emph{Random
duality theory} (RDT) based machinery is then created and utilized to
statistically handle properties of the extended $\ell_0$ spherical perceptrons
and implicitly of the deep ReLU NNs. A sizeable set of numerical evaluations is
conducted as well to put the entire RDT machinery in practical use. From these
we observe a rapidly decreasing tendency in needed layers' expansions, i.e., we
observe a rapid \emph{expansion saturation effect}. Only $4$ layers of depth
are sufficient to closely approach level of no needed expansion -- a result
that fairly closely resembles observations made in practical experiments and
that has so far remained completely untouchable by any of the existing
mathematical methodologies.",2024-12-27T14:57:40Z,http://arxiv.org/abs/2412.19677v1,Mihailo Stojnic
"Optimizing Local-Global Dependencies for Accurate 3D Human Pose
  Estimation","Transformer-based methods have recently achieved significant success in 3D
human pose estimation, owing to their strong ability to model long-range
dependencies. However, relying solely on the global attention mechanism is
insufficient for capturing the fine-grained local details, which are crucial
for accurate pose estimation. To address this, we propose SSR-STF, a
dual-stream model that effectively integrates local features with global
dependencies to enhance 3D human pose estimation. Specifically, we introduce
SSRFormer, a simple yet effective module that employs the skeleton selective
refine attention (SSRA) mechanism to capture fine-grained local dependencies in
human pose sequences, complementing the global dependencies modeled by the
Transformer. By adaptively fusing these two feature streams, SSR-STF can better
learn the underlying structure of human poses, overcoming the limitations of
traditional methods in local feature extraction. Extensive experiments on the
Human3.6M and MPI-INF-3DHP datasets demonstrate that SSR-STF achieves
state-of-the-art performance, with P1 errors of 37.4 mm and 13.2 mm
respectively, outperforming existing methods in both accuracy and
generalization. Furthermore, the motion representations learned by our model
prove effective in downstream tasks such as human mesh recovery. Codes are
available at https://github.com/poker-xu/SSR-STF.",2024-12-27T14:54:12Z,http://arxiv.org/abs/2412.19676v1,"Guangsheng Xu, Guoyi Zhang, Lejia Ye, Shuwei Gan, Xiaohu Zhang, Xia Yang"
"DLScanner: A parameter space scanner package assisted by deep learning
  methods","In this paper, we introduce a scanner package enhanced by deep learning (DL)
techniques. The proposed package addresses two significant challenges
associated with previously developed DL-based methods: slow convergence in
high-dimensional scans and the limited generalization of the DL network when
mapping random points to the target space. To tackle the first issue, we
utilize a similarity learning network that maps sampled points into a
representation space. In this space, in-target points are grouped together
while out-target points are effectively pushed apart. This approach enhances
the scan convergence by refining the representation of sampled points. The
second challenge is mitigated by integrating a dynamic sampling strategy.
Specifically, we employ a VEGAS mapping to adaptively suggest new points for
the DL network while also improving the mapping when more points are collected.
Our proposed framework demonstrates substantial gains in both performance and
efficiency compared to other scanning methods.",2024-12-27T14:52:42Z,http://arxiv.org/abs/2412.19675v1,"A. Hammad, Raymundo Ramos"
"Interplay of quantum statistics and self-interference in extended
  colliders","Collision of quantum particles remains an effective way of probing their
mutual statistics. Colliders based on quantum point contacts in quantum Hall
edge states have been successfully used to probe the statistics of the
underlying quantum particles. Notwithstanding the extensive theoretical work
focusing on point-like colliders, when it comes to experiment, the colliders
are rarely point-like objects and can support a resonant level or multiple
tunneling points. We present a study of a paradigmatic extended
(non-point-like) fermionic collider which is readily generalized to colliding
bosons. As with particle interferometers, in an extended collider there is an
infinite number of trajectories for any single or multi-particle event.
Self-interference of the former can lead to an apparent bunching of fermions
when we compare the cross-current correlator with the classical benchmark
representing two active sources. In view of this apparent bunching behavior of
fermions, we identify an irreducible cross-current correlator which reveals the
true mutual statistics of fermions.",2024-12-27T14:50:36Z,http://arxiv.org/abs/2412.19674v1,"Sai Satyam Samal, Smitha Vishveshwara, Yuval Gefen, Jukka I. Väyrynen"
Port-Hamiltonian nonlinear systems,"Control theory often takes the mathematical model of the to-be-control-led
system for granted. In contrast, port-Hamiltonian systems theory bridges the
gap between modelling and control for physical systems. It provides a unified
framework for the modelling of complex multiphysics systems. At the same time
it offers powerful tools for analysis and control by identifying the underlying
physical structure, as reflected in, e.g., energy balance and other conserved
quantities. This leads to control schemes that \emph{exploit} the physical
structure, instead of compensating for it. As a result, the derived control
laws tend to be simple, physically interpretable, and robust with respect to
physical parameter variations.
  In this paper, after introducing port-Hamiltonian systems, the focus is on
'control by interconnection' for set-point stabilization of nonlinear physical
systems. Most of this theory is well-established, but novel developments using
'energy ports' instead of 'power ports' are also included.",2024-12-27T14:45:45Z,http://arxiv.org/abs/2412.19673v1,Arjan van der Schaft
Spectral form factors for curved spacetimes with horizon,"The spectral form factor is believed to provide a special type of behavior
called ""dip-ramp-plateau"" in chaotic quantum systems which originates from the
random matrix theory. A similar behavior could be observed for deterministic
systems, ranging from the Riemann zeta function to the scattering amplitudes of
different types. It has been shown recently, the same behavior is observed for
the spectral form factor when the normal modes of a scalar massless field
theory in the brickwall model of the BTZ black hole are substituted as
eigenvalues of some quantum Hamiltonian. At the same time, the level spacing
distribution of these eigenvalues differs from that associated with the random
matrix theory ensembles. In this paper, we generalize these results considering
the recently proposed generalized spectral form factor for the de Sitter and
BTZ spacetimes. We study the details of this complex-valued form factor for
integrable quantum systems and for backgrounds with a horizon comparing it with
the random matrix theory behavior. As a result, we confirm that the scalar
field normal modes once again exhibit features of chaos.",2024-12-27T14:43:35Z,http://arxiv.org/abs/2412.19672v1,"Dmitry S. Ageev, Vasilii V. Pushkarev, Anastasia N. Zueva"
"Toward Scalable Multirobot Control: Fast Policy Learning in Distributed
  MPC","Distributed model predictive control (DMPC) is promising in achieving optimal
cooperative control in multirobot systems (MRS). However, real-time DMPC
implementation relies on numerical optimization tools to periodically calculate
local control sequences online. This process is computationally demanding and
lacks scalability for large-scale, nonlinear MRS. This article proposes a novel
distributed learning-based predictive control (DLPC) framework for scalable
multirobot control. Unlike conventional DMPC methods that calculate open-loop
control sequences, our approach centers around a computationally fast and
efficient distributed policy learning algorithm that generates explicit
closed-loop DMPC policies for MRS without using numerical solvers. The policy
learning is executed incrementally and forward in time in each prediction
interval through an online distributed actor-critic implementation. The control
policies are successively updated in a receding-horizon manner, enabling fast
and efficient policy learning with the closed-loop stability guarantee. The
learned control policies could be deployed online to MRS with varying robot
scales, enhancing scalability and transferability for large-scale MRS.
Furthermore, we extend our methodology to address the multirobot safe learning
challenge through a force field-inspired policy learning approach. We validate
our approach's effectiveness, scalability, and efficiency through extensive
experiments on cooperative tasks of large-scale wheeled robots and multirotor
drones. Our results demonstrate the rapid learning and deployment of DMPC
policies for MRS with scales up to 10,000 units.",2024-12-27T14:31:52Z,http://arxiv.org/abs/2412.19669v1,"Xinglong Zhang, Wei Pan, Cong Li, Xin Xu, Xiangke Wang, Ronghua Zhang, Dewen Hu"
Engineering Digital Systems for Humanity: a Research Roadmap,"As testified by new regulations like the European AI Act, worries about the
human and societal impact of (autonomous) software technologies are becoming of
public concern. Human, societal, and environmental values, alongside
traditional software quality, are increasingly recognized as essential for
sustainability and long-term well-being. Traditionally, systems are engineered
taking into account business goals and technology drivers. Considering the
growing awareness in the community, in this paper, we argue that engineering of
systems should also consider human, societal, and environmental drivers. Then,
we identify the macro and technological challenges by focusing on humans and
their role while co-existing with digital systems. The first challenge
considers humans in a proactive role when interacting with digital systems,
i.e., taking initiative in making things happen instead of reacting to events.
The second concerns humans having a reactive role in interacting with digital
systems, i.e., humans interacting with digital systems as a reaction to events.
The third challenge focuses on humans with a passive role, i.e., they
experience, enjoy or even suffer the decisions and/or actions of digital
systems. The fourth challenge concerns the duality of trust and
trustworthiness, with humans playing any role. Building on the new human,
societal, and environmental drivers and the macro and technological challenges,
we identify a research roadmap of digital systems for humanity. The research
roadmap is concretized in a number of research directions organized into four
groups: development process, requirements engineering, software architecture
and design, and verification and validation.",2024-12-27T14:28:39Z,http://arxiv.org/abs/2412.19668v1,"Marco Autili, Martina De Sanctis, Paola Inverardi, Patrizio Pelliccione"
Multipole moments in stationary spacetimes,"Multipole moments in general relativity serve as a powerful tool for
characterising the gravitational field. In this paper, we review the
construction of the Geroch--Hansen multipole moments for stationary
asymptotically flat vacuum spacetimes. A particular focus is placed on the
well-definedness of these moments, which hinges on the uniqueness of the
one-point conformal completion in Geroch's asymptotic flatness definition.
Based on Geroch's approach, we formulate and prove a revised uniqueness result,
thereby filling in some gaps in the original approach. Uniqueness holds up to
certain conformal transformations, and we discuss how the multipole moments
behave under such transformations.",2024-12-27T14:27:01Z,http://arxiv.org/abs/2412.19667v1,Jorn van Voorthuizen
"Standardizing reverberation-mapped H$α$ and H$β$ active
  galactic nuclei using radius--luminosity relations involving monochromatic
  and broad H$α$ luminosities","We test the standardizability of a homogeneous sample of 41 lower-redshift
($0.00415\leq z \leq 0.474$) active galactic nuclei (AGNs) reverberation-mapped
(RM) using the broad H$\alpha$ and H$\beta$ emission lines. We find that these
sources can be standardized using four radius$-$luminosity ($R-L$) relations
incorporating H$\alpha$ and H$\beta$ time delays and monochromatic and broad
H$\alpha$ luminosities. Although the $R-L$ relation parameters are well
constrained and independent of the six cosmological models considered, the
resulting cosmological constraints are weak. The measured $R-L$ relations
exhibit slightly steeper slopes than predicted by a simple photoionization
model and steeper than those from previous higher-redshift H$\beta$ analyses
based on larger datasets. These differences likely reflect the absence of
high-accreting sources in our smaller, lower-redshift sample, which primarily
comprises lower-accreting AGNs. The inferred cosmological parameters are
consistent within 2$\sigma$ (or better) with those from better-established
cosmological probes. This contrasts with our earlier findings using a larger,
heterogeneous sample of 118 H$\beta$ AGNs, which yielded cosmological
constraints differing by $\gtrsim 2\sigma$ from better-established cosmological
probes. Our analysis demonstrates that sample homogeneity$-$specifically, the
use of a consistent time-lag determination method$-$is crucial for developing
RM AGNs as a cosmological probe.",2024-12-27T14:23:31Z,http://arxiv.org/abs/2412.19665v1,"Shulei Cao, Amit Kumar Mandal, Michal Zajaček, Bożena Czerny, Bharat Ratra"
The Value of Recall in Extensive-Form Games,"Imperfect-recall games, in which players may forget previously acquired
information, have found many practical applications, ranging from game
abstractions to team games and testing AI agents. In this paper, we quantify
the utility gain by endowing a player with perfect recall, which we call the
value of recall (VoR). While VoR can be unbounded in general, we parameterize
it in terms of various game properties, namely the structure of chance nodes
and the degree of absentmindedness (the number of successive times a player
enters the same information set). Further, we identify several pathologies that
arise with VoR, and show how to circumvent them. We also study the complexity
of computing VoR, and how to optimally apportion partial recall. Finally, we
connect VoR to other previously studied concepts in game theory, including the
price of anarchy. We use that connection in conjunction with the celebrated
smoothness framework to characterize VoR in a broad class of games.",2024-12-27T14:12:45Z,http://arxiv.org/abs/2412.19659v1,"Ratip Emin Berker, Emanuel Tewolde, Ioannis Anagnostides, Tuomas Sandholm, Vincent Conitzer"
"Uniform finiteness of measures of maximal entropy for $C^r$ surface
  diffeomorphisms with large entropy","We prove that for a $C^r$ surface diffeomorphism $f$ satisfying $h_{\rm
top}(f)&gt;\frac{\lambda_{\min}(f)}{r}$, the number of ergodic measures of maximal
entropy is upper semi-continuous at $f$. This result connects to the discussion
in \cite[Remark 1.9]{BCS22}.",2024-12-27T14:06:41Z,http://arxiv.org/abs/2412.19658v1,"Chiyi Luo, Dawei Yang"
Quantum Cluster State Model with Haagerup Fusion Category Symmetry,"We propose a (1+1)D lattice model, inspired by a weak Hopf algebra
generalization of the cluster state model, which realizes Haagerup fusion
category symmetry and features a tensor product Hilbert space. The construction
begins with a reconstruction of the Haagerup weak Hopf algebra $H_3$ from the
Haagerup fusion category, ensuring that the representation category of $H_3$ is
equivalent to Haagerup fusion category. Utilizing the framework of symmetry
topological field theory (SymTFT), we develop an ultra-thin weak Hopf quantum
double model, characterized by a smooth topological boundary condition. We show
that this model supports Haagerup fusion category symmetry. Finally, we solve
the ground state of the model in terms of a weak Hopf matrix product state,
which serves as a natural generalization of the cluster state, embodying
Haagerup fusion category symmetry.",2024-12-27T14:05:15Z,http://arxiv.org/abs/2412.19657v1,Zhian Jia
Movable Antenna Aided Physical Layer Security with No Eavesdropper CSI,"A novel movable antenna (MA)-aided secure transmission framework is proposed
to enhance the secrecy transmission rate without relying on the eavesdropper's
channel state information. Within this framework, a joint beamforming and
jamming scheme is proposed, where the power of the confidential signal is
minimized by optimizing the positions of the MAs, and the residual power is
used to jam the eavesdropper. An efficient gradient-based method is employed to
solve this non-convex problem. Numerical results are provided to demonstrate
the superiority of the MA-based framework over systems using traditional
fixed-position antennas in secure transmission.",2024-12-27T14:05:08Z,http://arxiv.org/abs/2412.19656v1,"Zhenqiao Cheng, Chongjun Ouyang, Xingqi Zhang"
"FreStega: A Plug-and-Play Method for Boosting Imperceptibility and
  Capacity in Generative Linguistic Steganography for Real-World Scenarios","Linguistic steganography embeds secret information in seemingly innocent
texts, safeguarding privacy in surveillance environments. Generative linguistic
steganography leverages the probability distribution of language models (LMs)
and applies steganographic algorithms to generate stego tokens, gaining
attention with recent Large Language Model (LLM) advancements. To enhance
security, researchers develop distribution-preserving stego algorithms to
minimize the gap between stego sampling and LM sampling. However, the reliance
on language model distributions, coupled with deviations from real-world cover
texts, results in insufficient imperceptibility when facing steganalysis
detectors in real-world scenarios. Moreover, LLM distributions tend to be more
deterministic, resulting in reduced entropy and, consequently, lower embedding
capacity. In this paper, we propose FreStega, a plug-and-play method to
reconstruct the distribution of language models used for generative linguistic
steganography. FreStega dynamically adjusts token probabilities from the
language model at each step of stegotext auto-regressive generation, leveraging
both sequential and spatial dimensions. In sequential adjustment, the
temperature is dynamically adjusted based on instantaneous entropy, enhancing
the diversity of stego texts and boosting embedding capacity. In the spatial
dimension, the distribution is aligned with guidance from the target domain
corpus, closely mimicking real cover text in the target domain. By reforming
the distribution, FreStega enhances the imperceptibility of stego text in
practical scenarios and improves steganographic capacity by 15.41\%, all
without compromising the quality of the generated text. FreStega serves as a
plug-and-play remedy to enhance the imperceptibility and embedding capacity of
existing distribution-preserving steganography methods in real-world scenarios.",2024-12-27T13:56:51Z,http://arxiv.org/abs/2412.19652v1,Kaiyi Pang
Compactifications and measures for rational maps,"We study extensions of the measure of maximal entropy to suitable
compactifications of the parameter space and the moduli space of rational maps
acting on the Riemann sphere. For parameter space, we consider a space which
resolves the discontinuity of the iterate map. We show that the measure of
maximal entropy extends continuously to this resolution space. For moduli
space, we consider a space which resolves the discontinuity of the iterate map
acting on its geometric invariant theory compactification. We show that the
measure of maximal entropy, barycentered and modulo rotations, also extends
continuously to this resolution space. Thus, answering in the positive a
question raised by DeMarco. A main ingredient is a description of limiting
dynamics for some sequences.",2024-12-27T13:56:38Z,http://arxiv.org/abs/2412.19651v1,"Jan Kiwi, Hongming Nie"
"Branes and Representations of DAHA $C^\vee C_1$: affine braid group
  action on category","We study the representation theory of the spherical double affine Hecke
algebra (DAHA) of $C^\vee C_1$, using brane quantization. By showing a
one-to-one correspondence between Lagrangian $A$-branes with compact support
and finite-dimensional representations of the spherical DAHA, we provide
evidence of derived equivalence between the $A$-brane category of
$\mathrm{SL}(2,\mathbb{C})$-character variety of a four-punctured sphere and
the representation category of DAHA of $C^\vee C_1$. The $D_4$ root system
plays an essential role in understanding both the geometry and representation
theory. In particular, this $A$-model approach reveals the action of an affine
braid group of type $D_4$ on the category. As a by-product, our geometric
investigation offers detailed information about the low-energy dynamics of the
SU(2) $N_f=4$ Seiberg-Witten theory.",2024-12-27T13:54:31Z,http://arxiv.org/abs/2412.19647v1,"Junkang Huang, Satoshi Nawata, Yutai Zhang, Shutong Zhuang"
"VideoMaker: Zero-shot Customized Video Generation with the Inherent
  Force of Video Diffusion Models","Zero-shot customized video generation has gained significant attention due to
its substantial application potential. Existing methods rely on additional
models to extract and inject reference subject features, assuming that the
Video Diffusion Model (VDM) alone is insufficient for zero-shot customized
video generation. However, these methods often struggle to maintain consistent
subject appearance due to suboptimal feature extraction and injection
techniques. In this paper, we reveal that VDM inherently possesses the force to
extract and inject subject features. Departing from previous heuristic
approaches, we introduce a novel framework that leverages VDM's inherent force
to enable high-quality zero-shot customized video generation. Specifically, for
feature extraction, we directly input reference images into VDM and use its
intrinsic feature extraction process, which not only provides fine-grained
features but also significantly aligns with VDM's pre-trained knowledge. For
feature injection, we devise an innovative bidirectional interaction between
subject features and generated content through spatial self-attention within
VDM, ensuring that VDM has better subject fidelity while maintaining the
diversity of the generated video.Experiments on both customized human and
object video generation validate the effectiveness of our framework.",2024-12-27T13:49:25Z,http://arxiv.org/abs/2412.19645v1,"Tao Wu, Yong Zhang, Xiaodong Cun, Zhongang Qi, Junfu Pu, Huanzhang Dou, Guangcong Zheng, Ying Shan, Xi Li"
"Simple Barban--Davenport--Halberstam type asymptotics for general
  sequences","We prove two estimates for the Barban--Davenport--Halberstam type variance of
a general complex sequence in arithmetic progressions. The proofs are
elementary, and our estimates are capable of yielding an asymptotic for the
variance when the sequence is sufficiently nice, and is either somewhat sparse
or is sufficiently like the integers in its divisibility by small moduli.
  As a concrete application, we deduce a Barban--Davenport--Halberstam type
variance asymptotic for the $y$-smooth numbers less than $x$, on a wide range
of the parameters. This addresses a question considered by Granville and
Vaughan.",2024-12-27T13:48:30Z,http://arxiv.org/abs/2412.19644v1,Adam J. Harper
Global thermodynamics for isothermal fluids under gravity,"We develop a formulation of global thermodynamics for equilibrium systems
under the influence of gravity. The free energy for simple fluids is extended
to include a dependence on $(T, V, N, mgL)$, where $L$ represents the vertical
system length in the direction of gravity. A central idea in this formulation
is to uniquely fix the reference point of the gravitational potential, ensuring
a consistent thermodynamic framework. Using this framework, we derive the
probability density of thermodynamic quantities, which allows us to define a
variational function for determining equilibrium liquid-gas coexistence under
gravity. The resulting free energy landscape, derived from the variational
function, reveals the local stability of liquid-gas configurations.
Specifically, the liquid phase resides at the lower portion of the system due
to gravity, while the inverted configuration (with liquid on top) is also
locally stable in this landscape. Furthermore, we characterize the transition
between these liquid-gas configurations as a first-order phase transition using
the thermodynamic free energy of $(T,V,N,mgL)$. Finally, we validate the
predictions of global thermodynamics through molecular dynamics simulations,
demonstrating the applicability and accuracy of the proposed framework.",2024-12-27T13:47:08Z,http://arxiv.org/abs/2412.19643v1,"Naoko Nakagawa, Shin-ichi Sasa, Takamichi Hirao, Tsuyoshi Shiina, Kyosuke Tachi, Akira Yoshida"
"Mangasarian-Fromovitz-type constraint qualification and optimality
  conditions for smooth infinite programming problems","We introduce a constraint qualification condition (GPMFCQ) for smooth
infinite programming problems, where the nonlinear operator defining the
equality constraints has nonsurjective derivative at the local minimum. The
condition is a generalization of PMFCQ introduced by Morduhovich and Nghia. We
prove the existence of Lagrange multipliers by using either Hurwicz set or
Nonlinear Farkas Minkowski condition.",2024-12-27T13:42:03Z,http://arxiv.org/abs/2412.19642v1,"Ewa M. Bednarczuk, Krzysztof W. Leśniewski, Krzysztof E. Rutkowski"
ReNeg: Learning Negative Embedding with Reward Guidance,"In text-to-image (T2I) generation applications, negative embeddings have
proven to be a simple yet effective approach for enhancing generation quality.
Typically, these negative embeddings are derived from user-defined negative
prompts, which, while being functional, are not necessarily optimal. In this
paper, we introduce ReNeg, an end-to-end method designed to learn improved
Negative embeddings guided by a Reward model. We employ a reward feedback
learning framework and integrate classifier-free guidance (CFG) into the
training process, which was previously utilized only during inference, thus
enabling the effective learning of negative embeddings. We also propose two
strategies for learning both global and per-sample negative embeddings.
Extensive experiments show that the learned negative embedding significantly
outperforms null-text and handcrafted counterparts, achieving substantial
improvements in human preference alignment. Additionally, the negative
embedding learned within the same text embedding space exhibits strong
generalization capabilities. For example, using the same CLIP text encoder, the
negative embedding learned on SD1.5 can be seamlessly transferred to
text-to-image or even text-to-video models such as ControlNet, ZeroScope, and
VideoCrafter2, resulting in consistent performance improvements across the
board.",2024-12-27T13:31:55Z,http://arxiv.org/abs/2412.19637v1,"Xiaomin Li, Yixuan Liu, Takashi Isobe, Xu Jia, Qinpeng Cui, Dong Zhou, Dong Li, You He, Huchuan Lu, Zhongdao Wang, Emad Barsoum"
A Brief Overlook on Magnetoplasmadynamic Thrusters,"This paper presents a comprehensive analysis of Magnetoplasmadynamic
Thrusters (MPDT), examining their working principles, performance
characteristics, and potential applications in space propulsion. The study
focuses on both self-field and applied-field MPDT variants, detailing the
fundamental physics of plasma generation, acceleration mechanisms through
Lorentz forces, and plasma detachment processes. Through mathematical modeling
and experimental data analysis, the paper demonstrates MPDTs' capability to
achieve high specific impulse and efficient propellant utilization compared to
chemical propulsion systems. While highlighting their advantages for deep space
missions and satellite operations, the study also addresses key challenges,
including high power requirements and thermal management issues. The research
concludes that despite current technological limitations, MPDTs show promising
potential for future space exploration, particularly for long-duration missions
requiring sustained thrust.",2024-12-27T13:30:13Z,http://arxiv.org/abs/2412.19636v1,Egemen Gover
"High performance Black Phosphorus/Graphitic Carbon Nitride
  Heterostructure-based Wearable Sensor for Real-time Sweat Glucose Monitoring","Wearable, non-invasive glucose sensors capable of accurate and continuous
monitoring are crucial for managing metabolic conditions yet achieving high
sensitivity and stability in these devices remains challenging. In this work,
we present a Black Phosphorus/Graphitic Carbon Nitride (BP/g-CN)
heterostructure engineered to leverage phosphorus-nitrogen interactions for
enhanced electrochemical glucose oxidation activity. Compared to pristine gCN,
the BP-gCN heterostructure demonstrates a significantly improved
electrochemical surface area (ECSA) and nearly two-fold reduction in charge
transfer resistance (Rct), achieving remarkable glucose sensitivity of 1.1 uA
mM(^-1) cm(^-2) at physiological pH. Density functional theory (DFT)
calculations revealed stronger glucose adsorption and higher charge transfer on
the BP-gCN heterostructure compared to pristine gCN surface. These theoretical
insights complement the experimental findings, highlighting the superior
electrocatalytic performance of the heterostructure and the role of oxidized BP
surface. Furthermore, the BP-gCN sensor is integrated into a wearable device
platform with microfluidic layers and a Near Field Communication (NFC) chip,
forming a conformal skin patch that enables real-time sweat glucose monitoring.
This demonstration of a high-performance, non-enzymatic wearable glucose sensor
based on a heterostructure design underscores the potential of the device for
seamless health management and paves the way for next generation biosensing
platforms aimed at improving personalized and continuous health monitoring.",2024-12-27T13:22:58Z,http://arxiv.org/abs/2412.19633v1,"Ecem Ezgi Ozkahraman, Zafer Eroglu, Vladimir Efremov, Arooba Maryyam, Taher Abbasiasl, Ritu Das, Hadi Mirzajani, Berna Akgenc Hanedar, Levent Beker, Onder Metin"
"Primordial Black Hole Formation from the Upward Step Model: Avoiding
  Overproduction","We investigate the formation of primordial black holes (PBHs) in an upward
step inflationary model, where nonlinearities between curvature perturbations
and field fluctuations introduce a cutoff, deviating from the Gaussian case.
This necessitates a reevaluation of PBH formation, as $\mathcal{R}$ is not the
optimal variable for estimating abundance. Using the extended Press-Schechter
formalism, we show that non-Gaussianity modifies both the curvature
perturbation profile $\mathcal{R}(r)$ and the integration path in probability
space, significantly impacting PBH abundance. Our results reveal that the
abundance initially increases with the parameter $h$, which characterizes the
relaxation stage after the step. However, beyond a critical value ($h \simeq
5.9$), it sharply declines before rising again. Furthermore, we demonstrate
that non-Gaussianity introduces uncertainties in indirect PBH observations via
gravitational waves. Notably, we present an example where a positive $f_{\rm
NL}$ does not necessarily enhance PBH production, contrary to conventional
expectations. Finally, by accounting for non-perturbative effects, we resolve
the overproduction of PBHs suggested by pulsar timing array (PTA) data,
underscoring the critical importance of incorporating non-Gaussianity in future
studies.",2024-12-27T13:21:06Z,http://arxiv.org/abs/2412.19631v1,"Xiaoding Wang, Xiao-Han Ma, Yi-Fu Cai"
IMTP: Search-based Code Generation for In-memory Tensor Programs,"Processing-in-DRAM (DRAM-PIM) has emerged as a promising technology for
accelerating memory-intensive operations in modern applications, such as Large
Language Models (LLMs). Despite its potential, current software stacks for
DRAM-PIM face significant challenges, including reliance on hand-tuned
libraries that hinder programmability, limited support for high-level
abstractions, and the lack of systematic optimization frameworks. To address
these limitations, we present IMTP, a search-based optimizing tensor compiler
for UPMEM. Key features of IMTP include: (1) automated searches of the joint
search space for host and kernel tensor programs, (2) PIM-aware optimizations
for efficiently handling boundary conditions, and (3) improved search
algorithms for the expanded search space of UPMEM systems. Our experimental
results on UPMEM hardware demonstrate performance gains of up to 8.21x for
various UPMEM benchmark kernels and 5.33x for GPT-J layers. To the best of our
knowledge, IMTP is the first tensor compiler to provide fully automated,
autotuning-integrated code generation support for a DRAM-PIM system. By
bridging the gap between high-level tensor computation abstractions and
low-level hardware-specific requirements, IMTP establishes a foundation for
advancing DRAM-PIM programmability and enabling streamlined optimization.",2024-12-27T13:19:35Z,http://arxiv.org/abs/2412.19630v1,"Yongwon Shin, Dookyung Kang, Hyojin Sung"
On one-loop amplitudes in gauge theories,"We propose a new ``universal expansion"" for one-loop amplitudes with
arbitrary number of gluons in $D$ dimensions, which holds for general gauge
theories with gluons/fermions/scalars in the loop, including pure and
supersymmetric Yang-Mills theories. It expresses the $n$-gluon amplitudes as a
linear combination of universal scalar-loop amplitudes with $n{-}m$ gluons and
$m$ scalars, multiplied by gauge-invariant building blocks (defined for general
gauge theories); the integrands of these scalar-loop amplitudes are given in
terms of tree-level objects attached to the scalar loop, or by differential
operators acting on the most important part which is proportional to $D$ (with
$m=0$). We present closed-formula for these one-loop integrands and prove them
by showing that the single cuts are correctly reproduced by the gluing of an
additional pair of gluons (fermions/scalars) in the forward limit, plus $n$
gluons in a tree amplitude.",2024-12-27T13:15:17Z,http://arxiv.org/abs/2412.19629v1,"Qu Cao, Jin Dong, Song He, Fan Zhu"
An Exact Black Hole Scattering Amplitude,"General Relativity famously predicts precession of orbital motions in the
Schwarzschild metric. In this paper we show that by adding a NUT charge $N =
iM$ the precession vanishes to all orders in $G$ even for rotating black holes.
Moreover, we conjecture a generalization of the eikonal formula and show that
the classical integrable trajectories determine the full quantum amplitude for
this black hole, by means of exponentiation of the Post-Minkowskian radial
action. Several consequences of integrability in self-dual gravity are
discussed.",2024-12-27T13:13:27Z,http://arxiv.org/abs/2412.19627v1,"Alfredo Guevara, Uri Kol, Huy Tran"
An impediment to torsion from spectral geometry,"Modifications of standard general relativity that bring torsion into a game
have a long-standing history. However, no convincing arguments exist for or
against its presence in physically acceptable gravity models. In this Letter,
we provide an argument based on spectral geometry (using methods of
pseudo-differential calculus) that suggests that the torsion shall be excluded
from the consideration. We demonstrate that there is no well-defined functional
extending to the torsion-full case of the spectral formulation of the Einstein
tensor.",2024-12-27T13:05:26Z,http://arxiv.org/abs/2412.19626v1,"Arkadiusz Bochniak, Ludwik Dąbrowski, Andrzej Sitarz, Paweł Zalecki"
Reflexive modules and Auslander-type conditions,"We study the category $\mathop{\mathrm{ref}}\Lambda$ of reflexive modules
over a two-sided Noetherian ring $\Lambda$. We show that the category
$\mathop{\mathrm{ref}}\Lambda$ is quasi-abelian if and only if $\Lambda$
satisfies certain Auslander-type condition on the minimal injective resolution
of the ring itself. Furthermore, we establish a Morita theorem which
characterizes the category of reflexive modules among quasi-abelian categories
in terms of generator-cogenerators.",2024-12-27T13:02:12Z,http://arxiv.org/abs/2412.19625v1,Norihiro Hanihara
"An unholy trinity: TFNP, polynomial systems, and the quantum
  satisfiability problem","The theory of Total Function NP (TFNP) and its subclasses says that, even if
one is promised an efficiently verifiable proof exists for a problem, finding
this proof can be intractable. Despite the success of the theory at showing
intractability of problems such as computing Brouwer fixed points and Nash
equilibria, subclasses of TFNP remain arguably few and far between. In this
work, we define two new subclasses of TFNP borne of the study of complex
polynomial systems: Multi-homogeneous Systems (MHS) and Sparse Fundamental
Theorem of Algebra (SFTA). The first of these is based on B\'ezout's theorem
from algebraic geometry, marking the first TFNP subclass based on an algebraic
geometric principle. At the heart of our study is the computational problem
known as Quantum SAT (QSAT) with a System of Distinct Representatives (SDR),
first studied by [Laumann, L\""auchli, Moessner, Scardicchio, and Sondhi 2010].
Among other results, we show that QSAT with SDR is MHS-complete, thus giving
not only the first link between quantum complexity theory and TFNP, but also
the first TFNP problem whose classical variant (SAT with SDR) is easy but whose
quantum variant is hard. We also show how to embed the roots of a sparse,
high-degree, univariate polynomial into QSAT with SDR, obtaining that SFTA is
contained in a zero-error version of MHS. We conjecture this construction also
works in the low-error setting, which would imply SFTA is contained in MHS.",2024-12-27T12:57:06Z,http://arxiv.org/abs/2412.19623v1,"Marco Aldi, Sevag Gharibian, Dorian Rudolph"
Anisotropic Band Flattening in Twisted Bilayer of M-Valley MXenes,"Experimental studies on moir\'e materials have predominantly focused on
twisted hexagonal lattice with low-energy states near the $\Gamma$- or
K-points. These materials, characterized by isotropic low-energy dispersion,
are fundamentally distinct from those with anisotropic properties. Here we
introduce a series of semiconducting transition metal carbides (MXenes)
$M_2$C$T_2$ ($M$ = Ti, Zr, Hf, Sc, Y; $T$ = O, F, Cl) as a novel platform for
M-valley moir\'e materials. Take Ti$_2$CO$_2$ and Zr$_2$CO$_2$ as
representative examples, large-scale \emph{ab initio} calculations show that
their AB-stacked twisted homobilayer features three three-fold rotational
symmetry related M-valleys with time-reserval symmetry and giant anisotropic
band flattening. We derive a simplified moir\'e Hamiltonian for these systems
and conduct a detailed analysis of their band structures, where the origins of
anisotropic band flattening are clearly elucidated. This research broadens the
scope of moir\'e materials, where the valley- and spin-degenerate
two-dimensional array of quasi-one-dimensional system could serve as a
potential platform for realizing many interesting correlated phases.",2024-12-27T12:20:15Z,http://arxiv.org/abs/2412.19613v1,"Kejie Bao, Huan Wang, Zhaochen Liu, jing Wang"
Newman-Janis Algorithm from Taub-NUT Instantons,"We explicitly establish that the Kerr metric represents a pair of self-dual
and anti-self-dual gravitational dyons (Taub-NUT instantons). We show that the
Newman-Janis algorithm precisely originates from this fact. More generally,
this program of understanding four-dimensional black holes as systems of chiral
dyons extends to Kerr-Newman and Kerr-Taub-NUT solutions as well.",2024-12-27T12:16:20Z,http://arxiv.org/abs/2412.19611v1,Joon-Hwi Kim
"Machine Generated Product Advertisements: Benchmarking LLMs Against
  Human Performance","This study compares the performance of AI-generated and human-written product
descriptions using a multifaceted evaluation model. We analyze descriptions for
100 products generated by four AI models (Gemma 2B, LLAMA, GPT2, and ChatGPT 4)
with and without sample descriptions, against human-written descriptions. Our
evaluation metrics include sentiment, readability, persuasiveness, Search
Engine Optimization(SEO), clarity, emotional appeal, and call-to-action
effectiveness. The results indicate that ChatGPT 4 performs the best. In
contrast, other models demonstrate significant shortcomings, producing
incoherent and illogical output that lacks logical structure and contextual
relevance. These models struggle to maintain focus on the product being
described, resulting in disjointed sentences that do not convey meaningful
information. This research provides insights into the current capabilities and
limitations of AI in the creation of content for e-Commerce.",2024-12-27T12:11:50Z,http://arxiv.org/abs/2412.19610v1,Sanjukta Ghosh
"Bidding Games on Markov Decision Processes with Quantitative
  Reachability Objectives","Graph games are fundamental in strategic reasoning of multi-agent systems and
their environments. We study a new family of graph games which combine
stochastic environmental uncertainties and auction-based interactions among the
agents, formalized as bidding games on (finite) Markov decision processes
(MDP). Normally, on MDPs, a single decision-maker chooses a sequence of
actions, producing a probability distribution over infinite paths. In bidding
games on MDPs, two players -- called the reachability and safety players -- bid
for the privilege of choosing the next action at each step. The reachability
player's goal is to maximize the probability of reaching a target vertex,
whereas the safety player's goal is to minimize it. These games generalize
traditional bidding games on graphs, and the existing analysis techniques do
not extend. For instance, the central property of traditional bidding games is
the existence of a threshold budget, which is a necessary and sufficient budget
to guarantee winning for the reachability player. For MDPs, the threshold
becomes a relation between the budgets and probabilities of reaching the
target. We devise value-iteration algorithms that approximate thresholds and
optimal policies for general MDPs, and compute the exact solutions for acyclic
MDPs, and show that finding thresholds is at least as hard as solving
simple-stochastic games.",2024-12-27T12:10:00Z,http://arxiv.org/abs/2412.19609v1,"Guy Avni, Martin Kurečka, Kaushik Mallik, Petr Novotný, Suman Sadhukhan"
Photonic classification on a single diffractive layer,"Photonic computation started to shape the future of fast, efficient and
accessible computation. The advantages brought by light based Diffractive Deep
Neural Networks (D2NN), are shown to be overwhelmingly advantageous especially
in targeting classification problems. However, cost and complexity of
multi-layer systems are the main challenges that reduce the deployment of this
technology. In this study, we develop a simple yet extremely efficient way to
achieve optical classification using a single diffractive optical layer. A
spatial light modulator is used not only to emulate the classifying system but
also the input medium for the objects to be classified by the system. Using our
approach, we classify road traffic signs which has a direct application on
daily life and safety. We perform classification of road signs under the effect
of noise and show that we can successfully classify road signs with more than
75% accuracy under 20% noise/imperfection.",2024-12-27T12:09:28Z,http://arxiv.org/abs/2412.19607v1,"Anil J. Pekgöz, Emre Yüce"
"Enhancing Fine-grained Image Classification through Attentive Batch
  Training","Fine-grained image classification, which is a challenging task in computer
vision, requires precise differentiation among visually similar object
categories. In this paper, we propose 1) a novel module called Residual
Relationship Attention (RRA) that leverages the relationships between images
within each training batch to effectively integrate visual feature vectors of
batch images and 2) a novel technique called Relationship Position Encoding
(RPE), which encodes the positions of relationships between original images in
a batch and effectively preserves the relationship information between images
within the batch. Additionally, we design a novel framework, namely
Relationship Batch Integration (RBI), which utilizes RRA in conjunction with
RPE, allowing the discernment of vital visual features that may remain elusive
when examining a singular image representative of a particular class. Through
extensive experiments, our proposed method demonstrates significant
improvements in the accuracy of different fine-grained classifiers, with an
average increase of $(+2.78\%)$ and $(+3.83\%)$ on the CUB200-2011 and Stanford
Dog datasets, respectively, while achieving a state-of-the-art results
$(95.79\%)$ on the Stanford Dog dataset. Despite not achieving the same level
of improvement as in fine-grained image classification, our method still
demonstrates its prowess in leveraging general image classification by
attaining a state-of-the-art result of $(93.71\%)$ on the Tiny-Imagenet
dataset. Furthermore, our method serves as a plug-in refinement module and can
be easily integrated into different networks.",2024-12-27T12:07:58Z,http://arxiv.org/abs/2412.19606v1,"Duy M. Le, Bao Q. Bui, Anh Tran, Cong Tran, Cuong Pham"
"Versatile High-Power Monolithic All-Glass Fiber Amplifier for Pulsed
  Signals with a Wide Range of Repetition Rates","This study presents a compact, high-power monolithic all-glass spun tapered
double-clad fiber amplifier for single-stage amplification of narrow linewidth
picosecond pulsed signals from a few tens of mW to several hundred Watts of
average power and MW level of peak power, covering a wide range of repetition
rates. The absence of free-space elements in the amplifier module enhances its
overall reliability by omitting the dependency on the pump alignment and
internal back reflections. The versatile all-glass amplifier module delivers 50
ps pulses with over 2 MW peak power at 1 MHz, 50 ps pulses with over 625 W
average power at 20 MHz, and 20 ps pulses with over 645 W average power at 1
GHz, all exhibiting excellent spectral, spatial, and polarization
characteristics. This monolithic all-glass ultra-large mode area fiber
amplifier is verified as a robust solution for direct amplification of short
pulses attaining high peak/average power laser systems with excellent spectral,
spatial, and polarization characteristics.",2024-12-27T12:01:20Z,http://arxiv.org/abs/2412.19604v1,"Hossein Fathi, Ebrahim Aghayari, Andrey Grishchenko, Amit Yadav, Edik Rafailov, Regina Gumenyuk, Valery Filippov"
"Let Watermarks Speak: A Robust and Unforgeable Watermark for Language
  Models","Watermarking is an effective way to trace model-generated content. Current
watermark methods cannot resist forgery attacks, such as a deceptive claim that
the model-generated content is a response to a fabricated prompt. None of them
can be made unforgeable without degrading robustness.
  Unforgeability demands that the watermarked output is not only detectable but
also verifiable for integrity, indicating whether it has been modified. This
underscores the necessity and significance of a multi-bit watermarking scheme.
  Recent works try to build multi-bit scheme based on existing zero-bit
watermarking scheme, but they either degrades the robustness or brings a
significant computational burden. We aim to design a novel single-bit watermark
scheme, which provides the ability to embed 2 different watermark signals.
  This paper's main contribution is that we are the first to propose an
undetectable, robust, single-bit watermarking scheme. It has a comparable
robustness to the most advanced zero-bit watermarking schemes. Then we
construct a multi-bit watermarking scheme to use the hash value of prompt or
the newest generated content as the watermark signals, and embed them into the
following content, which guarantees the unforgeability.
  Additionally, we provide sufficient experiments on some popular language
models, while the other advanced methods with provable guarantees do not often
provide. The results show that our method is practically effective and robust.",2024-12-27T11:58:05Z,http://arxiv.org/abs/2412.19603v1,Minhao Bai
Arbitrarily Fast Tracking Multivariable Least-squares MRAC,"A novel least-squares model-reference direct adaptive control (LS MRAC)
algorithm for multivariable (MIMO) plants is presented. The controller
parameters are directly updated based on the output tracking error. The control
law is crucially modified to reduce the relative degree of the error model to
zero. A comprehensive Lyapunov-based stability analysis as well as a tracking
error convergence characterization is provided demonstrating that the LS MRAC
can achieve arbitrarily fast tracking while maintaining satisfactory parameter
convergence for quite large adaptation gains. Simulation results show a
significant improvement in tracking performance compared to previous methods.",2024-12-27T11:51:46Z,http://arxiv.org/abs/2412.19601v1,"Liu Hsu, Ramon R. Costa, Fernando Lizarralde, Alessandro Jacoud Peixoto"
Super-bath Quantum Eigensolver,"Simulating the dynamics of a system coupled to a suitable environment is a
promising approach in quantum computing for determining the ground state of
physical systems. However, this approach requires not only the
$\textit{existence}$ of an environment that allows the system to dissipate
energy and evolve to its ground state, but also the environment's
characteristics to be $\textit{known}$ in detail. In this paper, we propose an
algorithm with a sufficient condition for achieving polynomial-time complexity
in ground state preparation: the existence of an environment that enables the
system to evolve to its ground state in polynomial time, while such
environment's details may remain $\textit{unknown}$. The proposed algorithm is
Super-bath Quantum Eigensolver, which solves the system's ground state by
utilizing quasi-steady state preparation and simulating the coupling between
the system and the super-bath. Supported by experimental lifetime data of
nuclear metastable states, we suggest that our algorithm is applicable to
determine nuclear ground states in polynomial time. These results highlight the
potential advantage of quantum computing in addressing ground state problems in
real-world physical systems.",2024-12-27T11:46:47Z,http://arxiv.org/abs/2412.19599v1,"Tianren Wang, Zongkang Zhang, Bing-Nan Lu, Mauro Cirio, Ying Li"
Composite nature of the $T_{cc}$ state,"In 2021, LHCb collaboration reported a very narrow state in the $D^0D^0\pi^+$
mass spectrum just below the $D^{*+}D^0$ mass threshold. We consider the
influence of the Castillejo-Dalitz-Dyson (CDD) pole in the scattering amplitude
to derive a general treatment for the two-body final state interaction near its
threshold. The line shape (or the energy dependent event distribution) are then
obtained, where the parameters can be fixed by fitting to the experimental data
on the $D^0D^0\pi^+$ mass spectrum. Within our method the data are quite well
reproduced. The pole structure in the complex energy plane indicates the bound
state structure of the $T_{cc}$ state. The compositeness as a measure of
molecule component in its hadron wave function is predicted to be
$0.23_{-0.09}^{+0.40}$. The non-molecular component, e.g., the compact
tetraquark also takes a non-negligible portion.",2024-12-27T11:42:44Z,http://arxiv.org/abs/2412.19597v1,"Xian-Wei Kang, Wen-Shuo Ding"
"SocRATES: Towards Automated Scenario-based Testing of Social Navigation
  Algorithms","Current social navigation methods and benchmarks primarily focus on proxemics
and task efficiency. While these factors are important, qualitative aspects
such as perceptions of a robot's social competence are equally crucial for
successful adoption and integration into human environments. We propose a more
comprehensive evaluation of social navigation through scenario-based testing,
where specific human-robot interaction scenarios can reveal key robot
behaviors. However, creating such scenarios is often labor-intensive and
complex. In this work, we address this challenge by introducing a pipeline that
automates the generation of context-, and location-appropriate social
navigation scenarios, ready for simulation. Our pipeline transforms simple
scenario metadata into detailed textual scenarios, infers pedestrian and robot
trajectories, and simulates pedestrian behaviors, which enables more controlled
evaluation. We leverage the social reasoning and code-generation capabilities
of Large Language Models (LLMs) to streamline scenario generation and
translation. Our experiments show that our pipeline produces realistic
scenarios and significantly improves scenario translation over naive LLM
prompting. Additionally, we present initial feedback from a usability study
with social navigation experts and a case-study demonstrating a scenario-based
evaluation of three navigation algorithms.",2024-12-27T11:33:19Z,http://arxiv.org/abs/2412.19595v1,"Shashank Rao Marpally, Pranav Goyal, Harold Soh"
"Theoretical Investigation of (Zn, Co) co-Doped BaTiO3 for Advanced
  Energy and Photonic Applications","In light of recent advancements in energy technology, there is an urgent need
for lead-free barium titanate (BTO) -based materials that exhibit remarkable
ferroelectric and photoelectric properties. Notwithstanding the considerable
experimental advances, a theoretical understanding from the electron and atomic
perspectives remains elusive. This study employs the generalized gradient
approximation plane wave pseudopotential technique to investigate the
structural, electronic, ferroelectric, and optical properties of (Zn,Co)
co-doped BaTiO3 (BZCT) based on density functional theory. The objective is to
ascertain the extent of performance enhancement and the underlying mechanism of
(Zn,Co) co-doping on barium titanate. Our findings reveal that incorporating
(Zn,Co) into the BaTiO3 lattice significantly augments the tetragonality of the
unit cell. Moreover, the ferroelectric properties are enhanced, with a
spontaneous polarization stronger than that observed in pure BTO, exhibiting
excellent ferroelectricity. The results of the Hubbard+U algorithm indicate
that the band gap of BZCT is reduced. Concurrently, the enhanced ferroelectric
polarization increases the built-in electric field of the material,
facilitating the separation of photogenerated carriers and improving optical
absorption. Consequently, the optical absorption ability and photorefractive
ability are effectively enhanced. BZCT, with its high spontaneous polarization
and outstanding optical properties, can be a promising candidate material in
energy storage and photovoltaics.",2024-12-27T11:26:38Z,http://arxiv.org/abs/2412.19591v1,"Zheng Kang, Mei Wu, Yiyu Feng, Jiahao Li, Jieming Zhang, Haiyi Tian, Ancheng Wang, Yunkai Wu, Xu Wang"
"Robust phase estimation of the ground-state energy without controlled
  time evolution on a quantum device","Estimating the ground-state energy of Hamiltonians in quantum systems is an
important task. In this work, we demonstrate that the ground-state energy can
be accurately estimated without controlled time evolution by using adiabatic
state preparation (ASP) and Ramsey-type measurement. By considering the
symmetry of the Hamiltonian governing the time evolution during ASP, we can
prepare a superposition of the ground state and reference state whose
eigenvalue is known. This enables the estimation of the ground-state energy via
Ramsey-type measurement. Furthermore, our method is robust against
non-adiabatic transitions, making it suitable for use with early fault-tolerant
quantum computers and quantum annealing.",2024-12-27T11:20:45Z,http://arxiv.org/abs/2412.19590v1,"Hiroki Kuji, Yuta Shingu, Tetsuro Nikuni, Takashi Imoto, Kenji Sugisaki, Yuichiro Matsuzaki"
"ViDTA: Enhanced Drug-Target Affinity Prediction via Virtual Graph Nodes
  and Attention-based Feature Fusion","Drug-target interaction is fundamental in understanding how drugs affect
biological systems, and accurately predicting drug-target affinity (DTA) is
vital for drug discovery. Recently, deep learning methods have emerged as a
significant approach for estimating the binding strength between drugs and
target proteins. However, existing methods simply utilize the drug's local
information from molecular topology rather than global information.
Additionally, the features of drugs and proteins are usually fused with a
simple concatenation operation, limiting their effectiveness. To address these
challenges, we proposed ViDTA, an enhanced DTA prediction framework. We
introduce virtual nodes into the Graph Neural Network (GNN)-based drug feature
extraction network, which acts as a global memory to exchange messages more
efficiently. By incorporating virtual graph nodes, we seamlessly integrate
local and global features of drug molecular structures, expanding the GNN's
receptive field. Additionally, we propose an attention-based linear feature
fusion network for better capturing the interaction information between drugs
and proteins. Experimental results evaluated on various benchmarks including
Davis, Metz, and KIBA demonstrate that our proposed ViDTA outperforms the
state-of-the-art baselines.",2024-12-27T11:19:10Z,http://arxiv.org/abs/2412.19589v1,"Minghui Li, Zikang Guo, Yang Wu, Peijin Guo, Yao Shi, Shengshan Hu, Wei Wan, Shengqing Hu"
Elliptic triad,"The triad refers to embedding the Macdonald polynomials into the
Noumi-Shiraishi functions and their reduction to solutions of simple linear
equations at particular values of $t$. It provides an alternative definition of
Macdonald theory. We discuss lifting the triad to an elliptic generalization of
the Noumi-Shiraishi functions. The central unknown ingredient is linear
equations, for which we discuss various possible approaches, including
immediate elliptic deformation of periodicity conditions, (elliptic)
Ding-Iohara-Miki algebra operators, and elliptic Kostka coefficients.",2024-12-27T11:18:49Z,http://arxiv.org/abs/2412.19588v1,"A. Mironov, A. Morozov, A. Popolitov, Z. Zakirova"
"Ultralight Signal Classification Model for Automatic Modulation
  Recognition","The growing complexity of radar signals demands responsive and accurate
detection systems that can operate efficiently on resource-constrained edge
devices. Existing models, while effective, often rely on substantial
computational resources and large datasets, making them impractical for edge
deployment. In this work, we propose an ultralight hybrid neural network
optimized for edge applications, delivering robust performance across
unfavorable signal-to-noise ratios (mean accuracy of 96.3% at 0 dB) using less
than 100 samples per class, and significantly reducing computational overhead.",2024-12-27T11:03:26Z,http://arxiv.org/abs/2412.19585v1,"Alessandro Daniele Genuardi Oquendo, Agustín Matías Galante Cerviño, Nilotpal Sinha, Luc Andrea, Sam Mugel, Román Orús"
Stamps and Mathematics,"This study examines the potential of using math-themed postage stamps in
mathematics lessons as a tool to engage students and integrate the subject with
history, art, and culture. Since the first mathematical stamps appeared in the
early 20th century, featuring prominent scholars like Carl Friedrich Gauss and
Isaac Newton, they serve not only as philatelic artifacts but also as
historical carriers of knowledge. The paper presents several practical projects
to interest students, such as creating their own math stamps, investigating the
price trends of math-themed stamps, and developing a timeline of mathematical
discoveries depicted in philatelic issues. The proposed projects develop
students' mathematical skills in areas such as percentage calculations, general
arithmetic, working with time intervals, and statistical analysis. Students can
analyze shapes, symmetry, and patterns on stamps, study principles of
proportion, and explore geometric figures. Using stamps broadens students'
horizons, providing an opportunity to become familiar with renowned
mathematicians from different eras, countries, and cultures. This also offers
students a new perspective on the subject, presenting mathematical discoveries
as part of the world's cultural heritage. Postage stamps dedicated to
mathematics can become a powerful tool for visualizing theoretical knowledge,
stimulating interest in mathematics, and encouraging independent research among
students.",2024-12-27T10:52:54Z,http://arxiv.org/abs/2412.19579v1,Nataliya M. Ivanova
"Gauging or extending bulk and boundary conformal field theories:
  Application to bulk and domain wall problem in topological matter and their
  descriptions by (mock) modular covariant","We study gauging operations (or group extensions) in (smeared) boundary
conformal field theories (BCFTs) and bulk conformal field theories and their
applications to various phenomena in topologically ordered systems. We apply
the resultant theories to the correspondence between the renormalization group
(RG) flow of CFTs and the classification of topological quantum field theories
in the testable information of general classes of partition functions. One can
obtain the bulk topological properties of $2+1$ dimensional topological ordered
phase corresponding to the massive RG flow of $1+1$ dimensional systems, or
smeared BCFT. We present an obstruction of mass condensation for smeared BCFT
analogous to the Lieb-Shultz-Mattis theorem for noninvertible symmetry. Related
to the bulk topological degeneracies in $2+1$ dimensions and quantum phases in
$1+1$ dimensions we construct a new series of BCFT. We also investigate the
implications of the massless RG flow of $1+1$ dimensional CFT to $2+1$
dimensional topological order which corresponds to the earlier proposal by L.
Kong and H. Zheng in [Nucl. Phys. B 966 (2021), 115384], arXiv:1912.01760
closely related to the integer-spin simple current by Schellekens and
Gato-Rivera. We study the properties of the product of two CFTs connected by
the two kinds of massless flows. The (mock) modular covariants appearing in the
analysis seem to contain new ones. By applying the folding trick to the coupled
model, we provide a general method to solve the gapped and charged domain wall.
One can obtain the general phenomenology of the transportation of anyons
through the domain wall. Our work gives a unified direction for the future
theoretical and numerical studies of the topological phase based on the
established data of classifications of conformal field theories or modular
invariants.",2024-12-27T10:46:30Z,http://arxiv.org/abs/2412.19577v1,Yoshiki Fukusumi
Hybrid Population Monte Carlo,"Importance sampling (IS) is a powerful Monte Carlo (MC) technique for
approximating intractable integrals, for instance in Bayesian inference. The
performance of IS relies heavily on the appropriate choice of the so-called
proposal distribution. Adaptive IS (AIS) methods iteratively improve target
estimates by adapting the proposal distribution. Recent AIS research focuses on
enhancing proposal adaptation for high-dimensional problems, while addressing
the challenge of multi-modal targets. In this paper, a new class of AIS methods
is presented, utilizing a hybrid approach that incorporates weighted samples
and proposal distributions to enhance performance. This approach belongs to the
family of population Monte Carlo (PMC) algorithms, where a population of
proposals is adapted to better approximate the target distribution. The
proposed hybrid population Monte Carlo (HPMC) implements a novel two-step
adaptation mechanism. In the first step, a hybrid method is used to generate
the population of the preliminary proposal locations based on both weighted
samples and location parameters. We use Hamiltonian Monte Carlo (HMC) to
generate the preliminary proposal locations. HMC has a good exploratory
behavior, especially in high dimension scenarios. In the second step, the novel
cooperation algorithms are performing to find the final proposals for the next
iteration. HPMC achieves a significant performance improvement in
high-dimensional problems when compared to the state-of-the-art algorithms. We
discuss the statistical properties of HPMC and show its high performance in two
challenging benchmarks.",2024-12-27T10:46:13Z,http://arxiv.org/abs/2412.19576v1,"Ali Mousavi, Víctor Elvira"
"Superintegrability of the Wilson family of matrix models and moments of
  multivariable orthogonal polynomials","We present new examples of superintegrable matrix/eigenvalue models. These
examples arise as a result of the exploration of the relationship between the
theory of superintegrability and multivariate orthogonal polynomials. The new
superintegrable examples are built upon the multivariate generalizations of the
Meixner-Pollaczek and Wilson polynomials and their respective measures. From
the perspective of multivariate orthogonal polynomials in this work we propose
expressions for (generalized) moments of the respective multi-variable
measures. From the perspective of superintegrability we uncover a couple of new
phenomena such as the deviation from Schur polynomials as the superintegrable
basis without any deformation and new combinatorial structures appearing in the
answers.",2024-12-27T10:39:43Z,http://arxiv.org/abs/2412.19574v1,Victor Mishnyakov
Photoluminescence efficiency droop in Perovskites,"The commercialization prospects of perovskite light emitting diodes depend on
its luminescence efficiency under large carrier densities. The decrease in
luminescence efficiency under such high injection conditions could lead to an
undesired increase in power consumption with associated degradation and
stability concerns. Here, through detailed modeling of thermal transport and
carrier generation-recombination, we unravel the physical mechanisms that cause
luminescence droop under high injection conditions. We show that self-heating
leads to a reduction in the radiative recombination (both bimolecular and
excitonic). The resultant increase in non-radiative recombination and hence the
thermal dissipation acts as a positive feedback mechanism that leads to
efficiency droop in perovskites. Our model predictions, well supported by
experimental results, could be of broad interest towards the degradation-aware
thermal design of perovskite optoelectronics and stability.",2024-12-27T10:27:14Z,http://arxiv.org/abs/2412.19572v1,"Pradeep R. Nair, Karthik Raitani"
Quantum K-theory and Integrability,"In this note, we explore some recent advancements in enumerative algebraic
geometry, focusing particularly on the role of quantum K-theory of quiver
varieties as viewed through the lens of integrable systems. We highlight a
number of conjectures and open questions. This is a contribution to the
proceedings of the GLSM@30 conference, which was held in May 2023 at the Simons
Center for Geometry and Physics.",2024-12-27T10:18:45Z,http://arxiv.org/abs/2412.19570v1,Peter Koroteev
"Nonminimally coupled Dark Matter in Clusters of Galaxies: a fully
  comprehensive analysis","In this study, we explore how a non-minimal coupling between dark matter and
gravity can affect the behavior of dark matter in galaxy clusters. We have
considered the case of a disformal coupling, which leads to a modification of
the Poisson equation. Building on an earlier work, we expand the analysis
considering all possible disformal coupling scenarios and employing various
dark matter density profiles. In doing so, we aim to constrain the key
parameter in our model, the characteristic coupling length. To achieve this, we
analyze data from a combination of strong and weak lensing using three
statistical approaches: a single cluster fitting procedure, a joint analysis,
and one with stacked profiles. Our findings show that the coupling length is
typically very small, thus being fully consistent with general relativity,
although with an upper limit at $1\sigma$ which is of the order of $100$ kpc.",2024-12-27T10:12:29Z,http://arxiv.org/abs/2412.19569v1,"Saboura Zamani, Vincenzo Salzano, Dario Bettoni"
Safe Interval Randomized Path Planing For Manipulators,"Planning safe paths in 3D workspace for high DoF robotic systems, such as
manipulators, is a challenging problem, especially when the environment is
populated with the dynamic obstacles that need to be avoided. In this case the
time dimension should be taken into account that further increases the
complexity of planning. To mitigate this issue we suggest to combine
safe-interval path planning (a prominent technique in heuristic search) with
the randomized planning, specifically, with the bidirectional rapidly-exploring
random trees (RRT-Connect) - a fast and efficient algorithm for
high-dimensional planning. Leveraging a dedicated technique of fast computation
of the safe intervals we end up with an efficient planner dubbed SI-RRT. We
compare it with the state of the art and show that SI-RRT consistently
outperforms the competitors both in runtime and solution cost.
  Our implementation of SI-RRT is publicly available at
https://github.com/PathPlanning/ManipulationPlanning-SI-RRT",2024-12-27T10:10:52Z,http://arxiv.org/abs/2412.19567v1,"Nuraddin Kerimov, Aleksandr Onegin, Konstantin Yakovlev"
Effective field theory of the quantum skyrmion Hall effect,"Motivated by phenomenology of myriad recently-identified topologically
non-trivial phases of matter, we introduce effective field theories (EFTs) for
the quantum skyrmion Hall effect (QSkHE). We employ a single, unifying
generalisation for this purpose: in essence, a lowest Landau level projection
defining a non-commutative, fuzzy sphere with position coordinates proportional
to SU(2) generators of matrix representation size $N\times N$, may host an
intrinsically 2+1 dimensional, topologically non-trivial many-body state for
small $N$ as well as large $N$. That is, isospin degrees of freedom associated
with a matrix Lie algebra with $N \times N$ generators potentially encode some
finite number of spatial dimensions for $N\ge 2$, a regime in which isospin has
previously been treated as a label. This statement extends to more general
$p$-branes subjected to severe fuzzification as well as membranes. As a
consequence of this generalisation, systems with $d$ Cartesian spatial
coordinates and isospin degrees of freedom encoding an additional $\delta$
fuzzy coset space coordinates can realise topologically non-trivial states of
intrinsic dimensionality up to $d$+$\delta$+1. We therefore identify gauge
theories with extra fuzzy dimensions generalised to retain dependence upon
gauge fields over fuzzy coset spaces even for severe fuzzification (small $N$),
as EFTs for the QSkHE. We furthermore generalise these EFTs to space manifolds
with local product structure exploiting the dimensional hierarchy of (fuzzy)
spheres. For this purpose, we introduce methods of anisotropic fuzzification
and propose formulating topological invariants on fuzzy coset spaces as
artifacts of projecting matrix Lie algebras to occupied subspaces. Importantly,
we focus on phenomenology indicating the 2+1 D SU(2) gauge theory should be
generalised using this machinery, and serves as a minimal EFT of the QSkHE.",2024-12-27T10:09:48Z,http://arxiv.org/abs/2412.19565v1,"Vinay Patil, Rafael Flores-Calderón, Ashley M. Cook"
Stochastic resetting in a nonequilibrium environment,"This study examines the dynamics of a tracer particle diffusing in a
nonequilibrium medium under stochastic resetting. The nonequilibrium state is
induced by harmonic coupling between the tracer and bath particles, generating
memory effects with exponential decay in time. We explore the tracer's behavior
under a Poissonian resetting protocol, where resetting does not disturb the
bath environment, with a focus on key dynamical behavior and first-passage
properties, both in the presence and absence of an external force. The
interplay between coupling strength and diffusivity of bath particle
significantly impacts both the tracer's relaxation dynamics and search time,
with external forces further modulating these effects. Our analysis identifies
distinct hot and cold bath particles based on their diffusivities, revealing
that coupling to a hot particle facilitates the searching process, whereas
coupling to a cold particle hinders it. Using a combination of numerical
simulations and analytical methods, this study provides a comprehensive
framework for understanding resetting mechanisms in non-Markovian systems, with
potential applications to complex environments such as active and viscoelastic
media, where memory-driven dynamics and nonequilibrium interactions are
significant.",2024-12-27T10:06:52Z,http://arxiv.org/abs/2412.19564v1,Koushik Goswami
"A three-variable transcendental invariant of planar knotoids via Gauss
  diagrams","As a generalization of the classical knots, knotoids are equivalence classes
of immersions of the oriented unit interval in a surface. In recent years, a
variety of invariants of spherical and planar knotoids have been constructed as
extensions of invariants of classical and virtual knots. In this paper we
introduce a three-variable transcendental invariant of planar knotoids which is
defined over an index function of a Gauss diagram. We describe properties of
this invariant and show that it is a Vassiliev invariant of order one. We also
discuss the Gordian distance between planar knotoids and provide lower bounds
on the Gordian distance of homotopic planar knotoids by using the
transcendental invariant.",2024-12-27T09:51:30Z,http://arxiv.org/abs/2412.19554v1,"Wandi Feng, Fengling Li, Andrei Vesnin"
"Contrast-Optimized Basis Functions for Self-Navigated Motion Correction
  in Quantitative MRI","Purpose: The long scan times of quantitative MRI techniques make motion
artifacts more likely. For MR-Fingerprinting-like approaches, this problem can
be addressed with self-navigated retrospective motion correction based on
reconstructions in a singular value decomposition (SVD) subspace. However, the
SVD promotes high signal intensity in all tissues, which limits the contrast
between tissue types and ultimately reduces the accuracy of registration. The
purpose of this paper is to rotate the subspace for maximum contrast between
two types of tissue and improve the accuracy of motion estimates.
  Methods: A subspace is derived that promotes contrasts between brain
parenchyma and CSF, achieved through the generalized eigendecomposition of mean
autocorrelation matrices, followed by a Gram-Schmidt process to maintain
orthogonality. We tested our motion correction method on 85 scans with varying
motion levels, acquired with a 3D hybrid-state sequence optimized for
quantitative magnetization transfer imaging.
  Results: A comparative analysis shows that the contrast-optimized basis
significantly improve the parenchyma-CSF contrast, leading to smoother motion
estimates and reduced artifacts in the quantitative maps.
  Conclusion: The proposed contrast-optimized subspace improves the accuracy of
the motion estimation.",2024-12-27T09:49:35Z,http://arxiv.org/abs/2412.19552v1,"Elisa Marchetto, Sebastian Flassbeck, Andrew Mao, Jakob Assländer"
"Explicit propagation reversal bounds for bistable differential equations
  on trees","In this paper we provide explicit description of the pinning region and
propagation reversal phenomenon for the bistable reaction diffusion equation on
regular biinfinite trees. In contrast to the general existence results for
smooth bistabilities, the closed-form formulas are enabled by the choice of the
piecewise linear McKean's caricature. We construct exact pinned waves and show
their stability. The results are qualitatively similar to the propagation
reversal results for smooth bistabilities. Major exception consists in the
unboundedness of the pinning region in the case of the bistable McKean's
caricature. Consequently, the propagation reversal also occurs for arbitrarily
large diffusion.",2024-12-27T09:28:13Z,http://arxiv.org/abs/2412.19548v1,Petr Stehlík
Quantiles under ambiguity and risk sharing,"Choquet capacities and integrals are central concepts in decision making
under ambiguity or model uncertainty, pioneered by Schmeidler. Motivated by
risk optimization problems for quantiles under ambiguity, we study the subclass
of Choquet integrals, called Choquet quantiles, which generalizes the usual
(probabilistic) quantiles, also known as Value-at-Risk in finance, from
probabilities to capacities. Choquet quantiles share many features with
probabilistic quantiles, in terms of axiomatic representation, optimization
formulas, and risk sharing. We characterize Choquet quantiles via only one
axiom, called ordinality. We prove that the inf-convolution of Choquet
quantiles is again a Choquet quantile, leading to explicit optimal allocations
in risk sharing problems for quantile agents under ambiguity. A new class of
risk measures, Choquet Expected Shortfall, is introduced, which enjoys most
properties of the coherent risk measure Expected Shortfall. Our theory is
complemented by optimization algorithms, numerical examples, and a stylized
illustration with financial data.",2024-12-27T09:22:19Z,http://arxiv.org/abs/2412.19546v1,"Peng Liu, Tiantian Mao, Ruodu Wang"
"Enhancing Media Literacy: The Effectiveness of (Human) Annotations and
  Bias Visualizations on Bias Detection","Marking biased texts is a practical approach to increase media bias awareness
among news consumers. However, little is known about the generalizability of
such awareness to new topics or unmarked news articles, and the role of
machine-generated bias labels in enhancing awareness remains unclear. This
study tests how news consumers may be trained and pre-bunked to detect media
bias with bias labels obtained from different sources ( (Human or AI) and in
various manifestations. We conducted two experiments with 470 and 846
participants, exposing them to various bias-labeling conditions. We
subsequently tested how much bias they could identify in unlabeled news
materials on new topics. The results show that both Human (t(467) = 4.55, p &lt;
.001, d = 0.42) and AI labels (t(467) = 2.49, p = .039, d = 0.23) increased
correct detection compared to the control group. Human labels demonstrate
larger effect sizes and higher statistical significance. The control group
(t(467) = 4.51, p &lt; .001, d = 0.21) also improves performance through mere
exposure to study materials. We also find that participants trained with marked
biased phrases detected bias most reliably (F(834,1) = 44.00, p &lt; .001,
{\eta}2part = 0.048). Our experimental framework provides theoretical
implications for systematically assessing the generalizability of learning
effects in identifying media bias. These findings also provide practical
implications for developing news-reading platforms that offer bias indicators
and designing media literacy curricula to enhance media bias awareness.",2024-12-27T09:19:22Z,http://arxiv.org/abs/2412.19545v1,"Timo Spinde, Fei Wu, Wolfgang Gaissmaier, Gianluca Demartini, Helge Giese"
"TARGA: Targeted Synthetic Data Generation for Practical Reasoning over
  Structured Data","Semantic parsing, which converts natural language questions into logic forms,
plays a crucial role in reasoning within structured environments. However,
existing methods encounter two significant challenges: reliance on extensive
manually annotated datasets and limited generalization capability to unseen
examples. To tackle these issues, we propose Targeted Synthetic Data Generation
(TARGA), a practical framework that dynamically generates high-relevance
synthetic data without manual annotation. Starting from the pertinent entities
and relations of a given question, we probe for the potential relevant queries
through layer-wise expansion and cross-layer combination. Then we generate
corresponding natural language questions for these constructed queries to
jointly serve as the synthetic demonstrations for in-context learning.
Experiments on multiple knowledge base question answering (KBQA) datasets
demonstrate that TARGA, using only a 7B-parameter model, substantially
outperforms existing non-fine-tuned methods that utilize close-sourced model,
achieving notable improvements in F1 scores on GrailQA(+7.7) and
KBQA-Agent(+12.2). Furthermore, TARGA also exhibits superior sample efficiency,
robustness, and generalization capabilities under non-I.I.D. settings.",2024-12-27T09:16:39Z,http://arxiv.org/abs/2412.19544v1,"Xiang Huang, Jiayu Shen, Shanshan Huang, Sitao Cheng, Xiaxia Wang, Yuzhong Qu"
Diverse Rare Sample Generation with Pretrained GANs,"Deep generative models are proficient in generating realistic data but
struggle with producing rare samples in low density regions due to their
scarcity of training datasets and the mode collapse problem. While recent
methods aim to improve the fidelity of generated samples, they often reduce
diversity and coverage by ignoring rare and novel samples. This study proposes
a novel approach for generating diverse rare samples from high-resolution image
datasets with pretrained GANs. Our method employs gradient-based optimization
of latent vectors within a multi-objective framework and utilizes normalizing
flows for density estimation on the feature space. This enables the generation
of diverse rare images, with controllable parameters for rarity, diversity, and
similarity to a reference image. We demonstrate the effectiveness of our
approach both qualitatively and quantitatively across various datasets and GANs
without retraining or fine-tuning the pretrained GANs.",2024-12-27T09:10:30Z,http://arxiv.org/abs/2412.19543v1,"Subeen Lee, Jiyeon Han, Soyeon Kim, Jaesik Choi"
Interacted Object Grounding in Spatio-Temporal Human-Object Interactions,"Spatio-temporal Human-Object Interaction (ST-HOI) understanding aims at
detecting HOIs from videos, which is crucial for activity understanding.
However, existing whole-body-object interaction video benchmarks overlook the
truth that open-world objects are diverse, that is, they usually provide
limited and predefined object classes. Therefore, we introduce a new open-world
benchmark: Grounding Interacted Objects (GIO) including 1,098 interacted
objects class and 290K interacted object boxes annotation. Accordingly, an
object grounding task is proposed expecting vision systems to discover
interacted objects. Even though today's detectors and grounding methods have
succeeded greatly, they perform unsatisfactorily in localizing diverse and rare
objects in GIO. This profoundly reveals the limitations of current vision
systems and poses a great challenge. Thus, we explore leveraging
spatio-temporal cues to address object grounding and propose a 4D
question-answering framework (4D-QA) to discover interacted objects from
diverse videos. Our method demonstrates significant superiority in extensive
experiments compared to current baselines. Data and code will be publicly
available at https://github.com/DirtyHarryLYL/HAKE-AVA.",2024-12-27T09:08:46Z,http://arxiv.org/abs/2412.19542v1,"Xiaoyang Liu, Boran Wen, Xinpeng Liu, Zizheng Zhou, Hongwei Fan, Cewu Lu, Lizhuang Ma, Yulong Chen, Yong-Lu Li"
"GHZ-W Genuinely Entangled Subspace Verification with Adaptive Local
  Measurements","Genuinely entangled subspaces (GESs) are valuable resources in quantum
information science. Among these, the three-qubit GHZ-W GES, spanned by the
three-qubit Greenberger-Horne-Zeilinger (GHZ) and W states, is a universal and
crucial entangled subspace resource for three-qubit systems. In this work, we
develop two adaptive verification strategies, the XZ strategy and the rotation
strategy, for the three-qubit GHZ-W GES using local measurements and one-way
classical communication. These strategies are experimentally feasible,
efficient and possess a concise analytical expression for the sample complexity
of the rotation strategy, which scales approximately as
$2.248/\epsilon\ln(1/\delta)$, where $\epsilon$ is the infidelity and
$1-\delta$ is the confidence level. Furthermore, we comprehensively analyze the
two-dimensional two-qubit subspaces and classify them into three distinct
types, including unverifiable entangled subspaces, revealing intrinsic
limitations in local verification of entangled subspaces.",2024-12-27T09:07:44Z,http://arxiv.org/abs/2412.19540v1,"Congcong Zheng, Ping Xu, Kun Wang, Zaichen Zhang"
On the approximation of spatial convolutions by PDE systems,"This paper considers the approximation problem for the spatial convolution
with a given integral kernel. The approximation to the spatial convolution by
PDE systems has been proposed to eliminate the analytical difficulties caused
by the integrals for the one-dimensional space. In this paper we establish a
PDE system approximation for the spatial convolutions in higher spatial
dimensions. We derive an approximation function for given arbitrary radial
integral kernels by linear sums of the Green functions. In the proof of this
methodology we introduce an appropriate integral transformation to show the
completeness of the basis constructed by the Green functions. From this theory,
it is possible to approximate the nonlocal operator of the convolution type
with any radial integral kernels by the linear sum of the solutions to the PDE
system. Finally, numerical examples for the approximation are demonstrated
using this method.",2024-12-27T09:07:27Z,http://arxiv.org/abs/2412.19539v1,"Hiroshi Ishii, Yoshitaro Tanaka"
"Scalable Hierarchical Reinforcement Learning for Hyper Scale Multi-Robot
  Task Planning","To improve the efficiency of warehousing system and meet huge customer
orders, we aim to solve the challenges of dimension disaster and dynamic
properties in hyper scale multi-robot task planning (MRTP) for robotic mobile
fulfillment system (RMFS). Existing research indicates that hierarchical
reinforcement learning (HRL) is an effective method to reduce these challenges.
Based on that, we construct an efficient multi-stage HRL-based multi-robot task
planner for hyper scale MRTP in RMFS, and the planning process is represented
with a special temporal graph topology. To ensure optimality, the planner is
designed with a centralized architecture, but it also brings the challenges of
scaling up and generalization that require policies to maintain performance for
various unlearned scales and maps. To tackle these difficulties, we first
construct a hierarchical temporal attention network (HTAN) to ensure basic
ability of handling inputs with unfixed lengths, and then design multi-stage
curricula for hierarchical policy learning to further improve the scaling up
and generalization ability while avoiding catastrophic forgetting.
Additionally, we notice that policies with hierarchical structure suffer from
unfair credit assignment that is similar to that in multi-agent reinforcement
learning, inspired of which, we propose a hierarchical reinforcement learning
algorithm with counterfactual rollout baseline to improve learning performance.
Experimental results demonstrate that our planner outperform other
state-of-the-art methods on various MRTP instances in both simulated and
real-world RMFS. Also, our planner can successfully scale up to hyper scale
MRTP instances in RMFS with up to 200 robots and 1000 retrieval racks on
unlearned maps while keeping superior performance over other methods.",2024-12-27T09:07:11Z,http://arxiv.org/abs/2412.19538v1,"Xuan Zhou, Xiang Shi, Lele Zhang, Chen Chen, Hongbo Li, Lin Ma, Fang Deng, Jie Chen"
"Potential Vector Fields in $\mathbb R^3$ and $α$-Meridional
  Mappings of the Second Kind $(α\in \mathbb R)$","This paper extends approach developed in a recent author's paper on analytic
models of potential fields in inhomogeneous media. New three-dimensional
analytic models of potential vector fields in some layered media are
constructed. Properties of various analytic models in Cartesian and cylindrical
coordinates in $\mathbb R^3$ are compared. The original properties of the
Jacobian matrix $\mathbf{J}(\vec V)$ of potential meridional fields $\vec V$ in
cylindrically layered media, where $\phi( \rho) = \rho^{-\alpha}$ $(\alpha \in
\mathbb R)$, lead to the concept of \emph{$\alpha$-meridional mappings of the
first and second kind}. The concept of \emph{$\alpha$-Meridional functions of
the first and second kind} naturally arises in this way. When $\alpha =1$, the
special concept of \emph{Radially holomorphic functions in $\mathbb R^3$},
introduced by G\""{u}rlebeck, Habetha and Spr\""{o}ssig in 2008, is developed in
more detail. Certain key properties of the radially holomorphic functions $G$
and functions reversed with respect to $G$ are first characterized. Surprising
properties of the radially holomorphic potentials represented by superposition
of the radially holomorphic exponential function $e^{\breve{\beta} x}$
$(\breve{\beta} \in \mathbb R)$ and function reversed with respect to
$e^{\breve{\beta} x}$ are demonstrated explicitly. The basic properties of the
radially holomorphic potential represented by the radially holomorphic
extension of the Joukowski transformation in $\mathbb R^3$ are studied.",2024-12-27T09:03:29Z,http://arxiv.org/abs/2412.19536v1,Dmitry Bryukhov
"StyleRWKV: High-Quality and High-Efficiency Style Transfer with
  RWKV-like Architecture","Style transfer aims to generate a new image preserving the content but with
the artistic representation of the style source. Most of the existing methods
are based on Transformers or diffusion models, however, they suffer from
quadratic computational complexity and high inference time. RWKV, as an
emerging deep sequence models, has shown immense potential for long-context
sequence modeling in NLP tasks. In this work, we present a novel framework
StyleRWKV, to achieve high-quality style transfer with limited memory usage and
linear time complexity. Specifically, we propose a Recurrent WKV (Re-WKV)
attention mechanism, which incorporates bidirectional attention to establish a
global receptive field. Additionally, we develop a Deformable Shifting
(Deform-Shifting) layer that introduces learnable offsets to the sampling grid
of the convolution kernel, allowing tokens to shift flexibly and adaptively
from the region of interest, thereby enhancing the model's ability to capture
local dependencies. Finally, we propose a Skip Scanning (S-Scanning) method
that effectively establishes global contextual dependencies. Extensive
experiments with analysis including qualitative and quantitative evaluations
demonstrate that our approach outperforms state-of-the-art methods in terms of
stylization quality, model complexity, and inference efficiency.",2024-12-27T09:01:15Z,http://arxiv.org/abs/2412.19535v1,"Miaomiao Dai, Qianyu Zhou, Lizhuang Ma"
"P3S-Diffusion:A Selective Subject-driven Generation Framework via Point
  Supervision","Recent research in subject-driven generation increasingly emphasizes the
importance of selective subject features. Nevertheless, accurately selecting
the content in a given reference image still poses challenges, especially when
selecting the similar subjects in an image (e.g., two different dogs). Some
methods attempt to use text prompts or pixel masks to isolate specific
elements. However, text prompts often fall short in precisely describing
specific content, and pixel masks are often expensive. To address this, we
introduce P3S-Diffusion, a novel architecture designed for context-selected
subject-driven generation via point supervision. P3S-Diffusion leverages
minimal cost label (e.g., points) to generate subject-driven images. During
fine-tuning, it can generate an expanded base mask from these points, obviating
the need for additional segmentation models. The mask is employed for
inpainting and aligning with subject representation. The P3S-Diffusion
preserves fine features of the subjects through Multi-layers Condition
Injection. Enhanced by the Attention Consistency Loss for improved training,
extensive experiments demonstrate its excellent feature preservation and image
generation capabilities.",2024-12-27T08:59:01Z,http://arxiv.org/abs/2412.19533v1,"Junjie Hu, Shuyong Gao, Lingyi Hong, Qishan Wang, Yuzhou Zhao, Yan Wang, Wenqiang Zhang"
"Optical probing of fractal and multifractal connection to structural
  disorder in weakly optical disordered media: Application to cancer detection","The light scattering experiment establishes a relationship between refractive
index fluctuations and fractal dimension in weakly scattering tissue-like
media. Based on the box-counting approach, an analytical model is developed and
shows that the fractal dimension has a functional dependency on the structural
disorder or refractive index fluctuation for short-range correlation and
approximately linearly depends on each other for tissue-like media. Several
parametric imaging systems can be connected using this approach. Further,
tissue's weak multifractality optical scattering is explored using the
box-counting method. It is shown that with a functional transformation, the
distribution follows lognormal distributions.",2024-12-27T08:54:29Z,http://arxiv.org/abs/2412.19532v1,"Santanu Maity, Mousa Alrubayan, Ishmael Apachigwao, Dhruvil Solanki, Prabhakar Pradhan"
Is Your Text-to-Image Model Robust to Caption Noise?,"In text-to-image (T2I) generation, a prevalent training technique involves
utilizing Vision Language Models (VLMs) for image re-captioning. Even though
VLMs are known to exhibit hallucination, generating descriptive content that
deviates from the visual reality, the ramifications of such caption
hallucinations on T2I generation performance remain under-explored. Through our
empirical investigation, we first establish a comprehensive dataset comprising
VLM-generated captions, and then systematically analyze how caption
hallucination influences generation outcomes. Our findings reveal that (1) the
disparities in caption quality persistently impact model outputs during
fine-tuning. (2) VLMs confidence scores serve as reliable indicators for
detecting and characterizing noise-related patterns in the data distribution.
(3) even subtle variations in caption fidelity have significant effects on the
quality of learned representations. These findings collectively emphasize the
profound impact of caption quality on model performance and highlight the need
for more sophisticated robust training algorithm in T2I. In response to these
observations, we propose a approach leveraging VLM confidence score to mitigate
caption noise, thereby enhancing the robustness of T2I models against
hallucination in caption.",2024-12-27T08:53:37Z,http://arxiv.org/abs/2412.19531v1,"Weichen Yu, Ziyan Yang, Shanchuan Lin, Qi Zhao, Jianyi Wang, Liangke Gui, Matt Fredrikson, Lu Jiang"
"The Value of AI Advice: Personalized and Value-Maximizing AI Advisors
  Are Necessary to Reliably Benefit Experts and Organizations","Despite advances in AI's performance and interpretability, AI advisors can
undermine experts' decisions and increase the time and effort experts must
invest to make decisions. Consequently, AI systems deployed in high-stakes
settings often fail to consistently add value across contexts and can even
diminish the value that experts alone provide. Beyond harm in specific domains,
such outcomes impede progress in research and practice, underscoring the need
to understand when and why different AI advisors add or diminish value. To
bridge this gap, we stress the importance of assessing the value AI advice
brings to real-world contexts when designing and evaluating AI advisors.
Building on this perspective, we characterize key pillars -- pathways through
which AI advice impacts value -- and develop a framework that incorporates
these pillars to create reliable, personalized, and value-adding advisors. Our
results highlight the need for system-level, value-driven development of AI
advisors that advise selectively, are tailored to experts' unique behaviors,
and are optimized for context-specific trade-offs between decision improvements
and advising costs. They also reveal how the lack of inclusion of these pillars
in the design of AI advising systems may be contributing to the failures
observed in practical applications.",2024-12-27T08:50:54Z,http://arxiv.org/abs/2412.19530v1,"Nicholas Wolczynski, Maytal Saar-Tsechansky, Tong Wang"
"Real-time Reflectance Generation for UAV Multispectral Imagery using an
  Onboard Downwelling Spectrometer in Varied Weather Conditions","Advancements in unmanned aerial vehicle (UAV) remote sensing with spectral
imaging enable efficient assessment of critical agronomic traits. However,
existing reflectance calibration or generation methods suffer from limited
prediction accuracy and practical flexibility. This study explores reliable and
cost-efficient methods for the accurate conversion of digital number values
acquired from a multispectral imager into reflectance, leveraging real-time
solar spectra as references. To ensure consistent measurements of incident
light, an upward gimbal-mounted downwelling spectrometer was attached to the
UAV, and a sinusoidal model was developed to correct for solar position
variability. Using principal component analysis on the reference solar spectrum
for band selection, a multiple linear regression model with four sensitive
bands (4-Band MLR) and a 30 nm bandwidth achieved performance comparable to the
direct correction method. The root mean square error (RMSE) for reflectance
prediction improved by 86.1% compared to the empirical line method under
fluctuating cloudy conditions and by 59.6% compared to the downwelling light
sensor method averaged across different weather conditions. The RMSE was
calculated as 2.24% in a ground-based diurnal validation, and 2.03% in a UAV
campaign conducted at various times throughout a sunny day. Implementing the
4-Band MLR model enhanced the consistency of canopy reflectance within a
homogeneous vegetation area by 95.0% during spectral imaging in a large rice
field under significant cloud fluctuations. Additionally, improvements of 86.0%
and 90.3% were noted for two vegetation indices: the normalized difference
vegetation index (NDVI; a ratio index) and the difference vegetation index
(DVI; a non-ratio index), respectively.",2024-12-27T08:41:46Z,http://arxiv.org/abs/2412.19527v1,"Jiayang Xie, Yutao Shen, Haiyan Cen"
"PLN and NARS Often Yield Similar strength $\times$ confidence Given
  Highly Uncertain Term Probabilities","We provide a comparative analysis of the deduction, induction, and abduction
formulas used in Probabilistic Logic Networks (PLN) and the Non-Axiomatic
Reasoning System (NARS), two uncertain reasoning frameworks aimed at AGI. One
difference between the two systems is that, at the level of individual
inference rules, PLN directly leverages both term and relationship
probabilities, whereas NARS only leverages relationship frequencies and has no
simple analogue of term probabilities. Thus we focus here on scenarios where
there is high uncertainty about term probabilities, and explore how this
uncertainty influences the comparative inferential conclusions of the two
systems. We compare the product of strength and confidence ($s\times c$) in PLN
against the product of frequency and confidence ($f\times c$) in NARS
(quantities we refer to as measuring the ""power"" of an uncertain statement) in
cases of high term probability uncertainty, using heuristic analyses and
elementary numerical computations. We find that in many practical situations
with high term probability uncertainty, PLN and NARS formulas give very similar
results for the power of an inference conclusion, even though they sometimes
come to these similar numbers in quite different ways.",2024-12-27T08:31:19Z,http://arxiv.org/abs/2412.19524v1,Ben Goertzel
"Exploiting Domain-Specific Parallel Data on Multilingual Language Models
  for Low-resource Language Translation","Neural Machine Translation (NMT) systems built on multilingual
sequence-to-sequence Language Models (msLMs) fail to deliver expected results
when the amount of parallel data for a language, as well as the language's
representation in the model are limited. This restricts the capabilities of
domain-specific NMT systems for low-resource languages (LRLs). As a solution,
parallel data from auxiliary domains can be used either to fine-tune or to
further pre-train the msLM. We present an evaluation of the effectiveness of
these two techniques in the context of domain-specific LRL-NMT. We also explore
the impact of domain divergence on NMT model performance. We recommend several
strategies for utilizing auxiliary parallel data in building domain-specific
NMT models for LRLs.",2024-12-27T08:25:52Z,http://arxiv.org/abs/2412.19522v1,"Surangika Ranathungaa, Shravan Nayak, Shih-Ting Cindy Huang, Yanke Mao, Tong Su, Yun-Hsiang Ray Chan, Songchen Yuan, Anthony Rinaldi, Annie En-Shiun Lee"
"Lévy Score Function and Score-Based Particle Algorithm for Nonlinear
  Lévy--Fokker--Planck Equations","The score function for the diffusion process, also known as the gradient of
the log-density, is a basic concept to characterize the probability flow with
important applications in the score-based diffusion generative modelling and
the simulation of It\^{o} stochastic differential equations. However, neither
the probability flow nor the corresponding score function for the
diffusion-jump process are known. This paper delivers mathematical derivation,
numerical algorithm, and error analysis focusing on the corresponding score
function in non-Gaussian systems with jumps and discontinuities represented by
the nonlinear L\'{e}vy--Fokker--Planck equations. We propose the L\'{e}vy score
function for such stochastic equations, which features a nonlocal
double-integral term, and we develop its training algorithm by minimizing the
proposed loss function from samples. Based on the equivalence of the
probability flow with deterministic dynamics, we develop a self-consistent
score-based transport particle algorithm to sample the interactive L\'{e}vy
stochastic process at discrete time grid points. We provide error bound for the
Kullback--Leibler divergence between the numerical and true probability density
functions by overcoming the nonlocal challenges in the L\'{e}vy score. The full
error analysis with the Monte Carlo error and the time discretization error is
furthermore established. To show the usefulness and efficiency of our approach,
numerical examples from applications in biology and finance are tested.",2024-12-27T08:23:04Z,http://arxiv.org/abs/2412.19520v1,"Yuanfei Huang, Chengyu Liu, Xiang Zhou"
"Improved measurements of neutron lifetime with cold neutron beam at
  J-PARC","The ``neutron lifetime puzzle'' arises from the discrepancy between neutron
lifetime measurements obtained using the beam method, which measures decay
products, and the bottle method, which measures the disappearance of neutrons.
To resolve this puzzle, we conducted an experiment using a pulsed cold neutron
beam at J-PARC. In this experiment, the neutron lifetime is determined from the
ratio of neutron decay counts to $^3$He(n,p)$^3$H reactions in a gas detector.
This experiment belongs to the beam method but differs from previous
experiments that measured protons, as it instead detects electrons, enabling
measurements with distinct systematic uncertainties. By enlarging the beam
transport system and reducing systematic uncertainties, we achieved a fivefold
improvement in precision. Analysis of all acquired data yielded a neutron
lifetime of $\tau_{\rm n}=877.2~\pm~1.7_{\rm(stat.)}~^{+4.0}_{-3.6}{}_{\rm
(sys.)}$ s. This result is consistent with bottle method measurements but
exhibits a 2.3$\sigma$ tension with the average value obtained from the
proton-detection-based beam method.",2024-12-27T08:19:54Z,http://arxiv.org/abs/2412.19519v1,"Y. Fuwa, T. Hasegawa, K. Hirota, T. Hoshino, R. Hosokawa, G. Ichikawa, S. Ieki, T. Ino, Y. Iwashita, M. Kitaguchi, R. Kitahara, S. Makise, K. Mishima, T. Mogi, N. Nagakura, H. Oide, H. Okabe, H. Otono, Y. Seki, D. Sekiba, T. Shima, H. E. Shimizu, H. M. Shimizu, N. Sumi, H. Sumino, M. Tanida, H. Uehara, T. Yamada, S. Yamashita, K. Yano, T. Yoshioka"
"Estimation of System Parameters Including Repeated Cross-Sectional Data
  through Emulator-Informed Deep Generative Model","Differential equations (DEs) are crucial for modeling the evolution of
natural or engineered systems. Traditionally, the parameters in DEs are
adjusted to fit data from system observations. However, in fields such as
politics, economics, and biology, available data are often independently
collected at distinct time points from different subjects (i.e., repeated
cross-sectional (RCS) data). Conventional optimization techniques struggle to
accurately estimate DE parameters when RCS data exhibit various
heterogeneities, leading to a significant loss of information. To address this
issue, we propose a new estimation method called the emulator-informed
deep-generative model (EIDGM), designed to handle RCS data. Specifically, EIDGM
integrates a physics-informed neural network-based emulator that immediately
generates DE solutions and a Wasserstein generative adversarial network-based
parameter generator that can effectively mimic the RCS data. We evaluated EIDGM
on exponential growth, logistic population models, and the Lorenz system,
demonstrating its superior ability to accurately capture parameter
distributions. Additionally, we applied EIDGM to an experimental dataset of
Amyloid beta 40 and beta 42, successfully capturing diverse parameter
distribution shapes. This shows that EIDGM can be applied to model a wide range
of systems and extended to uncover the operating principles of systems based on
limited data.",2024-12-27T08:19:23Z,http://arxiv.org/abs/2412.19517v1,"Hyunwoo Cho, Sung Woong Cho, Hyeontae Jo, Hyung Ju Hwang"
Dynamical phase transitions in certain non-ergodic stochastic processes,"We present a class of stochastic processes in which the large deviation
functions of time-integrated observables exhibit singularities that relate to
dynamical phase transitions of trajectories. These illustrative examples
include Brownian motion with a death rate or in the presence of an absorbing
wall, for which we consider a set of empirical observables such as the net
displacement, local time, residence time, and area under the trajectory. Using
a backward Fokker-Planck approach, we derive the large deviation functions of
these observables, and demonstrate how singularities emerge from a competition
between survival and diffusion. Furthermore, we analyse this scenario using an
alternative approach with tilted operators, showing that at the singular point,
the effective dynamics undergoes an abrupt transition. Extending this approach,
we show that similar transitions may generically arise in Markov chains with
transient states. This scenario is robust and generalizable for non-Markovian
dynamics and for many-body systems, potentially leading to multiple dynamical
phase transitions.",2024-12-27T08:16:47Z,http://arxiv.org/abs/2412.19516v1,"Yogeesh Reddy Yerrababu, Satya N. Majumdar, Tridib Sadhu"
"Real-time classification of EEG signals using Machine Learning
  deployment","The prevailing educational methods predominantly rely on traditional
classroom instruction or online delivery, often limiting the teachers' ability
to engage effectively with all the students simultaneously. A more intrinsic
method of evaluating student attentiveness during lectures can enable the
educators to tailor the course materials and their teaching styles in order to
better meet the students' needs. The aim of this paper is to enhance teaching
quality in real time, thereby fostering a higher student engagement in the
classroom activities. By monitoring the students' electroencephalography (EEG)
signals and employing machine learning algorithms, this study proposes a
comprehensive solution for addressing this challenge. Machine learning has
emerged as a powerful tool for simplifying the analysis of complex variables,
enabling the effective assessment of the students' concentration levels based
on specific parameters. However, the real-time impact of machine learning
models necessitates a careful consideration as their deployment is concerned.
This study proposes a machine learning-based approach for predicting the level
of students' comprehension with regard to a certain topic. A browser interface
was introduced that accesses the values of the system's parameters to determine
a student's level of concentration on a chosen topic. The deployment of the
proposed system made it necessary to address the real-time challenges faced by
the students, consider the system's cost, and establish trust in its efficacy.
This paper presents the efforts made for approaching this pertinent issue
through the implementation of innovative technologies and provides a framework
for addressing key considerations for future research directions.",2024-12-27T08:14:28Z,http://arxiv.org/abs/2412.19515v1,"Swati Chowdhuri, Satadip Saha, Samadrita Karmakar, Ankur Chanda"
"HBT interferometry and Quantum Detection of Primordial Gravitational
  Waves in Hořava-Lifshitz Gravity","Ho\v{r}ava-Lifshitz gravity (to be precise, its projectable version) is
recognized as a renormalizable, unitary, and asymptotically free quantum field
theory of gravity. Notably, one of its cosmological predictions is that it can
produce scale-invariant primordial density fluctuations and primordial
gravitational waves without relying on inflation. In this paper, we investigate
the quantum nature of the primordial gravitational waves generated in
Ho\v{r}ava-Lifshitz gravity. It has been suggested that, for some inflationary
models, the non-classicality of primordial gravitational waves in the squeezed
coherent quantum state can be detected using the Hanbury Brown - Twiss (HBT)
interferometry. We show that in Ho\v{r}ava-Lifshitz gravity, scale-invariant
primordial gravitational waves can be generated during both the
radiation-dominated and matter-dominated eras of the Universe. Moreover, the
frequency range of their quantum signatures is shown to extend beyond that of
inflationary models.",2024-12-27T08:13:52Z,http://arxiv.org/abs/2412.19514v1,"Sugumi Kanno, Hiroki Matsui, Shinji Mukohyama"
"Confidence v.s. Critique: A Decomposition of Self-Correction Capability
  for LLMs","Large Language Models (LLMs) can correct their self-generated responses, but
a decline in accuracy after self-correction is also witnessed. To have a deeper
understanding of self-correction, we endeavor to decompose, evaluate, and
analyze the self-correction behaviors of LLMs. By enumerating and analyzing
answer correctness before and after self-correction, we decompose the
self-correction capability into confidence (being confident to correct answers)
and critique (turning wrong answers to correct) capabilities, and propose two
metrics from a probabilistic perspective to measure these 2 capabilities, along
with another metric for overall self-correction capability evaluation. Based on
our decomposition and evaluation metrics, we conduct extensive experiments and
draw some empirical conclusions. For example, we find different models can
exhibit distinct behaviors: some models are confident while others are more
critical. We also find the trade-off between the two capabilities (i.e.
improving one can lead to a decline in the other) when manipulating model
self-correction behavior by prompts or in-context learning. Further, we find a
simple yet efficient strategy to improve self-correction capability by
transforming Supervision Fine-Tuning (SFT) data format, and our strategy
outperforms vanilla SFT in both capabilities and achieves much higher accuracy
after self-correction. Our code will be publicly available on GitHub.",2024-12-27T08:09:11Z,http://arxiv.org/abs/2412.19513v1,"Zhe Yang, Yichang Zhang, Yudong Wang, Ziyao Xu, Junyang Lin, Zhifang Sui"
"Parameter Efficient Fine-Tuning for Deep Learning-Based Full-Waveform
  Inversion","Seismic full waveform inversion (FWI) has seen promising advancements through
deep learning. Existing approaches typically focus on task-specific models
trained and evaluated in isolation that lead to limited generalization across
different geological scenarios. In this work we introduce a task-agnostic
foundational model for FWI that captures general features across tasks. We
first demonstrate that full fine-tuning of this foundational model outperforms
task-specific models built from scratch by delivering superior performance
across multiple benchmarks. Building upon this we employ parameter-efficient
fine-tuning (PEFT) to further reduce computational overhead. By fine-tuning
only a small fraction of the model parameters PEFT achieves comparable results
to full fine-tuning while significantly lowering memory and computational
requirements. Additionally, PEFT excels in out-of-distribution tasks where it
outperforms both full fine-tuning and task-specific models. These findings
establish the value of foundational modeling for FWI and highlight PEFT as an
effective strategy for efficient and scalable adaptation across diverse tasks.",2024-12-27T08:00:12Z,http://arxiv.org/abs/2412.19510v1,"Koustav Ghosal, Abhranta Panigrahi, Arnav Chavan, ArunSingh, Deepak Gupta"
A Theoretical Study of Cavity-modulated Topological Anderson Insulators,"Strong light-matter interaction has been demonstrated feasible for
controlling phases of matter. In this work, the interplay with disorder is
studied and rich phenomena are demonstrated. Specifically, the topological
phases of the disordered longer-range Su-Schrieffer-Heeger (SSH) model coupled
with cavity photons are studied numerically. It is found that cavity photons
modify the hopping amplitudes, resulting in the change of phase transition
boundaries, and disorder induced topological Anderson insulating (TAI) phases
even in the presence of cavity photons. The critical disorder strength at the
phase transitions, determined by localization lengths, can be modulated by
cavity photons through the modified hopping amplitudes. Our work extends the
study of cavity-coupled solid state systems to disordered lattices.",2024-12-27T07:54:32Z,http://arxiv.org/abs/2412.19508v1,"Yu-Cheng Shaw, Hsiu-Chuan Hsu, J. S. You"
"DrivingWorld: ConstructingWorld Model for Autonomous Driving via Video
  GPT","Recent successes in autoregressive (AR) generation models, such as the GPT
series in natural language processing, have motivated efforts to replicate this
success in visual tasks. Some works attempt to extend this approach to
autonomous driving by building video-based world models capable of generating
realistic future video sequences and predicting ego states. However, prior
works tend to produce unsatisfactory results, as the classic GPT framework is
designed to handle 1D contextual information, such as text, and lacks the
inherent ability to model the spatial and temporal dynamics essential for video
generation. In this paper, we present DrivingWorld, a GPT-style world model for
autonomous driving, featuring several spatial-temporal fusion mechanisms. This
design enables effective modeling of both spatial and temporal dynamics,
facilitating high-fidelity, long-duration video generation. Specifically, we
propose a next-state prediction strategy to model temporal coherence between
consecutive frames and apply a next-token prediction strategy to capture
spatial information within each frame. To further enhance generalization
ability, we propose a novel masking strategy and reweighting strategy for token
prediction to mitigate long-term drifting issues and enable precise control.
Our work demonstrates the ability to produce high-fidelity and consistent video
clips of over 40 seconds in duration, which is over 2 times longer than
state-of-the-art driving world models. Experiments show that, in contrast to
prior works, our method achieves superior visual quality and significantly more
accurate controllable future video generation. Our code is available at
https://github.com/YvanYin/DrivingWorld.",2024-12-27T07:44:07Z,http://arxiv.org/abs/2412.19505v1,"Xiaotao Hu, Wei Yin, Mingkai Jia, Junyuan Deng, Xiaoyang Guo, Qian Zhang, Xiaoxiao Long, Ping Tan"
"Nonlinear valley Hall effect in a bilayer transition metal
  dichalcogenide","Valley-contrasting Hall transport conventionally relies on the inversion
symmetry breaking in two-dimensional systems, which greatly limits the
selection range of valley materials. In particular, while monolayer transition
metal dichalcogenides have been widely utilized as a well-known class of valley
materials in valleytronics, the centrosymmetric nature hinders the realization
of valley-contrasting properties in the bilayer counterparts. Here, taking
MoS$_{2}$ as an example, we discover valley-contrasting transport in bilayer
transition metal dichalcogenides by exploring nonlinear transport regime. Using
effective models and first-principles calculations, our work demonstrates that
nonvanishing nonlinear valley Hall conductivities emerge in a uniaxially
strained MoS$_{2}$ bilayer, owing to strain-induced band tilts of Dirac
fermions. With the aid of small spin-orbit-coupling induced band splittings,
the conduction bands generate much remarkable nonlinear valley Hall
conductivity. Moreover, the nonlinear conductivities are highly tunable through
modulating the strength and the direction of the strain, chemical potential,
and interlayer gap. Our findings not only expands material choices for
valleytronic applications, but also provides opportunities for designing
advanced electronic devices that leverage nonlinear valley transports.",2024-12-27T07:36:06Z,http://arxiv.org/abs/2412.19502v1,"Zhichao Zhou, Ruijing Fang, Zhen Zhang, Xiaoyu Wang, Jiayan Rong, Xiao Li"
"Multimodal Symmetric Circular Distributions Based on Nonnegative
  Trigonometric Sums and a Likelihood Ratio Test for Reflective Symmetry","Fern\'andez-Dur\'an (2004) developed a family of circular distributions based
on nonnegative trigonometric sums (NNTS) which is flexible for modeling
datasets exhibiting multimodality and asymmetry. Many datasets involving angles
in the natural sciences, such as animal movement in biology, are expected to
exhibit reflective symmetry with respect to a central angle (axis) of symmetry.
Testing for symmetry in the underlying circular density from which these angles
are generated is crucial. Additionally, such densities often display
multimodality. This paper identifies the conditions under which NNTS
distributions are reflective symmetric and develops a likelihood ratio test for
reflective symmetry. The proposed methodology is demonstrated through
applications to simulated and real datasets.",2024-12-27T07:35:16Z,http://arxiv.org/abs/2412.19501v1,"Juan José Fernández-Durán, María Mercedes Gregorio-Domínguez"
"RobotDiffuse: Motion Planning for Redundant Manipulator based on
  Diffusion Model","Redundant manipulators, with their higher Degrees of Freedom (DOFs), offer
enhanced kinematic performance and versatility, making them suitable for
applications like manufacturing, surgical robotics, and human-robot
collaboration. However, motion planning for these manipulators is challenging
due to increased DOFs and complex, dynamic environments. While traditional
motion planning algorithms struggle with high-dimensional spaces, deep
learning-based methods often face instability and inefficiency in complex
tasks. This paper introduces RobotDiffuse, a diffusion model-based approach for
motion planning in redundant manipulators. By integrating physical constraints
with a point cloud encoder and replacing the U-Net structure with an
encoder-only transformer, RobotDiffuse improves the model's ability to capture
temporal dependencies and generate smoother, more coherent motion plans. We
validate the approach using a complex simulator, and release a new dataset with
35M robot poses and 0.14M obstacle avoidance scenarios. Experimental results
demonstrate the effectiveness of RobotDiffuse and the promise of diffusion
models for motion planning tasks. The code can be accessed at
https://github.com/ACRoboT-buaa/RobotDiffuse.",2024-12-27T07:34:54Z,http://arxiv.org/abs/2412.19500v1,"Xiaohan Zhang, Xudong Mou, Rui Wang, Tianyu Wo, Ningbo Gu, Tiejun Wang, Cangbai Xu, Xudong Liu"
Casevo: A Cognitive Agents and Social Evolution Simulator,"In this paper, we introduce a multi-agent simulation framework Casevo
(Cognitive Agents and Social Evolution Simulator), that integrates large
language models (LLMs) to simulate complex social phenomena and decision-making
processes. Casevo is designed as a discrete-event simulator driven by agents
with features such as Chain of Thoughts (CoT), Retrieval-Augmented Generation
(RAG), and Customizable Memory Mechanism. Casevo enables dynamic social
modeling, which can support various scenarios such as social network analysis,
public opinion dynamics, and behavior prediction in complex social systems. To
demonstrate the effectiveness of Casevo, we utilize one of the U.S. 2020
midterm election TV debates as a simulation example. Our results show that
Casevo facilitates more realistic and flexible agent interactions, improving
the quality of dynamic social phenomena simulation. This work contributes to
the field by providing a robust system for studying large-scale, high-fidelity
social behaviors with advanced LLM-driven agents, expanding the capabilities of
traditional agent-based modeling (ABM). The open-source code repository address
of casevo is https://github.com/rgCASS/casevo.",2024-12-27T07:33:49Z,http://arxiv.org/abs/2412.19498v1,"Zexun Jiang, Yafang Shi, Maoxu Li, Hongjiang Xiao, Yunxiao Qin, Qinglan Wei, Ye Wang, Yuan Zhang"
"Multi-Condition Fault Diagnosis of Dynamic Systems: A Survey, Insights,
  and Prospects","With the increasing complexity of industrial production systems, accurate
fault diagnosis is essential to ensure safe and efficient system operation.
However, due to changes in production demands, dynamic process adjustments, and
complex external environmental disturbances, multiple operating conditions
frequently arise during production. The multi-condition characteristics pose
significant challenges to traditional fault diagnosis methods. In this context,
multi-condition fault diagnosis has gradually become a key area of research,
attracting extensive attention from both academia and industry. This paper aims
to provide a systematic and comprehensive review of existing research in the
field. Firstly, the mathematical definition of the problem is presented,
followed by an overview of the current research status. Subsequently, the
existing literature is reviewed and categorized from the perspectives of
single-model and multi-model approaches. In addition, standard evaluation
metrics and typical real-world application scenarios are summarized and
analyzed. Finally, the key challenges and prospects in the field are thoroughly
discussed.",2024-12-27T07:33:46Z,http://arxiv.org/abs/2412.19497v1,"Pengyu Han, Zeyi Liu, Xiao He, Steven X. Ding, Donghua Zhou"
"Multi-P$^2$A: A Multi-perspective Benchmark on Privacy Assessment for
  Large Vision-Language Models","Large Vision-Language Models (LVLMs) exhibit impressive potential across
various tasks but also face significant privacy risks, limiting their practical
applications. Current researches on privacy assessment for LVLMs is limited in
scope, with gaps in both assessment dimensions and privacy categories. To
bridge this gap, we propose Multi-P$^2$A, a comprehensive benchmark for
evaluating the privacy preservation capabilities of LVLMs in terms of privacy
awareness and leakage. Privacy awareness measures the model's ability to
recognize the privacy sensitivity of input data, while privacy leakage assesses
the risk of the model unintentionally disclosing privacy information in its
output. We design a range of sub-tasks to thoroughly evaluate the model's
privacy protection offered by LVLMs. Multi-P$^2$A covers 26 categories of
personal privacy, 15 categories of trade secrets, and 18 categories of state
secrets, totaling 31,962 samples. Based on Multi-P$^2$A, we evaluate the
privacy preservation capabilities of 21 open-source and 2 closed-source LVLMs.
Our results reveal that current LVLMs generally pose a high risk of
facilitating privacy breaches, with vulnerabilities varying across personal
privacy, trade secret, and state secret.",2024-12-27T07:33:39Z,http://arxiv.org/abs/2412.19496v1,"Jie Zhang, Xiangkui Cao, Zhouyu Han, Shiguang Shan, Xilin Chen"
Retrieval-augmented Generation for GenAI-enabled Semantic Communications,"Semantic communication (SemCom) is an emerging paradigm aiming at
transmitting only task-relevant semantic information to the receiver, which can
significantly improve communication efficiency. Recent advancements in
generative artificial intelligence (GenAI) have empowered GenAI-enabled SemCom
(GenSemCom) to further expand its potential in various applications. However,
current GenSemCom systems still face challenges such as semantic inconsistency,
limited adaptability to diverse tasks and dynamic environments, and the
inability to leverage insights from past transmission. Motivated by the success
of retrieval-augmented generation (RAG) in the domain of GenAI, this paper
explores the integration of RAG in GenSemCom systems. Specifically, we first
provide a comprehensive review of existing GenSemCom systems and the
fundamentals of RAG techniques. We then discuss how RAG can be integrated into
GenSemCom. Following this, we conduct a case study on semantic image
transmission using an RAG-enabled diffusion-based SemCom system, demonstrating
the effectiveness of the proposed integration. Finally, we outline future
directions for advancing RAG-enabled GenSemCom systems.",2024-12-27T07:30:01Z,http://arxiv.org/abs/2412.19494v1,"Shunpu Tang, Ruichen Zhang, Yuxuan Yan, Qianqian Yang, Dusit Niyato, Xianbin Wang, Shiwen Mao"
"Two superconducting thin films systems with potential integration of
  different quantum functionalities","Quantum computation based on superconducting circuits utilizes
superconducting qubits with Josephson tunnel junctions. Engineering
high-coherence qubits requires materials optimization. In this work, we present
two superconducting thin film systems, grown on silicon (Si), and one obtained
from the other via annealing. Cobalt (Co) thin films grown on Si were found to
be superconducting [EPL 131 (2020) 47001]. These films also happen to be a
self-organised hybrid superconductor/ferromagnet/superconductor (S/F/S)
structure. The S/F/S hybrids are important for superconducting $\pi$-qubits
[PRL 95 (2005) 097001] and in quantum information processing. Here we present
our results on the superconductivity of a hybrid Co film followed by the
superconductivity of a CoSi$_2$ film, which was prepared by annealing the Co
film. CoSi$_2$, with its $1/f$ noise about three orders of magnitude smaller
compared to the most commonly used superconductor aluminium (Al), is a
promising material for high-coherence qubits. The hybrid Co film revealed
superconducting transition temperature $T_c$ = 5 K and anisotropy in the upper
critical field between the in-plane and out-of-plane directions. The anisotropy
was of the order of ratio of lateral dimensions to thickness of the
superconducting Co grains, suggesting a quasi-2D nature of superconductivity.
On the other hand, CoSi$_2$ film showed a $T_c$ of 900 mK. In the resistivity
vs. temperature curve, we observe a peak near $T_c$. Magnetic field scan as a
function of $T$ shows a monotonic increase in intensity of this peak with
temperature. The origin of the peak has been explained in terms of parallel
resistive model for the particular measurement configuration. Although our
CoSi$_2$ film contains grain boundaries, we observed a perpendicular critical
field of 15 mT and a critical current density of 3.8x10$^7$ A/m$^2$, comparable
with epitaxial CoSi$_2$ films.",2024-12-27T07:23:20Z,http://arxiv.org/abs/2412.19493v1,"Snehal Mandal, Biplab Biswas, Suvankar Purakait, Anupam Roy, Biswarup Satpati, Indranil Das, B. N. Dev"
Towards Open-Vocabulary Remote Sensing Image Semantic Segmentation,"Recently, deep learning based methods have revolutionized remote sensing
image segmentation. However, these methods usually rely on a pre-defined
semantic class set, thus needing additional image annotation and model training
when adapting to new classes. More importantly, they are unable to segment
arbitrary semantic classes. In this work, we introduce Open-Vocabulary Remote
Sensing Image Semantic Segmentation (OVRSISS), which aims to segment arbitrary
semantic classes in remote sensing images. To address the lack of OVRSISS
datasets, we develop LandDiscover50K, a comprehensive dataset of 51,846 images
covering 40 diverse semantic classes. In addition, we propose a novel framework
named GSNet that integrates domain priors from special remote sensing models
and versatile capabilities of general vision-language models. Technically,
GSNet consists of a Dual-Stream Image Encoder (DSIE), a Query-Guided Feature
Fusion (QGFF), and a Residual Information Preservation Decoder (RIPD). DSIE
first captures comprehensive features from both special models and general
models in dual streams. Then, with the guidance of variable vocabularies, QGFF
integrates specialist and generalist features, enabling them to complement each
other. Finally, RIPD is proposed to aggregate multi-source features for more
accurate mask predictions. Experiments show that our method outperforms other
methods by a large margin, and our proposed LandDiscover50K improves the
performance of OVRSISS methods. The proposed dataset and method will be made
publicly available at https://github.com/yecy749/GSNet.",2024-12-27T07:20:30Z,http://arxiv.org/abs/2412.19492v1,"Chengyang Ye, Yunzhi Zhuge, Pingping Zhang"
User Willingness-aware Sales Talk Dataset,"User willingness is a crucial element in the sales talk process that affects
the achievement of the salesperson's or sales system's objectives. Despite the
importance of user willingness, to the best of our knowledge, no previous study
has addressed the development of automated sales talk dialogue systems that
explicitly consider user willingness. A major barrier is the lack of sales talk
datasets with reliable user willingness data. Thus, in this study, we developed
a user willingness-aware sales talk collection by leveraging the ecological
validity concept, which is discussed in the field of human-computer
interaction. Our approach focused on three types of user willingness essential
in real sales interactions. We created a dialogue environment that closely
resembles real-world scenarios to elicit natural user willingness, with
participants evaluating their willingness at the utterance level from multiple
perspectives. We analyzed the collected data to gain insights into practical
user willingness-aware sales talk strategies. In addition, as a practical
application of the constructed dataset, we developed and evaluated a sales
dialogue system aimed at enhancing the user's intent to purchase.",2024-12-27T07:16:10Z,http://arxiv.org/abs/2412.19490v1,"Asahi Hentona, Jun Baba, Shiki Sato, Reina Akama"
RAIN: Real-time Animation of Infinite Video Stream,"Live animation has gained immense popularity for enhancing online engagement,
yet achieving high-quality, real-time, and stable animation with diffusion
models remains challenging, especially on consumer-grade GPUs. Existing methods
struggle with generating long, consistent video streams efficiently, often
being limited by latency issues and degraded visual quality over extended
periods. In this paper, we introduce RAIN, a pipeline solution capable of
animating infinite video streams in real-time with low latency using a single
RTX 4090 GPU. The core idea of RAIN is to efficiently compute frame-token
attention across different noise levels and long time-intervals while
simultaneously denoising a significantly larger number of frame-tokens than
previous stream-based methods. This design allows RAIN to generate video frames
with much shorter latency and faster speed, while maintaining long-range
attention over extended video streams, resulting in enhanced continuity and
consistency. Consequently, a Stable Diffusion model fine-tuned with RAIN in
just a few epochs can produce video streams in real-time and low latency
without much compromise in quality or consistency, up to infinite long. Despite
its advanced capabilities, the RAIN only introduces a few additional 1D
attention blocks, imposing minimal additional burden. Experiments in benchmark
datasets and generating super-long videos demonstrating that RAIN can animate
characters in real-time with much better quality, accuracy, and consistency
than competitors while costing less latency. All code and models will be made
publicly available.",2024-12-27T07:13:15Z,http://arxiv.org/abs/2412.19489v1,"Zhilei Shu, Ruili Feng, Yang Cao, Zheng-Jun Zha"
"Block cross-interactive residual smoothing for Lanczos-type solvers for
  linear systems with multiple right-hand sides","Lanczos-type solvers for large sparse linear systems often exhibit large
oscillations in the residual norms. In finite precision arithmetic, large
oscillations increase the residual gap (the difference between the recursively
updated residual and the explicitly computed residual) and a loss of attainable
accuracy of the approximations. This issue is addressed using cross-interactive
residual smoothing (CIRS). This approach improves convergence behavior and
reduces the residual gap. Similar to how the standard Lanczos-type solvers have
been extended to global and block versions for solving systems with multiple
right-hand sides, CIRS can also be extended to these versions. While we have
developed a global CIRS scheme (Gl-CIRS) in our previous study [K. Aihara, A.
Imakura, and K. Morikuni, SIAM J. Matrix Anal. Appl., 43 (2022),
pp.1308--1330], in this study, we propose a block version (Bl-CIRS).
Subsequently, we demonstrate the effectiveness of Bl-CIRS from various
perspectives, such as theoretical insights into the convergence behaviors of
the residual and approximation norms, numerical experiments on model problems,
and a detailed rounding error analysis for the residual gap. For Bl-CIRS,
orthonormalizing the columns of direction matrices is crucial in effectively
reducing the residual gap. This analysis also complements our previous study
and evaluates the residual gap of the block Lanczos-type solvers.",2024-12-27T07:08:29Z,http://arxiv.org/abs/2412.19488v1,"Kensuke Aihara, Akira Imakura, Keiichi Morikuni"
UniBrain: A Unified Model for Cross-Subject Brain Decoding,"Brain decoding aims to reconstruct original stimuli from fMRI signals,
providing insights into interpreting mental content. Current approaches rely
heavily on subject-specific models due to the complex brain processing
mechanisms and the variations in fMRI signals across individuals. Therefore,
these methods greatly limit the generalization of models and fail to capture
cross-subject commonalities. To address this, we present UniBrain, a unified
brain decoding model that requires no subject-specific parameters. Our approach
includes a group-based extractor to handle variable fMRI signal lengths, a
mutual assistance embedder to capture cross-subject commonalities, and a
bilevel feature alignment scheme for extracting subject-invariant features. We
validate our UniBrain on the brain decoding benchmark, achieving comparable
performance to current state-of-the-art subject-specific models with extremely
fewer parameters. We also propose a generalization benchmark to encourage the
community to emphasize cross-subject commonalities for more general brain
decoding. Our code is available at https://github.com/xiaoyao3302/UniBrain.",2024-12-27T07:03:47Z,http://arxiv.org/abs/2412.19487v1,"Zicheng Wang, Zhen Zhao, Luping Zhou, Parashkev Nachev"
Comprehensive Bayesian Exploration of Froggatt-Nielsen Mechanism,"The Froggatt-Nielsen (FN) mechanism successfully explains the hierarchical
structure of fermion Yukawa couplings by introducing a U(1) flavor symmetry
with distinct charge assignments for different fermion generations. While some
FN charge assignments have been proposed, their evaluation has largely relied
on heuristic approaches. This paper systematically investigates viable FN
charge assignments within the Standard Model, including both the quark and
lepton sectors, using Bayesian statistical analysis. The study explores
scenarios involving both the seesaw mechanism and dimension-five operators for
neutrino mass generation. A comprehensive parameter scan over FN charges
reveals a wide range of charge assignments consistent with observed fermion
masses and mixing angles. Interestingly, negative FN charges and significant
generational differences in charges are found to be viable, contrary to
conventional assumptions. The analysis also compares the seesaw mechanism and
dimension-five operator scenarios, finding no strong preference between them
for optimal charge assignments. Furthermore, predictions for the lightest
neutrino mass and effective Majorana mass relevant for neutrinoless double-beta
decay are presented, highlighting regions of parameter space accessible to
upcoming experiments. Finally, implications for nucleon decay are studied,
demonstrating that different FN charge assignments predict significantly
different nucleon decay lifetimes and branching ratios, providing a potential
experimental probe for FN models.",2024-12-27T06:41:17Z,http://arxiv.org/abs/2412.19484v1,"Masahiro Ibe, Satoshi Shirai, Keiichi Watanabe"
Learning Radiance Fields from a Single Snapshot Compressive Image,"In this paper, we explore the potential of Snapshot Compressive Imaging (SCI)
technique for recovering the underlying 3D scene structure from a single
temporal compressed image. SCI is a cost-effective method that enables the
recording of high-dimensional data, such as hyperspectral or temporal
information, into a single image using low-cost 2D imaging sensors. To achieve
this, a series of specially designed 2D masks are usually employed, reducing
storage and transmission requirements and offering potential privacy
protection. Inspired by this, we take one step further to recover the encoded
3D scene information leveraging powerful 3D scene representation capabilities
of neural radiance fields (NeRF). Specifically, we propose SCINeRF, in which we
formulate the physical imaging process of SCI as part of the training of NeRF,
allowing us to exploit its impressive performance in capturing complex scene
structures. In addition, we further integrate the popular 3D Gaussian Splatting
(3DGS) framework and propose SCISplat to improve 3D scene reconstruction
quality and training/rendering speed by explicitly optimizing point clouds into
3D Gaussian representations. To assess the effectiveness of our method, we
conduct extensive evaluations using both synthetic data and real data captured
by our SCI system. Experimental results demonstrate that our proposed approach
surpasses the state-of-the-art methods in terms of image reconstruction and
novel view synthesis. Moreover, our method also exhibits the ability to render
high frame-rate multi-view consistent images in real time by leveraging SCI and
the rendering capabilities of 3DGS. Codes will be available at:
https://github.com/WU- CVGL/SCISplat.",2024-12-27T06:40:44Z,http://arxiv.org/abs/2412.19483v1,"Yunhao Li, Xiang Liu, Xiaodong Wang, Xin Yuan, Peidong Liu"
Generative Adversarial Network on Motion-Blur Image Restoration,"In everyday life, photographs taken with a camera often suffer from motion
blur due to hand vibrations or sudden movements. This phenomenon can
significantly detract from the quality of the images captured, making it an
interesting challenge to develop a deep learning model that utilizes the
principles of adversarial networks to restore clarity to these blurred pixels.
In this project, we will focus on leveraging Generative Adversarial Networks
(GANs) to effectively deblur images affected by motion blur. A GAN-based
Tensorflow model is defined, training and evaluating by GoPro dataset which
comprises paired street view images featuring both clear and blurred versions.
This adversarial training process between Discriminator and Generator helps to
produce increasingly realistic images over time. Peak Signal-to-Noise Ratio
(PSNR) and Structural Similarity Index Measure (SSIM) are the two evaluation
metrics used to provide quantitative measures of image quality, allowing us to
evaluate the effectiveness of the deblurring process. Mean PSNR in 29.1644 and
mean SSIM in 0.7459 with average 4.6921 seconds deblurring time are achieved in
this project. The blurry pixels are sharper in the output of GAN model shows a
good image restoration effect in real world applications.",2024-12-27T06:12:50Z,http://arxiv.org/abs/2412.19479v1,Zhengdong Li
"An Overview of Machine Learning-Driven Resource Allocation in IoT
  Networks","In the wake of disruptive IoT technologies generating massive amounts of
diverse data, Machine Learning (ML) will play a crucial role in bringing
intelligence to Internet of Things (IoT) networks. This paper provides a
comprehensive analysis of the current state of resource allocation within IoT
networks, focusing specifically on two key categories: Low-Power IoT Networks
and Mobile IoT Networks. We delve into the resource allocation strategies that
are crucial for optimizing network performance and energy efficiency in these
environments. Furthermore, the paper explores the transformative role of
Machine Learning (ML), Deep Learning (DL), and Reinforcement Learning (RL) in
enhancing IoT functionalities. We highlight a range of applications and use
cases where these advanced technologies can significantly improve
decision-making and optimization processes. In addition to the opportunities
presented by ML, DL, and RL, we also address the potential challenges that
organizations may face when implementing these technologies in IoT settings.
These challenges include crucial accuracy, low flexibility and adaptability,
and high computational cost, etc. Finally, the paper identifies promising
avenues for future research, emphasizing the need for innovative solutions to
overcome existing hurdles and improve the integration of ML, DL, and RL into
IoT networks. By providing this holistic perspective, we aim to contribute to
the ongoing discourse on resource allocation strategies and the application of
intelligent technologies in the IoT landscape.",2024-12-27T06:11:28Z,http://arxiv.org/abs/2412.19478v1,Zhengdong Li
"Exploiting Dynamic Sparsity for Near-Field Spatial Non-Stationary
  XL-MIMO Channel Tracking","This work considers a spatial non-stationary channel tracking problem in
broadband extremely large-scale multiple-input-multiple-output (XL-MIMO)
systems. In the case of spatial non-stationary, each scatterer has a certain
visibility region (VR) over antennas and power change may occur among visible
antennas. Concentrating on the temporal correlation of XL-MIMO channels, we
design a three-layer Markov prior model and hierarchical two-dimensional (2D)
Markov model to exploit the dynamic sparsity of sparse channel vectors and VRs,
respectively. Then, we formulate the channel tracking problem as a bilinear
measurement process, and a novel dynamic alternating maximum a posteriori
(DA-MAP) framework is developed to solve the problem. The DA-MAP contains four
basic modules: channel estimation module, VR detection module, grid update
module, and temporal correlated module. Specifically, the first module is an
inverse-free variational Bayesian inference (IF-VBI) estimator that avoids
computational intensive matrix inverse each iteration; the second module is a
turbo compressive sensing (Turbo-CS) algorithm that only needs small-scale
matrix operations in a parallel fashion; the third module refines the
polar-delay domain grid; and the fourth module can process the temporal prior
information to ensure high-efficiency channel tracking. Simulations show that
the proposed method can achieve a significant channel tracking performance
while achieving low computational overhead.",2024-12-27T06:00:30Z,http://arxiv.org/abs/2412.19475v1,"Wenkang Xu amd An Liu, Min-jian Zhao, Giuseppe Caire, Yik-Chung Wu"
"Traversing Quantum Control Robustness Landscapes: A New Paradigm for
  Quantum Gate Engineering","The optimization of robust quantum control is often tailored to specific
tasks and suffers from inefficiencies due to the complexity of cost functions
that account for gate infidelity, noise susceptibility, and intricate
constraints. Our recent findings suggest a more efficient approach through the
engineering of quantum gates, beginning with any arbitrary robust control
configuration. We first introduce the Quantum Control Robustness Landscape
(QCRL), a conceptual framework that maps control parameters to noise
susceptibility. This framework facilitates a systematic investigation of
equally robust controls for diverse quantum operations. By navigating through
the level sets of the QCRL, our algorithm Robustness-Invariant Pulse Variation
allows for the variation of control pulses while preserving robustness.
Numerical simulations demonstrate that our single- and two-qubit gates exceed
the quantum error correction threshold even with substantial noise, thereby
relaxing the extremely stringent noise protection mechanisms in quantum
computing systems. This methodology opens up a new paradigm for quantum gate
engineering capable of effectively suppressing generic noise.",2024-12-27T05:56:38Z,http://arxiv.org/abs/2412.19473v1,"Huiqi Xue, Xiu-Hao Deng"
"Quantum Transport through Asymmetrical Molecular channel Azulene: Role
  of Orbital Interference","We investigate electron transport through azulene molecule with four distinct
electrode contact geometries using the non-equilibrium Green's function
formalism within the tight-binding Hamiltonian. Employing the Q-matrix
approach, we analyze quantum interference (QI) among the molecular orbitals in
each contact configuration. Our results reveal distinct transmission profiles
and varying current responses among configurations, with the configuration 1-3
displaying the highest conductivity at higher bias due to strong constructive
interference of the Highest Occupied Molecular Orbital (HOMO). Conversely,
configuration 5-7 exhibit weak conductance and antiresonance at the Fermi
energy, attributed to dominant destructive interference among the frontier
molecular orbitals. Configuration 2-6 is found to exhibit asymmetric I-V
characteristics, due to the dipolar nature of the azulene molecule. These
findings underscore the significance of QI effects in shaping the transport
properties of azulene, and molecule-based devices in general.",2024-12-27T05:56:07Z,http://arxiv.org/abs/2412.19472v1,"Koushik R. Das, Sudipta Dutta"
"Meta-Learning-Based Delayless Subband Adaptive Filter using Complex
  Self-Attention for Active Noise Control","Active noise control typically employs adaptive filtering to generate
secondary noise, where the least mean square algorithm is the most widely used.
However, traditional updating rules are linear and exhibit limited
effectiveness in addressing nonlinear environments and nonstationary noise. To
tackle this challenge, we reformulate the active noise control problem as a
meta-learning problem and propose a meta-learning-based delayless subband
adaptive filter with deep neural networks. The core idea is to utilize a neural
network as an adaptive algorithm that can adapt to different environments and
types of noise. The neural network will train under noisy observations,
implying that it recognizes the optimized updating rule without true labels. A
single-headed attention recurrent neural network is devised with learnable
feature embedding to update the adaptive filter weight efficiently, enabling
accurate computation of the secondary source to attenuate the unwanted primary
noise. In order to relax the time constraint on updating the adaptive filter
weights, the delayless subband architecture is employed, which will allow the
system to be updated less frequently as the downsampling factor increases. In
addition, the delayless subband architecture does not introduce additional time
delays in active noise control systems. A skip updating strategy is introduced
to decrease the updating frequency further so that machines with limited
resources have more possibility to board our meta-learning-based model.
Extensive multi-condition training ensures generalization and robustness
against various types of noise and environments. Simulation results demonstrate
that our meta-learning-based model achieves superior noise reduction
performance compared to traditional methods.",2024-12-27T05:51:40Z,http://arxiv.org/abs/2412.19471v1,"Pengxing Feng, Hing Cheung So"
Movable Antenna-Aided Near-Field Integrated Sensing and Communication,"Integrated sensing and communication (ISAC) is emerging as a pivotal
technology for next-generation wireless networks. However, existing ISAC
systems are based on fixed-position antennas (FPAs), which inevitably incur a
loss in performance when balancing the trade-off between sensing and
communication. Movable antenna (MA) technology offers promising potential to
enhance ISAC performance by enabling flexible antenna movement. Nevertheless,
exploiting more spatial channel variations requires larger antenna moving
regions, which may invalidate the conventional far-field assumption for
channels between transceivers. Therefore, this paper utilizes the MA to enhance
sensing and communication capabilities in near-field ISAC systems, where a
full-duplex base station (BS) is equipped with multiple transmit and receive
MAs movable in large-size regions to simultaneously sense multiple targets and
serve multiple uplink (UL) and downlink (DL) users for communication. We aim to
maximize the weighted sum of sensing and communication rates (WSR) by jointly
designing the transmit beamformers, sensing signal covariance matrices, receive
beamformers, and MA positions at the BS, as well as the UL power allocation.
The resulting optimization problem is challenging to solve, while we propose an
efficient two-layer random position (RP) algorithm to tackle it. In addition,
to reduce movement delay and cost, we design an antenna position matching (APM)
algorithm based on the greedy strategy to minimize the total MA movement
distance. Extensive simulation results demonstrate the substantial performance
improvement achieved by deploying MAs in near-field ISAC systems. Moreover, the
results show the effectiveness of the proposed APM algorithm in reducing the
antenna movement distance, which is helpful for energy saving and time overhead
reduction for MA-aided near-field ISAC systems with large moving regions.",2024-12-27T05:45:35Z,http://arxiv.org/abs/2412.19470v1,"Jingze Ding, Zijian Zhou, Xiaodan Shao, Bingli Jiao, Rui Zhang"
"Knowledge Graph-Based Multi-Agent Path Planning in Dynamic Environments
  using WAITR","This paper addresses the challenge of multi-agent path planning for efficient
data collection in dynamic, uncertain environments, exemplified by autonomous
underwater vehicles (AUVs) navigating the Gulf of Mexico. Traditional greedy
algorithms, though computationally efficient, often fall short in long-term
planning due to their short-sighted nature, missing crucial data collection
opportunities and increasing exposure to hazards. To address these limitations,
we introduce WAITR (Weighted Aggregate Inter-Temporal Reward), a novel
path-planning framework that integrates a knowledge graph with pathlet-based
planning, segmenting the environment into dynamic, speed-adjusted sub-regions
(pathlets). This structure enables coordinated, adaptive planning, as agents
can operate within time-bound regions while dynamically responding to
environmental changes. WAITR's cumulative scoring mechanism balances immediate
data collection with long-term optimization of Points of Interest (POIs),
ensuring safer navigation and comprehensive data coverage. Experimental results
show that WAITR substantially improves POI coverage and reduces exposure to
hazards, achieving up to 27.1\% greater event coverage than traditional greedy
methods.",2024-12-27T05:43:41Z,http://arxiv.org/abs/2412.19469v1,"Ted Edward Holmberg, Elias Ioup, Mahdi Abdelguerfi"
A Time Series Analysis of Assertions in the Linux Kernel,"Assertions are a classical and typical software development technique. These
are extensively used also in operating systems and their kernels, including the
Linux kernel. The paper patches a gap in existing knowledge by empirically
examining the longitudinal evolution of assertion use in the Linux kernel.
According to the results, the use of assertions that cause a kernel panic has
slightly but not substantially decreased from the kernel's third to the sixth
release series. At the same time, however, the use of softer assertion variants
has increased; these do not cause a panic by default but instead produce
warnings. With these time series results, the paper contributes to the existing
but limited empirical knowledge base about operating system kernels and their
long-term evolution.",2024-12-27T05:22:09Z,http://arxiv.org/abs/2412.19465v1,Jukka Ruohonen
"MNet-SAt: A Multiscale Network with Spatial-enhanced Attention for
  Segmentation of Polyps in Colonoscopy","Objective: To develop a novel deep learning framework for the automated
segmentation of colonic polyps in colonoscopy images, overcoming the
limitations of current approaches in preserving precise polyp boundaries,
incorporating multi-scale features, and modeling spatial dependencies that
accurately reflect the intricate and diverse morphology of polyps. Methods: To
address these limitations, we propose a novel Multiscale Network with
Spatial-enhanced Attention (MNet-SAt) for polyp segmentation in colonoscopy
images. This framework incorporates four key modules: Edge-Guided Feature
Enrichment (EGFE) preserves edge information for improved boundary quality;
Multi-Scale Feature Aggregator (MSFA) extracts and aggregates multi-scale
features across channel spatial dimensions, focusing on salient regions;
Spatial-Enhanced Attention (SEAt) captures spatial-aware global dependencies
within the multi-scale aggregated features, emphasizing the region of interest;
and Channel-Enhanced Atrous Spatial Pyramid Pooling (CE-ASPP) resamples and
recalibrates attentive features across scales. Results: We evaluated MNet-SAt
on the Kvasir-SEG and CVC-ClinicDB datasets, achieving Dice Similarity
Coefficients of 96.61% and 98.60%, respectively. Conclusion: Both quantitative
(DSC) and qualitative assessments highlight MNet-SAt's superior performance and
generalization capabilities compared to existing methods. Significance:
MNet-SAt's high accuracy in polyp segmentation holds promise for improving
clinical workflows in early polyp detection and more effective treatment,
contributing to reduced colorectal cancer mortality rates.",2024-12-27T05:17:29Z,http://arxiv.org/abs/2412.19464v1,"Chandravardhan Singh Raghaw, Aryan Yadav, Jasmer Singh Sanjotra, Shalini Dangi, Nagendra Kumar"
Pulse-induced memory-like effect in cyclotron motion?,"We study how a charged particle moving in a uniform magnetic field along its
standard circular path (cyclotron motion) reacts to a short-duration,
homogeneous, uniform electric field pulse injected in the plane perpendicular
to the magnetic field. A `permanent' change in the radius of the initial circle
and a shift of its centre is noted at later times, after the pulse is switched
off. The magnitude of the velocity undergoes a change too, akin to a `velocity
kick'. In summary, our results suggest a pulse-induced `electromagnetic
memory-like effect', which is not quite a `wave memory', but, nevertheless, has
similar features within a simple, non-relativistic context.",2024-12-27T05:13:53Z,http://arxiv.org/abs/2412.19460v1,Sayan Kar
"DriveEditor: A Unified 3D Information-Guided Framework for Controllable
  Object Editing in Driving Scenes","Vision-centric autonomous driving systems require diverse data for robust
training and evaluation, which can be augmented by manipulating object
positions and appearances within existing scene captures. While recent
advancements in diffusion models have shown promise in video editing, their
application to object manipulation in driving scenarios remains challenging due
to imprecise positional control and difficulties in preserving high-fidelity
object appearances. To address these challenges in position and appearance
control, we introduce DriveEditor, a diffusion-based framework for object
editing in driving videos. DriveEditor offers a unified framework for
comprehensive object editing operations, including repositioning, replacement,
deletion, and insertion. These diverse manipulations are all achieved through a
shared set of varying inputs, processed by identical position control and
appearance maintenance modules. The position control module projects the given
3D bounding box while preserving depth information and hierarchically injects
it into the diffusion process, enabling precise control over object position
and orientation. The appearance maintenance module preserves consistent
attributes with a single reference image by employing a three-tiered approach:
low-level detail preservation, high-level semantic maintenance, and the
integration of 3D priors from a novel view synthesis model. Extensive
qualitative and quantitative evaluations on the nuScenes dataset demonstrate
DriveEditor's exceptional fidelity and controllability in generating diverse
driving scene edits, as well as its remarkable ability to facilitate downstream
tasks.",2024-12-27T04:49:36Z,http://arxiv.org/abs/2412.19458v1,"Yiyuan Liang, Zhiying Yan, Liqun Chen, Jiahuan Zhou, Luxin Yan, Sheng Zhong, Xu Zou"
Focusing Image Generation to Mitigate Spurious Correlations,"Instance features in images exhibit spurious correlations with background
features, affecting the training process of deep neural classifiers. This leads
to insufficient attention to instance features by the classifier, resulting in
erroneous classification outcomes. In this paper, we propose a data
augmentation method called Spurious Correlations Guided Synthesis (SCGS) that
mitigates spurious correlations through image generation model. This approach
does not require expensive spurious attribute (group) labels for the training
data and can be widely applied to other debiasing methods. Specifically, SCGS
first identifies the incorrect attention regions of a pre-trained classifier on
the training images, and then uses an image generation model to generate new
training data based on these incorrect attended regions. SCGS increases the
diversity and scale of the dataset to reduce the impact of spurious
correlations on classifiers. Changes in the classifier's attention regions and
experimental results on three different domain datasets demonstrate that this
method is effective in reducing the classifier's reliance on spurious
correlations.",2024-12-27T04:48:56Z,http://arxiv.org/abs/2412.19457v1,"Xuewei Li, Zhenzhen Nie, Mei Yu, Zijian Zhang, Jie Gao, Tianyi Xu, Zhiqiang Liu"
"NijiGAN: Transform What You See into Anime with Contrastive
  Semi-Supervised Learning and Neural Ordinary Differential Equations","Generative AI has transformed the animation industry. Several models have
been developed for image-to-image translation, particularly focusing on
converting real-world images into anime through unpaired translation.
Scenimefy, a notable approach utilizing contrastive learning, achieves high
fidelity anime scene translation by addressing limited paired data through
semi-supervised training. However, it faces limitations due to its reliance on
paired data from a fine-tuned StyleGAN in the anime domain, often producing
low-quality datasets. Additionally, Scenimefy's high parameter architecture
presents opportunities for computational optimization. This research introduces
NijiGAN, a novel model incorporating Neural Ordinary Differential Equations
(NeuralODEs), which offer unique advantages in continuous transformation
modeling compared to traditional residual networks. NijiGAN successfully
transforms real-world scenes into high fidelity anime visuals using half of
Scenimefy's parameters. It employs pseudo-paired data generated through
Scenimefy for supervised training, eliminating dependence on low-quality paired
data and improving the training process. Our comprehensive evaluation includes
ablation studies, qualitative, and quantitative analysis comparing NijiGAN to
similar models. The testing results demonstrate that NijiGAN produces
higher-quality images compared to AnimeGAN, as evidenced by a Mean Opinion
Score (MOS) of 2.192, it surpasses AnimeGAN's MOS of 2.160. Furthermore, our
model achieved a Frechet Inception Distance (FID) score of 58.71, outperforming
Scenimefy's FID score of 60.32. These results demonstrate that NijiGAN achieves
competitive performance against existing state-of-the-arts, especially
Scenimefy as the baseline model.",2024-12-27T04:46:44Z,http://arxiv.org/abs/2412.19455v1,"Kevin Putra Santoso, Anny Yuniarti, Dwiyasa Nakula, Dimas Prihady Setyawan, Adam Haidar Azizi, Jeany Aurellia P. Dewati, Farah Dhia Fadhila, Maria T. Elvara Bumbungan"
Significant circular Unruh effect at small acceleration,"We study the transition rates of an atom rotating in a circular orbit, which
is coupled with fluctuating electromagnetic fields in vacuum. We find that when
the rotational angular velocity exceeds the transition frequency of the atom,
the excitation rate can reach the same order of magnitude as the emission rate,
even with an extremely low centripetal acceleration resulting from a very small
orbital radius. For experimentally accessible centripetal accelerations, the
excitation rate of centripetally accelerated atoms can be up to ten to the
power of two hundred thousand times that of linearly accelerated atoms with the
same acceleration. Our result suggests that the circular version of the Unruh
effect can be significant even at very small centripetal accelerations,
contrary to the common belief that a large Unruh effect requires large
acceleration. This finding sheds new light on the experimental detection of the
circular Unruh effect.",2024-12-27T04:45:56Z,http://arxiv.org/abs/2412.19454v1,"Yuebing Zhou, Jiawei Hu, Hongwei Yu"
"Exponentially accurate open quantum simulation via randomized
  dissipation with minimal ancilla","Simulating open quantum systems is an essential technique for understanding
complex physical phenomena and advancing quantum technologies. Some quantum
algorithms for simulating Lindblad dynamics achieve logarithmically short
circuit depth in terms of accuracy $\varepsilon$ by coherently encoding all
possible jump processes with a large ancilla consumption. Minimizing the space
complexity while achieving such a logarithmic depth remains an important
challenge. In this work, we present a quantum algorithm for simulating general
Lindblad dynamics with multiple jump operators aimed at an observable
estimation, that achieves both a logarithmically short circuit depth and a
minimum ancilla size. Toward simulating an exponentially accurate Taylor
expansion of the Lindblad propagator to ensure the circuit depth of
$\mathcal{O} (\log(1/\varepsilon))$, we develop a novel random circuit
compilation method that leverages dissipative processes with only a single jump
operator; importantly, the proposed method requires the minimal-size, $4 +
\lceil \log M \rceil$, ancilla qubits where each single jump operator has at
most $M$ Pauli strings. This work represents a significant step towards making
open quantum system simulations more feasible on early fault-tolerant quantum
computing devices.",2024-12-27T04:43:19Z,http://arxiv.org/abs/2412.19453v1,"Jumpei Kato, Kaito Wada, Kosuke Ito, Naoki Yamamoto"
"Signatures of Core-Envelope Rotational Misalignment in the Mixed-Mode
  Asteroseismology of Kepler-56","Existing asteroseismic rotational measurements assume that stars rotate
around a single axis. However, tidal torques from misaligned companions, or
their possible engulfment, may bring the rotational axis of a star's envelope
out of alignment with its core, breaking azimuthal symmetry. I derive
perturbative expressions for asteroseismic signatures of such hitherto
unexamined rotational configurations, under the ``shellular approximation'' of
constant rotation rates on radially stratified mass shells. In the aligned
case, the distribution of power between multiplet components is determined by
the inclination of the rotational axis; radial differential misalignment causes
this to vary from multiplet to multiplet. I examine in particular detail the
phenomenology of gravitoacoustic mixed modes as seen in evolved sub- and red
giants, where near-resonance avoided crossings may break geometrical
degeneracies. Upon applying the revised asteroseismic observational methodology
that results from this theoretical discussion to revisit Kepler-56 -- a red
giant with a misaligned planetary system -- I find that its core and envelope
rotate around different rotational axes. While the rotational axis of its core
is indeed misaligned from the orbit normal of its transiting planets
(consistently with earlier studies), its envelope's rotational axis is close to
lying in the sky plane, and may well be aligned with them. More detailed
asteroseismic modelling, and spectroscopic follow-up, will be required to fully
elucidate the full spin-orbit geometry of the Kepler-56 system, and potentially
discriminate between hypotheses for how it formed.",2024-12-27T04:40:25Z,http://arxiv.org/abs/2412.19451v1,J. M. Joel Ong
"Find the Intention of Instruction: Comprehensive Evaluation of
  Instruction Understanding for Large Language Models","One of the key strengths of Large Language Models (LLMs) is their ability to
interact with humans by generating appropriate responses to given instructions.
This ability, known as instruction-following capability, has established a
foundation for the use of LLMs across various fields and serves as a crucial
metric for evaluating their performance. While numerous evaluation benchmarks
have been developed, most focus solely on clear and coherent instructions.
However, we have noted that LLMs can become easily distracted by
instruction-formatted statements, which may lead to an oversight of their
instruction comprehension skills. To address this issue, we introduce the
Intention of Instruction (IoInst) benchmark. This benchmark evaluates LLMs'
capacity to remain focused and understand instructions without being misled by
extraneous instructions. The primary objective of this benchmark is to identify
the appropriate instruction that accurately guides the generation of a given
context. Our findings suggest that even recently introduced state-of-the-art
models still lack instruction understanding capability. Along with the
proposition of IoInst in this study, we also present broad analyses of the
several strategies potentially applicable to IoInst.",2024-12-27T04:37:39Z,http://arxiv.org/abs/2412.19450v1,"Hyeonseok Moon, Jaehyung Seo, Seungyoon Lee, Chanjun Park, Heuiseok Lim"
The Cozero part of the pointfree version of $C_c (X)$,"Let $\mathcal C_{c}(L):= \{\alpha\in \mathcal{R}(L) \mid R_{\alpha} \, \text{
is a countable subset of } \, \mathbb R \}$, where $R_\alpha:=\{r\in\mathbb R
\mid {\mathrm{coz}}(\alpha-r)\neq\top\}$ for every $\alpha\in\mathcal R (L).$
By using idempotent elements, it is going to prove that
${{\mathrm{Coz}}}_c[L]:= \{{\mathrm{coz}}(\alpha) \mid \alpha\in\mathcal{C}_c
(L) \}$ is a $\sigma$-frame for every completely regular frame $L,$ and from
this, we conclude that it is regular, paracompact, perfectly normal and an
Alexandroff algebra frame such that each cover of it is shrinkable. Also, we
show that $L$ is a zero-dimensional frame if and only if $ L$ is a
$c$-completely regular frame.",2024-12-27T04:35:56Z,http://arxiv.org/abs/2412.19448v1,"Ali Akbar Estaji, Maryam Taha"
Gauge symmetry and partially Lagrangian systems,"We consider the classical field theory whose equations of motion follow from
the least action principle, but the class of admissible trajectories is
restricted by differential equations. The key element of the proposed
construction is the complete gauge symmetry of these additional equations . The
unfree variation of the trajectories reduces to the infinitesimal gauge
symmetry transformation of the equations restricting the trajectories. We
explicitly derive the equations that follow from the requirement that this
gauge variation of the action vanishes. The system of equations for conditional
extrema is not Lagrangian as such, but it admits an equivalent Hamiltonian
formulation with a non-canonical Poisson bracket. The bracket is degenerate, in
general. Alternatively, the equations restricting dynamics could be added to
the action with Lagrange multipliers with unrestricted variation of the
original variables. In this case, we would arrive at the Lagrangian equations
for original variables involving Lagrange multipliers and for Lagrange
multipliers themselves. In general, these two methods are not equivalent
because the multipliers can bring extra degrees of freedom compared to the case
of equations derived by unfree variation of the action. We illustrate the
general method with two examples. The first example is the particle in central
field with varying trajectories restricted by equation of conservation of
angular momentum. The phase space gets one more dimension, and there is an
extra conserved quantity $K$ which is responsible for precession of
trajectories. $K=0$ corresponds to the trajectories of usual Lagrangian
dynamics. The second example is the linearized gravity with Einstein-Hilbert
action and the class of varying fields is restricted by linearized Nordstr\""om
equation. This conditional extrema problem is shown to lead to the linearized
Cotton gravity equations.",2024-12-27T04:34:18Z,http://arxiv.org/abs/2412.19447v1,"Simon Lyakhovich, nikit Sinelnikov"
"Adrenaline: Adaptive Rendering Optimization System for Scalable Cloud
  Gaming","Cloud gaming requires a low-latency network connection, making it a prime
candidate for being hosted at the network edge. However, an edge server is
provisioned with a fixed compute capacity, causing an issue for multi-user
service and resulting in users having to wait before they can play when the
server is occupied. In this work, we present a new insight that when a user's
network condition results in use of lossy compression, the end-to-end visual
quality more degrades for frames of high rendering quality, wasting the
server's computing resources. We leverage this observation to build Adrenaline,
a new system which adaptively optimizes the game rendering qualities by
considering the user-side visual quality and server-side rendering cost. The
rendering quality optimization of Adrenaline is done via a scoring mechanism
quantifying the effectiveness of server resource usage on the user-side gaming
quality. Our open-sourced implementation of Adrenaline demonstrates easy
integration with modern game engines. In our evaluations, Adrenaline achieves
up to 24% higher service quality and 2x more users served with the same
resource footprint compared to other baselines.",2024-12-27T04:25:32Z,http://arxiv.org/abs/2412.19446v1,"Jin Heo, Ketan Bhardwaj, Ada Gavrilovska"
"Correspondence between quasinormal modes and grey-body factors for
  massive fields in Schwarzschild-de Sitter spacetime","Recently, a correspondence between quasinormal modes and grey-body factors of
black holes has been established. This correspondence is known to be exact in
the eikonal regime for a large class of asymptotically flat black holes and
approximate when the multipole number \( \ell \) is small. In this work, we
demonstrate that there exists a regime where the correspondence holds with
unprecedented accuracy even for the lowest multipole numbers: specifically, for
perturbations of massive fields in the background of asymptotically de Sitter
black holes, provided the field mass is not very small. We also fill the gap in
the existing literature via finding the grey-body factors of a massive scalar
field in the Schwarzschild- de Sitter background, when $\mu M/m_{P}$ is not
small.",2024-12-27T04:21:53Z,http://arxiv.org/abs/2412.19443v1,Zainab Malik
"A Survey on Large Language Model Acceleration based on KV Cache
  Management","Large Language Models (LLMs) have revolutionized a wide range of domains such
as natural language processing, computer vision, and multi-modal tasks due to
their ability to comprehend context and perform logical reasoning. However, the
computational and memory demands of LLMs, particularly during inference, pose
significant challenges when scaling them to real-world, long-context, and
real-time applications. Key-Value (KV) cache management has emerged as a
critical optimization technique for accelerating LLM inference by reducing
redundant computations and improving memory utilization. This survey provides a
comprehensive overview of KV cache management strategies for LLM acceleration,
categorizing them into token-level, model-level, and system-level
optimizations. Token-level strategies include KV cache selection, budget
allocation, merging, quantization, and low-rank decomposition, while
model-level optimizations focus on architectural innovations and attention
mechanisms to enhance KV reuse. System-level approaches address memory
management, scheduling, and hardware-aware designs to improve efficiency across
diverse computing environments. Additionally, the survey provides an overview
of both text and multimodal datasets and benchmarks used to evaluate these
strategies. By presenting detailed taxonomies and comparative analyses, this
work aims to offer useful insights for researchers and practitioners to support
the development of efficient and scalable KV cache management techniques,
contributing to the practical deployment of LLMs in real-world applications.
The curated paper list for KV cache management is in:
\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",2024-12-27T04:17:57Z,http://arxiv.org/abs/2412.19442v1,"Haoyang Li, Yiming Li, Anxin Tian, Tianhao Tang, Zhanchao Xu, Xuejia Chen, Nicole Hu, Wei Dong, Qing Li, Lei Chen"
"Low driving-force stable bending cooling via fatigue-resistant
  hierarchical NiTi shape memory alloy","Elastocaloric cooling with shape memory alloys (SMAs) is emerging as a
promising candidate for next-generation, environmentally friendly
refrigeration. However, its development is hindered by the large driving force
and low efficiency associated with uniaxial loading modes. In response, we
present an innovative elastocaloric air cooling approach that utilizes the
bending of NiTi beams, offering a low-force and energy-efficient solution. We
achieve a continuous maximum temperature drop of 0.91 K and a dissipated energy
of 15.1 N mm at a low specific driving force of 220 N. Notably, the specimen
achieves over 5 million cycles under a maximum surface tensile strain of 1.94%
for macroscopic cyclic bending, via a pre-strained, warm laser shock peening
(pw-LSP) method. This unprecedented fatigue resistance originates from the
formation of a hierarchical microstructure and a large compressive residual
stress of over 1 GPa. This work demonstrates the great potential of bending
induced elastocaloric cooling in the near future.",2024-12-27T04:13:06Z,http://arxiv.org/abs/2412.19440v1,"Kai Yan, Kangjie Chu, Peng Hua, Pengbo Wei, Hanlin Gu, Qiming Zhuang, Weifeng He, Fuzeng Ren, Qingping Sun, Robert O. Ritchie"
