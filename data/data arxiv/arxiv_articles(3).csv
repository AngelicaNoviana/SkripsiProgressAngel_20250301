Title,Summary,Published,Link,Authors
Entangled dual-comb spectroscopy,"Optical frequency combs have emerged as a cornerstone for a wide range of
areas, including spectroscopy, ranging, optical clocks, time and frequency
transfer, waveform synthesis, and communications. However, quantum mechanical
fluctuations of the optical carrier impose fundamental performance limits on
the precision of traditional classical laser frequency combs, particularly in
their use for interferometry and spectroscopy. Entanglement, as a
quintessential quantum resource, allows for surpassing the fundamental limits
of classical systems. Here, we introduce and experimentally demonstrate
entangled dual-comb spectroscopy (EDCS) that surmounts the fundamental limits
of classical DCS. EDCS builds on tailored entangled spectral structures of the
frequency combs, enabling simultaneous detection of all comb lines below the
standard quantum limit of classical DCS. Applying EDCS in gas detection, we
achieve a 2.6 dB enhancement in signal-to-noise ratio and a 1.7-fold reduction
in integration time over classical DCS, rendering EDCS particularly suited for
dynamic chemical and biological sensing, where fast, precise measurements
subject to power constraints are required. EDCS represents a new paradigm for
quantum frequency combs, underscoring their prospects in a plethora of
applications in precision metrology, spectroscopy, and timekeeping.",2024-12-27T18:57:59Z,http://arxiv.org/abs/2412.19800v1,"Abdulkarim Hariri, Shuai Liu, Haowei Shi, Quntao Zhuang, Xudong Fan, Zheshen Zhang"
"Generalized Grade-of-Membership Estimation for High-dimensional Locally
  Dependent Data","This work focuses on the mixed membership models for multivariate categorical
data widely used for analyzing survey responses and population genetics data.
These grade of membership (GoM) models offer rich modeling power but present
significant estimation challenges for high-dimensional polytomous data. Popular
existing approaches, such as Bayesian MCMC inference, are not scalable and lack
theoretical guarantees in high-dimensional settings. To address this, we first
observe that data from this model can be reformulated as a three-way
(quasi-)tensor, with many subjects responding to many items with varying
numbers of categories. We introduce a novel and simple approach that flattens
the three-way quasi-tensor into a ""fat"" matrix, and then perform a singular
value decomposition of it to estimate parameters by exploiting the singular
subspace geometry. Our fast spectral method can accommodate a broad range of
data distributions with arbitrarily locally dependent noise, which we formalize
as the generalized-GoM models. We establish finite-sample entrywise error
bounds for the generalized-GoM model parameters. This is supported by a new
sharp two-to-infinity singular subspace perturbation theory for locally
dependent and flexibly distributed noise, a contribution of independent
interest. Simulations and applications to data in political surveys, population
genetics, and single-cell sequencing demonstrate our method's superior
performance.",2024-12-27T18:51:15Z,http://arxiv.org/abs/2412.19796v1,"Ling Chen, Chengzhu Huang, Yuqi Gu"
"g-factor theory of Si/SiGe quantum dots: spin-valley and giant
  renormalization effects","Understanding the $g$-factor physics of Si/SiGe quantum dots is crucial for
realizing high-quality spin qubits. While previous work has explained some
aspects of $g$-factor physics in idealized geometries, the results do not
extend to general cases and they miss several important features. Here, we
construct a theory that gives $g$ in terms of readily computable matrix
elements, and can be applied to all Si/SiGe heterostructures of current
interest. As a concrete example, which currently has no $g$-factor
understanding, we study the so-called Wiggle Well structure, containing Ge
concentration oscillations inside the quantum well. Here we find a significant
renormalization of the $g$-factor compared to conventional Si/SiGe quantum
wells. We also uncover a giant $g$-factor suppression of order
$\mathcal{O}(1)$, which arises due to spin-valley coupling, and occurs at
locations of low valley splitting. Our work therefore opens up new avenues for
$g$-factor engineering in Si/SiGe quantum dots.",2024-12-27T18:50:38Z,http://arxiv.org/abs/2412.19795v1,"Benjamin D. Woods, Merritt P. Losert, Robert Joynt, Mark Friesen"
InfAlign: Inference-aware language model alignment,"Language model alignment has become a critical step in training modern
generative language models. The goal of alignment is to finetune a reference
model such that the win rate of a sample from the aligned model over a sample
from the reference model is high, subject to a KL divergence constraint. Today,
we are increasingly using inference-time algorithms (e.g., Best-of-N,
controlled decoding, tree search) to decode from language models rather than
standard sampling. However, the alignment objective does not capture such
inference-time decoding procedures. We show that the existing alignment
framework is sub-optimal in view of such inference-time methods. We then modify
the alignment objective and propose a framework for inference-aware alignment
(IAPO). We prove that for any inference-time decoding algorithm, the optimal
solution that optimizes the inference-time win rate of the aligned policy
against the reference policy is the solution to the typical RLHF problem with a
transformation of the reward. This motivates us to provide the KL-regularized
calibrate-and-transform RL (CTRL) algorithm to solve this problem, which
involves a reward calibration step and a KL-regularized reward maximization
step with a transformation of the calibrated reward. We particularize our study
to two important inference-time strategies: best-of-N sampling and best-of-N
jailbreaking, where N responses are sampled from the model and the one with the
highest or lowest reward is selected. We propose specific transformations for
these strategies and demonstrate that our framework offers significant
improvements over existing state-of-the-art methods for language model
alignment. Empirically, we outperform baselines that are designed without
taking inference-time decoding into consideration by 8-12% and 4-9% on
inference-time win rates over the Anthropic helpfulness and harmlessness dialog
benchmark datasets.",2024-12-27T18:45:36Z,http://arxiv.org/abs/2412.19792v1,"Ananth Balashankar, Ziteng Sun, Jonathan Berant, Jacob Eisenstein, Michael Collins, Adrian Hutter, Jong Lee, Chirag Nagpal, Flavien Prost, Aradhana Sinha, and Ananda Theertha Suresh, Ahmad Beirami"
"Data-driven analysis of anomalous transport and three-wave-coupling
  effects in E x B plasma discharges","Collisionless cross-field electron transport in an E x B configuration
relevant for electric propulsion is studied using data from a (z, {\theta})
full-PIC simulation. Higher-order spectral analysis shows that transport is
dominated by the in-phase interaction of the oscillations of the azimuthal
electric field and the electron density associated to the first electron
cyclotron drift instability (ECDI) mode. A secondary contribution emanates from
a lower-frequency mode, not predicted by linear ECDI theory, while higher modes
have a minor direct impact on transport. However, a bicoherence analysis
reveals that strong phase couplings exist among the ECDI modes, and a sparse
symbolic regression spectral model, based on the three-wave coupling equations,
suggests an inverse energy cascade as the most likely explanation, thus
suggesting that higher modes contribute indirectly to transport by quadratic
power transfer to the first mode. This work provides new insights into the
dynamics of anomalous plasma transport in E x B sources and the underlying
processes governing energy distribution across different scales, and supports
the validity of weak turbulence theory to examine their behavior.",2024-12-27T18:43:24Z,http://arxiv.org/abs/2412.19789v1,"Borja Bayón-Buján, Enrique Bello-Benítez, Jiewei Zhou, Mario Merino"
Can AI Help with Your Personal Finances?,"In recent years, Large Language Models (LLMs) have emerged as a
transformative development in artificial intelligence (AI), drawing significant
attention from industry and academia. Trained on vast datasets, these
sophisticated AI systems exhibit impressive natural language processing and
content generation capabilities. This paper explores the potential of LLMs to
address key challenges in personal finance, focusing on the United States. We
evaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini,
Anthropic's Claude, and Meta's Llama, to assess their effectiveness in
providing accurate financial advice on topics such as mortgages, taxes, loans,
and investments. Our findings show that while these models achieve an average
accuracy rate of approximately 70%, they also display notable limitations in
certain areas. Specifically, LLMs struggle to provide accurate responses for
complex financial queries, with performance varying significantly across
different topics. Despite these limitations, the analysis reveals notable
improvements in newer versions of these models, highlighting their growing
utility for individuals and financial advisors. As these AI systems continue to
evolve, their potential for advancing AI-driven applications in personal
finance becomes increasingly promising.",2024-12-27T18:25:27Z,http://arxiv.org/abs/2412.19784v1,"Oudom Hean, Utsha Saha, Binita Saha"
"Machine Learning for Sentiment Analysis of Imported Food in Trinidad and
  Tobago","This research investigates the performance of various machine learning
algorithms (CNN, LSTM, VADER, and RoBERTa) for sentiment analysis of Twitter
data related to imported food items in Trinidad and Tobago. The study addresses
three primary research questions: the comparative accuracy and efficiency of
the algorithms, the optimal configurations for each model, and the potential
applications of the optimized models in a live system for monitoring public
sentiment and its impact on the import bill. The dataset comprises tweets from
2018 to 2024, divided into imbalanced, balanced, and temporal subsets to assess
the impact of data balancing and the COVID-19 pandemic on sentiment trends. Ten
experiments were conducted to evaluate the models under various configurations.
Results indicated that VADER outperformed the other models in both multi-class
and binary sentiment classifications. The study highlights significant changes
in sentiment trends pre- and post-COVID-19, with implications for import
policies.",2024-12-27T18:25:08Z,http://arxiv.org/abs/2412.19781v1,"Cassandra Daniels, Koffka Khan"
Tensor Network Estimation of Distribution Algorithms,"Tensor networks are a tool first employed in the context of many-body quantum
physics that now have a wide range of uses across the computational sciences,
from numerical methods to machine learning. Methods integrating tensor networks
into evolutionary optimization algorithms have appeared in the recent
literature. In essence, these methods can be understood as replacing the
traditional crossover operation of a genetic algorithm with a tensor
network-based generative model. We investigate these methods from the point of
view that they are Estimation of Distribution Algorithms (EDAs). We find that
optimization performance of these methods is not related to the power of the
generative model in a straightforward way. Generative models that are better
(in the sense that they better model the distribution from which their training
data is drawn) do not necessarily result in better performance of the
optimization algorithm they form a part of. This raises the question of how
best to incorporate powerful generative models into optimization routines. In
light of this we find that adding an explicit mutation operator to the output
of the generative model often improves optimization performance.",2024-12-27T18:22:47Z,http://arxiv.org/abs/2412.19780v1,"John Gardiner, Javier Lopez-Piqueres"
"Symbolic Approximations to Ricci-flat Metrics Via Extrinsic Symmetries
  of Calabi-Yau Hypersurfaces","Ever since Yau's non-constructive existence proof of Ricci-flat metrics on
Calabi-Yau manifolds, finding their explicit construction remains a major
obstacle to development of both string theory and algebraic geometry. Recent
computational approaches employ machine learning to create novel neural
representations for approximating these metrics, offering high accuracy but
limited interpretability. In this paper, we analyse machine learning
approximations to flat metrics of Fermat Calabi-Yau n-folds and some of their
one-parameter deformations in three dimensions in order to discover their new
properties. We formalise cases in which the flat metric has more symmetries
than the underlying manifold, and prove that these symmetries imply that the
flat metric admits a surprisingly compact representation for certain choices of
complex structure moduli. We show that such symmetries uniquely determine the
flat metric on certain loci, for which we present an analytic form. We also
incorporate our theoretical results into neural networks to achieve
state-of-the-art reductions in Ricci curvature for multiple Calabi-Yau
manifolds. We conclude by distilling the ML models to obtain for the first time
closed form expressions for Kahler metrics with near-zero scalar curvature.",2024-12-27T18:19:26Z,http://arxiv.org/abs/2412.19778v1,"Viktor Mirjanić, Challenger Mishra"
"On the numerical solution of Lasserre relaxations of unconstrained
  binary quadratic optimization problem","The aim of this paper is to solve linear semidefinite programs arising from
higher-order Lasserre relaxations of unconstrained binary quadratic
optimization problems. For this we use an interior point method with a
preconditioned conjugate gradient method solving the linear systems. The
preconditioner utilizes the low-rank structure of the solution of the
relaxations. In order to fully exploit this, we need to re-write the moment
relaxations. To treat the arising linear equality constraints we use an
$\ell_1$-penalty approach within the interior-point solver. The efficiency of
this approach is demonstrated by numerical experiments with the MAXCUT and
other randomly generated problems and a comparison with a state-of-the-art
semidefinite solver and the ADMM method. We further propose a hybrid
ADMM-interior-point method that proves to be efficient for certain problem
classes. As a by-product, we observe that the second-order relaxation is often
high enough to deliver a globally optimal solution of the original problem.",2024-12-27T18:17:45Z,http://arxiv.org/abs/2412.19776v1,"Soodeh Habibi, Michal Kocvara, Michael Stingl"
"Analysis of Premature Death Rates in Texas Counties: The Impact of Air
  Quality, Socioeconomic Factors, and COPD Prevalence","Understanding factors contributing to premature mortality is critical for
public health planning. This study examines the relationships between premature
death rates and multiple risk factors across several Texas counties, utilizing
EPA air quality data, Census information, and county health records from recent
years. We analyze the impact of air quality (PM2.5 levels), socioeconomic
factors (median household income), and health conditions (COPD prevalence)
through statistical analysis and modeling techniques. Results reveal COPD
prevalence as a strong predictor of premature death rates, with higher
prevalence associated with a substantial increase in years of potential life
lost. While socioeconomic factors show a significant negative correlation, air
quality demonstrates more complex indirect relationships. These findings
emphasize the need for integrated public health interventions that prioritize
key health conditions while addressing underlying socioeconomic disparities.",2024-12-27T18:12:04Z,http://arxiv.org/abs/2412.19774v1,"Richard Rich, Ernesto Diaz"
"On the uplift of 4D wormholes in Braneworld models and their 5D
  structure","Recent developments in the consistent embedding of general 4D static and
spherically-symmetric spacetimes in arbitrary single-brane braneworld models
[Phys.Rev.D 109 (2024) 4, L041501] initiated the program of studying the bulk
structure of braneworld wormholes. In this article, adopting a completely
generic approach, we derive the general conditions that the metric functions of
any braneworld spacetime must satisfy to describe a wormhole structure in the
bulk. Particular emphasis is placed on clarifying the proper uplift of 4D
wormholes, expressed in terms of various radial coordinates on the brane, and
we demonstrate the important role of the circumferential radius metric function
for the embedding. Additionally, the flare-out conditions for braneworld
wormholes are presented for the first time and are found to differ from the
case of flat extra dimensions. To illustrate the method, we first perform the
uplift into the Randall-Sundrum II braneworld model for three well-known 4D
wormhole spacetimes; the effective braneworld wormhole solutions of
Casadio-Fabbri-Mazzacurati and Bronnikov-Kim, and the Simpson-Visser spacetime.
Subsequently, we study their bulk features by means of curvature invariants,
flare-out conditions, energy conditions and embedding diagrams. Our analysis
reveals that the assumption of a warped extra dimension has non-trivial
implications for the structure of 5D wormholes.",2024-12-27T18:12:03Z,http://arxiv.org/abs/2412.19773v1,"Thomas Pappas, Theodoros Nakas"
Direct estimates of irreversibility from time series,"The arrow of time can be quantified through the Kullback-Leibler divergence
($D_{KL}$) between the distributions of forward and reverse trajectories in a
system. Many approaches to estimate this rely on specific models, but the use
of incorrect models can introduce uncontrolled errors. Here, we describe a
model-free method that uses trajectory data directly to estimate the evidence
for irreversibility over finite windows of time. To do this we build on
previous work to identify and correct for errors that arise from limited sample
size. Importantly, our approach accurately recovers $D_{KL} = 0$ in systems
that adhere to detailed balance, and the correct nonzero $D_{KL}$ for data
generated by well understood models of nonequilibrium systems. We apply our
method to trajectories of neural activity in the retina as it responds to
naturalistic inputs, and find evidence of irreversibility in single neurons,
emphasizing the non-Markovian character of these data. These results open new
avenues for investigating how the brain represents the arrow of time.",2024-12-27T18:10:53Z,http://arxiv.org/abs/2412.19772v1,"Trevor GrandPre, Gianluca Teza, William Bialek"
Generative Video Propagation,"Large-scale video generation models have the inherent ability to
realistically model natural scenes. In this paper, we demonstrate that through
a careful design of a generative video propagation framework, various video
tasks can be addressed in a unified way by leveraging the generative power of
such models. Specifically, our framework, GenProp, encodes the original video
with a selective content encoder and propagates the changes made to the first
frame using an image-to-video generation model. We propose a data generation
scheme to cover multiple video tasks based on instance-level video segmentation
datasets. Our model is trained by incorporating a mask prediction decoder head
and optimizing a region-aware loss to aid the encoder to preserve the original
content while the generation model propagates the modified region. This novel
design opens up new possibilities: In editing scenarios, GenProp allows
substantial changes to an object's shape; for insertion, the inserted objects
can exhibit independent motion; for removal, GenProp effectively removes
effects like shadows and reflections from the whole video; for tracking,
GenProp is capable of tracking objects and their associated effects together.
Experiment results demonstrate the leading performance of our model in various
video tasks, and we further provide in-depth analyses of the proposed
framework.",2024-12-27T17:42:29Z,http://arxiv.org/abs/2412.19761v1,"Shaoteng Liu, Tianyu Wang, Jui-Hsien Wang, Qing Liu, Zhifei Zhang, Joon-Young Lee, Yijun Li, Bei Yu, Zhe Lin, Soo Ye Kim, Jiaya Jia"
"Enhancing Cognitive Diagnosis by Modeling Learner Cognitive Structure
  State","Cognitive diagnosis represents a fundamental research area within intelligent
education, with the objective of measuring the cognitive status of individuals.
Theoretically, an individual's cognitive state is essentially equivalent to
their cognitive structure state. Cognitive structure state comprises two key
components: knowledge state (KS) and knowledge structure state (KUS). The
knowledge state reflects the learner's mastery of individual concepts, a widely
studied focus within cognitive diagnosis. In contrast, the knowledge structure
state-representing the learner's understanding of the relationships between
concepts-remains inadequately modeled. A learner's cognitive structure is
essential for promoting meaningful learning and shaping academic performance.
Although various methods have been proposed, most focus on assessing KS and
fail to assess KUS. To bridge this gap, we propose an innovative and effective
framework-CSCD (Cognitive Structure State-based Cognitive Diagnosis)-which
introduces a novel framework to modeling learners' cognitive structures in
diagnostic assessments, thereby offering new insights into cognitive structure
modeling. Specifically, we employ an edge-feature-based graph attention network
to represent the learner's cognitive structure state, effectively integrating
KS and KUS. Extensive experiments conducted on real datasets demonstrate the
superior performance of this framework in terms of diagnostic accuracy and
interpretability.",2024-12-27T17:41:39Z,http://arxiv.org/abs/2412.19759v1,"Zhifu Chen, Hengnian Gu, Jin Peng Zhou, Dongdai Zhou"
"""Did my figure do justice to the answer?"" : Towards Multimodal Short
  Answer Grading with Feedback (MMSAF)","Personalized feedback plays a vital role in a student's learning process.
While existing systems are adept at providing feedback over MCQ-based
evaluation, this work focuses more on subjective and open-ended questions,
which is similar to the problem of Automatic Short Answer Grading (ASAG) with
feedback. Additionally, we introduce the Multimodal Short Answer grading with
Feedback (MMSAF) problem over the traditional ASAG feedback problem to address
the scenario where the student answer and reference answer might contain
images. Moreover, we introduce the MMSAF dataset with 2197 data points along
with an automated framework for generating such data sets. Our evaluations on
existing LLMs over this dataset achieved an overall accuracy of 55\% on Level
of Correctness labels, 75\% on Image Relevance labels and a score of 4.27 out
of 5 in correctness level of LLM generated feedback as rated by experts. As per
experts, Pixtral achieved a rating of above 4 out of all metrics, indicating
that it is more aligned to human judgement, and that it is the best solution
for assisting students.",2024-12-27T17:33:39Z,http://arxiv.org/abs/2412.19755v1,"Pritam Sil, Bhaskaran Raman, Pushpak Bhattacharyya"
Complement or substitute? How AI increases the demand for human skills,"The question of whether AI substitutes or complements human work is central
to debates on the future of work. This paper examines the impact of AI on skill
demand and compensation in the U.S. economy, analysing 12 million online job
vacancies from 2018 to 2023. It investigates internal effects (within-job
substitution and complementation) and external effects (across occupations,
industries, and regions). Our findings reveal a significant increase in demand
for AI-complementary skills, such as digital literacy, teamwork, and
resilience, alongside rising wage premiums for these skills in AI roles like
Data Scientist. Conversely, substitute skills, including customer service and
text review, have declined in both demand and value within AI-related
positions. Examining external effects, we find a notable rise in demand for
complementary skills in non-AI roles linked to the growth of AI-related jobs in
specific industries or regions. At the same time, there is a moderate decline
in non-AI roles requiring substitute skills. Overall, AI's complementary effect
is up to 50% larger than its substitution effect, resulting in net positive
demand for skills. These results, replicated for the UK and Australia,
highlight AI's transformative impact on workforce skill requirements. They
suggest reskilling efforts should prioritise not only technical AI skills but
also complementary skills like ethics and digital literacy.",2024-12-27T17:26:30Z,http://arxiv.org/abs/2412.19754v1,"Elina Mäkelä, Fabian Stephany"
"IMAGINE: An 8-to-1b 22nm FD-SOI Compute-In-Memory CNN Accelerator With
  an End-to-End Analog Charge-Based 0.15-8POPS/W Macro Featuring
  Distribution-Aware Data Reshaping","Charge-domain compute-in-memory (CIM) SRAMs have recently become an enticing
compromise between computing efficiency and accuracy to process sub-8b
convolutional neural networks (CNNs) at the edge. Yet, they commonly make use
of a fixed dot-product (DP) voltage swing, which leads to a loss in effective
ADC bits due to data-dependent clipping or truncation effects that waste
precious conversion energy and computing accuracy. To overcome this, we present
IMAGINE, a workload-adaptive 1-to-8b CIM-CNN accelerator in 22nm FD-SOI. It
introduces a 1152x256 end-to-end charge-based macro with a multi-bit DP based
on an input-serial, weight-parallel accumulation that avoids power-hungry DACs.
An adaptive swing is achieved by combining a channel-wise DP array split with a
linear in-ADC implementation of analog batch-normalization (ABN), obtaining a
distribution-aware data reshaping. Critical design constraints are relaxed by
including the post-silicon equivalent noise within a CIM-aware CNN training
framework. Measurement results showcase an 8b system-level energy efficiency of
40TOPS/W at 0.3/0.6V, with competitive accuracies on MNIST and CIFAR-10.
Moreover, the peak energy and area efficiencies of the 187kB/mm2 macro
respectively reach up to 0.15-8POPS/W and 2.6-154TOPS/mm2, scaling with the
8-to-1b computing precision. These results exceed previous charge-based designs
by 3-to-5x while being the first work to provide linear in-memory rescaling.",2024-12-27T17:18:15Z,http://arxiv.org/abs/2412.19750v1,"Adrian Kneip, Martin Lefebvre, Pol Maistriaux, David Bol"
"Enhancing Adversarial Robustness of Deep Neural Networks Through
  Supervised Contrastive Learning","Adversarial attacks exploit the vulnerabilities of convolutional neural
networks by introducing imperceptible perturbations that lead to
misclassifications, exposing weaknesses in feature representations and decision
boundaries. This paper presents a novel framework combining supervised
contrastive learning and margin-based contrastive loss to enhance adversarial
robustness. Supervised contrastive learning improves the structure of the
feature space by clustering embeddings of samples within the same class and
separating those from different classes. Margin-based contrastive loss,
inspired by support vector machines, enforces explicit constraints to create
robust decision boundaries with well-defined margins. Experiments on the
CIFAR-100 dataset with a ResNet-18 backbone demonstrate robustness performance
improvements in adversarial accuracy under Fast Gradient Sign Method attacks.",2024-12-27T17:14:52Z,http://arxiv.org/abs/2412.19747v1,"Longwei Wang, Navid Nayyem, Abdullah Rakin"
"AAM-SEALS: Developing Aerial-Aquatic Manipulators in SEa, Air, and Land
  Simulator","Current simulators lack the ability to accurately model integrated
environments that encompass sea, air, and land. To address this gap, we
introduce Aerial-Aquatic Manipulators (AAMs) in SEa, Air, and Land Simulator
(SEALS), a comprehensive and photorealistic simulator designed for AAMs to
operate and learn in these diverse environments. The development of AAM-SEALS
tackles several significant challenges, including the creation of integrated
controllers for flying, swimming, and manipulation, and the high-fidelity
simulation of aerial dynamics and hydrodynamics leveraging particle physics.
Our evaluation demonstrates smooth operation and photorealistic transitions
across air, water, and their interfaces. We quantitatively validate the
fidelity of particle-based hydrodynamics by comparing position-tracking errors
across real-world and simulated systems. AAM-SEALS promises to benefit a broad
range of robotics communities, including robot learning, aerial robotics,
underwater robotics, mobile manipulation, and robotic simulators. We will
open-source our code and data to foster the advancement of research in these
fields. Please access our project website at: https:
//aam-seals.github.io/aam-seals-v1/",2024-12-27T17:13:14Z,http://arxiv.org/abs/2412.19744v1,"William Wang Yang, Karthikeya Kona, Yashveer Jain, Abhinav Bhamidipati, Tomer Atzili, Xiaomin Lin, Yantian Zha"
"Physics of 2D magnets and magnetic thin films: Surface structure and
  surface phase transition, criticality and skyrmions","Recently, there is an increasing renewed interest in 2D magnetism such as Van
der Waals magnets. The physics of 2D magnetism and ultra-thin magnetic films
has a long history. This chapter is a review devoted to some fundamental
theoretical properties of 2D magnets and and magnetic thin films including
frustrated systems and topological spin textures. These properties allow to
understand macroscopic behaviors experimentally observed in thin films and
superlattices where the surface and the interface play a crucial role. The
chapter begins with a review on 2D magnets, their spin structures and phase
transitions. Next, the case of thin films is considered. The theory of surface
spin waves is discussed in various situations with and without surface
reconstruction of spin ordering. Various interactions are taken into account:
surface interaction different from the bulk one, competing interactions,
Dzyaloshinskii-Moriya interaction. Surface phase transitions are shown in some
particularly striking cases. Finally, some cases of topological spin textures
called ""skyrmions"" are reviewed. All the results shown in this chapter have
been published in various research papers cited in the text. Therefore, we will
discuss some important results but avoid to enter complicated methods. Instead,
the reader is referred to original papers for detailed demonstrations.",2024-12-27T17:06:31Z,http://arxiv.org/abs/2412.19741v1,Hung T. Diep
Hard Photon Triggered Jets in $p$-$p$ and $A$-$A$ Collisions,"An investigation of high transverse momentum (high-$p_T$) photon triggered
jets in proton-proton ($p$-$p$) and ion-ion ($A$-$A$) collisions at
$\sqrt{s_{NN}} = 0.2$ and $5.02~\mathrm{TeV}$ is carried out, using the
multistage description of in-medium jet evolution. Monte Carlo simulations of
hard scattering and energy loss in heavy-ion collisions are performed using
parameters tuned in a previous study of the nuclear modification factor
($R_{AA}$) for inclusive jets and high-$p_T$ hadrons. We obtain a good
reproduction of the experimental data for photon triggered jet $R_{AA}$, as
measured by the ATLAS detector, the distribution of the ratio of jet to photon
$p_T$ ($X_{\rm J \gamma}$), measured by both CMS and ATLAS, and the photon-jet
azimuthal correlation as measured by CMS. We obtain a moderate description of
the photon triggered jet $I_{AA}$, as measured by STAR. A noticeable
improvement in the comparison is observed when one goes beyond prompt photons
and includes bremsstrahlung and decay photons, revealing their significance in
certain kinematic regions, particularly at $X_{J\gamma} &gt; 1$. Moreover,
azimuthal angle correlations demonstrate a notable impact of non-prompt photons
on the distribution, emphasizing their role in accurately describing
experimental results. This work highlights the success of the multistage model
of jet modification to straightforwardly predict (this set of) photon triggered
jet observables. This comparison, along with the role played by non-prompt
photons, has important consequences on the inclusion of such observables in a
future Bayesian analysis.",2024-12-27T16:56:15Z,http://arxiv.org/abs/2412.19738v1,"C. Sirimanna, Y. Tachibana, A. Majumder, A. Angerami, R. Arora, S. A. Bass, Y. Chen, R. Datta, L. Du, R. Ehlers, H. Elfner, R. J. Fries, C. Gale, Y. He, B. V. Jacak, P. M. Jacobs, S. Jeon, Y. Ji, F. Jonas, L. Kasper, M. Kordell II, A. Kumar, R. Kunnawalkam-Elayavalli, J. Latessa, Y. -J. Lee, R. Lemmon, M. Luzum, S. Mak, A. Mankolli, C. Martin, H. Mehryar, T. Mengel, C. Nattrass, J. Norman, C. Parker, J. -F. Paquet, J. H. Putschke, H. Roch, G. Roland, B. Schenke, L. Schwiebert, A. Sengupta, C. Shen, M. Singh, D. Soeder, R. A. Soltz, I. Soudi, J. Velkovska, G. Vujanovic, X. -N. Wang, X. Wu, W. Zhao"
"Adaptive Context-Aware Multi-Path Transmission Control for VR/AR
  Content: A Deep Reinforcement Learning Approach","This paper introduces the Adaptive Context-Aware Multi-Path Transmission
Control Protocol (ACMPTCP), an efficient approach designed to optimize the
performance of Multi-Path Transmission Control Protocol (MPTCP) for
data-intensive applications such as augmented and virtual reality (AR/VR)
streaming. ACMPTCP addresses the limitations of conventional MPTCP by
leveraging deep reinforcement learning (DRL) for agile end-to-end path
management and optimal bandwidth allocation, facilitating path realignment
across diverse network environments.",2024-12-27T16:56:12Z,http://arxiv.org/abs/2412.19737v1,"Shakil Ahmed, Saifur Rahman Sabuj, Ashfaq Khokhar"
"A General Framework of Brain Region Detection And Genetic Variants
  Selection in Imaging Genetics","Imaging genetics is a growing field that employs structural or functional
neuroimaging techniques to study individuals with genetic risk variants
potentially linked to specific illnesses. This area presents considerable
challenges to statisticians due to the heterogeneous information and different
data forms it involves. In addition, both imaging and genetic data are
typically high-dimensional, creating a ""big data squared"" problem. Moreover,
brain imaging data contains extensive spatial information. Simply vectorizing
tensor images and treating voxels as independent features can lead to
computational issues and disregard spatial structure. This paper presents a
novel statistical method for imaging genetics modeling while addressing all
these challenges. We explore a Canonical Correlation Analysis based linear
model for the joint modeling of brain imaging, genetic information, and
clinical phenotype, enabling the simultaneous detection of significant brain
regions and selection of important genetic variants associated with the
phenotype outcome. Scalable algorithms are developed to tackle the ""big data
squared"" issue. We apply the proposed method to explore the reaction speed, an
indicator of cognitive functions, and its associations with brain MRI and
genetic factors using the UK Biobank database. Our study reveals a notable
connection between the caudate nucleus region of brain and specific significant
SNPs, along with their respective regulated genes, and the reaction speed.",2024-12-27T16:54:11Z,http://arxiv.org/abs/2412.19735v1,"Siqiang Su, Zhenghao Li, Long Feng, Ting Li"
"Dynamics, data and reconstruction","Data-driven learning is prevalent in many fields of science, mathematics and
engineering. The goal of data-driven learning of dynamical systems is to
interpret timeseries as a continuous observation of an underlying dynamical
system. This task is not well-posed for a variety of reasons. A dynamical
system may have multiple sub-systems co-existing within it. The nature of the
dataset depends on the portion of the phase space being viewed, and may thus my
confined to a sub-system. Secondly these sub-systems may be topologically
inter-weaved, so may be inseparable computationally. Thirdly, two timeseries
sampled separately from different dynamical systems may be close or even
indistinguishable. So there is no unqiue source for the timeseries. We show how
these ambiguities are circumvented if one considers dynamical systems and
measurement maps collectively. This is made possible in a category theoretical
framework, in which reconstruction is unique up to equivalences. We introduce
two categories of observed dynamical systems and timeseries-data. These are
related to the well known category of dynamical systems via functors. This
enables a functorial interpretation of the task of reconstruction as well.",2024-12-27T16:49:52Z,http://arxiv.org/abs/2412.19734v1,"Suddhasattwa Das, Tomoharu Suda"
"Generative Pretrained Embedding and Hierarchical Irregular Time Series
  Representation for Daily Living Activity Recognition","Within the evolving landscape of smart homes, the precise recognition of
daily living activities using ambient sensor data stands paramount. This paper
not only aims to bolster existing algorithms by evaluating two distinct
pretrained embeddings suited for ambient sensor activations but also introduces
a novel hierarchical architecture. We delve into an architecture anchored on
Transformer Decoder-based pre-trained embeddings, reminiscent of the GPT
design, and contrast it with the previously established state-of-the-art (SOTA)
ELMo embeddings for ambient sensors. Our proposed hierarchical structure
leverages the strengths of each pre-trained embedding, enabling the discernment
of activity dependencies and sequence order, thereby enhancing classification
precision. To further refine recognition, we incorporate into our proposed
architecture an hour-of-the-day embedding. Empirical evaluations underscore the
preeminence of the Transformer Decoder embedding in classification endeavors.
Additionally, our innovative hierarchical design significantly bolsters the
efficacy of both pre-trained embeddings, notably in capturing inter-activity
nuances. The integration of temporal aspects subtly but distinctively augments
classification, especially for time-sensitive activities. In conclusion, our
GPT-inspired hierarchical approach, infused with temporal insights, outshines
the SOTA ELMo benchmark.",2024-12-27T16:43:52Z,http://arxiv.org/abs/2412.19732v1,"Damien Bouchabou, Sao Mai Nguyen"
"Learning to Forget: Bayesian Time Series Forecasting using Recurrent
  Sparse Spectrum Signature Gaussian Processes","The signature kernel is a kernel between time series of arbitrary length and
comes with strong theoretical guarantees from stochastic analysis. It has found
applications in machine learning such as covariance functions for Gaussian
processes. A strength of the underlying signature features is that they provide
a structured global description of a time series. However, this property can
quickly become a curse when local information is essential and forgetting is
required; so far this has only been addressed with ad-hoc methods such as
slicing the time series into subsegments. To overcome this, we propose a
principled, data-driven approach by introducing a novel forgetting mechanism
for signatures. This allows the model to dynamically adapt its context length
to focus on more recent information. To achieve this, we revisit the recently
introduced Random Fourier Signature Features, and develop Random Fourier
Decayed Signature Features (RFDSF) with Gaussian processes (GPs). This results
in a Bayesian time series forecasting algorithm with variational inference,
that offers a scalable probabilistic algorithm that processes and transforms a
time series into a joint predictive distribution over time steps in one pass
using recurrence. For example, processing a sequence of length $10^4$ steps in
$\approx 10^{-2}$ seconds and in $&lt; 1\text{GB}$ of GPU memory. We demonstrate
that it outperforms other GP-based alternatives and competes with
state-of-the-art probabilistic time series forecasting algorithms.",2024-12-27T16:31:09Z,http://arxiv.org/abs/2412.19727v1,"Csaba Tóth, Masaki Adachi, Michael A. Osborne, Harald Oberhauser"
EEG-Reptile: An Automatized Reptile-Based Meta-Learning Library for BCIs,"Meta-learning, i.e., ""learning to learn"", is a promising approach to enable
efficient BCI classifier training with limited amounts of data. It can
effectively use collections of in some way similar classification tasks, with
rapid adaptation to new tasks where only minimal data are available. However,
applying meta-learning to existing classifiers and BCI tasks requires
significant effort. To address this issue, we propose EEG-Reptile, an automated
library that leverages meta-learning to improve classification accuracy of
neural networks in BCIs and other EEG-based applications. It utilizes the
Reptile meta-learning algorithm to adapt neural network classifiers of EEG data
to the inter-subject domain, allowing for more efficient fine-tuning for a new
subject on a small amount of data. The proposed library incorporates an
automated hyperparameter tuning module, a data management pipeline, and an
implementation of the Reptile meta-learning algorithm. EEG-Reptile automation
level allows using it without deep understanding of meta-learning. We
demonstrate the effectiveness of EEG-Reptile on two benchmark datasets (BCI IV
2a, Lee2019 MI) and three neural network architectures (EEGNet, FBCNet,
EEG-Inception). Our library achieved improvement in both zero-shot and few-shot
learning scenarios compared to traditional transfer learning approaches.",2024-12-27T16:24:31Z,http://arxiv.org/abs/2412.19725v1,"Daniil A. Berdyshev, Artem M. Grachev, Sergei L. Shishkin, Bogdan L. Kozyrskiy"
"Exploring low-rank structure for an inverse scattering problem with
  far-field data","The inverse scattering problem exhibits an inherent low-rank structure due to
its ill-posed nature; however developing low-rank structures for the inverse
scattering problem remains challenging. In this work, we introduce a novel
low-rank structure tailored for solving the inverse scattering problem. The
particular low-rank structure is given by the generalized prolate spheroidal
wave functions, computed stably and accurately via a Sturm-Liouville problem.
We first process the far-field data to obtain a post-processed data set within
a disk domain. Subsequently, the post-processed data are projected onto a
low-rank space given by the low-rank structure. The unknown is approximately
solved in this low-rank space, by dropping higher-order terms. The low-rank
structure leads to a H\""{o}lder-logarithmic type stability estimate for
arbitrary unknown functions, and a Lipschitz stability estimate for unknowns
belonging to a finite dimensional low-rank space. Various numerical experiments
are conducted to validate its performance, encompassing assessments of
resolution capability, robustness against randomly added noise and modeling
errors, and demonstration of increasing stability.",2024-12-27T16:24:20Z,http://arxiv.org/abs/2412.19724v1,"Yuyuan Zhou, Lorenzo Audibert, Shixu Meng, Bo Zhang"
"OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse
  Task Synthesis","Graphical User Interface (GUI) agents powered by Vision-Language Models
(VLMs) have demonstrated human-like computer control capability. Despite their
utility in advancing digital automation, a critical bottleneck persists:
collecting high-quality trajectory data for training. Common practices for
collecting such data rely on human supervision or synthetic data generation
through executing pre-defined tasks, which are either resource-intensive or
unable to guarantee data quality. Moreover, these methods suffer from limited
data diversity and significant gaps between synthetic data and real-world
environments. To address these challenges, we propose OS-Genesis, a novel GUI
data synthesis pipeline that reverses the conventional trajectory collection
process. Instead of relying on pre-defined tasks, OS-Genesis enables agents
first to perceive environments and perform step-wise interactions, then
retrospectively derive high-quality tasks to enable trajectory-level
exploration. A trajectory reward model is then employed to ensure the quality
of the generated trajectories. We demonstrate that training GUI agents with
OS-Genesis significantly improves their performance on highly challenging
online benchmarks. In-depth analysis further validates OS-Genesis's efficiency
and its superior data quality and diversity compared to existing synthesis
methods. Our codes, data, and checkpoints are available at
\href{https://qiushisun.github.io/OS-Genesis-Home/}{OS-Genesis Homepage}.",2024-12-27T16:21:58Z,http://arxiv.org/abs/2412.19723v1,"Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, Ben Kao, Guohao Li, Junxian He, Yu Qiao, Zhiyong Wu"
"Quantum correlations in a gravitational collapse simulation with
  SpheriCo.jl","We report on work using a newly developed code, SpheriCo.jl, that computes
the gravitational collapse of a spherical scalar field, where the scalar can be
either a classical field, or a quantum field operator. By utilising
summation-by-parts methods for the numerical derivatives we are able to
simulate the collapse longer than was possible previously due to enhanced
numerical stability. We present a suite of tests for the code that tests its
accuracy and stability, both for the classical and quantum fields. We are able
to observe critical behavior of gravitational collapse for the classical setup,
in agreement with expected results. The code is also used to compute two-point
correlation functions, with results that hint at a non-trivial correlation
across the horizon of Hawking quanta.",2024-12-27T16:20:27Z,http://arxiv.org/abs/2412.19722v1,"Benjamin Berczi, Magdalena Eriksson, Thanasis Giannakopoulos, Paul M. Saffin"
Sharpening Neural Implicit Functions with Frequency Consolidation Priors,"Signed Distance Functions (SDFs) are vital implicit representations to
represent high fidelity 3D surfaces. Current methods mainly leverage a neural
network to learn an SDF from various supervisions including signed distances,
3D point clouds, or multi-view images. However, due to various reasons
including the bias of neural network on low frequency content, 3D unaware
sampling, sparsity in point clouds, or low resolutions of images, neural
implicit representations still struggle to represent geometries with high
frequency components like sharp structures, especially for the ones learned
from images or point clouds. To overcome this challenge, we introduce a method
to sharpen a low frequency SDF observation by recovering its high frequency
components, pursuing a sharper and more complete surface. Our key idea is to
learn a mapping from a low frequency observation to a full frequency coverage
in a data-driven manner, leading to a prior knowledge of shape consolidation in
the frequency domain, dubbed frequency consolidation priors. To better
generalize a learned prior to unseen shapes, we introduce to represent
frequency components as embeddings and disentangle the embedding of the low
frequency component from the embedding of the full frequency component. This
disentanglement allows the prior to generalize on an unseen low frequency
observation by simply recovering its full frequency embedding through a
test-time self-reconstruction. Our evaluations under widely used benchmarks or
real scenes show that our method can recover high frequency component and
produce more accurate surfaces than the latest methods. The code, data, and
pre-trained models are available at \url{https://github.com/chenchao15/FCP}.",2024-12-27T16:18:46Z,http://arxiv.org/abs/2412.19720v1,"Chao Chen, Yu-Shen Liu, Zhizhong Han"
"Trading Off Energy Storage and Payload -- An Analytical Model for
  Freight Train Configuration","To support planning of alternative fuel technology (e.g., battery-electric
locomotives) deployment for decarbonizing non-electrified freight rail, we
develop a convex optimization formulation with a closed-form solution to
determine the optimal number of energy storage tender cars in a train. The
formulation shares a similar structure to an Economic Order Quantity (EOQ)
model. For given market characteristics, cost forecasts, and technology
parameters, our model captures the trade-offs between inventory carrying costs
associated with trip times (including delays due to charging/refueling) and
ordering costs associated with train dispatch and operation (energy, amortized
equipment, and labor costs). To illustrate the framework, we find the optimal
number of battery-electric energy tender cars in 22,501 freight markets
(origin-destination pairs and commodities) for U.S. Class I railroads. The
results display heterogeneity in optimal configurations with lighter, yet more
time-sensitive shipments (e.g., intermodal) utilizing more battery tender cars.
For heavier commodities (e.g., coal) with lower holding costs, single battery
tender car configurations are generally optimal. The results also show that the
optimal train configurations are sensitive to delays associated with recharging
or swapping tender cars.",2024-12-27T16:18:35Z,http://arxiv.org/abs/2412.19719v1,"Max T. M. Ng, Adrian Hernandez, Pablo L. Durango-Cohen, Hani S. Mahmassani"
"Text2Insight: Transform natural language text into insights seamlessly
  using multi-model architecture","The growing demand for dynamic, user-centric data analysis and visualization
is evident across domains like healthcare, finance, and research. Traditional
visualization tools often fail to meet individual user needs due to their
static and predefined nature. To address this gap, Text2Insight is introduced
as an innovative solution that delivers customized data analysis and
visualizations based on user-defined natural language requirements. Leveraging
a multi-model architecture, Text2Insight transforms user inputs into actionable
insights and dynamic visualizations.
  The methodology begins with analyzing the input dataset to extract structural
details such as columns and values. A pre-trained Llama3 model converts the
user's natural language query into an SQL query, which is further refined using
a Named Entity Recognition (NER) model for accuracy. A chart predictor
determines the most suitable visualization type, while the Llama3 model
generates insights based on the SQL query's results. The output is a
user-friendly and visually informative chart. To enhance analysis capabilities,
the system integrates a question-answering model and a predictive model using
the BERT framework. These models provide insights into historical data and
predict future trends.
  Performance evaluation of Text2Insight demonstrates its effectiveness,
achieving high accuracy (99%), precision (100%), recall (99%), and F1-score
(99%), with a BLEU score of 0.5. The question-answering model attained an
accuracy of 89% and the predictive model achieved 70% accuracy. These results
validate Text2Insight as a robust and viable solution for transforming natural
language text into dynamic, user-specific data analysis and visualizations.",2024-12-27T16:17:22Z,http://arxiv.org/abs/2412.19718v1,Pradeep Sain
Low-Regularity Global solution for fractional NLS in modulation spaces,"We establish global well-posedness for the mass sub-critical nonlinear
fractional Schr\""odinger equation
  $$iu_t + (-\Delta)^\frac{\beta}{2} u \pm (|u|^{\alpha}u)=0$$
  with radial initial data in modulation spaces $M^{p,\frac{p}{p-1}}(\mathbb
R^n)$ with $2&lt;p$ sufficiently close to $2.$ Our order of dispersion $\beta$
lies in $(2n/ (2n-1), 2)$ for $n \geq 2$.",2024-12-27T16:15:10Z,http://arxiv.org/abs/2412.19714v1,"Divyang G. Bhimani, Diksha Dhingra, Vijay Kumar Sohani"
"ProKAN: Progressive Stacking of Kolmogorov-Arnold Networks for Efficient
  Liver Segmentation","The growing need for accurate and efficient 3D identification of tumors,
particularly in liver segmentation, has spurred considerable research into deep
learning models. While many existing architectures offer strong performance,
they often face challenges such as overfitting and excessive computational
costs. An adjustable and flexible architecture that strikes a balance between
time efficiency and model complexity remains an unmet requirement. In this
paper, we introduce proKAN, a progressive stacking methodology for
Kolmogorov-Arnold Networks (KANs) designed to address these challenges. Unlike
traditional architectures, proKAN dynamically adjusts its complexity by
progressively adding KAN blocks during training, based on overfitting behavior.
This approach allows the network to stop growing when overfitting is detected,
preventing unnecessary computational overhead while maintaining high accuracy.
Additionally, proKAN utilizes KAN's learnable activation functions modeled
through B-splines, which provide enhanced flexibility in learning complex
relationships in 3D medical data. Our proposed architecture achieves
state-of-the-art performance in liver segmentation tasks, outperforming
standard Multi-Layer Perceptrons (MLPs) and fixed KAN architectures. The
dynamic nature of proKAN ensures efficient training times and high accuracy
without the risk of overfitting. Furthermore, proKAN provides better
interpretability by allowing insight into the decision-making process through
its learnable coefficients. The experimental results demonstrate a significant
improvement in accuracy, Dice score, and time efficiency, making proKAN a
compelling solution for 3D medical image segmentation tasks.",2024-12-27T16:14:06Z,http://arxiv.org/abs/2412.19713v1,"Bhavesh Gyanchandani, Aditya Oza, Abhinav Roy"
"Causal machine learning for heterogeneous treatment effects in the
  presence of missing outcome data","When estimating heterogeneous treatment effects, missing outcome data can
complicate treatment effect estimation, causing certain subgroups of the
population to be poorly represented. In this work, we discuss this commonly
overlooked problem and consider the impact that missing at random (MAR) outcome
data has on causal machine learning estimators for the conditional average
treatment effect (CATE). We then propose two de-biased machine learning
estimators for the CATE, the mDR-learner and mEP-learner, which address the
issue of under-representation by integrating inverse probability of censoring
weights into the DR-learner and EP-learner respectively. We show that under
reasonable conditions, these estimators are oracle efficient, and illustrate
their favorable performance through simulated data settings, comparing them to
existing CATE estimators, including comparison to estimators which use common
missing data techniques. Guidance on the implementation of these estimators is
provided and we present an example of their application using the ACTG175
trial, exploring treatment effect heterogeneity when comparing Zidovudine
mono-therapy against alternative antiretroviral therapies among HIV-1-infected
individuals.",2024-12-27T16:10:03Z,http://arxiv.org/abs/2412.19711v1,"Matthew Pryce, Karla Diaz-Ordaz, Ruth H. Keogh, Stijn Vansteelandt"
"Schwinger pair production in spacetime fields: Moiré patterns,
  Aharonov-Bohm phases and Sturm-Liouville eigenvalues","We use a worldline-instanton formalism to study the momentum spectrum of
Schwinger pair production in spacetime fields with multiple stationary points.
We show that the interference structure changes fundamentally when going from
purely time-dependent to space-time-dependent fields. For example, it was known
that two time-dependent pulses give interference if they are anti-parallel,
i.e. $E_z(t)-E_z(t-\Delta t)$, but here we show that two spacetime pulses will
typically give interference if they instead are parallel, i.e.
$E_z(t,z)+E_z(t-\Delta t,z-\Delta z)$. We take into account the fact that the
momenta of the electron, $p_z$, and of the positron, $p'_z$, are independent
for $E_z(t,z)$ (it would be $p_z+p'_z=0$ for $E(t)$), and find a type of fields
which give moir\'e patterns in the $p_z-p'_z$ plane. Depending on the
separation of two pulses, we also find an Aharonov-Bohm phase. We also study
complex momentum saddle points in order to obtain the integrated probability
from the spectrum. Finally, we calculate an asymptotic expansion for the
eigenvalues of the Sturm-Liouville equation that corresponds to the
saddle-point approximation of the worldline path integral, use that expansion
to compute the product of eigenvalues, and compare with the result obtained
with the Gelfand-Yaglom method.",2024-12-27T16:03:02Z,http://arxiv.org/abs/2412.19709v1,"Gianluca Degli Esposti, Greger Torgrimsson"
"All Finite (Anti)Hermitian Irreducible Representations of the de Sitter
  and Anti-de Sitter Lie Algebras and Their Lorentz Structure","Because of the importance of unitarity in quantum physics, work on the
representations of the de Sitter group has focussed on the unitary case, which
necessarily means infinite dimensional matrices for this non-compact group.
Here we address the finite dimensional representations resulting from the
requirement that the Lie algebra generators are either Hermitian or
anti-Hermitian. The complete classification of all such irreducible
representations is found and their matrix elements specified. These irreducible
representations (irreps) are based on backbones defined as the homogeneous
Lorentz sub-algebra and consisting of direct sums of the finite irreps of the
homogeneous Lorentz algebra (HLA). Only two types of such backbones arise (see
5.1a,b herein). Consequently, only certain dimensions of representation are
possible, namely 4, 5, 10, 14, 20, 30, 35, 55, 56, 91, etc or generally either
1/6 N(N+1)(N+2) or 1/6 N(N+1)(2N+1) where N=2,3,4,etc is the number of HLA
irreps in the backbone (minimum 2). The two Casimir invariants can be specified
in terms of a single integral or half-integral parameter, p. For irreps based
on (5.1a), -C1=p(p+1)-2 and C2=0 with p taking values 2,3,4,etc. For irreps
based on (5.1b), -C1=2(p^2-1) and -C2= p^2 (p^2-1) with p taking values
3/2,2,5/2,3,etc. These correspond to the same expressions found for the unitary
representations, -C1=p(p+1)+(q+1)(q-2) and -C2=p(p+1)q(q-1) with q=0 and q=p
respectively for the two types of irrep. There is thus a far more restricted
set of finite irreps with Hermitian or anti-Hermitian generators than for the
discrete infinite dimensional unitary irreps. The corresponding irreps of the
anti-de Sitter group follow immediately from the replacement of the 4-momentum
operators from V to iV.",2024-12-27T16:02:41Z,http://arxiv.org/abs/2412.19708v1,Richard A. W. Bradford
Toward Adaptive Reasoning in Large Language Models with Thought Rollback,"Large language models (LLMs) have been routinely used to solve various tasks
using step-by-step reasoning. However, the structure of intermediate reasoning
steps, or thoughts, is rigid and unidirectional, such as chains, trees, or
acyclic-directed graphs. Consequently, the resulting inflexible and
forward-only reasoning may not address challenging tasks and fail when the LLM
frequently gives false responses, i.e., ``hallucinations''. This paper proposes
a new reasoning framework, called Thought Rollback (TR), allowing LLMs to
adaptively build thought structure while maintaining effective reasoning toward
problem-solving under ``hallucinations''. The core mechanism of TR is rolling
back thoughts, which allows LLMs to perform error analysis on thoughts, and
thus roll back to any previously mistaken thought for revision. Subsequently,
by including such trial-and-error in the prompt to guide the LLM, each rollback
leads to one more reliable reasoning path. Therefore, starting with a simple
prompt without human annotations, LLM with TR adaptively and gradually explores
thoughts for a correct solution. Comprehensive experiments on mathematical
problems and multi-task reasoning demonstrate the state-of-the-art performance
of TR in terms of problem-solving rate and interaction cost. For instance, the
solving rate of GPT-4 with TR outperforms the current best by $9\%$ on the MATH
dataset.",2024-12-27T16:02:34Z,http://arxiv.org/abs/2412.19707v1,"Sijia Chen, Baochun Li"
"Noise Sensitivity of the Semidefinite Programs for Direct Data-Driven
  LQR","In this paper, we study the noise sensitivity of the semidefinite program
(SDP) proposed for direct data-driven infinite-horizon linear quadratic
regulator (LQR) problem for discrete-time linear time-invariant systems. While
this SDP is shown to find the true LQR controller in the noise-free setting, we
show that it leads to a trivial solution with zero gain matrices when data is
corrupted by noise, even when the noise is arbitrarily small. We then study a
variant of the SDP that includes a robustness promoting regularization term and
prove that regularization does not fully eliminate the sensitivity issue. In
particular, the solution of the regularized SDP converges in probability also
to a trivial solution.",2024-12-27T15:59:42Z,http://arxiv.org/abs/2412.19705v1,"Xiong Zeng, Laurent Bako, Necmiye Ozay"
"Reparameterization Invariance of FRW Model: Supervariable and BRST
  Approaches","We perform the Becchi-Rouet-Stora-Tyutin (BRST) quantization of a (0 +
1)-dimensional non-interacting cosmological Friedmann-Robertson-Walker (FRW)
model. This quantization leverages the classical infinitesimal and continuous
reparameterization symmetry transformations of the system. To derive the
nilpotent reparameterization invariant (anti-)BRST symmetry transformations for
the scale factor and corresponding momentum variables of the FRW model, we
employ the modified Bonora-Tonin supervariable approach (MBTSA) to BRST
formalism. Through this approach, we also establish the (anti-)BRST invariant
Curci-Ferrari (CF)-type restriction for this cosmological reparameterization
invariant model. Further, we obtain the nilpotent (anti-)BRST symmetry
transformations for other variables within the model using the (anti-)chiral
supervariable approach (ACSA) to BRST formalism. Within the framework of ACSA,
the CF-type restriction is demonstrated through two key aspects: (i) the
invariance of the coupled Lagrangians under symmetry transformations, and (ii)
the absolute anticommutativity of the conserved (anti-)BRST charges. Notably,
applying the MBTSA to a physical cosmological system, specifically a
one-dimensional one, constitutes a novel contribution to this work.
Additionally, in the application of ACSA, we restrict our analysis to
(anti-)chiral super expansions of supervariables, leading to the unique
observation of the absolute anticommutativity of the (anti-)BRST charges.
Moreover, we highlight that the CF-type restriction demonstrates a universal
nature, remaining consistent across any reparameterization invariant models in
D-dimensional spacetime.",2024-12-27T15:58:29Z,http://arxiv.org/abs/2412.19704v1,"B. Chauhan, R. Tripathi"
"Numerical inverse scattering transform for the defocusing nonlinear
  Schrödinger equation with box-type initial conditions on a nonzero
  background","We present a method to solve numerically the Cauchy problem for the
defocusing nonlinear Schr\""{o}dinger (NLS) equation with a box-type initial
condition (IC) having a nontrivial background of amplitude $q_o&gt;0$ as $x\to \pm
\infty$ by implementing numerically the associated Inverse Scattering Transform
(IST). The Riemann--Hilbert problem associated to the inverse transform is
solved numerically by means of appropriate contour deformations in the complex
plane following the numerical implementation of the Deift-Zhou nonlinear
steepest descent method. In this work, the box parameters are chosen so that
there is no discrete spectrum (i.e., no solitons). In particular, the numerical
method is demonstrated to be accurate within the two asymptotic regimes
corresponding to two different regions of the $(x,t)$-plane depending on
whether $|x/(2t)| &lt; q_o$ or $|x/(2t)| &gt; q_o$, as $t \to \infty$.",2024-12-27T15:57:44Z,http://arxiv.org/abs/2412.19703v1,"Aikaterini Gkogkou, Barbara Prinari, Thomas Trogdon"
"Search for the double Dalitz decays $η/η' \to e^+e^-μ^+μ^-$
  and $η' \to μ^+μ^-μ^+μ^-$","Using a data sample of $(10087 \pm 44) \times {10^{6}}$ $J/{\psi}$ events
collected with the BESIII detector, we search for the decays $\eta/\eta'\to
e^+e^-\mu^+\mu^-$ and $\eta' \to \mu^+\mu^-\mu^+\mu^-$ via the radiative decays
$J/{\psi}\to\gamma\eta$/$\gamma\eta'$. No excess of events over expected
background is observed for any of the decays of interest. At 90% confidence
level, we report the first upper limits on the branching fractions of $\eta'
\to e^{+}e^{-}\mu^{+}\mu^{-}$ and $\eta' \to \mu^{+}\mu^{-}\mu^{+}\mu^{-}$ to
be $ 1.75 \times {10^{-6}}$ and $5.28 \times {10^{-7}}$, respectively. In
addition, we set an upper limit on the branching fraction of $\eta \to
e^{+}e^{-}\mu^{+}\mu^{-}$ to be $6.88 \times {10^{-6}}$, which improves the
previous result by about two orders of magnitude.",2024-12-27T15:55:02Z,http://arxiv.org/abs/2412.19702v1,"BESIII Collaboration, M. Ablikim, M. N. Achasov, P. Adlarson, O. Afedulidis, X. C. Ai, R. Aliberti, A. Amoroso, Y. Bai, O. Bakina, I. Balossino, Y. Ban, H. -R. Bao, V. Batozskaya, K. Begzsuren, N. Berger, M. Berlowski, M. Bertani, D. Bettoni, F. Bianchi, E. Bianco, A. Bortone, I. Boyko, R. A. Briere, A. Brueggemann, H. Cai, X. Cai, A. Calcaterra, G. F. Cao, N. Cao, S. A. Cetin, X. Y. Chai, J. F. Chang, G. R. Che, Y. Z. Che, G. Chelkov, C. Chen, C. H. Chen, Chao Chen, G. Chen, H. S. Chen, H. Y. Chen, M. L. Chen, S. J. Chen, S. L. Chen, S. M. Chen, T. Chen, X. R. Chen, X. T. Chen, Y. B. Chen, Y. Q. Chen, Z. J. Chen, Z. Y. Chen, S. K. Choi, G. Cibinetto, F. Cossio, J. J. Cui, H. L. Dai, J. P. Dai, A. Dbeyssi, R. E. de Boer, D. Dedovich, C. Q. Deng, Z. Y. Deng, A. Denig, I. Denysenko, M. Destefanis, F. De Mori, B. Ding, X. X. Ding, Y. Ding, Y. Ding, J. Dong, L. Y. Dong, M. Y. Dong, X. Dong, M. C. Du, S. X. Du, Y. Y. Duan, Z. H. Duan, P. Egorov, Y. H. Fan, J. Fang, J. Fang, S. S. Fang, W. X. Fang, Y. Fang, Y. Q. Fang, R. Farinelli, L. Fava, F. Feldbauer, G. Felici, C. Q. Feng, J. H. Feng, Y. T. Feng, M. Fritsch, C. D. Fu, J. L. Fu, Y. W. Fu, H. Gao, X. B. Gao, Y. N. Gao, Yang Gao, S. Garbolino, I. Garzia, L. Ge, P. T. Ge, Z. W. Ge, C. Geng, E. M. Gersabeck, A. Gilman, K. Goetzen, L. Gong, W. X. Gong, W. Gradl, S. Gramigna, M. Greco, M. H. Gu, Y. T. Gu, C. Y. Guan, A. Q. Guo, L. B. Guo, M. J. Guo, R. P. Guo, Y. P. Guo, A. Guskov, J. Gutierrez, K. L. Han, T. T. Han, F. Hanisch, X. Q. Hao, F. A. Harris, K. K. He, K. L. He, F. H. Heinsius, C. H. Heinz, Y. K. Heng, C. Herold, T. Holtmann, P. C. Hong, G. Y. Hou, X. T. Hou, Y. R. Hou, Z. L. Hou, B. Y. Hu, H. M. Hu, J. F. Hu, Q. P. Hu, S. L. Hu, T. Hu, Y. Hu, G. S. Huang, K. X. Huang, L. Q. Huang, X. T. Huang, Y. P. Huang, Y. S. Huang, T. Hussain, F. Hölzken, N. Hüsken, N. in der Wiesche, J. Jackson, S. Janchiv, J. H. Jeong, Q. Ji, Q. P. Ji, W. Ji, X. B. Ji, X. L. Ji, Y. Y. Ji, X. Q. Jia, Z. K. Jia, D. Jiang, H. B. Jiang, P. C. Jiang, S. S. Jiang, T. J. Jiang, X. S. Jiang, Y. Jiang, J. B. Jiao, J. K. Jiao, Z. Jiao, S. Jin, Y. Jin, M. Q. Jing, X. M. Jing, T. Johansson, S. Kabana, N. Kalantar-Nayestanaki, X. L. Kang, X. S. Kang, M. Kavatsyuk, B. C. Ke, V. Khachatryan, A. Khoukaz, R. Kiuchi, O. B. Kolcu, B. Kopf, M. Kuessner, X. Kui, N. Kumar, A. Kupsc, W. Kühn, L. Lavezzi, T. T. Lei, Z. H. Lei, M. Lellmann, T. Lenz, C. Li, C. Li, C. H. Li, Cheng Li, D. M. Li, F. Li, G. Li, H. B. Li, H. J. Li, H. N. Li, Hui Li, J. R. Li, J. S. Li, K. Li, K. L. Li, L. J. Li, L. K. Li, Lei Li, M. H. Li, P. R. Li, Q. M. Li, Q. X. Li, R. Li, S. X. Li, T. Li, W. D. Li, W. G. Li, X. Li, X. H. Li, X. L. Li, X. Y. Li, X. Z. Li, Y. G. Li, Z. J. Li, Z. Y. Li, C. Liang, H. Liang, H. Liang, Y. F. Liang, Y. T. Liang, G. R. Liao, Y. P. Liao, J. Libby, A. Limphirat, C. C. Lin, C. X. Lin, D. X. Lin, T. Lin, B. J. Liu, B. X. Liu, C. Liu, C. X. Liu, F. Liu, F. H. Liu, Feng Liu, G. M. Liu, H. Liu, H. B. Liu, H. H. Liu, H. M. Liu, Huihui Liu, J. B. Liu, J. Y. Liu, K. Liu, K. Y. Liu, Ke Liu, L. Liu, L. C. Liu, Lu Liu, M. H. Liu, P. L. Liu, Q. Liu, S. B. Liu, T. Liu, W. K. Liu, W. M. Liu, X. Liu, X. Liu, Y. Liu, Y. Liu, Y. B. Liu, Z. A. Liu, Z. D. Liu, Z. Q. Liu, X. C. Lou, F. X. Lu, H. J. Lu, J. G. Lu, X. L. Lu, Y. Lu, Y. P. Lu, Z. H. Lu, C. L. Luo, J. R. Luo, M. X. Luo, T. Luo, X. L. Luo, X. R. Lyu, Y. F. Lyu, F. C. Ma, H. Ma, H. L. Ma, J. L. Ma, L. L. Ma, L. R. Ma, M. M. Ma, Q. M. Ma, R. Q. Ma, T. Ma, X. T. Ma, X. Y. Ma, Y. M. Ma, F. E. Maas, I. MacKay, M. Maggiora, S. Malde, Y. J. Mao, Z. P. Mao, S. Marcello, Z. X. Meng, J. G. Messchendorp, G. Mezzadri, H. Miao, T. J. Min, R. E. Mitchell, X. H. Mo, B. Moses, N. Yu. Muchnoi, J. Muskalla, Y. Nefedov, F. Nerling, L. S. Nie, I. B. Nikolaev, Z. Ning, S. Nisar, Q. L. Niu, W. D. Niu, Y. Niu, S. L. Olsen, S. L. Olsen, Q. Ouyang, S. Pacetti, X. Pan, Y. Pan, A. Pathak, Y. P. Pei, M. Pelizaeus, H. P. Peng, Y. Y. Peng, K. Peters, J. L. Ping, R. G. Ping, S. Plura, V. Prasad, F. Z. Qi, H. Qi, H. R. Qi, M. Qi, T. Y. Qi, S. Qian, W. B. Qian, C. F. Qiao, X. K. Qiao, J. J. Qin, L. Q. Qin, L. Y. Qin, X. P. Qin, X. S. Qin, Z. H. Qin, J. F. Qiu, Z. H. Qu, C. F. Redmer, K. J. Ren, A. Rivetti, M. Rolo, G. Rong, Ch. Rosner, M. Q. Ruan, S. N. Ruan, N. Salone, A. Sarantsev, Y. Schelhaas, K. Schoenning, M. Scodeggio, K. Y. Shan, W. Shan, X. Y. Shan, Z. J. Shang, J. F. Shangguan, L. G. Shao, M. Shao, C. P. Shen, H. F. Shen, W. H. Shen, X. Y. Shen, B. A. Shi, H. Shi, H. C. Shi, J. L. Shi, J. Y. Shi, Q. Q. Shi, S. Y. Shi, X. Shi, J. J. Song, T. Z. Song, W. M. Song, Y. J. Song, Y. X. Song, S. Sosio, S. Spataro, F. Stieler, S. S Su, Y. J. Su, G. B. Sun, G. X. Sun, H. Sun, H. K. Sun, J. F. Sun, K. Sun, L. Sun, S. S. Sun, T. Sun, W. Y. Sun, Y. Sun, Y. J. Sun, Y. Z. Sun, Z. Q. Sun, Z. T. Sun, C. J. Tang, G. Y. Tang, J. Tang, M. Tang, Y. A. Tang, L. Y. Tao, Q. T. Tao, M. Tat, J. X. Teng, V. Thoren, W. H. Tian, Y. Tian, Z. F. Tian, I. Uman, Y. Wan, S. J. Wang, B. Wang, B. L. Wang, Bo Wang, D. Y. Wang, F. Wang, H. J. Wang, J. J. Wang, J. P. Wang, K. Wang, L. L. Wang, M. Wang, N. Y. Wang, S. Wang, S. Wang, T. Wang, T. J. Wang, W. Wang, W. Wang, W. P. Wang, X. Wang, X. F. Wang, X. J. Wang, X. L. Wang, X. N. Wang, Y. Wang, Y. D. Wang, Y. F. Wang, Y. H. Wang, Y. L. Wang, Y. N. Wang, Y. Q. Wang, Yaqian Wang, Yi Wang, Z. Wang, Z. L. Wang, Z. Y. Wang, Ziyi Wang, D. H. Wei, F. Weidner, S. P. Wen, Y. R. Wen, U. Wiedner, G. Wilkinson, M. Wolke, L. Wollenberg, C. Wu, J. F. Wu, L. H. Wu, L. J. Wu, X. Wu, X. H. Wu, Y. Wu, Y. H. Wu, Y. J. Wu, Z. Wu, L. Xia, X. M. Xian, B. H. Xiang, T. Xiang, D. Xiao, G. Y. Xiao, S. Y. Xiao, Y. L. Xiao, Z. J. Xiao, C. Xie, X. H. Xie, Y. Xie, Y. G. Xie, Y. H. Xie, Z. P. Xie, T. Y. Xing, C. F. Xu, C. J. Xu, G. F. Xu, H. Y. Xu, M. Xu, Q. J. Xu, Q. N. Xu, W. Xu, W. L. Xu, X. P. Xu, Y. Xu, Y. C. Xu, Z. S. Xu, F. Yan, L. Yan, W. B. Yan, W. C. Yan, X. Q. Yan, H. J. Yang, H. L. Yang, H. X. Yang, J. H. Yang, T. Yang, Y. Yang, Y. F. Yang, Y. F. Yang, Y. X. Yang, Z. W. Yang, Z. P. Yao, M. Ye, M. H. Ye, J. H. Yin, Junhao Yin, Z. Y. You, B. X. Yu, C. X. Yu, G. Yu, J. S. Yu, M. C. Yu, T. Yu, X. D. Yu, Y. C. Yu, C. Z. Yuan, J. Yuan, J. Yuan, L. Yuan, S. C. Yuan, Y. Yuan, Z. Y. Yuan, C. X. Yue, A. A. Zafar, F. R. Zeng, S. H. Zeng, X. Zeng, Y. Zeng, Y. J. Zeng, Y. J. Zeng, X. Y. Zhai, Y. C. Zhai, Y. H. Zhan, A. Q. Zhang, B. L. Zhang, B. X. Zhang, D. H. Zhang, G. Y. Zhang, H. Zhang, H. Zhang, H. C. Zhang, H. H. Zhang, H. H. Zhang, H. Q. Zhang, H. R. Zhang, H. Y. Zhang, J. Zhang, J. Zhang, J. J. Zhang, J. L. Zhang, J. Q. Zhang, J. S. Zhang, J. W. Zhang, J. X. Zhang, J. Y. Zhang, J. Z. Zhang, Jianyu Zhang, L. M. Zhang, Lei Zhang, P. Zhang, Q. Y. Zhang, R. Y. Zhang, S. H. Zhang, Shulei Zhang, X. M. Zhang, X. Y Zhang, X. Y. Zhang, Y. Zhang, Y. Zhang, Y. T. Zhang, Y. H. Zhang, Y. M. Zhang, Yan Zhang, Z. D. Zhang, Z. H. Zhang, Z. L. Zhang, Z. Y. Zhang, Z. Y. Zhang, Z. Z. Zhang, G. Zhao, J. Y. Zhao, J. Z. Zhao, L. Zhao, Lei Zhao, M. G. Zhao, N. Zhao, R. P. Zhao, S. J. Zhao, Y. B. Zhao, Y. X. Zhao, Z. G. Zhao, A. Zhemchugov, B. Zheng, B. M. Zheng, J. P. Zheng, W. J. Zheng, Y. H. Zheng, B. Zhong, X. Zhong, H. Zhou, J. Y. Zhou, L. P. Zhou, S. Zhou, X. Zhou, X. K. Zhou, X. R. Zhou, X. Y. Zhou, Y. Z. Zhou, Z. C. Zhou, A. N. Zhu, J. Zhu, K. Zhu, K. J. Zhu, K. S. Zhu, L. Zhu, L. X. Zhu, S. H. Zhu, T. J. Zhu, W. D. Zhu, Y. C. Zhu, Z. A. Zhu, J. H. Zou, J. Zu"
"First-principles investigation of thermodynamics and electronic
  transitions in vacancy-ordered rare-earth perovskite nickelates","Controlled introduction of oxygen vacancies offers an effective route to
induce metal-to-insulator transition in strongly correlated rare-earth
nickelates ($R$NiO$_3$) at room temperature. However, the role played by the
rare-earth cations on the structure, thermodynamic stability, and electronic
properties of oxygen-deficient nickelates remains unclear. Here, we employ
density functional theory calculations with Hubbard corrections (DFT + $U$) to
investigate the whole family of $R$NiO$_{2.5}$ ($R$ = Pr-Er) compounds in two
commonly observed oxygen-vacancy ordered configurations, namely brownmillerite,
and square planar. We find that square planar polymorph is always more stable
($\sim$0.4 eV/u.f) than the brownmillerite for all rare-earth cations, owing to
the exceedingly low volumetric strains (&lt; 1\%). Formation energy of
$R$NiO$_{2.5}$ gradually increases with decreasing size of $R$ owing to
stronger Ni-O covalent interactions in pristine $R$NiO$_3$ with small $R^{3+}$
cations. This necessitates more oxygen-lean environments for synthesis of
$R$NiO$_{2.5}$ with smaller $R^{3+}$ cations. Analysis of the density of states
and band structures reveals that electronic structure of $R$NiO$_{2.5}$ is
governed by two factors: (a) localization of electron on NiO$_6$ octahedra
yielding a Mott insulating state with strong correlations as Ni $e_g$ is half
filled, and (b) crystal field splitting in the NiO$_4$ tetrahedra/square planar
polyhedra. Brownmillerite $R$NiO$_{2.5}$ is metallic, while square planar
$R$NiO$_{2.5}$ is an insulator with a predicted gap of $\sim$ 0.2-0.3 eV,
depending on the $R^{3+}$ cation. Crystal orbital Hamilton population (COHP)
analysis indicates that the Ni-O bond belonging to square-planar NiO$_4$
polyhedra exhibit much greater covalent character than those in NiO$_6$
octahedra in square planar $R$NiO$_{2.5}$.",2024-12-27T15:52:08Z,http://arxiv.org/abs/2412.19700v1,"Devang Bhagat, Ranga Teja Pidathala, Badri Narayanan"
Differentiable groupoid objects and their abstract Lie algebroids,"The infinitesimal counterpart of a Lie groupoid is its Lie algebroid. As a
vector bundle, it is given by the source vertical tangent bundle restricted to
the identity bisection. Its sections can be identified with the invariant
vector fields on the groupoid, which are closed under the Lie bracket. We
generalize this differentiation procedure to groupoid objects in any category
with an abstract tangent structure in the sense of Rosick\'{y} and a scalar
multiplication by a ring object that plays the role of the real numbers. We
identify the categorical conditions that the groupoid object must satisfy to
admit a natural notion of invariant vector fields. Then we show that invariant
vector fields are closed under the Lie bracket defined by Rosick\'{y} and
satisfy the Leibniz rule with respect to ring-valued morphisms on the base of
the groupoid. The result is what we define axiomatically as an abstract Lie
algebroid, by generalizing the underlying vector bundle to a module object in
the slice category over its base. Examples include diffeomorphism groups,
bisection groups of Lie groupoids, the diffeological symmetry groupoids of
general relativity (Blohmann/Fernandes/Weinstein), symmetry groupoids in
Lagrangian Field Theory, holonomy groupoids of singular foliations, elastic
diffeological groupoids, and groupoid objects in differentiable stacks.",2024-12-27T15:50:02Z,http://arxiv.org/abs/2412.19697v1,"Lory Aintablian, Christian Blohmann"
"An Integrated Optimization and Deep Learning Pipeline for Predicting
  Live Birth Success in IVF Using Feature Optimization and Transformer-Based
  Models","In vitro fertilization (IVF) is a widely utilized assisted reproductive
technology, yet predicting its success remains challenging due to the
multifaceted interplay of clinical, demographic, and procedural factors. This
study develops a robust artificial intelligence (AI) pipeline aimed at
predicting live birth outcomes in IVF treatments. The pipeline uses anonymized
data from 2010 to 2018, obtained from the Human Fertilization and Embryology
Authority (HFEA). We evaluated the prediction performance of live birth success
as a binary outcome (success/failure) by integrating different feature
selection methods, such as principal component analysis (PCA) and particle
swarm optimization (PSO), with different traditional machine learning-based
classifiers including random forest (RF) and decision tree, as well as deep
learning-based classifiers including custom transformer-based model and a tab
transformer model with an attention mechanism. Our research demonstrated that
the best performance was achieved by combining PSO for feature selection with
the TabTransformer-based deep learning model, yielding an accuracy of 99.50%
and an AUC of 99.96%, highlighting its significant performance to predict live
births. This study establishes a highly accurate AI pipeline for predicting
live birth outcomes in IVF, demonstrating its potential to enhance personalized
fertility treatments.",2024-12-27T15:46:59Z,http://arxiv.org/abs/2412.19696v1,"Arezoo Borji, Hossam Haick, Birgit Pohn, Antonia Graf, Jana Zakall, S M Ragib Shahriar Islam, Gernot Kronreif, Daniel Kovatchki, Heinz Strohmer, Sepideh Hatamikia"
Sharp Bounds on Lengths of Linear Recolouring Sequences,"A recolouring sequence, between $k$-colourings $\alpha$ and $\beta$ of a
graph $G$, transforms $\alpha$ into $\beta$ by recolouring one vertex at a
time, such that after each recolouring step we again have a proper
$k$-colouring of $G$. The diameter of the $k$-recolouring graph,
$\textrm{diam}~\mathcal{C}_k(G)$, is the maximum over all pairs $\alpha$ and
$\beta$ of the minimum length of a recolouring sequence from $\alpha$ to
$\beta$. Much previous work has focused on determining the asymptotics of
$\textrm{diam}~\mathcal{C}_k(G)$: Is it $\Theta(|G|)$? Is it $\Theta(|G|^2)$?
Or even larger? Here we focus on graphs for which
$\textrm{diam}~\mathcal{C}_k(G)=\Theta(|G|)$, and seek to determine more
precisely the multiplicative constant implicit in the $\Theta()$. In
particular, for each $k\ge 3$, for all positive integers $p$ and $q$ we exactly
determine $\textrm{diam}~\mathcal{C}_k(K_{p,q})$, up to a small additive
constant. We also sharpen a recolouring lemma that has been used in multiple
papers, proving an optimal version. This improves the multiplicative constant
in various prior results. Finally, we investigate plausible relationships
between similar reconfiguration graphs.",2024-12-27T15:42:55Z,http://arxiv.org/abs/2412.19695v1,"Stijn Cambie, Wouter Cames van Batenburg, Daniel W. Cranston"
"Nonperturbative effects in triple-differential dijet and Z+jet
  production at the LHC","In comparisons of precision collider data to the most accurate highest-order
calculations in perturbative quantum chromodynamics (QCD), it is required to
correct for nonperturbative effects. Such effects are typically studied using
Monte Carlo event generators that complement fixed-order predictions with
perturbative parton showers and models for the nonperturbative effects of the
Underlying Event and hadronisation. Thereby, the final state of collision
events can be predicted at the level of stable particles, which serve as input
for full detector simulations.
  This article investigates the impact of nonperturbative effects on two
processes that may be used for precision determinations of the strong coupling
constant and the proton structure: the triple-differential dijet and Z+jet
production. While nonperturbative effects impact both processes, significant
differences among them are observed and further investigated. Indications are
found that the Underlying Event and hadronisation cannot fully explain these
differences and the perturbative modelling may play a significant role as well.",2024-12-27T15:42:07Z,http://arxiv.org/abs/2412.19694v1,"Stefan Gieseke, Maximilian Horzela, Manjit Kaur, Dari Leonardi, Klaus Rabbertz, Aayushi Singla, Cedric Verstege"
"Wannier states and spin supersolid physics in the triangular
  antiferromagnet K$_2$Co(SeO$_3$)$_2$","We use a combination of ultra-high-resolution inelastic neutron scattering
and Monte Carlo numerical simulations to study the thermodynamics and the
structure of spin excitations in the spin-supersolid phase of the triangular
lattice XXZ easy axis antiferromagnet K$_2$Co(SeO$_3$)$_2$ and its evolution in
a magnetic field. BKT transitions heralding the onset of Ising and supersolid
order are detected. Above the supersolid phase the value of Wannier entropy is
experimentally recovered. At low temperatures, with an experimental resolution
of about 23 $\mu$eV, no discrete coherent magnon modes are resolved within a
broad continuum of scattering. In addition to gapless excitations, a
pseudo-Goldstone mode with a small energy gap of 0.06 meV is found. A second
excitation continuum is seen at higher energy, in place of single-spin-flip
excitations of the Ising model. In applied fields the continuum gradually
morphs into coherent spin waves, with the Goldstone and pseudo-Goldstone
sectors showing distinct evolution. The agreement between experiment and
numerical simulations is excellent on the quantitative level.",2024-12-27T15:40:31Z,http://arxiv.org/abs/2412.19693v1,"M. Zhu, Leandro M. Chinellato, V. Romerio, N. Murai, S. Ohira-Kawamura, Christian Balz, Z. Yan, S. Gvasaliya, Yasuyuki Kato, C. D. Batista, A. Zheludev"
"Quantum Many-Body Lattice C-R-T Symmetry: Fractionalization, Anomaly,
  and Symmetric Mass Generation","Charge conjugation (C), mirror reflection (R), and time reversal (T)
symmetries, along with internal symmetries, are essential for massless Majorana
and Dirac fermions. These symmetries are sufficient to rule out potential
fermion bilinear mass terms, thereby establishing a gapless free fermion fixed
point phase, pivotal for symmetric mass generation (SMG) transition. In this
work, we systematically study the anomaly of C-R-T-internal symmetry in all
spacetime dimensions by analyzing the projective representation (i.e. the
fractionalization) of the C-R-T-internal symmetry group in the quantum
many-body Hilbert space on the lattice. By discovering the
fermion-flavor-number-dependent C-R-T-internal symmetry's anomaly structure, we
demonstrate an alternative way to derive the minimal flavor number for SMG,
which shows consistency with known results from K\""ahler-Dirac fermion or
cobordism classification. Our findings reveal that, in general spatial
dimensions, either 8 copies of staggered Majorana fermions or 4 copies of
staggered Dirac fermions admit SMG. By directly searching for 4-fermion
interactions that form commuting stabilizers respecting all symmetry
constraints, we can prove the explicit SMG gapping retained a unique ground
state in the codespace. Furthermore, we establish the correspondence between
the symmetry operators of staggered fermions and free fermions, which is
instrumental in facilitating the analysis of symmetry fractionalization at the
field theory level.",2024-12-27T15:36:31Z,http://arxiv.org/abs/2412.19691v1,"Yang-Yang Li, Juven Wang, Yi-Zhuang You"
"Terms that define nuclei on residuated lattices: a case study of
  BL-algebras","A nucleus $\gamma$ on a (bounded commutative integral) residuated lattice
$\mathbf{A}$ is a closure operator that satisfies the inequality $\gamma(a)
\cdot \gamma(b) \leq \gamma(a \cdot b)$ for all $a,b \in A$. In this article,
among several results, a description of an arbitrary nucleus on a residuated
lattice is given. Special attention is given to terms that define a nucleus on
every structure of a variety, as a means of generalizing the double negation
operation. Some general results about these terms are presented, together with
examples. The main result of this article consists of the description of all
terms of this kind for every given subvariety of BL-algebras. We exhibit
interesting nontrivial examples.",2024-12-27T15:34:51Z,http://arxiv.org/abs/2412.19690v1,"Sebastián Buss, Diego Castaño, José Patricio Díaz Varela"
"A Review on the Integration of Artificial Intelligence and Medical
  Imaging in IVF Ovarian Stimulation","Artificial intelligence (AI) has emerged as a powerful tool to enhance
decision-making and optimize treatment protocols in in vitro fertilization
(IVF). In particular, AI shows significant promise in supporting
decision-making during the ovarian stimulation phase of the IVF process. This
review evaluates studies focused on the applications of AI combined with
medical imaging in ovarian stimulation, examining methodologies, outcomes, and
current limitations. Our analysis of 13 studies on this topic reveals that,
reveal that while AI algorithms demonstrated notable potential in predicting
optimal hormonal dosages, trigger timing, and oocyte retrieval outcomes, the
medical imaging data utilized predominantly came from two-dimensional (2D)
ultrasound which mainly involved basic quantifications, such as follicle size
and number, with limited use of direct feature extraction or advanced image
analysis techniques. This points to an underexplored opportunity where advanced
image analysis approaches, such as deep learning, and more diverse imaging
modalities, like three-dimensional (3D) ultrasound, could unlock deeper
insights. Additionally, the lack of explainable AI (XAI) in most studies raises
concerns about the transparency and traceability of AI-driven decisions - key
factors for clinical adoption and trust. Furthermore, many studies relied on
single-center designs and small datasets, which limit the generalizability of
their findings. This review highlights the need for integrating advanced
imaging analysis techniques with explainable AI methodologies, as well as the
importance of leveraging multicenter collaborations and larger datasets.
Addressing these gaps has the potential to enhance ovarian stimulation
management, paving the way for efficient, personalized, and data-driven
treatment pathways that improve IVF outcomes.",2024-12-27T15:29:08Z,http://arxiv.org/abs/2412.19688v1,"Jana Zakall, Birgit Pohn, Antonia Graf, Daniel Kovatchki, Arezoo Borji, Ragib Shahriar Islam, Hossam Haick, Heinz Strohmer, Sepideh Hatamikia"
"DFT based comparative analysis of physical properties of binary metallic
  diborides XB$_2$ (X = Cr, Mo and W)","Transition-metal borides (TMBs) have long attracted attention of the
researchers because of their unique mechanical and electrical properties
including superconductivity. We have explored the structural, mechanical,
electronic, optical, and some thermophysical properties of XB$_2$ (X = Cr, Mo
and W) binary metallic diborides in detail employing density functional theory
based first-principles method. Many of the physical properties, including
direction-dependent mechanical properties, optical properties, and
thermo-mechanical properties are being investigated for the first time.",2024-12-27T15:28:42Z,http://arxiv.org/abs/2412.19687v1,"Razu Ahmed, Md. Sohel Rana, Md. Sajidul Islam, S. H. Naqib"
"Boosting Private Domain Understanding of Efficient MLLMs: A Tuning-free,
  Adaptive, Universal Prompt Optimization Framework","Efficient multimodal large language models (EMLLMs), in contrast to
multimodal large language models (MLLMs), reduce model size and computational
costs and are often deployed on resource-constrained devices. However, due to
data privacy concerns, existing open-source EMLLMs rarely have access to
private domain-specific data during the pre-training process, making them
difficult to directly apply in device-specific domains, such as certain
business scenarios. To address this weakness, this paper focuses on the
efficient adaptation of EMLLMs to private domains, specifically in two areas:
1) how to reduce data requirements, and 2) how to avoid parameter fine-tuning.
Specifically, we propose a tun\textbf{\underline{I}}ng-free,
a\textbf{\underline{D}}aptiv\textbf{\underline{E}},
univers\textbf{\underline{AL}} \textbf{\underline{Prompt}} Optimization
Framework, abbreviated as \textit{\textbf{\ourmethod{}}} which consists of two
stages: 1) Predefined Prompt, based on the reinforcement searching strategy,
generate a prompt optimization strategy tree to acquire optimization priors; 2)
Prompt Reflection initializes the prompt based on optimization priors, followed
by self-reflection to further search and refine the prompt. By doing so,
\ourmethod{} elegantly generates the ``ideal prompts'' for processing private
domain-specific data. Note that our method requires no parameter fine-tuning
and only a small amount of data to quickly adapt to the data distribution of
private data. Extensive experiments across multiple tasks demonstrate that our
proposed \ourmethod{} significantly improves both efficiency and performance
compared to baselines.",2024-12-27T15:21:17Z,http://arxiv.org/abs/2412.19684v1,"Jiang Liu, Bolin Li, Haoyuan Li, Tianwei Lin, Wenqiao Zhang, Tao Zhong, Zhelun Yu, Jinghao Wei, Hao Cheng, Hao Jiang, Zheqi Lv, Juncheng Li, Siliang Tang, Yueting Zhuang"
"A Hybrid Technique for Plant Disease Identification and Localisation in
  Real-time","Over the past decade, several image-processing methods and algorithms have
been proposed for identifying plant diseases based on visual data. DNN (Deep
Neural Networks) have recently become popular for this task. Both traditional
image processing and DNN-based methods encounter significant performance issues
in real-time detection owing to computational limitations and a broad spectrum
of plant disease features. This article proposes a novel technique for
identifying and localising plant disease based on the Quad-Tree decomposition
of an image and feature learning simultaneously. The proposed algorithm
significantly improves accuracy and faster convergence in high-resolution
images with relatively low computational load. Hence it is ideal for deploying
the algorithm in a standalone processor in a remotely operated image
acquisition and disease detection system, ideally mounted on drones and robots
working on large agricultural fields. The technique proposed in this article is
hybrid as it exploits the advantages of traditional image processing methods
and DNN-based models at different scales, resulting in faster inference. The F1
score is approximately 0.80 for four disease classes corresponding to potato
and tomato crops.",2024-12-27T15:20:45Z,http://arxiv.org/abs/2412.19682v1,"Mahendra Kumar Gohil, Anirudha Bhattacharjee, Rwik Rana, Kishan Lal, Samir Kumar Biswas, Nachiketa Tiwari, Bishakh Bhattacharya"
Identifying clusters in Czekanowski's diagram,"Visualizing data through Czekanowski's diagram has as its aim the
illustration of the relationships between objects. Often, obvious clusters of
observations are directly visible. However, it is not straightforward to
precisely delineate these clusters. This paper presents the development of the
package RMaCzek, which now includes features for cluster identification in
Czekanowski diagrams.",2024-12-27T15:04:06Z,http://arxiv.org/abs/2412.19679v1,"Krzysztof Bartoszek, Ying Luo"
Deep ReLU networks -- injectivity capacity upper bounds,"We study deep ReLU feed forward neural networks (NN) and their injectivity
abilities. The main focus is on \emph{precisely} determining the so-called
injectivity capacity. For any given hidden layers architecture, it is defined
as the minimal ratio between number of network's outputs and inputs which
ensures unique recoverability of the input from a realizable output. A strong
recent progress in precisely studying single ReLU layer injectivity properties
is here moved to a deep network level. In particular, we develop a program that
connects deep $l$-layer net injectivity to an $l$-extension of the $\ell_0$
spherical perceptrons, thereby massively generalizing an isomorphism between
studying single layer injectivity and the capacity of the so-called
(1-extension) $\ell_0$ spherical perceptrons discussed in [82]. \emph{Random
duality theory} (RDT) based machinery is then created and utilized to
statistically handle properties of the extended $\ell_0$ spherical perceptrons
and implicitly of the deep ReLU NNs. A sizeable set of numerical evaluations is
conducted as well to put the entire RDT machinery in practical use. From these
we observe a rapidly decreasing tendency in needed layers' expansions, i.e., we
observe a rapid \emph{expansion saturation effect}. Only $4$ layers of depth
are sufficient to closely approach level of no needed expansion -- a result
that fairly closely resembles observations made in practical experiments and
that has so far remained completely untouchable by any of the existing
mathematical methodologies.",2024-12-27T14:57:40Z,http://arxiv.org/abs/2412.19677v1,Mihailo Stojnic
"Optimizing Local-Global Dependencies for Accurate 3D Human Pose
  Estimation","Transformer-based methods have recently achieved significant success in 3D
human pose estimation, owing to their strong ability to model long-range
dependencies. However, relying solely on the global attention mechanism is
insufficient for capturing the fine-grained local details, which are crucial
for accurate pose estimation. To address this, we propose SSR-STF, a
dual-stream model that effectively integrates local features with global
dependencies to enhance 3D human pose estimation. Specifically, we introduce
SSRFormer, a simple yet effective module that employs the skeleton selective
refine attention (SSRA) mechanism to capture fine-grained local dependencies in
human pose sequences, complementing the global dependencies modeled by the
Transformer. By adaptively fusing these two feature streams, SSR-STF can better
learn the underlying structure of human poses, overcoming the limitations of
traditional methods in local feature extraction. Extensive experiments on the
Human3.6M and MPI-INF-3DHP datasets demonstrate that SSR-STF achieves
state-of-the-art performance, with P1 errors of 37.4 mm and 13.2 mm
respectively, outperforming existing methods in both accuracy and
generalization. Furthermore, the motion representations learned by our model
prove effective in downstream tasks such as human mesh recovery. Codes are
available at https://github.com/poker-xu/SSR-STF.",2024-12-27T14:54:12Z,http://arxiv.org/abs/2412.19676v1,"Guangsheng Xu, Guoyi Zhang, Lejia Ye, Shuwei Gan, Xiaohu Zhang, Xia Yang"
Port-Hamiltonian nonlinear systems,"Control theory often takes the mathematical model of the to-be-control-led
system for granted. In contrast, port-Hamiltonian systems theory bridges the
gap between modelling and control for physical systems. It provides a unified
framework for the modelling of complex multiphysics systems. At the same time
it offers powerful tools for analysis and control by identifying the underlying
physical structure, as reflected in, e.g., energy balance and other conserved
quantities. This leads to control schemes that \emph{exploit} the physical
structure, instead of compensating for it. As a result, the derived control
laws tend to be simple, physically interpretable, and robust with respect to
physical parameter variations.
  In this paper, after introducing port-Hamiltonian systems, the focus is on
'control by interconnection' for set-point stabilization of nonlinear physical
systems. Most of this theory is well-established, but novel developments using
'energy ports' instead of 'power ports' are also included.",2024-12-27T14:45:45Z,http://arxiv.org/abs/2412.19673v1,Arjan van der Schaft
Lattice properties of the sharp partial order,"The aim of this paper is to study lattice properties of the sharp partial
order for complex matrices having index at most 1. We investigate the down-set
of a fixed matrix $B$ under this partial order via isomorphisms with two
different partially ordered sets of projectors. These are, respectively, the
set of projectors that commute with a certain (nonsingular) block of a
Hartwig-Spindelb\""ock decomposition of $B$ and the set of projectors that
commute with the Jordan canonical form of that block. Using these isomorphisms,
we study the lattice structure of the down-sets and we give properties of them.
Necessary and sufficient conditions under which the down-set of B is a lattice
were found, in which case we describe its elements completely. We also show
that every down-set of $B$ has a distinguished Boolean subalgebra and we give a
description of its elements. We characterize the matrices that are above a
given matrix in terms of its Jordan canonical form. Mitra (1987) showed that
the set of all $n \times n$ complex matrices having index at most 1 with $n\geq
4$ is not a lower semilattice. We extend this result to $n=3$ and prove that it
is a lower semilattice with $n=2$. We also answer negatively a conjecture given
by Mitra, Bhimasankaram and Malik (2010). As a last application, we
characterize solutions of some matrix equations via the established
isomorphisms.",2024-12-27T14:38:40Z,http://arxiv.org/abs/2412.19671v1,"Cecilia R. Cimadamore, Laura A. Rueda, Néstor Thome, Melina V. Verdecchia"
Multipole moments in stationary spacetimes,"Multipole moments in general relativity serve as a powerful tool for
characterising the gravitational field. In this paper, we review the
construction of the Geroch--Hansen multipole moments for stationary
asymptotically flat vacuum spacetimes. A particular focus is placed on the
well-definedness of these moments, which hinges on the uniqueness of the
one-point conformal completion in Geroch's asymptotic flatness definition.
Based on Geroch's approach, we formulate and prove a revised uniqueness result,
thereby filling in some gaps in the original approach. Uniqueness holds up to
certain conformal transformations, and we discuss how the multipole moments
behave under such transformations.",2024-12-27T14:27:01Z,http://arxiv.org/abs/2412.19667v1,Jorn van Voorthuizen
The Value of Recall in Extensive-Form Games,"Imperfect-recall games, in which players may forget previously acquired
information, have found many practical applications, ranging from game
abstractions to team games and testing AI agents. In this paper, we quantify
the utility gain by endowing a player with perfect recall, which we call the
value of recall (VoR). While VoR can be unbounded in general, we parameterize
it in terms of various game properties, namely the structure of chance nodes
and the degree of absentmindedness (the number of successive times a player
enters the same information set). Further, we identify several pathologies that
arise with VoR, and show how to circumvent them. We also study the complexity
of computing VoR, and how to optimally apportion partial recall. Finally, we
connect VoR to other previously studied concepts in game theory, including the
price of anarchy. We use that connection in conjunction with the celebrated
smoothness framework to characterize VoR in a broad class of games.",2024-12-27T14:12:45Z,http://arxiv.org/abs/2412.19659v1,"Ratip Emin Berker, Emanuel Tewolde, Ioannis Anagnostides, Tuomas Sandholm, Vincent Conitzer"
"Asymmetrical Reciprocity-based Federated Learning for Resolving
  Disparities in Medical Diagnosis","Geographic health disparities pose a pressing global challenge, particularly
in underserved regions of low- and middle-income nations. Addressing this issue
requires a collaborative approach to enhance healthcare quality, leveraging
support from medically more developed areas. Federated learning emerges as a
promising tool for this purpose. However, the scarcity of medical data and
limited computation resources in underserved regions make collaborative
training of powerful machine learning models challenging. Furthermore, there
exists an asymmetrical reciprocity between underserved and developed regions.
To overcome these challenges, we propose a novel cross-silo federated learning
framework, named FedHelp, aimed at alleviating geographic health disparities
and fortifying the diagnostic capabilities of underserved regions.
Specifically, FedHelp leverages foundational model knowledge via one-time API
access to guide the learning process of underserved small clients, addressing
the challenge of insufficient data. Additionally, we introduce a novel
asymmetric dual knowledge distillation module to manage the issue of asymmetric
reciprocity, facilitating the exchange of necessary knowledge between developed
large clients and underserved small clients. We validate the effectiveness and
utility of FedHelp through extensive experiments on both medical image
classification and segmentation tasks. The experimental results demonstrate
significant performance improvement compared to state-of-the-art baselines,
particularly benefiting clients in underserved regions.",2024-12-27T13:59:58Z,http://arxiv.org/abs/2412.19654v1,"Jiaqi Wang, Ziyi Yin, Quanzeng You, Lingjuan Lyu, Fenglong Ma"
"Distributed Download from an External Data Source in Faulty Majority
  Settings","We extend the study of retrieval problems in distributed networks, focusing
on improving the efficiency and resilience of protocols in the \emph{Data
Retrieval (DR) Model}. The DR Model consists of a complete network (i.e., a
clique) with $k$ peers, up to $\beta k$ of which may be Byzantine (for $\beta
\in [0, 1)$), and a trusted \emph{External Data Source} comprising an array $X$
of $n$ bits ($n \gg k$) that the peers can query. Additionally, the peers can
also send messages to each other. In this work, we focus on the Download
problem that requires all peers to learn $X$. Our primary goal is to minimize
the maximum number of queries made by any honest peer and additionally optimize
time.
  We begin with a randomized algorithm for the Download problem that achieves
optimal query complexity up to a logarithmic factor. For the stronger dynamic
adversary that can change the set of Byzantine peers from one round to the
next, we achieve the optimal time complexity in peer-to-peer communication but
with larger messages. In broadcast communication where all peers (including
Byzantine peers) are required to send the same message to all peers, with
larger messages, we achieve almost optimal time and query complexities for a
dynamic adversary. Finally, in a more relaxed crash fault model, where peers
stop responding after crashing, we address the Download problem in both
synchronous and asynchronous settings. Using a deterministic protocol, we
obtain nearly optimal results for both query complexity and message sizes in
these scenarios.",2024-12-27T13:55:00Z,http://arxiv.org/abs/2412.19649v1,"John Augustine, Soumyottam Chatterjee, Valerie King, Manish Kumar, Shachar Meir, David Peleg"
"Enhancing Vision-Language Tracking by Effectively Converting Textual
  Cues into Visual Cues","Vision-Language Tracking (VLT) aims to localize a target in video sequences
using a visual template and language description. While textual cues enhance
tracking potential, current datasets typically contain much more image data
than text, limiting the ability of VLT methods to align the two modalities
effectively. To address this imbalance, we propose a novel plug-and-play method
named CTVLT that leverages the strong text-image alignment capabilities of
foundation grounding models. CTVLT converts textual cues into interpretable
visual heatmaps, which are easier for trackers to process. Specifically, we
design a textual cue mapping module that transforms textual cues into target
distribution heatmaps, visually representing the location described by the
text. Additionally, the heatmap guidance module fuses these heatmaps with the
search image to guide tracking more effectively. Extensive experiments on
mainstream benchmarks demonstrate the effectiveness of our approach, achieving
state-of-the-art performance and validating the utility of our method for
enhanced VLT.",2024-12-27T13:54:32Z,http://arxiv.org/abs/2412.19648v1,"X. Feng, D. Zhang, S. Hu, X. Li, M. Wu, J. Zhang, X. Chen, K. Huang"
"Chimera: A Block-Based Neural Architecture Search Framework for
  Event-Based Object Detection","Event-based cameras are sensors that simulate the human eye, offering
advantages such as high-speed robustness and low power consumption. Established
Deep Learning techniques have shown effectiveness in processing event data.
Chimera is a Block-Based Neural Architecture Search (NAS) framework
specifically designed for Event-Based Object Detection, aiming to create a
systematic approach for adapting RGB-domain processing methods to the event
domain. The Chimera design space is constructed from various macroblocks,
including Attention blocks, Convolutions, State Space Models, and
MLP-mixer-based architectures, which provide a valuable trade-off between local
and global processing capabilities, as well as varying levels of complexity.
The results on the PErson Detection in Robotics (PEDRo) dataset demonstrated
performance levels comparable to leading state-of-the-art models, alongside an
average parameter reduction of 1.6 times.",2024-12-27T13:50:44Z,http://arxiv.org/abs/2412.19646v1,"Diego A. Silva, Ahmed Elsheikh, Kamilya Smagulova, Mohammed E. Fouda, Ahmed M. Eltawil"
A Brief Overlook on Magnetoplasmadynamic Thrusters,"This paper presents a comprehensive analysis of Magnetoplasmadynamic
Thrusters (MPDT), examining their working principles, performance
characteristics, and potential applications in space propulsion. The study
focuses on both self-field and applied-field MPDT variants, detailing the
fundamental physics of plasma generation, acceleration mechanisms through
Lorentz forces, and plasma detachment processes. Through mathematical modeling
and experimental data analysis, the paper demonstrates MPDTs' capability to
achieve high specific impulse and efficient propellant utilization compared to
chemical propulsion systems. While highlighting their advantages for deep space
missions and satellite operations, the study also addresses key challenges,
including high power requirements and thermal management issues. The research
concludes that despite current technological limitations, MPDTs show promising
potential for future space exploration, particularly for long-duration missions
requiring sustained thrust.",2024-12-27T13:30:13Z,http://arxiv.org/abs/2412.19636v1,Egemen Gover
A non-semisimple non-invertible symmetry,"We investigate the action of a non-semisimple, non-invertible symmetry on
spin chains, whose topological defects encode the category of modules over the
Taft algebra of dimension 4. Sacrificing Hermiticity, we construct several
symmetric, frustration-free, gapped Hamiltonians with real spectra and analyse
their ground state subspaces. Our study reveals two intriguing phenomena.
First, we identify an $\mathbb{S}^1$-parametrised family of symmetric states,
all of which belong to the same gapped phase with respect to the invertible
subsymmetry, yet transform inequivalently under the non-semisimple symmetry.
Second, we find a model where a product state and the so-called W state
spontaneously break the symmetry. We further relate the indistinguishability of
these two states in the infinite-volume limit to the notion that they are
associated with a simple object and its projective cover, respectively, in a
non-semisimple module category.",2024-12-27T13:27:24Z,http://arxiv.org/abs/2412.19635v1,"Clement Delcamp, Edmund Heng, Matthew Yu"
Deep Linear Hawkes Processes,"Marked temporal point processes (MTPPs) are used to model sequences of
different types of events with irregular arrival times, with broad applications
ranging from healthcare and social networks to finance. We address shortcomings
in existing point process models by drawing connections between modern deep
state-space models (SSMs) and linear Hawkes processes (LHPs), culminating in an
MTPP that we call the deep linear Hawkes process (DLHP). The DLHP modifies the
linear differential equations in deep SSMs to be stochastic jump differential
equations, akin to LHPs. After discretizing, the resulting recurrence can be
implemented efficiently using a parallel scan. This brings parallelism and
linear scaling to MTPP models. This contrasts with attention-based MTPPs, which
scale quadratically, and RNN-based MTPPs, which do not parallelize across the
sequence length. We show empirically that DLHPs match or outperform existing
models across a broad range of metrics on eight real-world datasets. Our
proposed DLHP model is the first instance of the unique architectural
capabilities of SSMs being leveraged to construct a new class of MTPP models.",2024-12-27T13:23:58Z,http://arxiv.org/abs/2412.19634v1,"Yuxin Chang, Alex Boyd, Cao Xiao, Taha Kass-Hout, Parminder Bhatia, Padhraic Smyth, Andrew Warrington"
"Primordial Black Hole Formation from the Upward Step Model: Avoiding
  Overproduction","We investigate the formation of primordial black holes (PBHs) in an upward
step inflationary model, where nonlinearities between curvature perturbations
and field fluctuations introduce a cutoff, deviating from the Gaussian case.
This necessitates a reevaluation of PBH formation, as $\mathcal{R}$ is not the
optimal variable for estimating abundance. Using the extended Press-Schechter
formalism, we show that non-Gaussianity modifies both the curvature
perturbation profile $\mathcal{R}(r)$ and the integration path in probability
space, significantly impacting PBH abundance. Our results reveal that the
abundance initially increases with the parameter $h$, which characterizes the
relaxation stage after the step. However, beyond a critical value ($h \simeq
5.9$), it sharply declines before rising again. Furthermore, we demonstrate
that non-Gaussianity introduces uncertainties in indirect PBH observations via
gravitational waves. Notably, we present an example where a positive $f_{\rm
NL}$ does not necessarily enhance PBH production, contrary to conventional
expectations. Finally, by accounting for non-perturbative effects, we resolve
the overproduction of PBHs suggested by pulsar timing array (PTA) data,
underscoring the critical importance of incorporating non-Gaussianity in future
studies.",2024-12-27T13:21:06Z,http://arxiv.org/abs/2412.19631v1,"Xiaoding Wang, Xiao-Han Ma, Yi-Fu Cai"
"RecConv: Efficient Recursive Convolutions for Multi-Frequency
  Representations","Recent advances in vision transformers (ViTs) have demonstrated the advantage
of global modeling capabilities, prompting widespread integration of
large-kernel convolutions for enlarging the effective receptive field (ERF).
However, the quadratic scaling of parameter count and computational complexity
(FLOPs) with respect to kernel size poses significant efficiency and
optimization challenges. This paper introduces RecConv, a recursive
decomposition strategy that efficiently constructs multi-frequency
representations using small-kernel convolutions. RecConv establishes a linear
relationship between parameter growth and decomposing levels which determines
the effective kernel size $k\times 2^\ell$ for a base kernel $k$ and $\ell$
levels of decomposition, while maintaining constant FLOPs regardless of the ERF
expansion. Specifically, RecConv achieves a parameter expansion of only
$\ell+2$ times and a maximum FLOPs increase of $5/3$ times, compared to the
exponential growth ($4^\ell$) of standard and depthwise convolutions.
RecNeXt-M3 outperforms RepViT-M1.1 by 1.9 $AP^{box}$ on COCO with similar
FLOPs. This innovation provides a promising avenue towards designing efficient
and compact networks across various modalities. Codes and models can be found
at \url{https://github.com/suous/RecNeXt}.",2024-12-27T13:13:52Z,http://arxiv.org/abs/2412.19628v1,"Mingshu Zhao, Yi Luo, Yong Ouyang"
"Measurement of the branching fraction, polarization, and time-dependent
  $CP$ asymmetry in $B^0 \to ρ^+ρ^-$ decays and constraint on the CKM
  angle $φ_2$","We present a measurement of the branching fraction and fraction of
longitudinal polarization of $B^0 \to \rho^+ \rho^-$ decays, which have two
$\pi^0$'s in the final state. We also measure time-dependent $CP$ violation
parameters for decays into longitudinally polarized $\rho^+ \rho^-$ pairs. This
analysis is based on a data sample containing $(387\pm6) \times 10^6$ \BBbar
pairs collected with the Belle~II detector at the SuperKEKB asymmetric-energy
$e^+e^-$ collider in 2019-2022. We obtain ${B}(B^0\to\rho^+\rho^-) = (2.88
^{+0.23}_{-0.22} {}^{+0.29}_{-0.27}) \times 10^{-5}, f_{L} = 0.921
^{+0.024}_{-0.025} {}^{+0.017}_{-0.015}$, $S = -0.26\pm0.19\pm0.08$, and $C =
-0.02\pm0.12^{+0.06}_{-0.05}$, where the first uncertainties are statistical
and the second are systematic. We use these results to perform an isospin
analysis to constrain the CKM angle $\phi_2$ and obtain two solutions; the
result consistent with other Standard Model constraints is $\phi_2 =
(92.6^{+4.5}_{-4.8})^\circ$.",2024-12-27T13:00:10Z,http://arxiv.org/abs/2412.19624v1,"Belle II Collaboration, I. Adachi, L. Aggarwal, H. Ahmed, N. Akopov, M. Alhakami, A. Aloisio, N. Althubiti, N. Anh Ky, D. M. Asner, H. Atmacan, V. Aushev, M. Aversano, R. Ayad, V. Babu, N. K. Baghel, P. Bambade, Sw. Banerjee, M. Barrett, M. Bartl, J. Baudot, A. Baur, A. Beaubien, J. Becker, J. V. Bennett, V. Bertacchi, M. Bertemes, E. Bertholet, M. Bessner, S. Bettarini, B. Bhuyan, D. Biswas, A. Bobrov, D. Bodrov, A. Bolz, A. Bondar, J. Borah, A. Boschetti, A. Bozek, M. Bračko, P. Branchini, R. A. Briere, T. E. Browder, A. Budano, S. Bussino, Q. Campagna, M. Campajola, G. Casarosa, C. Cecchi, J. Cerasoli, M. -C. Chang, P. Chang, R. Cheaib, P. Cheema, B. G. Cheon, K. Chilikin, K. Chirapatpimol, H. -E. Cho, K. Cho, S. -J. Cho, S. -K. Choi, S. Choudhury, J. Cochran, L. Corona, J. X. Cui, E. De La Cruz-Burelo, S. A. De La Motte, G. De Nardo, G. De Pietro, R. de Sangro, M. Destefanis, S. Dey, F. Di Capua, J. Dingfelder, Z. Doležal, I. Domínguez Jiménez, T. V. Dong, X. Dong, M. Dorigo, D. Dossett, K. Dugic, G. Dujany, P. Ecker, J. Eppelt, P. Feichtinger, T. Ferber, T. Fillinger, C. Finck, G. Finocchiaro, A. Fodor, F. Forti, B. G. Fulsom, A. Gabrielli, E. Ganiev, M. Garcia-Hernandez, R. Garg, G. Gaudino, V. Gaur, A. Gaz, A. Gellrich, G. Ghevondyan, D. Ghosh, H. Ghumaryan, G. Giakoustidis, R. Giordano, A. Giri, P. Gironella Gironell, A. Glazov, B. Gobbo, R. Godang, O. Gogota, P. Goldenzweig, W. Gradl, E. Graziani, D. Greenwald, Z. Gruberová, Y. Guan, K. Gudkova, I. Haide, T. Hara, C. Harris, K. Hayasaka, S. Hazra, C. Hearty, M. T. Hedges, A. Heidelbach, I. Heredia de la Cruz, M. Hernández Villanueva, T. Higuchi, M. Hoek, M. Hohmann, R. Hoppe, P. Horak, C. -L. Hsu, T. Humair, T. Iijima, K. Inami, N. Ipsita, A. Ishikawa, R. Itoh, M. Iwasaki, D. Jacobi, W. W. Jacobs, E. -J. Jang, Y. Jin, A. Johnson, H. Junkerkalefeld, M. Kaleta, A. B. Kaliyar, J. Kandra, F. Keil, C. Ketter, C. Kiesling, C. -H. Kim, D. Y. Kim, J. -Y. Kim, K. -H. Kim, Y. -K. Kim, K. Kinoshita, P. Kodyš, T. Koga, S. Kohani, K. Kojima, A. Korobov, S. Korpar, E. Kovalenko, R. Kowalewski, P. Križan, P. Krokovny, T. Kuhr, Y. Kulii, R. Kumar, K. Kumara, T. Kunigo, A. Kuzmin, Y. -J. Kwon, S. Lacaprara, K. Lalwani, T. Lam, L. Lanceri, J. S. Lange, T. S. Lau, M. Laurenza, R. Leboucher, F. R. Le Diberder, M. J. Lee, C. Lemettais, P. Leo, L. K. Li, Q. M. Li, W. Z. Li, Y. Li, Y. B. Li, Y. P. Liao, J. Libby, J. Lin, S. Lin, M. H. Liu, Q. Y. Liu, Z. Q. Liu, D. Liventsev, S. Longo, T. Lueck, C. Lyu, Y. Ma, C. Madaan, M. Maggiora, S. P. Maharana, R. Maiti, G. Mancinelli, R. Manfredi, E. Manoni, M. Mantovano, D. Marcantonio, S. Marcello, C. Marinas, C. Martellini, A. Martens, A. Martini, T. Martinov, L. Massaccesi, M. Masuda, K. Matsuoka, D. Matvienko, S. K. Maurya, M. Maushart, J. A. McKenna, F. Meier, D. Meleshko, M. Merola, C. Miller, M. Mirra, S. Mitra, K. Miyabayashi, H. Miyake, G. B. Mohanty, S. Mondal, S. Moneta, H. -G. Moser, R. Mussa, I. Nakamura, M. Nakao, Y. Nakazawa, M. Naruki, Z. Natkaniec, A. Natochii, M. Nayak, G. Nazaryan, M. Neu, S. Nishida, S. Ogawa, R. Okubo, H. Ono, Y. Onuki, G. Pakhlova, S. Pardi, K. Parham, H. Park, J. Park, K. Park, S. -H. Park, A. Passeri, S. Patra, T. K. Pedlar, I. Peruzzi, R. Peschke, R. Pestotnik, L. E. Piilonen, P. L. M. Podesta-Lerma, T. Podobnik, S. Pokharel, C. Praz, S. Prell, E. Prencipe, M. T. Prim, H. Purwar, S. Raiz, K. Ravindran, J. U. Rehman, M. Reif, S. Reiter, M. Remnev, L. Reuter, D. Ricalde Herrmann, I. Ripp-Baudot, G. Rizzo, M. Roehrken, J. M. Roney, A. Rostomyan, N. Rout, Y. Sakai, D. A. Sanders, S. Sandilya, L. Santelj, V. Savinov, B. Scavino, C. Schwanda, A. J. Schwartz, Y. Seino, A. Selce, K. Senyo, J. Serrano, M. E. Sevior, C. Sfienti, W. Shan, X. D. Shi, T. Shillington, J. -G. Shiu, D. Shtol, B. Shwartz, A. Sibidanov, F. Simon, J. Skorupa, R. J. Sobie, M. Sobotzik, A. Soffer, A. Sokolov, E. Solovieva, S. Spataro, B. Spruck, W. Song, M. Starič, P. Stavroulakis, S. Stefkova, R. Stroili, J. Strube, M. Sumihama, K. Sumisawa, N. Suwonjandee, H. Svidras, M. Takizawa, U. Tamponi, K. Tanida, F. Tenchini, A. Thaller, O. Tittel, R. Tiwary, E. Torassa, K. Trabelsi, I. Tsaklidis, I. Ueda, T. Uglov, K. Unger, Y. Unno, K. Uno, S. Uno, P. Urquijo, Y. Ushiroda, S. E. Vahsen, R. van Tonder, K. E. Varvell, M. Veronesi, A. Vinokurova, V. S. Vismaya, L. Vitale, V. Vobbilisetti, R. Volpe, M. Wakai, S. Wallner, M. -Z. Wang, A. Warburton, M. Watanabe, S. Watanuki, C. Wessel, E. Won, X. P. Xu, B. D. Yabsley, S. Yamada, W. Yan, J. Yelton, J. H. Yin, K. Yoshihara, J. Yuan, Y. Yusa, L. Zani, V. Zhilich, J. S. Zhou, Q. D. Zhou, L. Zhu, R. Žlebčík"
Signatures of prediction during natural listening in MEG data?,"The brain uses contextual information and prior knowledge to anticipate
upcoming content during language comprehension. Recent research has shown
predictive signals can be revealed in pre-onset ECoG activity during
naturalistic narrative listening, by building encoding models based on word
embeddings from Large Language Models (LLMs). Similarly, evidence for
long-range predictive encoding has been observed in fMRI data, where
incorporating embeddings for multiple upcoming words in a narrative improves
alignment with brain activity. This study examines whether similar predictive
information can be detected in MEG, a technique with higher temporal resolution
than fMRI but a lower signal-to-noise ratio than ECoG. Our findings indicate
that MEG captures pre-onset representations up to 1 second before word onset,
consistent with ECoG results. However, unlike fMRI findings, incorporating
future word embeddings did not enhance MEG encoding, even for one word into the
future, which suggests that the pre-onset encoding may not reflect predictive
processing. This work demonstrates that MEG combined with LLMs is a valuable
approach for studying language processing in naturalistic narratives and
highlights the need to study further what constitutes evidence for prediction
during natural listening.",2024-12-27T12:49:03Z,http://arxiv.org/abs/2412.19622v1,"Sahel Azizpour, Britta U. Westner, Jakub Szewczyk, Umut Güçlü, Linda Geerligs"
"The Key Steps and Distinct Performance Trends of Pyrrolic vs. Pyridinic
  M-N-C Catalysts in Electrocatalytic Nitrate Reduction","Electrochemical nitrate reduction reaction(NO3RR)offers a sustainable route
for ambient ammonia synthesis. While metal-nitrogen-carbon (M-N-C) single-atom
catalysts have emerged as promising candidates for NO3RR, the
structure-activity relations underlying their catalytic behavior remain to be
elucidated. Through systematic analysis of reported experimental data and
pH-field coupled microkinetic modelling on a reversible hydrogen electrode
(RHE) scale, we reveal that the coordination-dependent activity originates from
distinct scaling relations governed by metal-intermediate interactions.
M-N-Pyrrolic catalysts demonstrate higher turnover frequencies for ammonia
production, whereas M-N-Pyridinic catalysts exhibit broader activity ranges
across the activity volcano plot. Meanwhile, the adsorption and protonation of
nitrate, which is a step often dismissed and/or assumed to be simultaneous in
many previous reports, is identified to be the rate-determining step (RDS) in
NO3RR. Remarkably, our subsequent experimental validation confirms the
theoretical predictions under both neutral and alkaline conditions. This study
offers a comprehensive mechanistic framework for interpreting the
electrocatalytic activity of M-N-C catalysts in NO3RR, showing that a classical
thermodynamic limiting-potential model is not sufficiently accurate to capture
the RDS and the catalytic performance trends of different materials (even on
M-N-Pyrrolic and M-N-Pyridinic catalysts). These findings provide brand new
insights into the reaction mechanism of NO3RR and establish fundamental design
principles for electrocatalytic ammonia synthesis.",2024-12-27T12:23:09Z,http://arxiv.org/abs/2412.19615v1,"Qiuling Jiang, Mingyao Gu, Tianyi Wang, Fangzhou Liu, Xin Yang, Di Zhang, Zhijian Wu, Ying Wang, Li Wei, Hao Li"
Anisotropic Band Flattening in Twisted Bilayer of M-Valley MXenes,"Experimental studies on moir\'e materials have predominantly focused on
twisted hexagonal lattice with low-energy states near the $\Gamma$- or
K-points. These materials, characterized by isotropic low-energy dispersion,
are fundamentally distinct from those with anisotropic properties. Here we
introduce a series of semiconducting transition metal carbides (MXenes)
$M_2$C$T_2$ ($M$ = Ti, Zr, Hf, Sc, Y; $T$ = O, F, Cl) as a novel platform for
M-valley moir\'e materials. Take Ti$_2$CO$_2$ and Zr$_2$CO$_2$ as
representative examples, large-scale \emph{ab initio} calculations show that
their AB-stacked twisted homobilayer features three three-fold rotational
symmetry related M-valleys with time-reserval symmetry and giant anisotropic
band flattening. We derive a simplified moir\'e Hamiltonian for these systems
and conduct a detailed analysis of their band structures, where the origins of
anisotropic band flattening are clearly elucidated. This research broadens the
scope of moir\'e materials, where the valley- and spin-degenerate
two-dimensional array of quasi-one-dimensional system could serve as a
potential platform for realizing many interesting correlated phases.",2024-12-27T12:20:15Z,http://arxiv.org/abs/2412.19613v1,"Kejie Bao, Huan Wang, Zhaochen Liu, jing Wang"
"Machine Generated Product Advertisements: Benchmarking LLMs Against
  Human Performance","This study compares the performance of AI-generated and human-written product
descriptions using a multifaceted evaluation model. We analyze descriptions for
100 products generated by four AI models (Gemma 2B, LLAMA, GPT2, and ChatGPT 4)
with and without sample descriptions, against human-written descriptions. Our
evaluation metrics include sentiment, readability, persuasiveness, Search
Engine Optimization(SEO), clarity, emotional appeal, and call-to-action
effectiveness. The results indicate that ChatGPT 4 performs the best. In
contrast, other models demonstrate significant shortcomings, producing
incoherent and illogical output that lacks logical structure and contextual
relevance. These models struggle to maintain focus on the product being
described, resulting in disjointed sentences that do not convey meaningful
information. This research provides insights into the current capabilities and
limitations of AI in the creation of content for e-Commerce.",2024-12-27T12:11:50Z,http://arxiv.org/abs/2412.19610v1,Sanjukta Ghosh
Infinitary combinatorics in condensed math and strong homology,"Recent advances in our understanding of higher derived limits carry multiple
implications in the fields of condensed and pyknotic mathematics, as well as
for the study of strong homology. These implications are thematically diverse,
pertaining, for example, to the sheaf theory of extremally disconnected spaces,
to Banach--Smith duality, to the productivity of compact projective condensed
anima, and to the structure of the derived category of condensed abelian
groups. Underlying each of these implications are the combinatorics of
multidimensionally coherent families of functions of small infinite cardinal
height, and it is for this reason that we convene accounts of them together
herein.",2024-12-27T12:03:05Z,http://arxiv.org/abs/2412.19605v1,"Jeffrey Bergfalk, Chris Lambie-Hanson"
Super-bath Quantum Eigensolver,"Simulating the dynamics of a system coupled to a suitable environment is a
promising approach in quantum computing for determining the ground state of
physical systems. However, this approach requires not only the
$\textit{existence}$ of an environment that allows the system to dissipate
energy and evolve to its ground state, but also the environment's
characteristics to be $\textit{known}$ in detail. In this paper, we propose an
algorithm with a sufficient condition for achieving polynomial-time complexity
in ground state preparation: the existence of an environment that enables the
system to evolve to its ground state in polynomial time, while such
environment's details may remain $\textit{unknown}$. The proposed algorithm is
Super-bath Quantum Eigensolver, which solves the system's ground state by
utilizing quasi-steady state preparation and simulating the coupling between
the system and the super-bath. Supported by experimental lifetime data of
nuclear metastable states, we suggest that our algorithm is applicable to
determine nuclear ground states in polynomial time. These results highlight the
potential advantage of quantum computing in addressing ground state problems in
real-world physical systems.",2024-12-27T11:46:47Z,http://arxiv.org/abs/2412.19599v1,"Tianren Wang, Zongkang Zhang, Bing-Nan Lu, Mauro Cirio, Ying Li"
Composite nature of the $T_{cc}$ state,"In 2021, LHCb collaboration reported a very narrow state in the $D^0D^0\pi^+$
mass spectrum just below the $D^{*+}D^0$ mass threshold. We consider the
influence of the Castillejo-Dalitz-Dyson (CDD) pole in the scattering amplitude
to derive a general treatment for the two-body final state interaction near its
threshold. The line shape (or the energy dependent event distribution) are then
obtained, where the parameters can be fixed by fitting to the experimental data
on the $D^0D^0\pi^+$ mass spectrum. Within our method the data are quite well
reproduced. The pole structure in the complex energy plane indicates the bound
state structure of the $T_{cc}$ state. The compositeness as a measure of
molecule component in its hadron wave function is predicted to be
$0.23_{-0.09}^{+0.40}$. The non-molecular component, e.g., the compact
tetraquark also takes a non-negligible portion.",2024-12-27T11:42:44Z,http://arxiv.org/abs/2412.19597v1,"Xian-Wei Kang, Wen-Shuo Ding"
"SocRATES: Towards Automated Scenario-based Testing of Social Navigation
  Algorithms","Current social navigation methods and benchmarks primarily focus on proxemics
and task efficiency. While these factors are important, qualitative aspects
such as perceptions of a robot's social competence are equally crucial for
successful adoption and integration into human environments. We propose a more
comprehensive evaluation of social navigation through scenario-based testing,
where specific human-robot interaction scenarios can reveal key robot
behaviors. However, creating such scenarios is often labor-intensive and
complex. In this work, we address this challenge by introducing a pipeline that
automates the generation of context-, and location-appropriate social
navigation scenarios, ready for simulation. Our pipeline transforms simple
scenario metadata into detailed textual scenarios, infers pedestrian and robot
trajectories, and simulates pedestrian behaviors, which enables more controlled
evaluation. We leverage the social reasoning and code-generation capabilities
of Large Language Models (LLMs) to streamline scenario generation and
translation. Our experiments show that our pipeline produces realistic
scenarios and significantly improves scenario translation over naive LLM
prompting. Additionally, we present initial feedback from a usability study
with social navigation experts and a case-study demonstrating a scenario-based
evaluation of three navigation algorithms.",2024-12-27T11:33:19Z,http://arxiv.org/abs/2412.19595v1,"Shashank Rao Marpally, Pranav Goyal, Harold Soh"
"Quasicrystal problem -- on rigidity of non-periodic structures from
  statistical mechanics point of view","We present a brief history of quasicrystals and a short introduction to
classical lattice-gas models of interacting particles. We discuss stability of
non-periodic tilings and one-dimensional sequences of symbols seen as ground
states of some hamiltonians. We argue that some sort of homogeneity, the
so-called Strict Boundary Condition, is necessary for stability of non-periodic
ground states against small perturbations of interactions and thermal
fluctuations.",2024-12-27T11:28:02Z,http://arxiv.org/abs/2412.19594v1,Jacek Miȩkisz
A counterexample to a Brenti-Carnevale conjecture,"Recently, F. Brenti put a preprint on the arXiv with several interesting open
problems on Coxeter groups and unimodality. In this note, we refute one of
these conjectures with a counterexample and provide supporting data related to
it. This work serves as an initial step toward further exploration of the
topic.",2024-12-27T11:26:53Z,http://arxiv.org/abs/2412.19593v1,"Nathan Chapelier-Laget, Jean Fromentin"
"Theoretical Investigation of (Zn, Co) co-Doped BaTiO3 for Advanced
  Energy and Photonic Applications","In light of recent advancements in energy technology, there is an urgent need
for lead-free barium titanate (BTO) -based materials that exhibit remarkable
ferroelectric and photoelectric properties. Notwithstanding the considerable
experimental advances, a theoretical understanding from the electron and atomic
perspectives remains elusive. This study employs the generalized gradient
approximation plane wave pseudopotential technique to investigate the
structural, electronic, ferroelectric, and optical properties of (Zn,Co)
co-doped BaTiO3 (BZCT) based on density functional theory. The objective is to
ascertain the extent of performance enhancement and the underlying mechanism of
(Zn,Co) co-doping on barium titanate. Our findings reveal that incorporating
(Zn,Co) into the BaTiO3 lattice significantly augments the tetragonality of the
unit cell. Moreover, the ferroelectric properties are enhanced, with a
spontaneous polarization stronger than that observed in pure BTO, exhibiting
excellent ferroelectricity. The results of the Hubbard+U algorithm indicate
that the band gap of BZCT is reduced. Concurrently, the enhanced ferroelectric
polarization increases the built-in electric field of the material,
facilitating the separation of photogenerated carriers and improving optical
absorption. Consequently, the optical absorption ability and photorefractive
ability are effectively enhanced. BZCT, with its high spontaneous polarization
and outstanding optical properties, can be a promising candidate material in
energy storage and photovoltaics.",2024-12-27T11:26:38Z,http://arxiv.org/abs/2412.19591v1,"Zheng Kang, Mei Wu, Yiyu Feng, Jiahao Li, Jieming Zhang, Haiyi Tian, Ancheng Wang, Yunkai Wu, Xu Wang"
"ViDTA: Enhanced Drug-Target Affinity Prediction via Virtual Graph Nodes
  and Attention-based Feature Fusion","Drug-target interaction is fundamental in understanding how drugs affect
biological systems, and accurately predicting drug-target affinity (DTA) is
vital for drug discovery. Recently, deep learning methods have emerged as a
significant approach for estimating the binding strength between drugs and
target proteins. However, existing methods simply utilize the drug's local
information from molecular topology rather than global information.
Additionally, the features of drugs and proteins are usually fused with a
simple concatenation operation, limiting their effectiveness. To address these
challenges, we proposed ViDTA, an enhanced DTA prediction framework. We
introduce virtual nodes into the Graph Neural Network (GNN)-based drug feature
extraction network, which acts as a global memory to exchange messages more
efficiently. By incorporating virtual graph nodes, we seamlessly integrate
local and global features of drug molecular structures, expanding the GNN's
receptive field. Additionally, we propose an attention-based linear feature
fusion network for better capturing the interaction information between drugs
and proteins. Experimental results evaluated on various benchmarks including
Davis, Metz, and KIBA demonstrate that our proposed ViDTA outperforms the
state-of-the-art baselines.",2024-12-27T11:19:10Z,http://arxiv.org/abs/2412.19589v1,"Minghui Li, Zikang Guo, Yang Wu, Peijin Guo, Yao Shi, Shengshan Hu, Wei Wan, Shengqing Hu"
DAS3R: Dynamics-Aware Gaussian Splatting for Static Scene Reconstruction,"We propose a novel framework for scene decomposition and static background
reconstruction from everyday videos. By integrating the trained motion masks
and modeling the static scene as Gaussian splats with dynamics-aware
optimization, our method achieves more accurate background reconstruction
results than previous works. Our proposed method is termed DAS3R, an
abbreviation for Dynamics-Aware Gaussian Splatting for Static Scene
Reconstruction. Compared to existing methods, DAS3R is more robust in complex
motion scenarios, capable of handling videos where dynamic objects occupy a
significant portion of the scene, and does not require camera pose inputs or
point cloud data from SLAM-based methods. We compared DAS3R against recent
distractor-free approaches on the DAVIS and Sintel datasets; DAS3R demonstrates
enhanced performance and robustness with a margin of more than 2 dB in PSNR.
The project's webpage can be accessed via \url{https://kai422.github.io/DAS3R/}",2024-12-27T10:59:46Z,http://arxiv.org/abs/2412.19584v1,"Kai Xu, Tze Ho Elden Tse, Jizong Peng, Angela Yao"
"A Comparative Study of Machine Unlearning Techniques for Image and Text
  Classification Models","Machine Unlearning has emerged as a critical area in artificial intelligence,
addressing the need to selectively remove learned data from machine learning
models in response to data privacy regulations. This paper provides a
comprehensive comparative analysis of six state-of-theart unlearning techniques
applied to image and text classification tasks. We evaluate their performance,
efficiency, and compliance with regulatory requirements, highlighting their
strengths and limitations in practical scenarios. By systematically analyzing
these methods, we aim to provide insights into their applicability,
challenges,and tradeoffs, fostering advancements in the field of ethical and
adaptable machine learning.",2024-12-27T10:58:55Z,http://arxiv.org/abs/2412.19583v1,"Omar M. Safa, Mahmoud M. Abdelaziz, Mustafa Eltawy, Mohamed Mamdouh, Moamen Gharib, Salaheldin Eltenihy, Nagia M. Ghanem, Mohamed M. Ismail"
"Readout of strongly coupled NV center-pair spin states with deep neural
  networks","Optically addressable electron spin clusters are of interest for quantum
computation, simulation and sensing. However, with interaction length scales of
a few tens of nanometers in the strong coupling regime, they are unresolved in
conventional confocal microscopy, making individual readout problematic. Here
we show that when using a single shot readout technique, collective states of
the combined register space become accessible. By using spin to charge
conversion of the defects we draw the connection between the intricate photon
count statistics with spin state tomography using deep neural networks. This
approach is particularly versatile with further scaling the number of
constituent spins in a cluster due to complexity of the analytical treatment.
We perform a proof of concept measurement of the correlated classical signal,
paving the way for using our technique in realistic applications.",2024-12-27T10:56:04Z,http://arxiv.org/abs/2412.19581v1,"Matthew Joliffe, Vadim Vorobyov, Jörg Wrachtrup"
"Muonium fine structure: theory update, tests of Lorentz violation and
  experimental prospects","We review the status of the QED calculations for the muonium
$2S_{1/2}-2P_{3/2}$ energy interval and provide the updated theoretical value
of $9874.357\pm0.001\,\mathrm{MHz}$. Additionally, we present a model for
probing Lorentz-violating coefficients within the Standard Model Extension
framework using the fine structure measurement in the presence and absence of a
weak external magnetic field, enabling novel tests of CPT and Lorentz symmetry.
Using Monte Carlo simulations, we estimate that a precision of $\sim
10\,\mathrm{kHz}$ on the isolated $2S_{1/2}, F=1 - 2P_{3/2}, F=1$ transition
could be achievable employing Ramsey's separate oscillatory fields (SOF)
technique. Collecting the required statics will become feasible with the
upcoming High-Intensity Muon Beam (HiMB) at the Paul Scherrer Institute (PSI)
in Switzerland. These advancements will enable precise tests of radiative QED
corrections and nuclear self-energy contributions, while also providing tests
of new physics and sensitivity to unconstrained coefficients for Lorentz
violation within the Standard Model Extension framework.",2024-12-27T10:52:57Z,http://arxiv.org/abs/2412.19580v1,"Philipp Blumer, Svenja Geissmann, Arnaldo J. Vargas, Gianluca Janka, Ben Ohayon, Paolo Crivelli"
"Graph-attention-based Casual Discovery with Trust Region-navigated
  Clipping Policy Optimization","In many domains of empirical sciences, discovering the causal structure
within variables remains an indispensable task. Recently, to tackle with
unoriented edges or latent assumptions violation suffered by conventional
methods, researchers formulated a reinforcement learning (RL) procedure for
causal discovery, and equipped REINFORCE algorithm to search for the
best-rewarded directed acyclic graph. The two keys to the overall performance
of the procedure are the robustness of RL methods and the efficient encoding of
variables. However, on the one hand, REINFORCE is prone to local convergence
and unstable performance during training. Neither trust region policy
optimization, being computationally-expensive, nor proximal policy optimization
(PPO), suffering from aggregate constraint deviation, is decent alternative for
combinatory optimization problems with considerable individual subactions. We
propose a trust region-navigated clipping policy optimization method for causal
discovery that guarantees both better search efficiency and steadiness in
policy optimization, in comparison with REINFORCE, PPO and our prioritized
sampling-guided REINFORCE implementation. On the other hand, to boost the
efficient encoding of variables, we propose a refined graph attention encoder
called SDGAT that can grasp more feature information without priori
neighbourhood information. With these improvements, the proposed method
outperforms former RL method in both synthetic and benchmark datasets in terms
of output results and optimization robustness.",2024-12-27T10:50:43Z,http://arxiv.org/abs/2412.19578v1,"Shixuan Liu, Yanghe Feng, Keyu Wu, Guangquan Cheng, Jincai Huang, Zhong Liu"
"Gauging or extending bulk and boundary conformal field theories:
  Application to bulk and domain wall problem in topological matter and their
  descriptions by (mock) modular covariant","We study gauging operations (or group extensions) in (smeared) boundary
conformal field theories (BCFTs) and bulk conformal field theories and their
applications to various phenomena in topologically ordered systems. We apply
the resultant theories to the correspondence between the renormalization group
(RG) flow of CFTs and the classification of topological quantum field theories
in the testable information of general classes of partition functions. One can
obtain the bulk topological properties of $2+1$ dimensional topological ordered
phase corresponding to the massive RG flow of $1+1$ dimensional systems, or
smeared BCFT. We present an obstruction of mass condensation for smeared BCFT
analogous to the Lieb-Shultz-Mattis theorem for noninvertible symmetry. Related
to the bulk topological degeneracies in $2+1$ dimensions and quantum phases in
$1+1$ dimensions we construct a new series of BCFT. We also investigate the
implications of the massless RG flow of $1+1$ dimensional CFT to $2+1$
dimensional topological order which corresponds to the earlier proposal by L.
Kong and H. Zheng in [Nucl. Phys. B 966 (2021), 115384], arXiv:1912.01760
closely related to the integer-spin simple current by Schellekens and
Gato-Rivera. We study the properties of the product of two CFTs connected by
the two kinds of massless flows. The (mock) modular covariants appearing in the
analysis seem to contain new ones. By applying the folding trick to the coupled
model, we provide a general method to solve the gapped and charged domain wall.
One can obtain the general phenomenology of the transportation of anyons
through the domain wall. Our work gives a unified direction for the future
theoretical and numerical studies of the topological phase based on the
established data of classifications of conformal field theories or modular
invariants.",2024-12-27T10:46:30Z,http://arxiv.org/abs/2412.19577v1,Yoshiki Fukusumi
"Superintegrability of the Wilson family of matrix models and moments of
  multivariable orthogonal polynomials","We present new examples of superintegrable matrix/eigenvalue models. These
examples arise as a result of the exploration of the relationship between the
theory of superintegrability and multivariate orthogonal polynomials. The new
superintegrable examples are built upon the multivariate generalizations of the
Meixner-Pollaczek and Wilson polynomials and their respective measures. From
the perspective of multivariate orthogonal polynomials in this work we propose
expressions for (generalized) moments of the respective multi-variable
measures. From the perspective of superintegrability we uncover a couple of new
phenomena such as the deviation from Schur polynomials as the superintegrable
basis without any deformation and new combinatorial structures appearing in the
answers.",2024-12-27T10:39:43Z,http://arxiv.org/abs/2412.19574v1,Victor Mishnyakov
"The possible long-term periodic variability of the extremely luminous
  quasar WISE J090924.01+000211.1","The extremely luminous infrared galaxy (ELIRG), WISE J090924.01+000211.1
(hereafter; WISE J0909+0002, $z=1.87$) is an extraordinary object with a quasar
aspect. This study performs monitoring observations of WISE J0909+0002 with the
105 cm Murikabushi telescope, Okayama and Akeno 50 cm telescopes/MITSuME ($g'$,
$R_{\rm c}$, and $I_{\rm c}$ bands), and the SaCRA 55 cm telescope/MuSaSHI
($r$, $i$, and $z$ bands). We obtain the following results by combining the
UV/optical light curves of the CRTS, Pan-STARRS, and ZTF archive data, and our
observational data: (1) the light curves of WISE J0909+0002 present
quasi-periodic (sinusoidal) oscillations with the rest-frame period of $\sim$
660$-$689 day; (2) the structure functions of WISE J0909+0002 do not show a
damped random walk (DRW) trend; (3) the mock DRW light curves present
periodic-like trend on rare occasions in 10000 simulations; (4) the
relativistic boost scenario is favored, since the relation between variability
amplitude and power-law slope ratio is consistent with the theoretical
prediction of this scenario, and a substantial parameter space exists between
the inclination angles and the black hole mass; (5) the circumbinary disk model
is difficult to explain the spectral energy distribution of our target; (6) the
significant radio flux density of WISE J0909+0002 is not detected from the VLA
FIRST Survey, thus the radio jet precession scenario is ruled out. From our
results, the Doppler boost scenario is likely as a cause of the periodic
variability, consequently the quasi-periodic oscillations in WISE J0909+0002 is
possibly interpreted by a supermassive blackhole binary. Additional
observations to investigate the continuity of the periodic trend would bring
new insights into mechanisms of the quasi-periodic oscillations and/or ELIRGs.",2024-12-27T10:33:11Z,http://arxiv.org/abs/2412.19573v1,"Takashi Horiuchi, Yoshiki Toba, Toru Misawa, Katsuhiro L. Murata, Keisuke Isogai, Yoichi Yatsu, Ichiro Takahashi, Mahito Sasada, Masafumi Niwano, Narikazu Higuchi, Shunsuke Hayatsu, Hibiki Seki, Yumiko Oasa, Rikuto Sato"
"Nonminimally coupled Dark Matter in Clusters of Galaxies: a fully
  comprehensive analysis","In this study, we explore how a non-minimal coupling between dark matter and
gravity can affect the behavior of dark matter in galaxy clusters. We have
considered the case of a disformal coupling, which leads to a modification of
the Poisson equation. Building on an earlier work, we expand the analysis
considering all possible disformal coupling scenarios and employing various
dark matter density profiles. In doing so, we aim to constrain the key
parameter in our model, the characteristic coupling length. To achieve this, we
analyze data from a combination of strong and weak lensing using three
statistical approaches: a single cluster fitting procedure, a joint analysis,
and one with stacked profiles. Our findings show that the coupling length is
typically very small, thus being fully consistent with general relativity,
although with an upper limit at $1\sigma$ which is of the order of $100$ kpc.",2024-12-27T10:12:29Z,http://arxiv.org/abs/2412.19569v1,"Saboura Zamani, Vincenzo Salzano, Dario Bettoni"
Multiplicative Chern insulator,"We study multiplicative Chern insulators (MCIs) as canonical examples of
multiplicative topological phases of matter. Constructing the MCI Bloch
Hamiltonian as a symmetry-protected tensor product of two topologically
non-trivial parent Chern insulators (CIs), we study two-dimensional (2D) MCIs
and introduce 3D mixed MCIs, constructed by requiring the two 2D parent
Hamiltonians share only one momentum component. We study the 2D MCI response to
time reversal symmetric flux insertion, observing a $4\pi$ Aharonov-Bohm
effect, relating these topological states to fractional quantum Hall states via
the effective field theory of the quantum skyrmion Hall effect. As part of this
response, we observe evidence of quantisation of a proposed topological
invariant for compactified many-body states, to a rational number, suggesting
higher-dimensional topology may also be relevant. Finally, we study effects of
bulk perturbations breaking the symmetry-protected tensor product structure of
the child Hamiltonian, finding the MCI evolves adiabatically into a topological
skyrmion phase.",2024-12-27T10:10:13Z,http://arxiv.org/abs/2412.19566v1,"Archi Banerjee, Michał J. Pacholski, Ashley M. Cook"
Effective field theory of the quantum skyrmion Hall effect,"Motivated by phenomenology of myriad recently-identified topologically
non-trivial phases of matter, we introduce effective field theories (EFTs) for
the quantum skyrmion Hall effect (QSkHE). We employ a single, unifying
generalisation for this purpose: in essence, a lowest Landau level projection
defining a non-commutative, fuzzy sphere with position coordinates proportional
to SU(2) generators of matrix representation size $N\times N$, may host an
intrinsically 2+1 dimensional, topologically non-trivial many-body state for
small $N$ as well as large $N$. That is, isospin degrees of freedom associated
with a matrix Lie algebra with $N \times N$ generators potentially encode some
finite number of spatial dimensions for $N\ge 2$, a regime in which isospin has
previously been treated as a label. This statement extends to more general
$p$-branes subjected to severe fuzzification as well as membranes. As a
consequence of this generalisation, systems with $d$ Cartesian spatial
coordinates and isospin degrees of freedom encoding an additional $\delta$
fuzzy coset space coordinates can realise topologically non-trivial states of
intrinsic dimensionality up to $d$+$\delta$+1. We therefore identify gauge
theories with extra fuzzy dimensions generalised to retain dependence upon
gauge fields over fuzzy coset spaces even for severe fuzzification (small $N$),
as EFTs for the QSkHE. We furthermore generalise these EFTs to space manifolds
with local product structure exploiting the dimensional hierarchy of (fuzzy)
spheres. For this purpose, we introduce methods of anisotropic fuzzification
and propose formulating topological invariants on fuzzy coset spaces as
artifacts of projecting matrix Lie algebras to occupied subspaces. Importantly,
we focus on phenomenology indicating the 2+1 D SU(2) gauge theory should be
generalised using this machinery, and serves as a minimal EFT of the QSkHE.",2024-12-27T10:09:48Z,http://arxiv.org/abs/2412.19565v1,"Vinay Patil, Rafael Flores-Calderón, Ashley M. Cook"
Three Theorems on Negami's Planar Cover Conjecture,"A long-standing Conjecture of S. Negami states that a connected graph has a
finite planar cover if and only if it embeds in the projective plane. It is
known that the Conjecture is equivalent to the fact that \emph{the graph
$K_{1,2, 2, 2}$ has no finite planar cover}. We prove three theorems showing
that the graph $K_{1,2, 2, 2}$ admits no planar cover with certain structural
properties, and that the minimal planar cover of $K_{1,2, 2, 2}$ (if it exists)
must be $4$-connected.",2024-12-27T10:01:38Z,http://arxiv.org/abs/2412.19560v1,"Dickson Annor, Yuri Nikolayevsky, Michael Payne"
"Structural Similarity in Deep Features: Image Quality Assessment Robust
  to Geometrically Disparate Reference","Image Quality Assessment (IQA) with references plays an important role in
optimizing and evaluating computer vision tasks. Traditional methods assume
that all pixels of the reference and test images are fully aligned. Such
Aligned-Reference IQA (AR-IQA) approaches fail to address many real-world
problems with various geometric deformations between the two images. Although
significant effort has been made to attack Geometrically-Disparate-Reference
IQA (GDR-IQA) problem, it has been addressed in a task-dependent fashion, for
example, by dedicated designs for image super-resolution and retargeting, or by
assuming the geometric distortions to be small that can be countered by
translation-robust filters or by explicit image registrations. Here we rethink
this problem and propose a unified, non-training-based Deep Structural
Similarity (DeepSSIM) approach to address the above problems in a single
framework, which assesses structural similarity of deep features in a simple
but efficient way and uses an attention calibration strategy to alleviate
attention deviation. The proposed method, without application-specific design,
achieves state-of-the-art performance on AR-IQA datasets and meanwhile shows
strong robustness to various GDR-IQA test cases. Interestingly, our test also
shows the effectiveness of DeepSSIM as an optimization tool for training image
super-resolution, enhancement and restoration, implying an even wider
generalizability. \footnote{Source code will be made public after the review is
completed.",2024-12-27T09:51:23Z,http://arxiv.org/abs/2412.19553v1,"Keke Zhang, Weiling Chen, Tiesong Zhao, Zhou Wang"
Boolean combinations of graphs,"Boolean combinations allow combining given combinatorial objects to obtain
new, potentially more complicated, objects. In this paper, we initiate a
systematic study of this idea applied to graphs. In order to understand
expressive power and limitations of boolean combinations in this context, we
investigate how they affect different combinatorial and structural properties
of graphs, in particular $\chi$-boundedness, as well as characterize the
structure of boolean combinations of graphs from various classes.",2024-12-27T09:47:53Z,http://arxiv.org/abs/2412.19551v1,"Sarosh Adenwalla, Samuel Braunfeld, John Sylvester, Viktor Zamaraev"
Quantiles under ambiguity and risk sharing,"Choquet capacities and integrals are central concepts in decision making
under ambiguity or model uncertainty, pioneered by Schmeidler. Motivated by
risk optimization problems for quantiles under ambiguity, we study the subclass
of Choquet integrals, called Choquet quantiles, which generalizes the usual
(probabilistic) quantiles, also known as Value-at-Risk in finance, from
probabilities to capacities. Choquet quantiles share many features with
probabilistic quantiles, in terms of axiomatic representation, optimization
formulas, and risk sharing. We characterize Choquet quantiles via only one
axiom, called ordinality. We prove that the inf-convolution of Choquet
quantiles is again a Choquet quantile, leading to explicit optimal allocations
in risk sharing problems for quantile agents under ambiguity. A new class of
risk measures, Choquet Expected Shortfall, is introduced, which enjoys most
properties of the coherent risk measure Expected Shortfall. Our theory is
complemented by optimization algorithms, numerical examples, and a stylized
illustration with financial data.",2024-12-27T09:22:19Z,http://arxiv.org/abs/2412.19546v1,"Peng Liu, Tiantian Mao, Ruodu Wang"
"TARGA: Targeted Synthetic Data Generation for Practical Reasoning over
  Structured Data","Semantic parsing, which converts natural language questions into logic forms,
plays a crucial role in reasoning within structured environments. However,
existing methods encounter two significant challenges: reliance on extensive
manually annotated datasets and limited generalization capability to unseen
examples. To tackle these issues, we propose Targeted Synthetic Data Generation
(TARGA), a practical framework that dynamically generates high-relevance
synthetic data without manual annotation. Starting from the pertinent entities
and relations of a given question, we probe for the potential relevant queries
through layer-wise expansion and cross-layer combination. Then we generate
corresponding natural language questions for these constructed queries to
jointly serve as the synthetic demonstrations for in-context learning.
Experiments on multiple knowledge base question answering (KBQA) datasets
demonstrate that TARGA, using only a 7B-parameter model, substantially
outperforms existing non-fine-tuned methods that utilize close-sourced model,
achieving notable improvements in F1 scores on GrailQA(+7.7) and
KBQA-Agent(+12.2). Furthermore, TARGA also exhibits superior sample efficiency,
robustness, and generalization capabilities under non-I.I.D. settings.",2024-12-27T09:16:39Z,http://arxiv.org/abs/2412.19544v1,"Xiang Huang, Jiayu Shen, Shanshan Huang, Sitao Cheng, Xiaxia Wang, Yuzhong Qu"
Diverse Rare Sample Generation with Pretrained GANs,"Deep generative models are proficient in generating realistic data but
struggle with producing rare samples in low density regions due to their
scarcity of training datasets and the mode collapse problem. While recent
methods aim to improve the fidelity of generated samples, they often reduce
diversity and coverage by ignoring rare and novel samples. This study proposes
a novel approach for generating diverse rare samples from high-resolution image
datasets with pretrained GANs. Our method employs gradient-based optimization
of latent vectors within a multi-objective framework and utilizes normalizing
flows for density estimation on the feature space. This enables the generation
of diverse rare images, with controllable parameters for rarity, diversity, and
similarity to a reference image. We demonstrate the effectiveness of our
approach both qualitatively and quantitatively across various datasets and GANs
without retraining or fine-tuning the pretrained GANs.",2024-12-27T09:10:30Z,http://arxiv.org/abs/2412.19543v1,"Subeen Lee, Jiyeon Han, Soyeon Kim, Jaesik Choi"
Interacted Object Grounding in Spatio-Temporal Human-Object Interactions,"Spatio-temporal Human-Object Interaction (ST-HOI) understanding aims at
detecting HOIs from videos, which is crucial for activity understanding.
However, existing whole-body-object interaction video benchmarks overlook the
truth that open-world objects are diverse, that is, they usually provide
limited and predefined object classes. Therefore, we introduce a new open-world
benchmark: Grounding Interacted Objects (GIO) including 1,098 interacted
objects class and 290K interacted object boxes annotation. Accordingly, an
object grounding task is proposed expecting vision systems to discover
interacted objects. Even though today's detectors and grounding methods have
succeeded greatly, they perform unsatisfactorily in localizing diverse and rare
objects in GIO. This profoundly reveals the limitations of current vision
systems and poses a great challenge. Thus, we explore leveraging
spatio-temporal cues to address object grounding and propose a 4D
question-answering framework (4D-QA) to discover interacted objects from
diverse videos. Our method demonstrates significant superiority in extensive
experiments compared to current baselines. Data and code will be publicly
available at https://github.com/DirtyHarryLYL/HAKE-AVA.",2024-12-27T09:08:46Z,http://arxiv.org/abs/2412.19542v1,"Xiaoyang Liu, Boran Wen, Xinpeng Liu, Zizheng Zhou, Hongwei Fan, Cewu Lu, Lizhuang Ma, Yulong Chen, Yong-Lu Li"
On the approximation of spatial convolutions by PDE systems,"This paper considers the approximation problem for the spatial convolution
with a given integral kernel. The approximation to the spatial convolution by
PDE systems has been proposed to eliminate the analytical difficulties caused
by the integrals for the one-dimensional space. In this paper we establish a
PDE system approximation for the spatial convolutions in higher spatial
dimensions. We derive an approximation function for given arbitrary radial
integral kernels by linear sums of the Green functions. In the proof of this
methodology we introduce an appropriate integral transformation to show the
completeness of the basis constructed by the Green functions. From this theory,
it is possible to approximate the nonlocal operator of the convolution type
with any radial integral kernels by the linear sum of the solutions to the PDE
system. Finally, numerical examples for the approximation are demonstrated
using this method.",2024-12-27T09:07:27Z,http://arxiv.org/abs/2412.19539v1,"Hiroshi Ishii, Yoshitaro Tanaka"
"Scalable Hierarchical Reinforcement Learning for Hyper Scale Multi-Robot
  Task Planning","To improve the efficiency of warehousing system and meet huge customer
orders, we aim to solve the challenges of dimension disaster and dynamic
properties in hyper scale multi-robot task planning (MRTP) for robotic mobile
fulfillment system (RMFS). Existing research indicates that hierarchical
reinforcement learning (HRL) is an effective method to reduce these challenges.
Based on that, we construct an efficient multi-stage HRL-based multi-robot task
planner for hyper scale MRTP in RMFS, and the planning process is represented
with a special temporal graph topology. To ensure optimality, the planner is
designed with a centralized architecture, but it also brings the challenges of
scaling up and generalization that require policies to maintain performance for
various unlearned scales and maps. To tackle these difficulties, we first
construct a hierarchical temporal attention network (HTAN) to ensure basic
ability of handling inputs with unfixed lengths, and then design multi-stage
curricula for hierarchical policy learning to further improve the scaling up
and generalization ability while avoiding catastrophic forgetting.
Additionally, we notice that policies with hierarchical structure suffer from
unfair credit assignment that is similar to that in multi-agent reinforcement
learning, inspired of which, we propose a hierarchical reinforcement learning
algorithm with counterfactual rollout baseline to improve learning performance.
Experimental results demonstrate that our planner outperform other
state-of-the-art methods on various MRTP instances in both simulated and
real-world RMFS. Also, our planner can successfully scale up to hyper scale
MRTP instances in RMFS with up to 200 robots and 1000 retrieval racks on
unlearned maps while keeping superior performance over other methods.",2024-12-27T09:07:11Z,http://arxiv.org/abs/2412.19538v1,"Xuan Zhou, Xiang Shi, Lele Zhang, Chen Chen, Hongbo Li, Lin Ma, Fang Deng, Jie Chen"
"Finger in Camera Speaks Everything: Unconstrained Air-Writing for
  Real-World","Air-writing is a challenging task that combines the fields of computer vision
and natural language processing, offering an intuitive and natural approach for
human-computer interaction. However, current air-writing solutions face two
primary challenges: (1) their dependency on complex sensors (e.g., Radar, EEGs
and others) for capturing precise handwritten trajectories, and (2) the absence
of a video-based air-writing dataset that covers a comprehensive vocabulary
range. These limitations impede their practicality in various real-world
scenarios, including the use on devices like iPhones and laptops. To tackle
these challenges, we present the groundbreaking air-writing Chinese character
video dataset (AWCV-100K-UCAS2024), serving as a pioneering benchmark for
video-based air-writing. This dataset captures handwritten trajectories in
various real-world scenarios using commonly accessible RGB cameras, eliminating
the need for complex sensors. AWCV-100K-UCAS2024 includes 8.8 million video
frames, encompassing the complete set of 3,755 characters from the GB2312-80
level-1 set (GB1). Furthermore, we introduce our baseline approach, the
video-based character recognizer (VCRec). VCRec adeptly extracts fingertip
features from sparse visual cues and employs a spatio-temporal sequence module
for analysis. Experimental results showcase the superior performance of VCRec
compared to existing models in recognizing air-written characters, both
quantitatively and qualitatively. This breakthrough paves the way for enhanced
human-computer interaction in real-world contexts. Moreover, our approach
leverages affordable RGB cameras, enabling its applicability in a diverse range
of scenarios. The code and data examples will be made public at
https://github.com/wmeiqi/AWCV.",2024-12-27T09:04:04Z,http://arxiv.org/abs/2412.19537v1,"Meiqi Wu, Kaiqi Huang, Yuanqiang Cai, Shiyu Hu, Yuzhong Zhao, Weiqiang Wang"
"Potential Vector Fields in $\mathbb R^3$ and $α$-Meridional
  Mappings of the Second Kind $(α\in \mathbb R)$","This paper extends approach developed in a recent author's paper on analytic
models of potential fields in inhomogeneous media. New three-dimensional
analytic models of potential vector fields in some layered media are
constructed. Properties of various analytic models in Cartesian and cylindrical
coordinates in $\mathbb R^3$ are compared. The original properties of the
Jacobian matrix $\mathbf{J}(\vec V)$ of potential meridional fields $\vec V$ in
cylindrically layered media, where $\phi( \rho) = \rho^{-\alpha}$ $(\alpha \in
\mathbb R)$, lead to the concept of \emph{$\alpha$-meridional mappings of the
first and second kind}. The concept of \emph{$\alpha$-Meridional functions of
the first and second kind} naturally arises in this way. When $\alpha =1$, the
special concept of \emph{Radially holomorphic functions in $\mathbb R^3$},
introduced by G\""{u}rlebeck, Habetha and Spr\""{o}ssig in 2008, is developed in
more detail. Certain key properties of the radially holomorphic functions $G$
and functions reversed with respect to $G$ are first characterized. Surprising
properties of the radially holomorphic potentials represented by superposition
of the radially holomorphic exponential function $e^{\breve{\beta} x}$
$(\breve{\beta} \in \mathbb R)$ and function reversed with respect to
$e^{\breve{\beta} x}$ are demonstrated explicitly. The basic properties of the
radially holomorphic potential represented by the radially holomorphic
extension of the Joukowski transformation in $\mathbb R^3$ are studied.",2024-12-27T09:03:29Z,http://arxiv.org/abs/2412.19536v1,Dmitry Bryukhov
"StyleRWKV: High-Quality and High-Efficiency Style Transfer with
  RWKV-like Architecture","Style transfer aims to generate a new image preserving the content but with
the artistic representation of the style source. Most of the existing methods
are based on Transformers or diffusion models, however, they suffer from
quadratic computational complexity and high inference time. RWKV, as an
emerging deep sequence models, has shown immense potential for long-context
sequence modeling in NLP tasks. In this work, we present a novel framework
StyleRWKV, to achieve high-quality style transfer with limited memory usage and
linear time complexity. Specifically, we propose a Recurrent WKV (Re-WKV)
attention mechanism, which incorporates bidirectional attention to establish a
global receptive field. Additionally, we develop a Deformable Shifting
(Deform-Shifting) layer that introduces learnable offsets to the sampling grid
of the convolution kernel, allowing tokens to shift flexibly and adaptively
from the region of interest, thereby enhancing the model's ability to capture
local dependencies. Finally, we propose a Skip Scanning (S-Scanning) method
that effectively establishes global contextual dependencies. Extensive
experiments with analysis including qualitative and quantitative evaluations
demonstrate that our approach outperforms state-of-the-art methods in terms of
stylization quality, model complexity, and inference efficiency.",2024-12-27T09:01:15Z,http://arxiv.org/abs/2412.19535v1,"Miaomiao Dai, Qianyu Zhou, Lizhuang Ma"
"Optical probing of fractal and multifractal connection to structural
  disorder in weakly optical disordered media: Application to cancer detection","The light scattering experiment establishes a relationship between refractive
index fluctuations and fractal dimension in weakly scattering tissue-like
media. Based on the box-counting approach, an analytical model is developed and
shows that the fractal dimension has a functional dependency on the structural
disorder or refractive index fluctuation for short-range correlation and
approximately linearly depends on each other for tissue-like media. Several
parametric imaging systems can be connected using this approach. Further,
tissue's weak multifractality optical scattering is explored using the
box-counting method. It is shown that with a functional transformation, the
distribution follows lognormal distributions.",2024-12-27T08:54:29Z,http://arxiv.org/abs/2412.19532v1,"Santanu Maity, Mousa Alrubayan, Ishmael Apachigwao, Dhruvil Solanki, Prabhakar Pradhan"
Is Your Text-to-Image Model Robust to Caption Noise?,"In text-to-image (T2I) generation, a prevalent training technique involves
utilizing Vision Language Models (VLMs) for image re-captioning. Even though
VLMs are known to exhibit hallucination, generating descriptive content that
deviates from the visual reality, the ramifications of such caption
hallucinations on T2I generation performance remain under-explored. Through our
empirical investigation, we first establish a comprehensive dataset comprising
VLM-generated captions, and then systematically analyze how caption
hallucination influences generation outcomes. Our findings reveal that (1) the
disparities in caption quality persistently impact model outputs during
fine-tuning. (2) VLMs confidence scores serve as reliable indicators for
detecting and characterizing noise-related patterns in the data distribution.
(3) even subtle variations in caption fidelity have significant effects on the
quality of learned representations. These findings collectively emphasize the
profound impact of caption quality on model performance and highlight the need
for more sophisticated robust training algorithm in T2I. In response to these
observations, we propose a approach leveraging VLM confidence score to mitigate
caption noise, thereby enhancing the robustness of T2I models against
hallucination in caption.",2024-12-27T08:53:37Z,http://arxiv.org/abs/2412.19531v1,"Weichen Yu, Ziyan Yang, Shanchuan Lin, Qi Zhao, Jianyi Wang, Liangke Gui, Matt Fredrikson, Lu Jiang"
Magnon-Phonon Coupling in Layered Antiferromagnet,"We present a fully analytical model of hybridization between magnon, and
phonons observed experimentally in magneto-Raman scattering in van der Waals
(vdW) antiferromagnets (AFM). Here, the representative material, FePS3, has
been shown to be a quasi-two-dimensional-Ising antiferromagnet, with additional
features of spin-phonon coupling in the Raman spectra emerging below the N\'eel
temperature (TN) of approximately 120 K. Using magneto-Raman spectroscopy as an
optical probe of magnetic structure, we show that one of these Raman-active
modes in the magnetically ordered state is a magnon with a frequency of 3.7 THz
(~ 122 cm-1). In addition, one magnon band and three phonon bands are coupled
via the magneto-elastic coupling evidenced by anti-crossing in the complete
spectra. We consider a simple model involving only in-plane nearest neighbor
exchange couplings (designed to give rise to a similar magnetic structure) and
perpendicular anisotropy in presence of an out-of-plane magnetic field. Exact
diagonalization of the Hamiltonian leads to energy bands which show that the
interaction term gives rise to avoided crossings between the hybridized magnon
and phonon branches. Realizing magnon-phonon coupling in two-dimensional (2D)
AFMs is important for the verification of the theoretical predictions on exotic
quantum transport phenomena like spin-caloritronics, topological magnonics,
etc.",2024-12-27T08:38:33Z,http://arxiv.org/abs/2412.19526v1,"Somsubhra Ghosh, Mainak Palit, Sujan Maity, Subhadeep Datta"
"Attribution for Enhanced Explanation with Transferable Adversarial
  eXploration","The interpretability of deep neural networks is crucial for understanding
model decisions in various applications, including computer vision.
AttEXplore++, an advanced framework built upon AttEXplore, enhances attribution
by incorporating transferable adversarial attack methods such as MIG and GRA,
significantly improving the accuracy and robustness of model explanations. We
conduct extensive experiments on five models, including CNNs (Inception-v3,
ResNet-50, VGG16) and vision transformers (MaxViT-T, ViT-B/16), using the
ImageNet dataset. Our method achieves an average performance improvement of
7.57\% over AttEXplore and 32.62\% compared to other state-of-the-art
interpretability algorithms. Using insertion and deletion scores as evaluation
metrics, we show that adversarial transferability plays a vital role in
enhancing attribution results. Furthermore, we explore the impact of
randomness, perturbation rate, noise amplitude, and diversity probability on
attribution performance, demonstrating that AttEXplore++ provides more stable
and reliable explanations across various models. We release our code at:
https://anonymous.4open.science/r/ATTEXPLOREP-8435/",2024-12-27T08:27:53Z,http://arxiv.org/abs/2412.19523v1,"Zhiyu Zhu, Jiayu Zhang, Zhibo Jin, Huaming Chen, Jianlong Zhou, Fang Chen"
"Exploiting Domain-Specific Parallel Data on Multilingual Language Models
  for Low-resource Language Translation","Neural Machine Translation (NMT) systems built on multilingual
sequence-to-sequence Language Models (msLMs) fail to deliver expected results
when the amount of parallel data for a language, as well as the language's
representation in the model are limited. This restricts the capabilities of
domain-specific NMT systems for low-resource languages (LRLs). As a solution,
parallel data from auxiliary domains can be used either to fine-tune or to
further pre-train the msLM. We present an evaluation of the effectiveness of
these two techniques in the context of domain-specific LRL-NMT. We also explore
the impact of domain divergence on NMT model performance. We recommend several
strategies for utilizing auxiliary parallel data in building domain-specific
NMT models for LRLs.",2024-12-27T08:25:52Z,http://arxiv.org/abs/2412.19522v1,"Surangika Ranathungaa, Shravan Nayak, Shih-Ting Cindy Huang, Yanke Mao, Tong Su, Yun-Hsiang Ray Chan, Songchen Yuan, Anthony Rinaldi, Annie En-Shiun Lee"
"Improved measurements of neutron lifetime with cold neutron beam at
  J-PARC","The ``neutron lifetime puzzle'' arises from the discrepancy between neutron
lifetime measurements obtained using the beam method, which measures decay
products, and the bottle method, which measures the disappearance of neutrons.
To resolve this puzzle, we conducted an experiment using a pulsed cold neutron
beam at J-PARC. In this experiment, the neutron lifetime is determined from the
ratio of neutron decay counts to $^3$He(n,p)$^3$H reactions in a gas detector.
This experiment belongs to the beam method but differs from previous
experiments that measured protons, as it instead detects electrons, enabling
measurements with distinct systematic uncertainties. By enlarging the beam
transport system and reducing systematic uncertainties, we achieved a fivefold
improvement in precision. Analysis of all acquired data yielded a neutron
lifetime of $\tau_{\rm n}=877.2~\pm~1.7_{\rm(stat.)}~^{+4.0}_{-3.6}{}_{\rm
(sys.)}$ s. This result is consistent with bottle method measurements but
exhibits a 2.3$\sigma$ tension with the average value obtained from the
proton-detection-based beam method.",2024-12-27T08:19:54Z,http://arxiv.org/abs/2412.19519v1,"Y. Fuwa, T. Hasegawa, K. Hirota, T. Hoshino, R. Hosokawa, G. Ichikawa, S. Ieki, T. Ino, Y. Iwashita, M. Kitaguchi, R. Kitahara, S. Makise, K. Mishima, T. Mogi, N. Nagakura, H. Oide, H. Okabe, H. Otono, Y. Seki, D. Sekiba, T. Shima, H. E. Shimizu, H. M. Shimizu, N. Sumi, H. Sumino, M. Tanida, H. Uehara, T. Yamada, S. Yamashita, K. Yano, T. Yoshioka"
"Estimation of System Parameters Including Repeated Cross-Sectional Data
  through Emulator-Informed Deep Generative Model","Differential equations (DEs) are crucial for modeling the evolution of
natural or engineered systems. Traditionally, the parameters in DEs are
adjusted to fit data from system observations. However, in fields such as
politics, economics, and biology, available data are often independently
collected at distinct time points from different subjects (i.e., repeated
cross-sectional (RCS) data). Conventional optimization techniques struggle to
accurately estimate DE parameters when RCS data exhibit various
heterogeneities, leading to a significant loss of information. To address this
issue, we propose a new estimation method called the emulator-informed
deep-generative model (EIDGM), designed to handle RCS data. Specifically, EIDGM
integrates a physics-informed neural network-based emulator that immediately
generates DE solutions and a Wasserstein generative adversarial network-based
parameter generator that can effectively mimic the RCS data. We evaluated EIDGM
on exponential growth, logistic population models, and the Lorenz system,
demonstrating its superior ability to accurately capture parameter
distributions. Additionally, we applied EIDGM to an experimental dataset of
Amyloid beta 40 and beta 42, successfully capturing diverse parameter
distribution shapes. This shows that EIDGM can be applied to model a wide range
of systems and extended to uncover the operating principles of systems based on
limited data.",2024-12-27T08:19:23Z,http://arxiv.org/abs/2412.19517v1,"Hyunwoo Cho, Sung Woong Cho, Hyeontae Jo, Hyung Ju Hwang"
"Confidence v.s. Critique: A Decomposition of Self-Correction Capability
  for LLMs","Large Language Models (LLMs) can correct their self-generated responses, but
a decline in accuracy after self-correction is also witnessed. To have a deeper
understanding of self-correction, we endeavor to decompose, evaluate, and
analyze the self-correction behaviors of LLMs. By enumerating and analyzing
answer correctness before and after self-correction, we decompose the
self-correction capability into confidence (being confident to correct answers)
and critique (turning wrong answers to correct) capabilities, and propose two
metrics from a probabilistic perspective to measure these 2 capabilities, along
with another metric for overall self-correction capability evaluation. Based on
our decomposition and evaluation metrics, we conduct extensive experiments and
draw some empirical conclusions. For example, we find different models can
exhibit distinct behaviors: some models are confident while others are more
critical. We also find the trade-off between the two capabilities (i.e.
improving one can lead to a decline in the other) when manipulating model
self-correction behavior by prompts or in-context learning. Further, we find a
simple yet efficient strategy to improve self-correction capability by
transforming Supervision Fine-Tuning (SFT) data format, and our strategy
outperforms vanilla SFT in both capabilities and achieves much higher accuracy
after self-correction. Our code will be publicly available on GitHub.",2024-12-27T08:09:11Z,http://arxiv.org/abs/2412.19513v1,"Zhe Yang, Yichang Zhang, Yudong Wang, Ziyao Xu, Junyang Lin, Zhifang Sui"
Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging,"Fine-tuning large language models (LLMs) for downstream tasks is a widely
adopted approach, but it often leads to safety degradation in safety-aligned
LLMs. Currently, many solutions address this issue by incorporating additional
safety data, which can be impractical in many cases. In this paper, we address
the question: How can we improve downstream task performance while preserving
safety in LLMs without relying on additional safety data? We propose a simple
and effective method that maintains the inherent safety of LLMs while enhancing
their downstream task performance: merging the weights of pre- and
post-fine-tuned safety-aligned models. Experimental results across various
downstream tasks, models, and merging methods demonstrate that this approach
effectively mitigates safety degradation while improving downstream task
performance, offering a practical solution for adapting safety-aligned LLMs.",2024-12-27T08:03:22Z,http://arxiv.org/abs/2412.19512v1,"Hua Farn, Hsuan Su, Shachi H Kumar, Saurav Sahay, Shang-Tse Chen, Hung-yi Lee"
Hybrid Local Causal Discovery,"Local causal discovery aims to learn and distinguish the direct causes and
effects of a target variable from observed data. Existing constraint-based
local causal discovery methods use AND or OR rules in constructing the local
causal skeleton, but using either rule alone is prone to produce cascading
errors in the learned local causal skeleton, and thus impacting the inference
of local causal relationships. On the other hand, directly applying score-based
global causal discovery methods to local causal discovery may randomly return
incorrect results due to the existence of local equivalence classes. To address
the above issues, we propose a Hybrid Local Causal Discovery algorithm, called
HLCD. Specifically, HLCD initially utilizes a constraint-based approach
combined with the OR rule to obtain a candidate skeleton and then employs a
score-based method to eliminate redundant portions in the candidate skeleton.
Furthermore, during the local causal orientation phase, HLCD distinguishes
between V-structures and equivalence classes by comparing the local structure
scores between the two, thereby avoiding orientation interference caused by
local equivalence classes. We conducted extensive experiments with seven
state-of-the-art competitors on 14 benchmark Bayesian network datasets, and the
experimental results demonstrate that HLCD significantly outperforms existing
local causal discovery algorithms.",2024-12-27T07:53:59Z,http://arxiv.org/abs/2412.19507v1,"Zhaolong Ling, Honghui Peng, Yiwen Zhang, Peng Zhou, Xingyu Wu, Kui Yu, Xindong Wu"
"RobotDiffuse: Motion Planning for Redundant Manipulator based on
  Diffusion Model","Redundant manipulators, with their higher Degrees of Freedom (DOFs), offer
enhanced kinematic performance and versatility, making them suitable for
applications like manufacturing, surgical robotics, and human-robot
collaboration. However, motion planning for these manipulators is challenging
due to increased DOFs and complex, dynamic environments. While traditional
motion planning algorithms struggle with high-dimensional spaces, deep
learning-based methods often face instability and inefficiency in complex
tasks. This paper introduces RobotDiffuse, a diffusion model-based approach for
motion planning in redundant manipulators. By integrating physical constraints
with a point cloud encoder and replacing the U-Net structure with an
encoder-only transformer, RobotDiffuse improves the model's ability to capture
temporal dependencies and generate smoother, more coherent motion plans. We
validate the approach using a complex simulator, and release a new dataset with
35M robot poses and 0.14M obstacle avoidance scenarios. Experimental results
demonstrate the effectiveness of RobotDiffuse and the promise of diffusion
models for motion planning tasks. The code can be accessed at
https://github.com/ACRoboT-buaa/RobotDiffuse.",2024-12-27T07:34:54Z,http://arxiv.org/abs/2412.19500v1,"Xiaohan Zhang, Xudong Mou, Rui Wang, Tianyu Wo, Ningbo Gu, Tiejun Wang, Cangbai Xu, Xudong Liu"
Tailoring Robust Quantum Anomalous Hall Effect via Entropy-Engineering,"Development of quantum materials and tailoring of their functional properties
is a fundamental interest in materials science. Here we propose a new design
concept for robust quantum anomalous Hall effect via entropy engineering in 2D
magnets. As a prototypical example, configurational entropy of monolayer
transition metal trihalide VCl$_3$ is manipulated by incorporating four
different transition-metal cations [Ti,Cr,Fe,Co] in the honeycomb structure
made of vanadium, such that all the in-plane mirror symmetries, inversion
and/or roto-inversion are broken. Monolayer VCl$_3$ is a ferromagnetic Dirac
half-metal in which spin-polarized Dirac dispersion at valley momenta is
accompanied by bulk states at the $\Gamma$-point and thus the spin-orbit
interaction driven quantum anomalous Hall phase does not exhibit fully gapped
bulk band dispersion. Entropy-driven bandstructure renormalization, especially
band flattening in combination with red and blue shifts at different momenta of
the Brillouin zone and crystal-field effects, transforms Dirac half-metal to a
Dirac spin gapless semiconductor and leads to a robust quantum anomalous Hall
phase with fully gapped bulk band dispersion, and thus, a purely topological
edge state transport without mixing with dissipative bulk channels. These
findings provide a paradigm to design entropy-driven 2D materials for the
realization of robust quantum anomalous Hall effect and quantum device
applications.",2024-12-27T07:33:57Z,http://arxiv.org/abs/2412.19499v1,"Syeda Amina Shabbir, Frank Fei Yun, Muhammad Nadeem, Xiaolin Wang"
"Multi-P$^2$A: A Multi-perspective Benchmark on Privacy Assessment for
  Large Vision-Language Models","Large Vision-Language Models (LVLMs) exhibit impressive potential across
various tasks but also face significant privacy risks, limiting their practical
applications. Current researches on privacy assessment for LVLMs is limited in
scope, with gaps in both assessment dimensions and privacy categories. To
bridge this gap, we propose Multi-P$^2$A, a comprehensive benchmark for
evaluating the privacy preservation capabilities of LVLMs in terms of privacy
awareness and leakage. Privacy awareness measures the model's ability to
recognize the privacy sensitivity of input data, while privacy leakage assesses
the risk of the model unintentionally disclosing privacy information in its
output. We design a range of sub-tasks to thoroughly evaluate the model's
privacy protection offered by LVLMs. Multi-P$^2$A covers 26 categories of
personal privacy, 15 categories of trade secrets, and 18 categories of state
secrets, totaling 31,962 samples. Based on Multi-P$^2$A, we evaluate the
privacy preservation capabilities of 21 open-source and 2 closed-source LVLMs.
Our results reveal that current LVLMs generally pose a high risk of
facilitating privacy breaches, with vulnerabilities varying across personal
privacy, trade secret, and state secret.",2024-12-27T07:33:39Z,http://arxiv.org/abs/2412.19496v1,"Jie Zhang, Xiangkui Cao, Zhouyu Han, Shiguang Shan, Xilin Chen"
"Disparate Model Performance and Stability in Machine Learning Clinical
  Support for Diabetes and Heart Diseases","Machine Learning (ML) algorithms are vital for supporting clinical
decision-making in biomedical informatics. However, their predictive
performance can vary across demographic groups, often due to the
underrepresentation of historically marginalized populations in training
datasets. The investigation reveals widespread sex- and age-related inequities
in chronic disease datasets and their derived ML models. Thus, a novel
analytical framework is introduced, combining systematic arbitrariness with
traditional metrics like accuracy and data complexity. The analysis of data
from over 25,000 individuals with chronic diseases revealed mild sex-related
disparities, favoring predictive accuracy for males, and significant
age-related differences, with better accuracy for younger patients. Notably,
older patients showed inconsistent predictive accuracy across seven datasets,
linked to higher data complexity and lower model performance. This highlights
that representativeness in training data alone does not guarantee equitable
outcomes, and model arbitrariness must be addressed before deploying models in
clinical settings.",2024-12-27T07:31:14Z,http://arxiv.org/abs/2412.19495v1,"Ioannis Bilionis, Ricardo C. Berrios, Luis Fernandez-Luque, Carlos Castillo"
"Two superconducting thin films systems with potential integration of
  different quantum functionalities","Quantum computation based on superconducting circuits utilizes
superconducting qubits with Josephson tunnel junctions. Engineering
high-coherence qubits requires materials optimization. In this work, we present
two superconducting thin film systems, grown on silicon (Si), and one obtained
from the other via annealing. Cobalt (Co) thin films grown on Si were found to
be superconducting [EPL 131 (2020) 47001]. These films also happen to be a
self-organised hybrid superconductor/ferromagnet/superconductor (S/F/S)
structure. The S/F/S hybrids are important for superconducting $\pi$-qubits
[PRL 95 (2005) 097001] and in quantum information processing. Here we present
our results on the superconductivity of a hybrid Co film followed by the
superconductivity of a CoSi$_2$ film, which was prepared by annealing the Co
film. CoSi$_2$, with its $1/f$ noise about three orders of magnitude smaller
compared to the most commonly used superconductor aluminium (Al), is a
promising material for high-coherence qubits. The hybrid Co film revealed
superconducting transition temperature $T_c$ = 5 K and anisotropy in the upper
critical field between the in-plane and out-of-plane directions. The anisotropy
was of the order of ratio of lateral dimensions to thickness of the
superconducting Co grains, suggesting a quasi-2D nature of superconductivity.
On the other hand, CoSi$_2$ film showed a $T_c$ of 900 mK. In the resistivity
vs. temperature curve, we observe a peak near $T_c$. Magnetic field scan as a
function of $T$ shows a monotonic increase in intensity of this peak with
temperature. The origin of the peak has been explained in terms of parallel
resistive model for the particular measurement configuration. Although our
CoSi$_2$ film contains grain boundaries, we observed a perpendicular critical
field of 15 mT and a critical current density of 3.8x10$^7$ A/m$^2$, comparable
with epitaxial CoSi$_2$ films.",2024-12-27T07:23:20Z,http://arxiv.org/abs/2412.19493v1,"Snehal Mandal, Biplab Biswas, Suvankar Purakait, Anupam Roy, Biswarup Satpati, Indranil Das, B. N. Dev"
"Multi-label Classification using Deep Multi-order Context-aware Kernel
  Networks","Multi-label classification is a challenging task in pattern recognition. Many
deep learning methods have been proposed and largely enhanced classification
performance. However, most of the existing sophisticated methods ignore context
in the models' learning process. Since context may provide additional cues to
the learned models, it may significantly boost classification performances. In
this work, we make full use of context information (namely geometrical
structure of images) in order to learn better context-aware similarities
(a.k.a. kernels) between images. We reformulate context-aware kernel design as
a feed-forward network that outputs explicit kernel mapping features. Our
obtained context-aware kernel network further leverages multiple orders of
patch neighbors within different distances, resulting into a more
discriminating Deep Multi-order Context-aware Kernel Network (DMCKN) for
multi-label classification. We evaluate the proposed method on the challenging
Corel5K and NUS-WIDE benchmarks, and empirical results show that our method
obtains competitive performances against the related state-of-the-art, and both
quantitative and qualitative performances corroborate its effectiveness and
superiority for multi-label image classification.",2024-12-27T07:16:11Z,http://arxiv.org/abs/2412.19491v1,"Mingyuan Jiu, Hailong Zhu, Hichem Sahbi"
User Willingness-aware Sales Talk Dataset,"User willingness is a crucial element in the sales talk process that affects
the achievement of the salesperson's or sales system's objectives. Despite the
importance of user willingness, to the best of our knowledge, no previous study
has addressed the development of automated sales talk dialogue systems that
explicitly consider user willingness. A major barrier is the lack of sales talk
datasets with reliable user willingness data. Thus, in this study, we developed
a user willingness-aware sales talk collection by leveraging the ecological
validity concept, which is discussed in the field of human-computer
interaction. Our approach focused on three types of user willingness essential
in real sales interactions. We created a dialogue environment that closely
resembles real-world scenarios to elicit natural user willingness, with
participants evaluating their willingness at the utterance level from multiple
perspectives. We analyzed the collected data to gain insights into practical
user willingness-aware sales talk strategies. In addition, as a practical
application of the constructed dataset, we developed and evaluated a sales
dialogue system aimed at enhancing the user's intent to purchase.",2024-12-27T07:16:10Z,http://arxiv.org/abs/2412.19490v1,"Asahi Hentona, Jun Baba, Shiki Sato, Reina Akama"
Comprehensive Bayesian Exploration of Froggatt-Nielsen Mechanism,"The Froggatt-Nielsen (FN) mechanism successfully explains the hierarchical
structure of fermion Yukawa couplings by introducing a U(1) flavor symmetry
with distinct charge assignments for different fermion generations. While some
FN charge assignments have been proposed, their evaluation has largely relied
on heuristic approaches. This paper systematically investigates viable FN
charge assignments within the Standard Model, including both the quark and
lepton sectors, using Bayesian statistical analysis. The study explores
scenarios involving both the seesaw mechanism and dimension-five operators for
neutrino mass generation. A comprehensive parameter scan over FN charges
reveals a wide range of charge assignments consistent with observed fermion
masses and mixing angles. Interestingly, negative FN charges and significant
generational differences in charges are found to be viable, contrary to
conventional assumptions. The analysis also compares the seesaw mechanism and
dimension-five operator scenarios, finding no strong preference between them
for optimal charge assignments. Furthermore, predictions for the lightest
neutrino mass and effective Majorana mass relevant for neutrinoless double-beta
decay are presented, highlighting regions of parameter space accessible to
upcoming experiments. Finally, implications for nucleon decay are studied,
demonstrating that different FN charge assignments predict significantly
different nucleon decay lifetimes and branching ratios, providing a potential
experimental probe for FN models.",2024-12-27T06:41:17Z,http://arxiv.org/abs/2412.19484v1,"Masahiro Ibe, Satoshi Shirai, Keiichi Watanabe"
Learning Radiance Fields from a Single Snapshot Compressive Image,"In this paper, we explore the potential of Snapshot Compressive Imaging (SCI)
technique for recovering the underlying 3D scene structure from a single
temporal compressed image. SCI is a cost-effective method that enables the
recording of high-dimensional data, such as hyperspectral or temporal
information, into a single image using low-cost 2D imaging sensors. To achieve
this, a series of specially designed 2D masks are usually employed, reducing
storage and transmission requirements and offering potential privacy
protection. Inspired by this, we take one step further to recover the encoded
3D scene information leveraging powerful 3D scene representation capabilities
of neural radiance fields (NeRF). Specifically, we propose SCINeRF, in which we
formulate the physical imaging process of SCI as part of the training of NeRF,
allowing us to exploit its impressive performance in capturing complex scene
structures. In addition, we further integrate the popular 3D Gaussian Splatting
(3DGS) framework and propose SCISplat to improve 3D scene reconstruction
quality and training/rendering speed by explicitly optimizing point clouds into
3D Gaussian representations. To assess the effectiveness of our method, we
conduct extensive evaluations using both synthetic data and real data captured
by our SCI system. Experimental results demonstrate that our proposed approach
surpasses the state-of-the-art methods in terms of image reconstruction and
novel view synthesis. Moreover, our method also exhibits the ability to render
high frame-rate multi-view consistent images in real time by leveraging SCI and
the rendering capabilities of 3DGS. Codes will be available at:
https://github.com/WU- CVGL/SCISplat.",2024-12-27T06:40:44Z,http://arxiv.org/abs/2412.19483v1,"Yunhao Li, Xiang Liu, Xiaodong Wang, Xin Yuan, Peidong Liu"
"Pre-training, Fine-tuning and Re-ranking: A Three-Stage Framework for
  Legal Question Answering","Legal question answering (QA) has attracted increasing attention from people
seeking legal advice, which aims to retrieve the most applicable answers from a
large-scale database of question-answer pairs. Previous methods mainly use a
dual-encoder architecture to learn dense representations of both questions and
answers. However, these methods could suffer from lacking domain knowledge and
sufficient labeled training data. In this paper, we propose a three-stage
(\underline{p}re-training, \underline{f}ine-tuning and \underline{r}e-ranking)
framework for \underline{l}egal \underline{QA} (called PFR-LQA), which promotes
the fine-grained text representation learning and boosts the performance of
dense retrieval with the dual-encoder architecture. Concretely, we first
conduct domain-specific pre-training on legal questions and answers through a
self-supervised training objective, allowing the pre-trained model to be
adapted to the legal domain. Then, we perform task-specific fine-tuning of the
dual-encoder on legal question-answer pairs by using the supervised learning
objective, leading to a high-quality dual-encoder for the specific downstream
QA task. Finally, we employ a contextual re-ranking objective to further refine
the output representations of questions produced by the document encoder, which
uses contextual similarity to increase the discrepancy between the anchor and
hard negative samples for better question re-ranking. We conduct extensive
experiments on a manually annotated legal QA dataset. Experimental results show
that our PFR-LQA method achieves better performance than the strong competitors
for legal question answering.",2024-12-27T06:33:42Z,http://arxiv.org/abs/2412.19482v1,"Shiwen Ni, Hao Cheng, Min Yang"
Generative Adversarial Network on Motion-Blur Image Restoration,"In everyday life, photographs taken with a camera often suffer from motion
blur due to hand vibrations or sudden movements. This phenomenon can
significantly detract from the quality of the images captured, making it an
interesting challenge to develop a deep learning model that utilizes the
principles of adversarial networks to restore clarity to these blurred pixels.
In this project, we will focus on leveraging Generative Adversarial Networks
(GANs) to effectively deblur images affected by motion blur. A GAN-based
Tensorflow model is defined, training and evaluating by GoPro dataset which
comprises paired street view images featuring both clear and blurred versions.
This adversarial training process between Discriminator and Generator helps to
produce increasingly realistic images over time. Peak Signal-to-Noise Ratio
(PSNR) and Structural Similarity Index Measure (SSIM) are the two evaluation
metrics used to provide quantitative measures of image quality, allowing us to
evaluate the effectiveness of the deblurring process. Mean PSNR in 29.1644 and
mean SSIM in 0.7459 with average 4.6921 seconds deblurring time are achieved in
this project. The blurry pixels are sharper in the output of GAN model shows a
good image restoration effect in real world applications.",2024-12-27T06:12:50Z,http://arxiv.org/abs/2412.19479v1,Zhengdong Li
"An Overview of Machine Learning-Driven Resource Allocation in IoT
  Networks","In the wake of disruptive IoT technologies generating massive amounts of
diverse data, Machine Learning (ML) will play a crucial role in bringing
intelligence to Internet of Things (IoT) networks. This paper provides a
comprehensive analysis of the current state of resource allocation within IoT
networks, focusing specifically on two key categories: Low-Power IoT Networks
and Mobile IoT Networks. We delve into the resource allocation strategies that
are crucial for optimizing network performance and energy efficiency in these
environments. Furthermore, the paper explores the transformative role of
Machine Learning (ML), Deep Learning (DL), and Reinforcement Learning (RL) in
enhancing IoT functionalities. We highlight a range of applications and use
cases where these advanced technologies can significantly improve
decision-making and optimization processes. In addition to the opportunities
presented by ML, DL, and RL, we also address the potential challenges that
organizations may face when implementing these technologies in IoT settings.
These challenges include crucial accuracy, low flexibility and adaptability,
and high computational cost, etc. Finally, the paper identifies promising
avenues for future research, emphasizing the need for innovative solutions to
overcome existing hurdles and improve the integration of ML, DL, and RL into
IoT networks. By providing this holistic perspective, we aim to contribute to
the ongoing discourse on resource allocation strategies and the application of
intelligent technologies in the IoT landscape.",2024-12-27T06:11:28Z,http://arxiv.org/abs/2412.19478v1,Zhengdong Li
"A C-Band Cryogenic GaAs MMIC Low-Noise Amplifier for Quantum
  Applications","Large-scale superconducting quantum computers require massive numbers of
high-performance cryogenic low-noise amplifiers (cryo-LNA) for qubit readout.
Here we present a C-Band monolithic microwave integrated circuit (MMIC)
cryo-LNA for this purpose. This cryo-LNA is based on 150 nm GaAs pseudomorphic
high electron mobility transistor (pHEMT) process and implemented with a
three-stage cascaded architecture, where the first stage adopts careful
impedance match to optimize the noise and return loss. The integration of
negative feedback loops adopted in the second and third-stage enhances the
overall stability. Moreover, the pHEMT-self bias and current multiplexing
circuitry structure facilitate the reduction of power consumption and require
only single bias line. Operating at an ambient temperature of 3.6 K and
consuming 15 mW, the cryo-LNA demonstrates good performance in the C-band,
reaching a 5 K equivalent noise temperature and an average gain of 40 dB. We
further benchmark this cryo-LNA with superconducting qubits, achieving an
average single-shot dispersive readout fidelity of 98.3% without assistance
from a quantum-limited parametric amplifier. The development of GaAs cryo-LNA
diversifies technical support necessary for large-scale quantum applications.",2024-12-27T06:04:01Z,http://arxiv.org/abs/2412.19477v1,"Zechen Guo, Daxiong Sun, Peisheng Huang, Xuandong Sun, Yuefeng Yuan, Jiawei Zhang, Wenhui Huang, Yongqi Liang, Jiawei Qiu, Jiajian Zhang, Ji Chu, Weijie Guo, Ji Jiang, Jingjing Niu, Wenhui Ren, Ziyu Tao, Xiayu Linpeng, Youpeng Zhong, Dapeng Yu"
"Effects of Reynolds number and spatial resolution on the pressure source
  terms in turbulent boundary layers","The increase in wall-pressure fluctuations with increasing friction Reynolds
number ($Re_{\tau}$) of a turbulent boundary layer (TBL) is well known in the
literature. However, very few studies have investigated the
$Re_{\tau}$-variation of the source terms of the pressure fluctuations, which
are solely a function of the spatial velocity gradients within the TBL. This
study quantifies the pressure source terms in a zero-pressure gradient TBL by
utilizing a published direct numerical simulation (DNS; Sillero et al. 2013,
Phys. Fluids) database across 1000 $\lesssim$ $Re_{\tau}$ $\lesssim$ 2000. It
is found that the magnitude of all source terms increases with $Re_{\tau}$
across the entire TBL thickness, with the turbulence-turbulence (non-linear)
interaction terms growing faster than the mean-shear (linear) source terms.
Further, we use the simulation database to mimic the scenario of particle image
velocimetry (PIV) experiments that are typically spatially under-resolved
compared to DNS data. It is used to quantify the effect of spatial resolution
on the accuracy of pressure source terms, which are estimated here for two
common PIV scenarios: (i) planar PIV in the streamwise-wall-normal plane, and
(ii) stereo-PIV in the spanwise-wall-normal plane of a ZPG TBL. This exercise
reveals significant attenuation of all pressure source terms compared to those
estimated from the original DNS, highlighting the challenges of accurately
estimating these source terms in a high $Re_{\tau}$ PIV experiment.",2024-12-27T05:58:49Z,http://arxiv.org/abs/2412.19474v1,"Aditya Agarwal, Rahul Deshpande"
"Knowledge Graph-Based Multi-Agent Path Planning in Dynamic Environments
  using WAITR","This paper addresses the challenge of multi-agent path planning for efficient
data collection in dynamic, uncertain environments, exemplified by autonomous
underwater vehicles (AUVs) navigating the Gulf of Mexico. Traditional greedy
algorithms, though computationally efficient, often fall short in long-term
planning due to their short-sighted nature, missing crucial data collection
opportunities and increasing exposure to hazards. To address these limitations,
we introduce WAITR (Weighted Aggregate Inter-Temporal Reward), a novel
path-planning framework that integrates a knowledge graph with pathlet-based
planning, segmenting the environment into dynamic, speed-adjusted sub-regions
(pathlets). This structure enables coordinated, adaptive planning, as agents
can operate within time-bound regions while dynamically responding to
environmental changes. WAITR's cumulative scoring mechanism balances immediate
data collection with long-term optimization of Points of Interest (POIs),
ensuring safer navigation and comprehensive data coverage. Experimental results
show that WAITR substantially improves POI coverage and reduces exposure to
hazards, achieving up to 27.1\% greater event coverage than traditional greedy
methods.",2024-12-27T05:43:41Z,http://arxiv.org/abs/2412.19469v1,"Ted Edward Holmberg, Elias Ioup, Mahdi Abdelguerfi"
Laws of Quantum Programming,"In this paper, we investigate the fundamental laws of quantum programming. We
extend a comprehensive set of Hoare et al.'s basic laws of classical
programming to the quantum setting. These laws characterise the algebraic
properties of quantum programs, such as the distributivity of sequential
composition over (quantum) if-statements and the unfolding of nested (quantum)
if-statements. At the same time, we clarify some subtle differences between
certain laws of classical programming and their quantum counterparts.
Additionally, we derive a fixpoint characterization of quantum while-loops and
a loop-based realisation of tail recursion in quantum programming. Furthermore,
we establish two normal form theorems: one for quantum circuits and one for
finite quantum programs. The theory in which these laws are established is
formalised in the Coq proof assistant, and all of these laws are mechanically
verified. As an application case of our laws, we present a formal derivation of
the principle of deferred measurements in dynamic quantum circuits.
  We expect that these laws can be utilized in correctness-preserving
transformation, compilation, and automatic code optimization in quantum
programming. In particular, because these laws are formally verified in Coq,
they can be confidently applied in quantum program development.",2024-12-27T05:16:35Z,http://arxiv.org/abs/2412.19463v1,"Mingsheng Ying, Li Zhou, Gilles Barthe"
A Prototype Unit for Image De-raining using Time-Lapse Data,"We address the challenge of single-image de-raining, a task that involves
recovering rain-free background information from a single rain image. While
recent advancements have utilized real-world time-lapse data for training,
enabling the estimation of consistent backgrounds and realistic rain streaks,
these methods often suffer from computational and memory consumption, limiting
their applicability in real-world scenarios. In this paper, we introduce a
novel solution: the Rain Streak Prototype Unit (RsPU). The RsPU efficiently
encodes rain streak-relevant features as real-time prototypes derived from
time-lapse data, eliminating the need for excessive memory resources. Our
de-raining network combines encoder-decoder networks with the RsPU, allowing us
to learn and encapsulate diverse rain streak-relevant features as concise
prototypes, employing an attention-based approach. To ensure the effectiveness
of our approach, we propose a feature prototype loss encompassing cohesion and
divergence components. This loss function captures both the compactness and
diversity aspects of the prototypical rain streak features within the RsPU. Our
method evaluates various de-raining benchmarks, accompanied by comprehensive
ablation studies. We show that it can achieve competitive results in various
rain images compared to state-of-the-art methods.",2024-12-27T05:04:56Z,http://arxiv.org/abs/2412.19459v1,"Jaehoon Cho, Minjung Yoo, Jini Yang, Sunok Kim"
"DriveEditor: A Unified 3D Information-Guided Framework for Controllable
  Object Editing in Driving Scenes","Vision-centric autonomous driving systems require diverse data for robust
training and evaluation, which can be augmented by manipulating object
positions and appearances within existing scene captures. While recent
advancements in diffusion models have shown promise in video editing, their
application to object manipulation in driving scenarios remains challenging due
to imprecise positional control and difficulties in preserving high-fidelity
object appearances. To address these challenges in position and appearance
control, we introduce DriveEditor, a diffusion-based framework for object
editing in driving videos. DriveEditor offers a unified framework for
comprehensive object editing operations, including repositioning, replacement,
deletion, and insertion. These diverse manipulations are all achieved through a
shared set of varying inputs, processed by identical position control and
appearance maintenance modules. The position control module projects the given
3D bounding box while preserving depth information and hierarchically injects
it into the diffusion process, enabling precise control over object position
and orientation. The appearance maintenance module preserves consistent
attributes with a single reference image by employing a three-tiered approach:
low-level detail preservation, high-level semantic maintenance, and the
integration of 3D priors from a novel view synthesis model. Extensive
qualitative and quantitative evaluations on the nuScenes dataset demonstrate
DriveEditor's exceptional fidelity and controllability in generating diverse
driving scene edits, as well as its remarkable ability to facilitate downstream
tasks.",2024-12-27T04:49:36Z,http://arxiv.org/abs/2412.19458v1,"Yiyuan Liang, Zhiying Yan, Liqun Chen, Jiahuan Zhou, Luxin Yan, Sheng Zhong, Xu Zou"
Focusing Image Generation to Mitigate Spurious Correlations,"Instance features in images exhibit spurious correlations with background
features, affecting the training process of deep neural classifiers. This leads
to insufficient attention to instance features by the classifier, resulting in
erroneous classification outcomes. In this paper, we propose a data
augmentation method called Spurious Correlations Guided Synthesis (SCGS) that
mitigates spurious correlations through image generation model. This approach
does not require expensive spurious attribute (group) labels for the training
data and can be widely applied to other debiasing methods. Specifically, SCGS
first identifies the incorrect attention regions of a pre-trained classifier on
the training images, and then uses an image generation model to generate new
training data based on these incorrect attended regions. SCGS increases the
diversity and scale of the dataset to reduce the impact of spurious
correlations on classifiers. Changes in the classifier's attention regions and
experimental results on three different domain datasets demonstrate that this
method is effective in reducing the classifier's reliance on spurious
correlations.",2024-12-27T04:48:56Z,http://arxiv.org/abs/2412.19457v1,"Xuewei Li, Zhenzhen Nie, Mei Yu, Zijian Zhang, Jie Gao, Tianyi Xu, Zhiqiang Liu"
"NijiGAN: Transform What You See into Anime with Contrastive
  Semi-Supervised Learning and Neural Ordinary Differential Equations","Generative AI has transformed the animation industry. Several models have
been developed for image-to-image translation, particularly focusing on
converting real-world images into anime through unpaired translation.
Scenimefy, a notable approach utilizing contrastive learning, achieves high
fidelity anime scene translation by addressing limited paired data through
semi-supervised training. However, it faces limitations due to its reliance on
paired data from a fine-tuned StyleGAN in the anime domain, often producing
low-quality datasets. Additionally, Scenimefy's high parameter architecture
presents opportunities for computational optimization. This research introduces
NijiGAN, a novel model incorporating Neural Ordinary Differential Equations
(NeuralODEs), which offer unique advantages in continuous transformation
modeling compared to traditional residual networks. NijiGAN successfully
transforms real-world scenes into high fidelity anime visuals using half of
Scenimefy's parameters. It employs pseudo-paired data generated through
Scenimefy for supervised training, eliminating dependence on low-quality paired
data and improving the training process. Our comprehensive evaluation includes
ablation studies, qualitative, and quantitative analysis comparing NijiGAN to
similar models. The testing results demonstrate that NijiGAN produces
higher-quality images compared to AnimeGAN, as evidenced by a Mean Opinion
Score (MOS) of 2.192, it surpasses AnimeGAN's MOS of 2.160. Furthermore, our
model achieved a Frechet Inception Distance (FID) score of 58.71, outperforming
Scenimefy's FID score of 60.32. These results demonstrate that NijiGAN achieves
competitive performance against existing state-of-the-arts, especially
Scenimefy as the baseline model.",2024-12-27T04:46:44Z,http://arxiv.org/abs/2412.19455v1,"Kevin Putra Santoso, Anny Yuniarti, Dwiyasa Nakula, Dimas Prihady Setyawan, Adam Haidar Azizi, Jeany Aurellia P. Dewati, Farah Dhia Fadhila, Maria T. Elvara Bumbungan"
"Feature Alignment-Based Knowledge Distillation for Efficient Compression
  of Large Language Models","This study proposes a knowledge distillation algorithm based on large
language models and feature alignment, aiming to effectively transfer the
knowledge of large pre-trained models into lightweight student models, thereby
reducing computational costs while maintaining high model performance.
Different from the traditional soft label distillation method, this method
introduces a multi-layer feature alignment strategy to deeply align the
intermediate features and attention mechanisms of the teacher model and the
student model, maximally retaining the semantic expression ability and context
modeling ability of the teacher model. In terms of method design, a multi-task
loss function is constructed, including feature matching loss, attention
alignment loss, and output distribution matching loss, to ensure multi-level
information transfer through joint optimization. The experiments were
comprehensively evaluated on the GLUE data set and various natural language
processing tasks. The results show that the proposed model performs very close
to the state-of-the-art GPT-4 model in terms of evaluation indicators such as
perplexity, BLEU, ROUGE, and CER. At the same time, it far exceeds baseline
models such as DeBERTa, XLNet, and GPT-3, showing significant performance
improvements and computing efficiency advantages. Research results show that
the feature alignment distillation strategy is an effective model compression
method that can significantly reduce computational overhead and storage
requirements while maintaining model capabilities. Future research can be
further expanded in the directions of self-supervised learning, cross-modal
feature alignment, and multi-task transfer learning to provide more flexible
and efficient solutions for the deployment and optimization of deep learning
models.",2024-12-27T04:37:06Z,http://arxiv.org/abs/2412.19449v1,"Shuo Wang, Chihang Wang, Jia Gao, Zhen Qi, Hongye Zheng, Xiaoxuan Liao"
Gauge symmetry and partially Lagrangian systems,"We consider the classical field theory whose equations of motion follow from
the least action principle, but the class of admissible trajectories is
restricted by differential equations. The key element of the proposed
construction is the complete gauge symmetry of these additional equations . The
unfree variation of the trajectories reduces to the infinitesimal gauge
symmetry transformation of the equations restricting the trajectories. We
explicitly derive the equations that follow from the requirement that this
gauge variation of the action vanishes. The system of equations for conditional
extrema is not Lagrangian as such, but it admits an equivalent Hamiltonian
formulation with a non-canonical Poisson bracket. The bracket is degenerate, in
general. Alternatively, the equations restricting dynamics could be added to
the action with Lagrange multipliers with unrestricted variation of the
original variables. In this case, we would arrive at the Lagrangian equations
for original variables involving Lagrange multipliers and for Lagrange
multipliers themselves. In general, these two methods are not equivalent
because the multipliers can bring extra degrees of freedom compared to the case
of equations derived by unfree variation of the action. We illustrate the
general method with two examples. The first example is the particle in central
field with varying trajectories restricted by equation of conservation of
angular momentum. The phase space gets one more dimension, and there is an
extra conserved quantity $K$ which is responsible for precession of
trajectories. $K=0$ corresponds to the trajectories of usual Lagrangian
dynamics. The second example is the linearized gravity with Einstein-Hilbert
action and the class of varying fields is restricted by linearized Nordstr\""om
equation. This conditional extrema problem is shown to lead to the linearized
Cotton gravity equations.",2024-12-27T04:34:18Z,http://arxiv.org/abs/2412.19447v1,"Simon Lyakhovich, nikit Sinelnikov"
"Comparative Performance Analysis of Quantum Machine Learning
  Architectures for Credit Card Fraud Detection","As financial fraud becomes increasingly complex, effective detection methods
are essential. Quantum Machine Learning (QML) introduces certain capabilities
that may enhance both accuracy and efficiency in this area. This study examines
how different quantum feature map and ansatz configurations affect the
performance of three QML-based classifiers-the Variational Quantum Classifier
(VQC), the Sampler Quantum Neural Network (SQNN), and the Estimator Quantum
Neural Network (EQNN)-when applied to two non-standardized financial fraud
datasets. Different quantum feature map and ansatz configurations are
evaluated, revealing distinct performance patterns. The VQC consistently
demonstrates strong classification results, achieving an F1 score of 0.88,
while the SQNN also delivers promising outcomes. In contrast, the EQNN
struggles to produce robust results, emphasizing the challenges presented by
non-standardized data. These findings highlight the importance of careful model
configuration in QML-based financial fraud detection. By showing how specific
feature maps and ansatz choices influence predictive success, this work guides
researchers and practitioners in refining QML approaches for complex financial
applications.",2024-12-27T04:17:34Z,http://arxiv.org/abs/2412.19441v1,"Mansour El Alami, Nouhaila Innan, Muhammad Shafique, Mohamed Bennai"
"Low-Rank Contextual Reinforcement Learning from Heterogeneous Human
  Feedback","Reinforcement learning from human feedback (RLHF) has become a cornerstone
for aligning large language models with human preferences. However, the
heterogeneity of human feedback, driven by diverse individual contexts and
preferences, poses significant challenges for reward learning. To address this,
we propose a Low-rank Contextual RLHF (LoCo-RLHF) framework that integrates
contextual information to better model heterogeneous feedback while maintaining
computational efficiency. Our approach builds on a contextual preference model,
leveraging the intrinsic low-rank structure of the interaction between user
contexts and query-answer pairs to mitigate the high dimensionality of feature
representations. Furthermore, we address the challenge of distributional shifts
in feedback through our Pessimism in Reduced Subspace (PRS) policy, inspired by
pessimistic offline reinforcement learning techniques. We theoretically
demonstrate that our policy achieves a tighter sub-optimality gap compared to
existing methods. Extensive experiments validate the effectiveness of
LoCo-RLHF, showcasing its superior performance in personalized RLHF settings
and its robustness to distribution shifts.",2024-12-27T04:02:46Z,http://arxiv.org/abs/2412.19436v1,"Seong Jin Lee, Will Wei Sun, Yufeng Liu"
"Residual Feature-Reutilization Inception Network for Image
  Classification","Capturing feature information effectively is of great importance in the field
of computer vision. With the development of convolutional neural networks
(CNNs), concepts like residual connection and multiple scales promote continual
performance gains in diverse deep learning vision tasks. In this paper, we
propose a novel CNN architecture that it consists of residual
feature-reutilization inceptions (ResFRI) or split-residual
feature-reutilization inceptions (Split-ResFRI). And it is composed of four
convolutional combinations of different structures connected by specially
designed information interaction passages, which are utilized to extract
multi-scale feature information and effectively increase the receptive field of
the model. Moreover, according to the network structure designed above,
Split-ResFRI can adjust the segmentation ratio of the input information,
thereby reducing the number of parameters and guaranteeing the model
performance. Specifically, in experiments based on popular vision datasets,
such as CIFAR10 ($97.94$\%), CIFAR100 ($85.91$\%) and Tiny Imagenet
($70.54$\%), we obtain state-of-the-art results compared with other modern
models under the premise that the model size is approximate and no additional
data is used.",2024-12-27T03:55:25Z,http://arxiv.org/abs/2412.19433v1,"Yuanpeng He, Wenjie Song, Lijian Li, Tianxiang Zhan, Wenpin Jiao"
Revisiting PCA for time series reduction in temporal dimension,"Revisiting PCA for Time Series Reduction in Temporal Dimension; Jiaxin Gao,
Wenbo Hu, Yuntian Chen; Deep learning has significantly advanced time series
analysis (TSA), enabling the extraction of complex patterns for tasks like
classification, forecasting, and regression. Although dimensionality reduction
has traditionally focused on the variable space-achieving notable success in
minimizing data redundancy and computational complexity-less attention has been
paid to reducing the temporal dimension. In this study, we revisit Principal
Component Analysis (PCA), a classical dimensionality reduction technique, to
explore its utility in temporal dimension reduction for time series data. It is
generally thought that applying PCA to the temporal dimension would disrupt
temporal dependencies, leading to limited exploration in this area. However,
our theoretical analysis and extensive experiments demonstrate that applying
PCA to sliding series windows not only maintains model performance, but also
enhances computational efficiency. In auto-regressive forecasting, the temporal
structure is partially preserved through windowing, and PCA is applied within
these windows to denoise the time series while retaining their statistical
information. By preprocessing time-series data with PCA, we reduce the temporal
dimensionality before feeding it into TSA models such as Linear, Transformer,
CNN, and RNN architectures. This approach accelerates training and inference
and reduces resource consumption. Notably, PCA improves Informer training and
inference speed by up to 40% and decreases GPU memory usage of TimesNet by 30%,
without sacrificing model accuracy. Comparative analysis against other
reduction methods further highlights the effectiveness of PCA in improving the
efficiency of TSA models.",2024-12-27T03:17:26Z,http://arxiv.org/abs/2412.19423v1,"Jiaxin Gao, Wenbo Hu, Yuntian Chen"
"Gx2Mol: De Novo Generation of Hit-like Molecules from Gene Expression
  Profiles via Deep Learning","De novo generation of hit-like molecules is a challenging task in the drug
discovery process. Most methods in previous studies learn the semantics and
syntax of molecular structures by analyzing molecular graphs or simplified
molecular input line entry system (SMILES) strings; however, they do not take
into account the drug responses of the biological systems consisting of genes
and proteins. In this study we propose a deep generative model, Gx2Mol, which
utilizes gene expression profiles to generate molecular structures with
desirable phenotypes for arbitrary target proteins. In the algorithm, a
variational autoencoder is employed as a feature extractor to learn the latent
feature distribution of the gene expression profiles. Then, a long short-term
memory is leveraged as the chemical generator to produce syntactically valid
SMILES strings that satisfy the feature conditions of the gene expression
profile extracted by the feature extractor. Experimental results and case
studies demonstrate that the proposed Gx2Mol model can produce new molecules
with potential bioactivities and drug-like properties.",2024-12-27T03:16:56Z,http://arxiv.org/abs/2412.19422v1,"Chen Li, Yuki Matsukiyo, Yoshihiro Yamanishi"
"A Matrix Logic Approach to Efficient Frequent Itemset Discovery in Large
  Data Sets","This paper proposes a frequent itemset mining algorithm based on the Boolean
matrix method, aiming to solve the storage and computational bottlenecks of
traditional frequent pattern mining algorithms in high-dimensional and
large-scale transaction databases. By representing the itemsets in the
transaction database as Boolean matrices, the algorithm uses Boolean logic
operations such as AND and OR to efficiently calculate the support of the
itemsets, avoiding the generation and storage of a large number of candidates
itemsets in traditional algorithms. The algorithm recursively mines frequent
itemsets through matrix operations and can flexibly adapt to different data
scales and support thresholds. In the experiment, the public Groceries dataset
was selected, and the running efficiency test and frequent itemset mining
effect test were designed to evaluate the algorithm's performance indicators
such as running time, memory usage, and number of frequent itemsets under
different transaction numbers and support thresholds. The experimental results
show that the algorithm can efficiently mine a large number of frequent
itemsets when the support threshold is low, and focus on strong association
rules with high support when the threshold is high. In addition, the changing
trends of running time and memory usage show that the Boolean matrix method can
still maintain good running efficiency when the number of transactions
increases significantly and has high scalability and robustness. Future
research can improve memory optimization and matrix block operations, and
combine distributed computing and deep learning models to further enhance the
algorithm's applicability and real-time processing capabilities in
ultra-large-scale data environments. The algorithm has broad application
potential and development prospects in the fields of market analysis,
recommendation systems, and network security.",2024-12-27T03:13:13Z,http://arxiv.org/abs/2412.19420v1,"Xuan Li, Tingyi Ruan, Yankaiqi Li, Quanchao Lu, Xiaoxuan Sun"
KALAHash: Knowledge-Anchored Low-Resource Adaptation for Deep Hashing,"Deep hashing has been widely used for large-scale approximate nearest
neighbor search due to its storage and search efficiency. However, existing
deep hashing methods predominantly rely on abundant training data, leaving the
more challenging scenario of low-resource adaptation for deep hashing
relatively underexplored. This setting involves adapting pre-trained models to
downstream tasks with only an extremely small number of training samples
available. Our preliminary benchmarks reveal that current methods suffer
significant performance degradation due to the distribution shift caused by
limited training samples. To address these challenges, we introduce
Class-Calibration LoRA (CLoRA), a novel plug-and-play approach that dynamically
constructs low-rank adaptation matrices by leveraging class-level textual
knowledge embeddings. CLoRA effectively incorporates prior class knowledge as
anchors, enabling parameter-efficient fine-tuning while maintaining the
original data distribution. Furthermore, we propose Knowledge-Guided Discrete
Optimization (KIDDO), a framework to utilize class knowledge to compensate for
the scarcity of visual information and enhance the discriminability of hash
codes. Extensive experiments demonstrate that our proposed method, Knowledge-
Anchored Low-Resource Adaptation Hashing (KALAHash), significantly boosts
retrieval performance and achieves a 4x data efficiency in low-resource
scenarios.",2024-12-27T03:04:54Z,http://arxiv.org/abs/2412.19417v1,"Shu Zhao, Tan Yu, Xiaoshuai Hao, Wenchao Ma, Vijaykrishnan Narayanan"
DIPS: Optimal Dynamic Index for Poisson $\boldsymbolπ$ps Sampling,"This paper addresses the Poisson $\pi$ps sampling problem, a topic of
significant academic interest in various domains and with practical data mining
applications, such as influence maximization. The problem includes a set
$\mathcal{S}$ of $n$ elements, where each element $v$ is assigned a weight
$w(v)$ reflecting its importance. The goal is to generate a random subset $X$
of $\mathcal{S}$, where each element $v \in \mathcal{S}$ is included in $X$
independently with probability $\frac{c\cdot w(v)}{\sum_{v \in \mathcal{S}}
w(v)}$, where $0&lt;c\leq 1$ is a constant. The subsets must be independent across
different queries. While the Poisson $\pi$ps sampling problem can be reduced to
the well-studied subset sampling problem, updates in Poisson $\pi$ps sampling,
such as adding a new element or removing an element, would cause the
probabilities of all $n$ elements to change in the corresponding subset
sampling problem, making this approach impractical for dynamic scenarios. To
address this, we propose a dynamic index specifically tailored for the Poisson
$\pi$ps sampling problem, supporting optimal expected $\mathcal{O}(1)$ query
time and $\mathcal{O}(1)$ index update time, with an optimal $\mathcal{O}(n)$
space cost. Our solution involves recursively partitioning the set by weights
and ultimately using table lookup. The core of our solution lies in addressing
the challenges posed by weight explosion and correlations between elements.
Empirical evaluations demonstrate that our approach achieves significant
speedups in update time while maintaining consistently competitive query time
compared to the subset-sampling-based methods.",2024-12-27T02:47:44Z,http://arxiv.org/abs/2412.19415v1,"Jinchao Huang, Sibo Wang"
"The Hobby-Eberly Telescope Dark Energy Experiment Survey (HETDEX) Active
  Galactic Nuclei Catalog: the Fourth Data Release","We present the Active Galactic Nuclei (AGN) catalog from the fourth data
release (HDR4) of the Hobby-Eberly Telescope Dark Energy Experiment Survey
(HETDEX). HETDEX is an untargeted spectroscopic survey. HDR4 contains 345,874
Integral Field Unit (IFU) observations from January 2017 to August 2023
covering an effective area of 62.9 deg2. With no imaging pre-selection, our
spectroscopic confirmed AGN sample includes low-luminosity AGN, narrow-line
AGN, and/or red AGN down to g~25. This catalog has 15,940 AGN across the
redshifts of z=0.1~4.6, giving a raw AGN number density of 253.4 deg-2. Among
them, 10,499 (66%) have redshifts either confirmed by line pairs or matched to
the Sloan Digital Sky Survey Quasar Catalog. For the remaining 5,441 AGN, 2,083
are single broad line AGN candidates, while the remaining 3,358 are single
intermediate broad line (full width at half maximum, FWHM ~ 1200 km s-1) AGN
candidates. A total of 4,060 (39%) of the 10,499 redshift-confirmed AGN have
emission-line regions $3\sigma$ more extended than the image quality which
could be strong outflows blowing into the outskirts of the host galaxies or
ionized intergalactic medium.",2024-12-27T02:45:20Z,http://arxiv.org/abs/2412.19414v1,"Chenxu Liu, Karl Gebhardt, Erin Mentuch Cooper, Dustin Davis, Donald P. Schneider, Matt J. Jarvis, Daniel J. Farrow, Steven L. Finkelstein, Oscar A. Chavez Ortiz, The HETDEX Collaboration"
MINIMA: Modality Invariant Image Matching,"Image matching for both cross-view and cross-modality plays a critical role
in multimodal perception. In practice, the modality gap caused by different
imaging systems/styles poses great challenges to the matching task. Existing
works try to extract invariant features for specific modalities and train on
limited datasets, showing poor generalization. In this paper, we present
MINIMA, a unified image matching framework for multiple cross-modal cases.
Without pursuing fancy modules, our MINIMA aims to enhance universal
performance from the perspective of data scaling up. For such purpose, we
propose a simple yet effective data engine that can freely produce a large
dataset containing multiple modalities, rich scenarios, and accurate matching
labels. Specifically, we scale up the modalities from cheap but rich RGB-only
matching data, by means of generative models. Under this setting, the matching
labels and rich diversity of the RGB dataset are well inherited by the
generated multimodal data. Benefiting from this, we construct MD-syn, a new
comprehensive dataset that fills the data gap for general multimodal image
matching. With MD-syn, we can directly train any advanced matching pipeline on
randomly selected modality pairs to obtain cross-modal ability. Extensive
experiments on in-domain and zero-shot matching tasks, including $19$
cross-modal cases, demonstrate that our MINIMA can significantly outperform the
baselines and even surpass modality-specific methods. The dataset and code are
available at https://github.com/LSXI7/MINIMA .",2024-12-27T02:39:50Z,http://arxiv.org/abs/2412.19412v1,"Xingyu Jiang, Jiangwei Ren, Zizhuo Li, Xin Zhou, Dingkang Liang, Xiang Bai"
"An arbitrary order mixed finite element method with boundary value
  correction for the Darcy flow on curved domains","We propose a boundary value correction method for the Brezzi-Douglas-Marini
mixed finite element discretization of the Darcy flow with non-homogeneous
Neumann boundary condition on 2D curved domains. The discretization is defined
on a body-fitted triangular mesh, i.e. the boundary nodes of the mesh lie on
the curved physical boundary. However, the boundary edges of the triangular
mesh, which are straight, may not coincide with the curved physical boundary. A
boundary value correction technique is then designed to transform the Neumann
boundary condition from the physical boundary to the boundary of the triangular
mesh. One advantage of the boundary value correction method is that it avoids
using curved mesh elements and thus reduces the complexity of implementation.
We prove that the proposed method reaches optimal convergence for arbitrary
order discretizations. Supporting numerical results are presented. Key words:
mixed finite element method, Neumann boundary condition, curved domain,
boundary value correction method.",2024-12-27T02:31:14Z,http://arxiv.org/abs/2412.19411v1,"Yongli Hou, Yanqiu Wang"
"MLLM-SUL: Multimodal Large Language Model for Semantic Scene
  Understanding and Localization in Traffic Scenarios","Multimodal large language models (MLLMs) have shown satisfactory effects in
many autonomous driving tasks. In this paper, MLLMs are utilized to solve joint
semantic scene understanding and risk localization tasks, while only relying on
front-view images. In the proposed MLLM-SUL framework, a dual-branch visual
encoder is first designed to extract features from two resolutions, and rich
visual information is conducive to the language model describing risk objects
of different sizes accurately. Then for the language generation, LLaMA model is
fine-tuned to predict scene descriptions, containing the type of driving
scenario, actions of risk objects, and driving intentions and suggestions of
ego-vehicle. Ultimately, a transformer-based network incorporating a regression
token is trained to locate the risk objects. Extensive experiments on the
existing DRAMA-ROLISP dataset and the extended DRAMA-SRIS dataset demonstrate
that our method is efficient, surpassing many state-of-the-art image-based and
video-based methods. Specifically, our method achieves 80.1% BLEU-1 score and
298.5% CIDEr score in the scene understanding task, and 59.6% accuracy in the
localization task. Codes and datasets are available at
https://github.com/fjq-tongji/MLLM-SUL.",2024-12-27T02:05:38Z,http://arxiv.org/abs/2412.19406v1,"Jiaqi Fan, Jianhua Wu, Jincheng Gao, Jianhao Yu, Yafei Wang, Hongqing Chu, Bingzhao Gao"
Spectral-Temporal Fusion Representation for Person-in-Bed Detection,"This study is based on the ICASSP 2025 Signal Processing Grand Challenge's
Accelerometer-Based Person-in-Bed Detection Challenge, which aims to determine
bed occupancy using accelerometer signals. The task is divided into two tracks:
""in bed"" and ""not in bed"" segmented detection, and streaming detection, facing
challenges such as individual differences, posture variations, and external
disturbances. We propose a spectral-temporal fusion-based feature
representation method with mixup data augmentation, and adopt Intersection over
Union (IoU) loss to optimize detection accuracy. In the two tracks, our method
achieved outstanding results of 100.00% and 95.55% in detection scores,
securing first place and third place, respectively.",2024-12-27T02:05:09Z,http://arxiv.org/abs/2412.19404v1,"Xuefeng Yang, Shiheng Zhang, Jian Guan, Feiyang Xiao, Wei Lu, Qiaoxi Zhu"
"Fully Data-driven but Interpretable Human Behavioural Modelling with
  Differentiable Discrete Choice Model","Discrete choice models are essential for modelling various decision-making
processes in human behaviour. However, the specification of these models has
depended heavily on domain knowledge from experts, and the fully automated but
interpretable modelling of complex human behaviours has been a long-standing
challenge. In this paper, we introduce the differentiable discrete choice model
(Diff-DCM), a fully data-driven method for the interpretable modelling,
learning, prediction, and control of complex human behaviours, which is
realised by differentiable programming. Solely from input features and choice
outcomes without any prior knowledge, Diff-DCM can estimate interpretable
closed-form utility functions that reproduce observed behaviours. Comprehensive
experiments with both synthetic and real-world data demonstrate that Diff-DCM
can be applied to various types of data and requires only a small amount of
computational resources for the estimations, which can be completed within tens
of seconds on a laptop without any accelerators. In these experiments, we also
demonstrate that, using its differentiability, Diff-DCM can provide useful
insights into human behaviours, such as an optimal intervention path for
effective behavioural changes. This study provides a strong basis for the fully
automated and reliable modelling, prediction, and control of human behaviours.",2024-12-27T01:53:18Z,http://arxiv.org/abs/2412.19403v1,"Fumiyasu Makinoshima, Tatsuya Mitomi, Fumiya Makihara, Eigo Segawa"
"Joint Optimization of Multimodal Transit Frequency and Shared Autonomous
  Vehicle Fleet Size with Hybrid Metaheuristic and Nonlinear Programming","This paper presents an optimization framework for the joint multimodal
transit frequency and shared autonomous vehicle (SAV) fleet size optimization,
a problem variant of the transit network frequency setting problem (TNFSP) that
explicitly considers mode choice behavior and route selection. To address the
non-linear non-convex optimization problem, we develop a hybrid solution
approach that combines metaheuristics (particle swarm optimization, PSO) with
local nonlinear programming (NLP) improvement, incorporating approximation
models for SAV waiting time, multimodal route choice, and mode choice. Applied
to the Chicago metropolitan area, our method achieves a 33.3% increase in
transit ridership.",2024-12-27T01:33:42Z,http://arxiv.org/abs/2412.19401v1,"Max T. M. Ng, Hani S. Mahmassani, Draco Tong, Omer Verbas, Taner Cokyasar"
"Spin alignment of vector mesons in local equilibrium by Zubarev's
  approach","We compute the $00$ element of the spin density matrix, denoted as
$\rho_{00}$ and called the spin alignment, up to the second order of the
gradient expansion in local equilibrium by Zubarev's approach. In the first
order, we obtain $\rho_{00}=1/3$, meaning that the contributions from thermal
vorticity and shear stress tensor are vanishing. The non-vanishing
contributions to $\rho_{00}-1/3$ appear in the second order of gradients in the
Belinfante and canonical cases. We also discuss the properties of the spin
density matrix under the time reversal transformation. The effective transport
coefficient for the spin alignment induced by the thermal shear stress tensor
is T-odd in the first order, implying that the first order effect is
dissipative.",2024-12-27T01:29:43Z,http://arxiv.org/abs/2412.19400v1,"Shi-Zheng Yang, Xin-Qing Xie, Shi Pu, Jian-Hua Gao, Qun Wang"
"Comparing Few to Rank Many: Active Human Preference Learning using
  Randomized Frank-Wolfe","We study learning of human preferences from a limited comparison feedback.
This task is ubiquitous in machine learning. Its applications such as
reinforcement learning from human feedback, have been transformational. We
formulate this problem as learning a Plackett-Luce model over a universe of $N$
choices from $K$-way comparison feedback, where typically $K \ll N$. Our
solution is the D-optimal design for the Plackett-Luce objective. The design
defines a data logging policy that elicits comparison feedback for a small
collection of optimally chosen points from all ${N \choose K}$ feasible
subsets. The main algorithmic challenge in this work is that even fast methods
for solving D-optimal designs would have $O({N \choose K})$ time complexity. To
address this issue, we propose a randomized Frank-Wolfe (FW) algorithm that
solves the linear maximization sub-problems in the FW method on randomly chosen
variables. We analyze the algorithm, and evaluate it empirically on synthetic
and open-source NLP datasets.",2024-12-27T01:10:17Z,http://arxiv.org/abs/2412.19396v1,"Kiran Koshy Thekumparampil, Gaurush Hiranandani, Kousha Kalantari, Shoham Sabach, Branislav Kveton"
"Two-echelon Electric Vehicle Routing Problem in Parcel Delivery: A
  Literature Review","Multi-echelon parcel delivery systems using electric vehicles (EVs) are
crucial for managing urban logistics complexity and promoting sustainability.
In multi-echelon systems, particularly within two-stage systems, larger
vehicles transport parcels from a central depot to satellite hubs, where
smaller EVs pick up the parcels and carry out last-mile deliveries. This system
could increase efficiency, reduce emissions, and improve service reliability.
The two-echelon electric vehicle routing problem (2E-EVRP), an extension of the
traditional two-echelon vehicle routing problem (2E-VRP), addresses EV-specific
challenges such as battery constraints and recharging stations to tackle
environmental impacts, urban congestion, and e-commerce demands. While
effectively reducing costs, energy use, and emissions, the 2E-EVRP faces
modeling challenges due to multi-echelon structures, EV limitations, and
recharging station selection. This paper systematically reviews 2E-EVRP
literature, analyzing key studies. It proposes a classification scheme to
categorize the papers based on the problem variants, objectives, constraints,
and solution methods. It identifies gaps such as delivery tardiness,
environmental trade-offs, multi-objective optimization, multiple depots, split
deliveries, and time-dependent travel conditions. Future research directions
include aligning models with urban policies, integrating parcel lockers,
enabling same-day delivery, and incorporating advanced technologies like
autonomous vehicles. Methodological advancements suggest using machine
learning, reinforcement learning, and simulation-based approaches to enhance
dynamic routing and real-time decision-making. These directions aim to expand
the 2E-EVRP applicability, addressing theoretical and practical challenges in
sustainable urban logistics for future works.",2024-12-27T01:05:59Z,http://arxiv.org/abs/2412.19395v1,"Nima Moradi, Niloufar Mirzavand Boroujeni, Navid Aftabi, Amin Aslani"
"An In-Depth Analysis of Adversarial Discriminative Domain Adaptation for
  Digit Classification","Domain adaptation is an active area of research driven by the growing demand
for robust machine learning models that perform well on real-world data.
Adversarial learning for deep neural networks (DNNs) has emerged as a promising
approach to improving generalization ability, particularly for image
classification. In this paper, we implement a specific adversarial learning
technique known as Adversarial Discriminative Domain Adaptation (ADDA) and
replicate digit classification experiments from the original ADDA paper. We
extend their findings by examining a broader range of domain shifts and provide
a detailed analysis of in-domain classification accuracy post-ADDA. Our results
demonstrate that ADDA significantly improves accuracy across certain domain
shifts with minimal impact on in-domain performance. Furthermore, we provide
qualitative analysis and propose potential explanations for ADDA's limitations
in less successful domain shifts. Code is at
https://github.com/eugenechoi2004/COS429_FINAL .",2024-12-27T00:36:40Z,http://arxiv.org/abs/2412.19391v1,"Eugene Choi, Julian Rodriguez, Edmund Young"
"A reduced-order framework for temperature estimation in food freezing
  from optimally located sensors, including turbulent conjugate flow scenarios","This article proposes a framework for estimating temperature fields in
food-freezing applications that significantly reduces computational load while
ensuring accurate temperature monitoring, representing a promising
technological tool for optimizing and controlling food engineering processes.
The strategy is based on (i) a mathematical model of a convection-dominated
problem coupling thermal convection and turbulence and (ii) a least-squares
approach for solving the inverse data assimilation problem, regularized by
projecting the governing dynamics onto a reduced-order model (ROM). The
unsteady freezing process considers an idealized salmon slice in a freezer
cabinet, modeled with temperature-dependent thermophysical properties. The
forward problem is approximated using a third-order WENO finite volume solver,
including an optimized second-order backward scheme for time discretization. We
employ our data assimilation framework to reconstruct the temperature field
from a limited number of sensor data and to estimate temperature distributions
within frozen food. Sensor placement is optimized using a new greedy algorithm,
relying on maximizing the observability of the reduced-order dynamics for a
fixed set of sensors. The proposed approach allows efficient extrapolation from
external sensor measurements to the internal temperature of the food, which is
crucial for maintaining food quality.",2024-12-27T00:26:36Z,http://arxiv.org/abs/2412.19387v1,"Felipe Galarce, Diego Rivera, Douglas Pacheco, Alfonso Caiazzo, Ernesto Castillo"
Resolvent-based estimation and control of a laminar airfoil wake,"We develop an optimal resolvent-based estimator and controller to predict and
attenuate unsteady vortex shedding fluctuations in the laminar wake of a NACA
0012 airfoil at an angle of attack of 6.5 degrees, chord-based Reynolds number
of 5000, and Mach number of 0.3. The resolvent-based estimation and control
framework offers several advantages over standard methods. Under equivalent
assumptions, the resolvent-based estimator and controller reproduce the Kalman
filter and LQG controller, respectively, but at substantially lower
computational cost using either an operator-based or data-driven
implementation. Unlike these methods, the resolvent-based approach can
naturally accommodate forcing terms (nonlinear terms from Navier-Stokes) with
colored-in-time statistics, significantly improving estimation accuracy and
control efficacy. Causality is optimally enforced using a Wiener-Hopf
formalism. We integrate these tools into a high-performance-computing-ready
compressible flow solver and demonstrate their effectiveness for estimating and
controlling velocity fluctuations in the wake of the airfoil immersed in clean
and noisy freestreams, the latter of which prevents the flow from falling into
a periodic limit cycle. Using four shear-stress sensors on the surface of the
airfoil, the resolvent-based estimator predicts a series of downstream targets
with approximately 3% and 30% error for the clean and noisy freestream
conditions, respectively. For the latter case, using four actuators on the
airfoil surface, the resolvent-based controller reduces the turbulent kinetic
energy in the wake by 98%.",2024-12-27T00:12:23Z,http://arxiv.org/abs/2412.19386v1,"Junoh Jung, Rutvij Bhagwat, Aaron Towne"
"The Internet of Value: Integrating Blockchain and Lightning Network
  Micropayments for Knowledge Markets","Q&amp;A websites rely on user-generated responses, with incentives such as
reputation scores or monetary rewards often offered. While some users may find
it intrinsically rewarding to assist others, studies indicate that payment can
improve the quality and speed of answers. However, traditional payment
processors impose minimum thresholds that many Q&amp;A inquiries fall below. The
introduction of Bitcoin enabled direct digital value transfer, yet frequent
micropayments remain challenging. Recent advancements like the Lightning
Network now allow frictionless micropayments by reducing costs and minimising
reliance on intermediaries. This development fosters an ""Internet of Value,""
where transferring even small amounts of money is as simple as sharing data.
This study investigates integrating Lightning Network-based micropayment
strategies into Q&amp;A platforms, aiming to create a knowledge market free of
minimum payment barriers. A survey was conducted to address the gap below the
$2 payment level identified in prior research. Responses confirmed that
incentives for asking and answering weaken as payments decrease. Findings
reveal even minimal payments, such as {\pounds}0.01, significantly encourage
higher quality and effort in responses. The study recommends micropayment
incentives for service-oriented applications, particularly Q&amp;A platforms. By
leveraging the Lightning Network to remove barriers, a more open marketplace
can emerge, improving engagement and outcomes. Further research is needed to
confirm if users follow through on reported intentions when spending funds.",2024-12-26T23:57:54Z,http://arxiv.org/abs/2412.19384v1,"Ellis Solaiman, Jorge Robins"
On the Quantum K-theory of Quiver Varieties at Roots of Unity,"Let $\Psi(\mathbb{z},\mathbb{a},q)$ be the fundamental solution matrix of the
quantum difference equation in the equivariant quantum K-theory for Nakajima
variety $X$. In this work, we prove that the operator $$
\Psi(\mathbb{z},\mathbb{a},q)
\Psi\left(\mathbb{z}^p,\mathbb{a}^p,q^{p^2}\right)^{-1} $$ has no poles at the
primitive complex $p$-th roots of unity $\zeta_p$ in the curve counting
parameter $q$. As a byproduct, we show that the eigenvalues of the iterated
product of the operators ${\bf M}_{\mathcal{L}}$ from the quantum difference
equation on $X$ $$ {\bf M}_{\mathcal{L}} (\mathbb{z},\mathbb{a},q) {\bf
M}_{\mathcal{L}} (\mathbb{z} q^{\mathcal{L}},\mathbb{a},q) \cdots {\bf
M}_{\mathcal{L}} (\mathbb{z} q^{(p-1)\mathcal{L}},\mathbb{a},q) $$ evaluated at
$\zeta_p$ are described by the Bethe equations for $X$ in which all variables
are substituted by their $p$-th powers.
  Finally, upon a reduction of the quantum difference equation on $X$ to the
quantum differential equation over the field with finite characteristic, the
above iterated product transforms into a Grothendiek-Katz $p$-curvature of the
corresponding differential connection whereas ${\bf M}_{\mathcal{L}}
(\mathbb{z}^p,\mathbb{a}^p,q^p)$ becomes a certain Frobenius twist of that
connection. In this way, we are reproducing, in part, the statement of a
theorem by Etingof and Varchenko.",2024-12-26T23:52:58Z,http://arxiv.org/abs/2412.19383v1,"Peter Koroteev, Andrey Smirnov"
"Reconstruction of non-trivial magnetization textures from magnetic field
  images using neural networks","Spatial imaging of magnetic stray fields from magnetic materials is a useful
tool for identifying the underlying magnetic configurations of the material.
However, transforming the magnetic image into a magnetization image is an
ill-poised problem, which can result in artefacts that limit the inferences
that can be made on the material under investigation. In this work, we develop
a neural network fitting approach that approximates this transformation,
reducing these artefacts. Additionally, we demonstrate that this approach
allows the inclusion of additional models and bounds that are not possible with
traditional reconstruction methods. These advantages allow for the
reconstruction of non-trivial magnetization textures with varying magnetization
directions in thin-film magnets, which was not possible previously. We
demonstrate this new capability by performing magnetization reconstructions on
a variety of topological spin textures.",2024-12-26T23:50:02Z,http://arxiv.org/abs/2412.19381v1,"David A. Broadway, Mykhailo Flaks, Adrien E. E. Dubois, Patrick Maletinsky"
"Guidelines for Correlative Imaging and Analysis of Reactive Lithium
  Metal Battery Materials","To unlock the full potential of lithium metal batteries, a deep understanding
of lithium metal reactivity and its solid electrolyte interphase is essential.
Correlative imaging, combining focused ion beam and electron microscopy offers
a powerful approach for multi-scale characterization. However, the extreme
reactivity of lithium metal and its SEI presents challenges in investigating
deposition and stripping mechanisms. In this work, we systematically evaluated
the storage stability of lithium metal in glovebox before and after
electrochemical deposition. We then assessed different FIB ion sources for
their impact on lithium metal lamella preparation for transmission electron
microscopy. Furthermore, we examined cryogenic-TEM transfer methods, optimizing
for minimal contamination during sample handling. Contrary to prior
assumptions, we demonstrate that high resolution imaging of pure lithium metal
at room temperature is achievable using inert gas transfer with an electron
dose rate exceeding 1000 e/A2/s, without significant detectable damage. In
contrast, SEI components, such as Li2CO3 and LiF display much greater
sensitivity to electron beams, requiring cryogenic conditions and precise dose
control for nano/atomic scale imaging. We quantified electron dose limits for
these SEI components to track their structural evolution under irradiation.
Based on these findings, we propose a robust protocol for lithium metal sample
handling - from storage to atomic-level characterization - minimizing damage
and contamination. This work paves the way for more accurate and reproducible
studies, accelerating the development of next-generation lithium metal
batteries by ensuing the preservation of native material properties during
analysis.",2024-12-26T23:20:36Z,http://arxiv.org/abs/2412.19376v1,"Shuang Bai, Zhao Liu, Diyi Cheng, Bingyu Lu, Nestor J. Zaluzec, Ganesh Raghavendran, Shen Wang, Thomas S. Marchese, Brandon van Leer, Letian Li, Lin Jiang, Adam Stokes, Joseph P. Cline, Rachel Osmundsen, Paul Barends, Alexander Bright, Minghao Zhang, Ying Shirley Meng"
"Minimal Batch Adaptive Learning Policy Engine for Real-Time Mid-Price
  Forecasting in High-Frequency Trading","High-frequency trading (HFT) has transformed modern financial markets, making
reliable short-term price forecasting models essential. In this study, we
present a novel approach to mid-price forecasting using Level 1 limit order
book (LOB) data from NASDAQ, focusing on 100 U.S. stocks from the S&amp;P 500 index
during the period from September to November 2022. Expanding on our previous
work with Radial Basis Function Neural Networks (RBFNN), which leveraged
automated feature importance techniques based on mean decrease impurity (MDI)
and gradient descent (GD), we introduce the Adaptive Learning Policy Engine
(ALPE) - a reinforcement learning (RL)-based agent designed for batch-free,
immediate mid-price forecasting. ALPE incorporates adaptive epsilon decay to
dynamically balance exploration and exploitation, outperforming a diverse range
of highly effective machine learning (ML) and deep learning (DL) models in
forecasting performance.",2024-12-26T22:49:53Z,http://arxiv.org/abs/2412.19372v1,"Adamantios Ntakaris, Gbenga Ibikunle"
Velocity-dependent self-interacting dark matter and composite Higgs,"We show that the mass of a self-interacting dark matter candidate,
specifically a Dirac fermion, can be generated by composite dynamics, with a
light scalar mediator emerging alongside the Higgs itself as composite
particles. These novel models naturally explain the halo structure problems at
various scales and alleviates the Standard Model naturalness problem
simultaneously. The relic density of the dark matter candidates is particle
anti-particle symmetric and due to thermal freeze-out. These models are
four-dimensional gauge theories with a minimal number of fermions charged under
a new confining gauge group. Finally, we demonstrate that these models satisfy
various constraints set by the dark matter relic density, Big Bang
Nucleosynthesis, Cosmic Microwave Background, as well as direct and indirect
detection experiments.",2024-12-26T22:42:42Z,http://arxiv.org/abs/2412.19371v1,Martin Rosenlyst
The Deconstruction of Flavor in the Privately Democratic Higgs Sector,"The Standard Model (SM) of particle physics fails to explain the observed
hierarchy in fermion masses or the origin of fermion-flavor structure. We
construct a model to explain these observations in the quark sector. We
introduce a spectrum of new particles consisting of six of each -- massive
singlet vector-like quarks (VLQs), singlet scalars, and SU(2)-doublet scalars.
SM quark masses are generated when the neutral components of the SU(2)-doublet
scalars acquire non-zero vacuum expectation values (VEVs). We impose global
symmetries to ensure that Yukawa couplings stay roughly flavor diagonal and
democratic (of the same order), as well as to suppress tree-level
flavor-changing neutral currents. Quark-mass hierarchy then follows from a
hierarchy in scalar VEVs. The singlet scalars also acquire weak-scale VEVs.
Together with the VLQs, they act as messengers between different generations of
quarks in the SM. These messenger particles are responsible for generating the
elements of the Cabibbo-Kobayashi-Masakawa (CKM) matrix which depend on the
ratios of the singlet VEVs and VLQ masses. Constructed this way, the CKM matrix
is found to be \emph{independent} of the SM fermion masses. Using the measured
values of the CKM matrix elements and assuming order-one couplings, we derive
constraints on the masses of the VLQs and discuss prospects for probing our
model in the near future.",2024-12-26T22:32:31Z,http://arxiv.org/abs/2412.19369v1,"Bhubanjyoti Bhattacharya, Suneth Jayawardana, Nausheen R. Shah"
"Variational integrators for stochastic Hamiltonian systems on Lie
  groups: properties and convergence","We derive variational integrators for stochastic Hamiltonian systems on Lie
groups using a discrete version of the stochastic Hamiltonian phase space
principle. The structure-preserving properties of the resulting scheme, such as
symplecticity, preservation of the Lie-Poisson structure, preservation of the
coadjoint orbits, and conservation of Casimir functions, are discussed, along
with a discrete Noether theorem for subgroup symmetries. We also consider in
detail the case of stochastic Hamiltonian systems with advected quantities,
studying the associated structure-preserving properties in relation to
semidirect product Lie groups. A full convergence proof for the scheme is
provided for the case of the Lie group of rotations. Several numerical examples
are presented, including simulations of the free rigid body and the heavy top.",2024-12-26T22:25:37Z,http://arxiv.org/abs/2412.19368v1,"François Gay-Balmaz, Meng Wu"
Large Language Models for Market Research: A Data-augmentation Approach,"Large Language Models (LLMs) have transformed artificial intelligence by
excelling in complex natural language processing tasks. Their ability to
generate human-like text has opened new possibilities for market research,
particularly in conjoint analysis, where understanding consumer preferences is
essential but often resource-intensive. Traditional survey-based methods face
limitations in scalability and cost, making LLM-generated data a promising
alternative. However, while LLMs have the potential to simulate real consumer
behavior, recent studies highlight a significant gap between LLM-generated and
human data, with biases introduced when substituting between the two. In this
paper, we address this gap by proposing a novel statistical data augmentation
approach that efficiently integrates LLM-generated data with real data in
conjoint analysis. Our method leverages transfer learning principles to debias
the LLM-generated data using a small amount of human data. This results in
statistically robust estimators with consistent and asymptotically normal
properties, in contrast to naive approaches that simply substitute human data
with LLM-generated data, which can exacerbate bias. We validate our framework
through an empirical study on COVID-19 vaccine preferences, demonstrating its
superior ability to reduce estimation error and save data and costs by 24.9\%
to 79.8\%. In contrast, naive approaches fail to save data due to the inherent
biases in LLM-generated data compared to human data. Another empirical study on
sports car choices validates the robustness of our results. Our findings
suggest that while LLM-generated data is not a direct substitute for human
responses, it can serve as a valuable complement when used within a robust
statistical framework.",2024-12-26T22:06:29Z,http://arxiv.org/abs/2412.19363v1,"Mengxin Wang, Dennis J. Zhang, Heng Zhang"
"Evaluating Convolutional Neural Networks for COVID-19 classification in
  chest X-ray images","Coronavirus Disease 2019 (COVID-19) pandemic rapidly spread globally,
impacting the lives of billions of people. The effective screening of infected
patients is a critical step to struggle with COVID-19, and treating the
patients avoiding this quickly disease spread. The need for automated and
scalable methods has increased due to the unavailability of accurate automated
toolkits. Recent researches using chest X-ray images suggest they include
relevant information about the COVID-19 virus. Hence, applying machine learning
techniques combined with radiological imaging promises to identify this disease
accurately. It is straightforward to collect these images once it is spreadly
shared and analyzed in the world. This paper presents a method for automatic
COVID-19 detection using chest Xray images through four convolutional neural
networks, namely: AlexNet, VGG-11, SqueezeNet, and DenseNet-121. This method
had been providing accurate diagnostics for positive or negative COVID-19
classification. We validate our experiments using a ten-fold cross-validation
procedure over the training and test sets. Our findings include the shallow
fine-tuning and data augmentation strategies that can assist in dealing with
the low number of positive COVID-19 images publicly available. The accuracy for
all CNNs is higher than 97.00%, and the SqueezeNet model achieved the best
result with 99.20%.",2024-12-26T22:05:30Z,http://arxiv.org/abs/2412.19362v1,"Leonardo Gabriel Ferreira Rodrigues, Danilo Ferreira da Silva, Larissa Ferreira Rodrigues, João Fernando Mari"
Dynamic Skill Adaptation for Large Language Models,"We present Dynamic Skill Adaptation (DSA), an adaptive and dynamic framework
to adapt novel and complex skills to Large Language Models (LLMs). Compared
with previous work which learns from human-curated and static data in random
orders, we propose to first automatically generate and organize the training
data by mimicking the learning pathways of human and then dynamically tailor
the training data based on the training dynamics. Specifically, inspired by the
learning structures and teaching strategies in the human education system, we
first construct a skill graph by decomposing complex skills into sub-skills and
arranging them based on their dependencies in human syllables. For every skill,
we utilize LLMs to generate both textbook-like data which contains detailed
descriptions of skills for pre-training and exercise-like data which targets at
explicitly utilizing the skills to solve problems for instruction-tuning.
Furthermore, during the instruction-tuning, we dynamically update the training
data which down-weight easy-to-learn examples, generate more complex examples,
and filter out data with errors. Experiments on large language models such as
LLAMA and Mistral demonstrate the effectiveness of our proposed methods in
adapting math reasoning skills and social study skills.",2024-12-26T22:04:23Z,http://arxiv.org/abs/2412.19361v1,"Jiaao Chen, Diyi Yang"
"Improving the network traffic classification using the Packet Vision
  approach","The network traffic classification allows improving the management, and the
network services offer taking into account the kind of application. The future
network architectures, mainly mobile networks, foresee intelligent mechanisms
in their architectural frameworks to deliver application-aware network
requirements. The potential of convolutional neural networks capabilities,
widely exploited in several contexts, can be used in network traffic
classification. Thus, it is necessary to develop methods based on the content
of packets transforming it into a suitable input for CNN technologies. Hence,
we implemented and evaluated the Packet Vision, a method capable of building
images from packets raw-data, considering both header and payload. Our approach
excels those found in state-of-the-art by delivering security and privacy by
transforming the raw-data packet into images. Therefore, we built a dataset
with four traffic classes evaluating the performance of three CNNs
architectures: AlexNet, ResNet-18, and SqueezeNet. Experiments showcase the
Packet Vision combined with CNNs applicability and suitability as a promising
approach to deliver outstanding performance in classifying network traffic.",2024-12-26T21:56:03Z,http://arxiv.org/abs/2412.19360v1,"Rodrigo Moreira, Larissa Ferreira Rodrigues, Pedro Frosi Rosa, Flávio de Oliveira Silva"
"Federated Hybrid Training and Self-Adversarial Distillation: Towards
  Robust Edge Networks","Federated learning (FL) is a distributed training technology that enhances
data privacy in mobile edge networks by allowing data owners to collaborate
without transmitting raw data to the edge server. However, data heterogeneity
and adversarial attacks pose challenges to develop an unbiased and robust
global model for edge deployment. To address this, we propose Federated hyBrid
Adversarial training and self-adversarial disTillation (FedBAT), a new
framework designed to improve both robustness and generalization of the global
model. FedBAT seamlessly integrates hybrid adversarial training and
self-adversarial distillation into the conventional FL framework from data
augmentation and feature distillation perspectives. From a data augmentation
perspective, we propose hybrid adversarial training to defend against
adversarial attacks by balancing accuracy and robustness through a weighted
combination of standard and adversarial training. From a feature distillation
perspective, we introduce a novel augmentation-invariant adversarial
distillation method that aligns local adversarial features of augmented images
with their corresponding unbiased global clean features. This alignment can
effectively mitigate bias from data heterogeneity while enhancing both the
robustness and generalization of the global model. Extensive experimental
results across multiple datasets demonstrate that FedBAT yields comparable or
superior performance gains in improving robustness while maintaining accuracy
compared to several baselines.",2024-12-26T21:32:08Z,http://arxiv.org/abs/2412.19354v1,"Yu Qiao, Apurba Adhikary, Kitae Kim, Eui-Nam Huh, Zhu Han, Choong Seon Hong"
ETTA: Elucidating the Design Space of Text-to-Audio Models,"Recent years have seen significant progress in Text-To-Audio (TTA) synthesis,
enabling users to enrich their creative workflows with synthetic audio
generated from natural language prompts. Despite this progress, the effects of
data, model architecture, training objective functions, and sampling strategies
on target benchmarks are not well understood. With the purpose of providing a
holistic understanding of the design space of TTA models, we set up a
large-scale empirical experiment focused on diffusion and flow matching models.
Our contributions include: 1) AF-Synthetic, a large dataset of high quality
synthetic captions obtained from an audio understanding model; 2) a systematic
comparison of different architectural, training, and inference design choices
for TTA models; 3) an analysis of sampling methods and their Pareto curves with
respect to generation quality and inference speed. We leverage the knowledge
obtained from this extensive analysis to propose our best model dubbed
Elucidated Text-To-Audio (ETTA). When evaluated on AudioCaps and MusicCaps,
ETTA provides improvements over the baselines trained on publicly available
data, while being competitive with models trained on proprietary data. Finally,
we show ETTA's improved ability to generate creative audio following complex
and imaginative captions -- a task that is more challenging than current
benchmarks.",2024-12-26T21:13:12Z,http://arxiv.org/abs/2412.19351v1,"Sang-gil Lee, Zhifeng Kong, Arushi Goel, Sungwon Kim, Rafael Valle, Bryan Catanzaro"
"On the Expressiveness and Length Generalization of Selective State-Space
  Models on Regular Languages","Selective state-space models (SSMs) are an emerging alternative to the
Transformer, offering the unique advantage of parallel training and sequential
inference. Although these models have shown promising performance on a variety
of tasks, their formal expressiveness and length generalization properties
remain underexplored. In this work, we provide insight into the workings of
selective SSMs by analyzing their expressiveness and length generalization
performance on regular language tasks, i.e., finite-state automaton (FSA)
emulation. We address certain limitations of modern SSM-based architectures by
introducing the Selective Dense State-Space Model (SD-SSM), the first selective
SSM that exhibits perfect length generalization on a set of various regular
language tasks using a single layer. It utilizes a dictionary of dense
transition matrices, a softmax selection mechanism that creates a convex
combination of dictionary matrices at each time step, and a readout consisting
of layer normalization followed by a linear map. We then proceed to evaluate
variants of diagonal selective SSMs by considering their empirical performance
on commutative and non-commutative automata. We explain the experimental
results with theoretical considerations. Our code is available at
https://github.com/IBM/selective-dense-state-space-model.",2024-12-26T20:53:04Z,http://arxiv.org/abs/2412.19350v1,"Aleksandar Terzić, Michael Hersche, Giacomo Camposampiero, Thomas Hofmann, Abu Sebastian, Abbas Rahimi"
"Semi-Supervised Learning from Small Annotated Data and Large Unlabeled
  Data for Fine-grained PICO Entity Recognition","Objective: Extracting PICO elements -- Participants, Intervention,
Comparison, and Outcomes -- from clinical trial literature is essential for
clinical evidence retrieval, appraisal, and synthesis. Existing approaches do
not distinguish the attributes of PICO entities. This study aims to develop a
named entity recognition (NER) model to extract PICO entities with fine
granularities.
  Materials and Methods: Using a corpus of 2,511 abstracts with PICO mentions
from 4 public datasets, we developed a semi-supervised method to facilitate the
training of a NER model, FinePICO, by combining limited annotated data of PICO
entities and abundant unlabeled data. For evaluation, we divided the entire
dataset into two subsets: a smaller group with annotations and a larger group
without annotations. We then established the theoretical lower and upper
performance bounds based on the performance of supervised learning models
trained solely on the small, annotated subset and on the entire set with
complete annotations, respectively. Finally, we evaluated FinePICO on both the
smaller annotated subset and the larger, initially unannotated subset. We
measured the performance of FinePICO using precision, recall, and F1.
  Results: Our method achieved precision/recall/F1 of 0.567/0.636/0.60,
respectively, using a small set of annotated samples, outperforming the
baseline model (F1: 0.437) by more than 16\%. The model demonstrates
generalizability to a different PICO framework and to another corpus, which
consistently outperforms the benchmark in diverse experimental settings
(p-value \textless0.001).
  Conclusion: This study contributes a generalizable and effective
semi-supervised approach to named entity recognition leveraging large unlabeled
data together with small, annotated data. It also initially supports
fine-grained PICO extraction.",2024-12-26T20:24:35Z,http://arxiv.org/abs/2412.19346v1,"Fangyi Chen, Gongbo Zhang, Yilu Fang, Yifan Peng, Chunhua Weng"
Advanced Scheduling of Electrolyzer Modules for Grid Flexibility,"As the transition to sustainable power generation progresses, green hydrogen
production via electrolysis is expected to gain importance as a means for
energy storage and flexible load to complement variable renewable generation.
With the increasing need for cost-effective and efficient hydrogen production,
electrolyzer optimization is essential to improve both energy efficiency and
profitability. This paper analyzes how the efficiency and modular setup of
alkaline hydrogen electrolyzers can improve hydrogen output of systems linked
to a fluctuating renewable power supply. To explore this, we propose a
day-ahead optimal scheduling problem of a hybrid wind and electrolyzer system.
The novelty of our approach lies in modeling the number and capacity of
electrolyzer modules, and capturing the modules' impact on the hydrogen
production and efficiency. We solve the resulting mixed-integer optimization
problem with several different combinations of number of modules, efficiency
and operating range parameters, using day-ahead market data from a wind farm
generator in the ERCOT system as an input. Our results demonstrate that the
proposed approach ensures that electrolyzer owners can better optimize the
operation of their systems, achieving greater hydrogen production and higher
revenue. Key findings include that as the number of modules in a system with
the same overall capacity increases, hydrogen production and revenue increases.",2024-12-26T20:24:01Z,http://arxiv.org/abs/2412.19345v1,"Angelina Lesniak, Andrea Gloppen Johnsen, Noah Rhodes, Line Roald"
"Sparse recovery from quadratic equations, part II: hardness and
  incoherence","We study the square root bottleneck in the recovery of sparse vectors from
quadratic equations. It is acknowledged that a sparse vector $ \mathbf x_0\in
\mathbb{R}^n$, $\| \mathbf x_0\|_0 = k$ can in theory be recovered from as few
as $O(k)$ generic quadratic equations but no polynomial time algorithm is known
for this task unless $m = \Omega(k^2)$. This bottleneck was in fact shown in
previous work to be essentially related to the initialization of descent
algorithms. Starting such algorithms sufficiently close to the planted signal
is known to imply convergence to this signal. In this paper, we show that as
soon as $m\gtrsim \mu_0^{-2}k \vee \mu_0^{-4}$ (up to log factors) where $\mu_0
= \| \mathbf x_0\|_\infty/\| \mathbf x_0\|_2$, it is possible to recover a
$k$-sparse vector $ \mathbf x_0\in \mathbb{R}^n$ from $m$ quadratic equations
of the form $\langle \mathbf A_i, \mathbf x \mathbf x^\intercal\rangle =
\langle \mathbf A_i, \mathbf x_0 \mathbf x_0^\intercal\rangle + \varepsilon_i $
by minimizing the classical empirical loss. The proof idea carries over to the
phase retrieval setting for which it provides an original initialization that
matches the current optimal sample complexity (see e.g. [Cai 2023]). In the
maximally incoherent regime $\mu_0^{-2}=k$, and for $m=o(k^2)$ we provide
evidence for topological hardness by showing that a property known as the
Overlap Gap Property (OGP), which originated in spin glass theory and is
conjectured to be indicative of algorithmic intractability when optimizing over
random structures, holds for a particular level of overparametrization. The key
ingredient of the proof is a lower bound on the tail of chi-squared random
variables which follows from the theory of moderate deviations.",2024-12-26T20:11:44Z,http://arxiv.org/abs/2412.19341v1,Augustin Cosse
Moduli spaces of polynomial maps and multipliers at small cycles,"Fix an integer $d \geq 2$. The space $\mathcal{P}_{d}$ of polynomial maps of
degree $d$ modulo conjugation by affine transformations is naturally an affine
variety over $\mathbb{Q}$ of dimension $d -1$. For each integer $P \geq 1$, the
elementary symmetric functions of the multipliers at all the cycles with period
$p \in \lbrace 1, \dotsc, P \rbrace$ induce a natural morphism
$\operatorname{Mult}_{d}^{(P)}$ defined on $\mathcal{P}_{d}$. In this article,
we show that the morphism $\operatorname{Mult}_{d}^{(2)}$ induced by the
multipliers at the cycles with periods $1$ and $2$ is both finite and
birational onto its image. In the case of polynomial maps, this strengthens
results by McMullen and by Ji and Xie stating that
$\operatorname{Mult}_{d}^{(P)}$ is quasifinite and birational onto its image
for all sufficiently large integers $P$. Our result arises as the combination
of the following two statements: $\mathord{\bullet}$ A sequence of polynomials
over $\mathbb{C}$ of degree $d$ with bounded multipliers at its cycles with
periods $1$ and $2$ is necessarily bounded in $\mathcal{P}_{d}(\mathbb{C})$.
$\mathord{\bullet}$ A generic conjugacy class of polynomials over $\mathbb{C}$
of degree $d$ is uniquely determined by its multipliers at its cycles with
periods $1$ and $2$.",2024-12-26T19:13:17Z,http://arxiv.org/abs/2412.19335v1,Valentin Huguin
"DPmoire: A tool for constructing accurate machine learning force fields
  in moiré systems","In moir\'e systems, the impact of lattice relaxation on electronic band
structures is significant, yet the computational demands of first-principles
relaxation are prohibitively high due to the large number of atoms involved. To
address this challenge, We introduce a robust methodology for the construction
of machine learning potentials specifically tailored for moir\'e structures and
present an open-source software package DPmoire designed to facilitate this
process. Utilizing this package, we have developed machine learning force
fields (MLFFs) for MX$_2$ (M = Mo, W; X = S, Se, Te) materials. Our approach
not only streamlines the computational process but also ensures accurate
replication of the detailed electronic and structural properties typically
observed in density functional theory (DFT) relaxations. The MLFFs were
rigorously validated against standard DFT results, confirming their efficacy in
capturing the complex interplay of atomic interactions within these layered
materials. This development not only enhances our ability to explore the
physical properties of moir\'e systems with reduced computational overhead but
also opens new avenues for the study of relaxation effects and their impact on
material properties in two-dimensional layered structures.",2024-12-26T19:00:40Z,http://arxiv.org/abs/2412.19333v1,"Jiaxuan Liu, Zhong Fang, Hongming Weng, Quansheng Wu"
Identifying Split Vacancies with Foundation Models and Electrostatics,"Point defects are ubiquitous in solid-state compounds, dictating many
functional properties such as conductivity, catalytic activity and carrier
recombination. Over the past decade, the prevalence of metastable defect
geometries and their importance to relevant properties has been increasingly
recognised. A particularly striking example of this is split vacancies, where
an isolated atomic vacancy transforms to a stoichiometry-conserving complex of
two vacancies and an interstitial ($V_X \rightarrow [V_X + X_i + V_X]$), which
can be accompanied by a dramatic lowering of the defect energy and change in
behaviour. Such species are particularly challenging to identify from
computation, due to the `non-local' nature of this reconstruction. Here, I
present an approach for efficiently identifying such species in solid-state
compounds, through tiered screening which combines geometric analysis,
electrostatic energies and foundation machine learning (ML) models. This
approach allows the screening of all compounds in the Materials Project
database (including all entries in the ICSD, along with several thousand
predicted metastable materials), identifying thousands of split vacancy
configurations, hitherto unknown. This study highlights both the potential
utility of machine-learning potentials for defect investigations, with
important caveats, and the importance of global optimisation approaches for
correctly identifying stable defect geometries.",2024-12-26T18:58:52Z,http://arxiv.org/abs/2412.19330v1,Seán R. Kavanagh
"Deep learning and whole-brain networks for biomarker discovery: modeling
  the dynamics of brain fluctuations in resting-state and cognitive tasks","Background: Brain network models offer insights into brain dynamics, but the
utility of model-derived bifurcation parameters as biomarkers remains
underexplored. Objective: This study evaluates bifurcation parameters from a
whole-brain network model as biomarkers for distinguishing brain states
associated with resting-state and task-based cognitive conditions. Methods:
Synthetic BOLD signals were generated using a supercritical Hopf brain network
model to train deep learning models for bifurcation parameter prediction.
Inference was performed on Human Connectome Project data, including both
resting-state and task-based conditions. Statistical analyses assessed the
separability of brain states based on bifurcation parameter distributions.
Results: Bifurcation parameter distributions differed significantly across task
and resting-state conditions ($p &lt; 0.0001$ for all but one comparison).
Task-based brain states exhibited higher bifurcation values compared to rest.
Conclusion: Bifurcation parameters effectively differentiate cognitive and
resting states, warranting further investigation as biomarkers for brain state
characterization and neurological disorder assessment.",2024-12-26T18:58:38Z,http://arxiv.org/abs/2412.19329v1,"Facundo Roffet, Gustavo Deco, Claudio Delrieux, Gustavo Patow"
"Resolving the Ambiguity of Complete-to-Partial Point Cloud Registration
  for Image-Guided Liver Surgery with Patches-to-Partial Matching","In image-guided liver surgery, the initial rigid alignment between
preoperative and intraoperative data, often represented as point clouds, is
crucial for providing sub-surface information from preoperative CT/MRI images
to the surgeon during the procedure. Currently, this alignment is typically
performed using semi-automatic methods, which, while effective to some extent,
are prone to errors that demand manual correction. Point cloud
correspondence-based registration methods are promising to serve as a fully
automatic solution. However, they may struggle in scenarios with limited
intraoperative surface visibility, a common challenge in liver surgery,
particularly in laparoscopic procedures, which we refer to as
complete-to-partial ambiguity. We first illustrate this ambiguity by evaluating
the performance of state-of-the-art learning-based point cloud registration
methods on our carefully constructed in silico and in vitro datasets. Then, we
propose a patches-to-partial matching strategy as a plug-and-play module to
resolve the ambiguity, which can be seamlessly integrated into learning-based
registration methods without disrupting their end-to-end structure. It has
proven effective and efficient in improving registration performance for cases
with limited intraoperative visibility. The constructed benchmark and the
proposed module establish a solid foundation for advancing applications of
point cloud correspondence-based registration methods in image-guided liver
surgery.",2024-12-26T18:58:29Z,http://arxiv.org/abs/2412.19328v1,"Zixin Yang, Jon S. Heiselman, Cheng Han, Kelly Merrell, Richard Simon, Cristian. A. Linte"
"Performance Control in Early Exiting to Deploy Large Models at the Same
  Cost of Smaller Ones","Early Exiting (EE) is a promising technique for speeding up inference by
adaptively allocating compute resources to data points based on their
difficulty. The approach enables predictions to exit at earlier layers for
simpler samples while reserving more computation for challenging ones. In this
study, we first present a novel perspective on the EE approach, showing that
larger models deployed with EE can achieve higher performance than smaller
models while maintaining similar computational costs. As existing EE approaches
rely on confidence estimation at each exit point, we further study the impact
of overconfidence on the controllability of the compute-performance trade-off.
We introduce Performance Control Early Exiting (PCEE), a method that enables
accuracy thresholding by basing decisions not on a data point's confidence but
on the average accuracy of samples with similar confidence levels from a
held-out validation set. In our experiments, we show that PCEE offers a simple
yet computationally efficient approach that provides better control over
performance than standard confidence-based approaches, and allows us to scale
up model sizes to yield performance gain while reducing the computational cost.",2024-12-26T18:54:32Z,http://arxiv.org/abs/2412.19325v1,"Mehrnaz Mofakhami, Reza Bayat, Ioannis Mitliagkas, Joao Monteiro, Valentina Zantedeschi"
Electron efficiency in LHC Run-2 with the ATLAS experiment,"The document presents a general overview of the electron reconstruction,
identification and isolation performance in the ATLAS experiment. The results
are obtained using 13 TeV proton-proton collision data collected during the LHC
Run-2. The electron reconstruction efficiency is higher than 97%, and the ratio
of data to Monte Carlo simulation efficiency is close to unity, with associated
uncertainties generally smaller than 0.1%. The electron identification is shown
for three working points, and depending on the electron $E_T$, it can be as low
as 60%, increasing to more than 80% above 50 GeV. The correction factors are
close to one, generally within 5%. Five isolation working points are
recommended in the ATLAS experiment, to successfully reject fake/non-prompt
electrons. Their dependency on the electron identification working points is
shown and discussed, as well as their pile-up dependency, and their performance
versus electron $E_T$ and $\eta$.
  Document based on a presentation at the XI International Conference on New
Frontiers in Physics (ICNFP 2022).
  keywords; prompt electrons, reconstruction, identification, isolation,
fake/non-prompt electrons",2024-12-26T18:50:43Z,http://arxiv.org/abs/2412.19323v1,Otilia Ducu
"A novel framework for MCDM based on Z numbers and soft likelihood
  function","The optimization on the structure of process of information management under
uncertain environment has attracted lots of attention from researchers around
the world. Nevertheless, how to obtain accurate and rational evaluation from
assessments produced by experts is still an open problem. Specially,
intuitionistic fuzzy set provides an effective solution in handling
indeterminate information. And Yager proposes a novel method for fusion of
probabilistic evidence to handle uncertain and conflicting information lately
which is called soft likelihood function. This paper devises a novel framework
of soft likelihood function based on information volume of fuzzy membership and
credibility measure for extracting truly useful and valuable information from
uncertainty. An application is provided to verify the validity and correctness
of the proposed framework. Besides, the comparisons with other existing methods
further demonstrate the superiority of the novel framework of soft likelihood
function.",2024-12-26T18:47:19Z,http://arxiv.org/abs/2412.19321v1,Yuanpeng He
Adaptive Conformal Inference by Betting,"Conformal prediction is a valuable tool for quantifying predictive
uncertainty of machine learning models. However, its applicability relies on
the assumption of data exchangeability, a condition which is often not met in
real-world scenarios. In this paper, we consider the problem of adaptive
conformal inference without any assumptions about the data generating process.
Existing approaches for adaptive conformal inference are based on optimizing
the pinball loss using variants of online gradient descent. A notable
shortcoming of such approaches is in their explicit dependence on and
sensitivity to the choice of the learning rates. In this paper, we propose a
different approach for adaptive conformal inference that leverages
parameter-free online convex optimization techniques. We prove that our method
controls long-term miscoverage frequency at a nominal level and demonstrate its
convincing empirical performance without any need of performing cumbersome
parameter tuning.",2024-12-26T18:42:08Z,http://arxiv.org/abs/2412.19318v1,"Aleksandr Podkopaev, Darren Xu, Kuang-Chih Lee"
"ATLAS searches for higgsinos with R-parity violating couplings in events
  with leptons","This document presents two searches for Supersymmetry through the direct
production of pairs of higgsinos decaying into final states with leptons and
($b$-) jets. The analyses are performed using 139~fb$^{-1}$ of the 13~TeV
proton-proton collision data collected with the ATLAS detector. The methods
used to estimate the Standard Model and detector backgrounds are discussed, as
well as their shortcomings. Finally, results in selected signal regions, and
some exclusion limits, are presented, illustrating the significant improvement
over the previous exclusion limits.
  Document based on a presentation at the XI International Conference on New
Frontiers in Physics (ICNFP 2022).",2024-12-26T18:41:55Z,http://arxiv.org/abs/2412.19317v1,Otilia Ducu
A note on common complements,"We discuss the structure of the set $\Delta$ consisting of pairs of closed
subspaces that have a common complement in a Hilbert space previously studied
by Lauzon and Treil (J. Funct. Anal. 212: 500--512, 2004). We prove that
$\Delta$ is the base space of a real analytic fiber bundle constructed in terms
of geometric objects associated to the Grassmann manifold. As a consequence we
determine the homotopy type of $\Delta$.",2024-12-26T18:39:56Z,http://arxiv.org/abs/2412.19316v1,"Esteban Andruchow, Eduardo Chiumiento"
Towards a Single ASR Model That Generalizes to Disordered Speech,"This study investigates the impact of integrating a dataset of disordered
speech recordings ($\sim$1,000 hours) into the fine-tuning of a near
state-of-the-art ASR baseline system. Contrary to what one might expect,
despite the data being less than 1% of the training data of the ASR system, we
find a considerable improvement in disordered speech recognition accuracy.
Specifically, we observe a 33% improvement on prompted speech, and a 26%
improvement on a newly gathered spontaneous, conversational dataset of
disordered speech. Importantly, there is no significant performance decline on
standard speech recognition benchmarks. Further, we observe that the proposed
tuning strategy helps close the gap between the baseline system and
personalized models by 64% highlighting the significant progress as well as the
room for improvement. Given the substantial benefits of our findings, this
experiment suggests that from a fairness perspective, incorporating a small
fraction of high quality disordered speech data in a training recipe is an easy
step that could be done to make speech technology more accessible for users
with speech disabilities.",2024-12-26T18:39:15Z,http://arxiv.org/abs/2412.19315v1,"Jimmy Tobin, Katrin Tomanek, Subhashini Venugopalan"
NNLO QCD corrections to unpolarized and polarized SIDIS,"The semi-inclusive deep-inelastic scattering (SIDIS) process requires the
presence of an identified hadron H$'$ in the final state, which arises from the
scattering of a lepton with an initial hadron P. By employing factorization in
quantum chromodynamics (QCD), SIDIS provides essential knowledge on the hadron
structure, enabling the exploration of parton distribution functions (PDFs) and
fragmentation functions (FFs). The coefficient functions for SIDIS can be
calculated in perturbative QCD and are currently known to the
next-to-next-to-leading order (NNLO) for the cases, where the incoming lepton
and the hadron P are either both polarized or unpolarized. We present a
detailed description of these NNLO computations, including a thorough
discussion of all the partonic channels, the calculation of the amplitudes and
master integrals for the phase-space integration as well as the renormalization
of ultraviolet divergences and mass factorization of infrared divergences in
dimensional regularization through NNLO. We provide an extensive
phenomenological analysis of the effects of NNLO corrections on SIDIS cross
sections for different PDFs and FFs and various kinematics, including those of
the future Electron-Ion Collider (EIC). We find that these corrections are not
only significant but also crucial for reducing the dependence on the
renormalization and factorization scales $\mu_R$ and $\mu_F$ to obtain stable
predictions.",2024-12-26T18:16:32Z,http://arxiv.org/abs/2412.19309v1,"Saurav Goyal, Roman N. Lee, Sven-Olaf Moch, Vaibhav Pathak, Narayan Rana, V. Ravindran"
"Theoretical models for longitudinal coupled-bunch instabilities driven
  by harmonic cavities in electron storage rings","We present a theoretical framework for analyzing longitudinal coupled-bunch
instabilities in double-rf systems with even filling patterns, accounting for
potential-well distortion and multiple azimuthal modes. The linearized Vlasov
equation is solved in the frequency-domain for an arbitrary rf potential to
derive the Lebedev equation. We unified different formulations, obtaining
results from recent publications as particular cases. Applications to Robinson
dipole-quadrupole mode coupling and the periodic transient beam loading
(PTBL)/mode-1 instability are presented. Notably, for the first time,
theoretical predictions of the mode-1 thresholds show excellent agreement with
experimental data. The analysis reveals that the PTBL instability is a
zero-frequency effect dependent on azimuthal mode interactions and resistant to
Landau damping, providing new insights into its mechanism. The methods are
implemented in the open-source package pycolleff, offering a useful
semi-analytical tool for studying instabilities in electron storage rings with
harmonic cavities.",2024-12-26T18:11:39Z,http://arxiv.org/abs/2412.19308v1,Murilo B. Alves
"Perceive, Query &amp; Reason: Enhancing Video QA with Question-Guided
  Temporal Queries","Video Question Answering (Video QA) is a challenging video understanding task
that requires models to comprehend entire videos, identify the most relevant
information based on contextual cues from a given question, and reason
accurately to provide answers. Recent advancements in Multimodal Large Language
Models (MLLMs) have transformed video QA by leveraging their exceptional
commonsense reasoning capabilities. This progress is largely driven by the
effective alignment between visual data and the language space of MLLMs.
However, for video QA, an additional space-time alignment poses a considerable
challenge for extracting question-relevant information across frames. In this
work, we investigate diverse temporal modeling techniques to integrate with
MLLMs, aiming to achieve question-guided temporal modeling that leverages
pre-trained visual and textual alignment in MLLMs. We propose T-Former, a novel
temporal modeling method that creates a question-guided temporal bridge between
frame-wise visual perception and the reasoning capabilities of LLMs. Our
evaluation across multiple video QA benchmarks demonstrates that T-Former
competes favorably with existing temporal modeling approaches and aligns with
recent advancements in video QA.",2024-12-26T17:53:14Z,http://arxiv.org/abs/2412.19304v1,"Roberto Amoroso, Gengyuan Zhang, Rajat Koner, Lorenzo Baraldi, Rita Cucchiara, Volker Tresp"
RecLM: Recommendation Instruction Tuning,"Modern recommender systems aim to deeply understand users' complex
preferences through their past interactions. While deep collaborative filtering
approaches using Graph Neural Networks (GNNs) excel at capturing user-item
relationships, their effectiveness is limited when handling sparse data or
zero-shot scenarios, primarily due to constraints in ID-based embedding
functions. To address these challenges, we propose a model-agnostic
recommendation instruction-tuning paradigm that seamlessly integrates large
language models with collaborative filtering. Our proposed Recommendation
Language Model (RecLM) enhances the capture of user preference diversity
through a carefully designed reinforcement learning reward function that
facilitates self-augmentation of language models. Comprehensive evaluations
demonstrate significant advantages of our approach across various settings, and
its plug-and-play compatibility with state-of-the-art recommender systems
results in notable performance enhancements.",2024-12-26T17:51:54Z,http://arxiv.org/abs/2412.19302v1,"Yangqin Jiang, Yuhao Yang, Lianghao Xia, Da Luo, Kangyi Lin, Chao Huang"
"Sample Complexity of Data-driven Multistage Stochastic Programming under
  Markovian Uncertainty","This work is motivated by the challenges of applying the sample average
approximation (SAA) method to multistage stochastic programming with an unknown
continuous-state Markov process. While SAA is widely used in static and
two-stage stochastic optimization, it becomes computationally intractable in
general multistage settings as the time horizon $T$ increases. Indeed, the
number of samples required to obtain a reasonably accurate solution grows
exponentially$\text{ -- }$a phenomenon known as the curse of dimensionality
with respect to the time horizon. To overcome this limitation, we propose a
novel data-driven approach, the Markov Recombining Scenario Tree (MRST) method,
which constructs an approximate problem using only two independent trajectories
of historical data. Our analysis demonstrates that the MRST method achieves
polynomial sample complexity in $T$, providing a more efficient alternative to
SAA. Numerical experiments on the Linear Quadratic Gaussian problem show that
MRST outperforms SAA, addressing the curse of dimensionality.",2024-12-26T17:46:27Z,http://arxiv.org/abs/2412.19299v1,"Hyuk Park, Grani A. Hanasusanto"
"Transformation of the trapped flux in a SC disc under electromagnetic
  exposure","Superconducting permanent magnets (SCs) with trapped magnetic flux are used
in technical devices (motors, generators, etc.). These magnets endure repeated
magnetic ""shocks"" during operation, which can affect their performance. In this
work, we investigated the dynamic behavior of magnetic induction in the trapped
flux in an SC disk when exposed to stepwise changes in the external magnetic
field, simulating these operational shocks. Our results reveal a direct
correlation between the stepwise changes in the magnetic field and the trapped
flux response, with each increase or decrease in the field inducing a
corresponding 40-50% change in trapped flux for a 600 G field step at
temperature of 5 K. The magnitude of these changes depends on the external
parameters and their dynamics could lead to additional energy dissipation and
potential heating, which may affect the reliability of SC magnets in
applications. A scaling analysis of the induction flux profiles, revealing
roughness exponents in the range of 0.435 to 0.475 was performed as well, and
we determined the Hausdorff dimension of the surface structure.",2024-12-26T17:45:19Z,http://arxiv.org/abs/2412.19298v1,"V. V. Chabanenko, I. Abaloszewa, V. F. Rusakov, O. I. Kuchuk, O. M. Chumak, A. Nabiałek, A. Abaloszew, A. Filippov, R. Puźniak"
Spatio-Temporal Differences in Bike Sharing Usage: A Tale of Six Cities,"This study investigates the spatio-temporal patterns of Bike Sharing System
(BSS) usage in six major cities: New York, London, Tokyo, Boston, Chicago and
Washington D.C. By analyzing data over a 30-day period with comparable climate
and average temperatures, we explored differences in BSS usage between weekdays
and weekends in those cities using Jensen-Shannon divergence (JSD) and rank
distribution analysis. Our findings reveal significant temporal differences in
BSS usage that were commonly observed in all cities, with weekday patterns
dominated by commute peaks and weekend patterns reflecting recreational
activities. Friday emerges as a transitional day, sharing the characteristics
of both weekdays and weekends. Meanwhile, docking station usage rank
distributions show remarkable consistency between weekdays and weekends for
most cities, with London being a unique anomaly. This study highlights the
potential of BSS data to uncover urban mobility patterns and the underlying
structures of cities. The results suggest that BSS usage reflects both
intrinsic user behavior and external influences such as urban planning.",2024-12-26T17:35:28Z,http://arxiv.org/abs/2412.19294v1,"Shu-ichi Kinoshita, Yuya Bando, Hiroki Sayama"
RAG with Differential Privacy,"Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to
provide *Large Language Models* (LLM) with fresh and relevant context,
mitigating the risk of hallucinations and improving the overall quality of
responses in environments with large and fast moving knowledge bases. However,
the integration of external documents into the generation process raises
significant privacy concerns. Indeed, when added to a prompt, it is not
possible to guarantee a response will not inadvertently expose confidential
data, leading to potential breaches of privacy and ethical dilemmas. This paper
explores a practical solution to this problem suitable to general knowledge
extraction from personal data. It shows *differentially private token
generation* is a viable approach to private RAG.",2024-12-26T17:34:26Z,http://arxiv.org/abs/2412.19291v1,Nicolas Grislain
"ViPCap: Retrieval Text-Based Visual Prompts for Lightweight Image
  Captioning","Recent lightweight image captioning models using retrieved data mainly focus
on text prompts. However, previous works only utilize the retrieved text as
text prompts, and the visual information relies only on the CLIP visual
embedding. Because of this issue, there is a limitation that the image
descriptions inherent in the prompt are not sufficiently reflected in the
visual embedding space. To tackle this issue, we propose ViPCap, a novel
retrieval text-based visual prompt for lightweight image captioning. ViPCap
leverages the retrieved text with image information as visual prompts to
enhance the ability of the model to capture relevant visual information. By
mapping text prompts into the CLIP space and generating multiple randomized
Gaussian distributions, our method leverages sampling to explore randomly
augmented distributions and effectively retrieves the semantic features that
contain image information. These retrieved features are integrated into the
image and designated as the visual prompt, leading to performance improvements
on the datasets such as COCO, Flickr30k, and NoCaps. Experimental results
demonstrate that ViPCap significantly outperforms prior lightweight captioning
models in efficiency and effectiveness, demonstrating the potential for a
plug-and-play solution.",2024-12-26T17:29:38Z,http://arxiv.org/abs/2412.19289v1,"Taewhan Kim, Soeun Lee, Si-Woo Kim, Dong-Jin Kim"
A semi-algebraic model for automatic loop parallelization,"In this work, we introduce a semi-algebraic model for automatic
parallelization of perfectly nested polynomial loops, which generalizes the
classical polyhedral model. This model supports the basic tasks for automatic
loop parallelization, such as the representation of the nested loop, the
dependence analysis, the computation of valid schedules, as well as the
transformation of the loop program with a valid schedule.",2024-12-26T17:19:56Z,http://arxiv.org/abs/2412.19287v1,Changbo Chen
Improving Generalization for AI-Synthesized Voice Detection,"AI-synthesized voice technology has the potential to create realistic human
voices for beneficial applications, but it can also be misused for malicious
purposes. While existing AI-synthesized voice detection models excel in
intra-domain evaluation, they face challenges in generalizing across different
domains, potentially becoming obsolete as new voice generators emerge. Current
solutions use diverse data and advanced machine learning techniques (e.g.,
domain-invariant representation, self-supervised learning), but are limited by
predefined vocoders and sensitivity to factors like background noise and
speaker identity. In this work, we introduce an innovative disentanglement
framework aimed at extracting domain-agnostic artifact features related to
vocoders. Utilizing these features, we enhance model learning in a flat loss
landscape, enabling escape from suboptimal solutions and improving
generalization. Extensive experiments on benchmarks show our approach
outperforms state-of-the-art methods, achieving up to 5.12% improvement in the
equal error rate metric in intra-domain and 7.59% in cross-domain evaluations.",2024-12-26T16:45:20Z,http://arxiv.org/abs/2412.19279v1,"Hainan Ren, Lin Li, Chun-Hao Liu, Xin Wang, Shu Hu"
Memory-Centric Computing: Recent Advances in Processing-in-DRAM,"Memory-centric computing aims to enable computation capability in and near
all places where data is generated and stored. As such, it can greatly reduce
the large negative performance and energy impact of data access and data
movement, by 1) fundamentally avoiding data movement, 2) reducing data access
latency &amp; energy, and 3) exploiting large parallelism of memory arrays. Many
recent studies show that memory-centric computing can largely improve system
performance &amp; energy efficiency. Major industrial vendors and startup companies
have recently introduced memory chips with sophisticated computation
capabilities. Going forward, both hardware and software stack should be
revisited and designed carefully to take advantage of memory-centric computing.
  This work describes several major recent advances in memory-centric
computing, specifically in Processing-in-DRAM, a paradigm where the operational
characteristics of a DRAM chip are exploited and enhanced to perform
computation on data stored in DRAM. Specifically, we describe 1) new techniques
that slightly modify DRAM chips to enable both enhanced computation capability
and easier programmability, 2) new experimental studies that demonstrate the
functionally-complete bulk-bitwise computational capability of real commercial
off-the-shelf DRAM chips, without any modifications to the DRAM chip or the
interface, and 3) new DRAM designs that improve access granularity &amp;
efficiency, unleashing the true potential of Processing-in-DRAM.",2024-12-26T16:31:40Z,http://arxiv.org/abs/2412.19275v1,"Onur Mutlu, Ataberk Olgun, Geraldo F. Oliveira, Ismail Emir Yuksel"
"Anvendelse av kunstig intelligens (KI) i Norge i norsk offentlig sektor
  2024","There are great expectations for the use of AI in Norway. On the other hand,
it is reported that the adoption of AI in Norway is slower than expected in
both the private and public sectors. Using responses from NOKIOS Technology
Radar 2017-2021, IT in Practice surveys conducted by Ramboll in 2021-2024, as
well as another national survey as part of a five-year cycle, this article
looks at reported and planned use of AI with a focus on local (municipalities)
and national government agencies. IT in practice is distributed to a large
number of Norwegian public agencies, with a response rate of over 5o percent.
The most recent data (2024) presented in this article is based on responses
from 335 public organizations, with 237 municipalities, and 98 public
organizations at the national or regional level. The survey confirms that the
use of AI is still at an early stage, although expectations are high for future
use.
  --
  Det er store forventninger til bruk av KI i Norge. P{\aa} den annen side
rapporteres det at adopsjonen av KI i Norge g{\aa}r tregere enn forventet
b{\aa}de i privat og offentlig sektor. Ved hjelp av svar fra NOKIOS
teknologiradar 2017-2021, IT i Praksis unders{\o}kelser utf{\o}rt av Ramb{\o}ll
i 2021-2024, samt en annen nasjonal unders{\o}kelse som en del av en
fem{\aa}rig syklus, ser vi i denne artikkelen p{\aa} rapportert og planlagt
bruk av KI med fokus p{\aa} lokale (kommuner) og nasjonale offentlige etater.
IT i praksis distribueres til en lang rekke norske offentlige virksomheter, med
en svarprosent p{\aa} over 50 prosent. De nyeste dataene (2024) presentert i
denne artikkelen er basert p{\aa} svar fra 335 offentlige organisasjoner, med
237 kommuner, og 98 offentlige organisasjoner p{\aa} nasjonalt eller regionalt
niv{\aa}. Unders{\o}kelsen bekrefter at bruken av KI fortsatt er p{\aa} et
tidlig stadium, selv om forventningene er h{\o}ye til fremtidig bruk.",2024-12-26T16:28:49Z,http://arxiv.org/abs/2412.19273v1,John Krogstie
"Investigating nuclear density profiles to reveal particle-hole
  configurations in the island of inversion","Background: In the mass regions with an abnormal shell structure, the
so-called ``island of inversion,"" the spin-parity of odd-mass nuclei provides
quantitative insights into the shell evolution. However, the experimental
determination of the spin-parity is often challenging, leaving it undetermined
in many nuclei. Purpose: We discuss how the shell structure affects the density
profiles of nuclei in the island of inversion and investigate whether these can
be probed from the total reaction and elastic scattering cross sections.
Method: The antisymmetrized molecular dynamics (AMD) is employed to generate
various particle-hole configurations and predict the energy levels of these
nuclei. The obtained density distributions are used as inputs to the Glauber
model, which is employed to calculate the total reaction and elastic scattering
cross sections for revealing their relationship to the particle-hole
configurations. Results: In addition to the well-known correlation between
nuclear deformation and radius, we show the correlations between the
particle-hole configurations and both central density and diffuseness. We show
that different particle-hole configurations are well reflected in the total
reaction and elastic scattering cross sections. Conclusion: The total reaction
and elastic scattering cross sections are useful probes to identify the
spin-parity of nuclei when different particle-hole configurations coexist.",2024-12-26T16:23:35Z,http://arxiv.org/abs/2412.19270v1,"R. Barman, W. Horiuchi, M. Kimura, R. Chatterjee"
Jet formation studies in AGN: a search for new targets,"In recent years, the jet formation region in active galaxies has been imaged
through mm-VLBI in few ideal targets, first and foremost M87. An important leap
forward for understanding jet launching could be made by identifying a larger
number of suitable objects, characterized by different accretion modes and jet
powers. In this article, we present 1 cm and 7 mm VLBI data of a sample of 16
poorly explored radio galaxies, comprising both High-Excitation (HEG) and
Low-Excitation Galaxies (LEG) and spanning a large range in radio power. The
combination of the sources vicinity (z&lt;0.1) with a large black hole mass
($\log{M_{\rm BH}}$&gt;8.5) results in a high spatial resolution in units of
Schwarzschild radii ($&lt;10^3-10^4$ $R_{\rm S}$), necessary for probing the
region where the jet is initially accelerated and collimated. We aim at
identifying the best candidates for follow-up observations with current and
future VLBI facilities. The observations were performed with the High
Sensitivity Array, including Effelsberg and the phased-VLA, which has allowed
us to characterize the sub-parsec properties of these faint jets and to
estimate their core brightness temperature and orientation. The number of
sources imaged on scales $\lesssim 10^3$ $R_{\rm S}$ is more than doubled by
our study. All targets were detected at both frequencies, and several present
two-sided jet structures. Several LEG jets show hints of limb-brightening. The
core brightness temperatures are generally below the equipartition value,
indicating that equipartition has not yet been reached and/or that the emission
is de-boosted. Among LEG, we identify 3C31, 3C66B, and 3C465 as the most
promising, combining a relatively high flux density (&gt;50 mJy) with superb
spatial resolution (&lt;500 $R_{\rm S}$) at 7 mm. The powerful HEG 3C452 is
interesting as well due to its highly symmetric, two-sided jet base.",2024-12-26T16:20:53Z,http://arxiv.org/abs/2412.19268v1,"B. Boccardi, L. Ricci, E. Madika, V. Bartolini, U. Bach, P. Grandi, E. Torresi, T. P. Krichbaum, J. A. Zensus"
Ultrafast opto-acoustics in single nickel cavities,"Mechanical stress produced in nano- and micro-scale structures can enhance
materials properties, such as the high mobility of silicon in modern
transistors or amplified magnetization dynamics in spintronic devices. Here, we
report on the dynamics of coherent acoustic phonons excited by femtosecond
light pulses and confined in a single freestanding nickel layer, acting as an
acoustic cavity. By combining Fourier transform analysis of the experimental
signal and numerical multi-physics simulations, we show that high-frequency (&gt;
10 GHz) longitudinal acoustic pulses can resonate inside the cavity and display
lower damping compared to a reference nickel film on SiO2 substrate given that
the conditions of total reflection are nearly met in the cavity. Our results
provide a thorough understanding of the opto-acoustic response in suspended
membranes of magnetic materials, which we foresee can be used to amplify
magnetization precession dynamics and to develop magneto-acousto-optical
modulators.",2024-12-26T16:17:03Z,http://arxiv.org/abs/2412.19267v1,"Alba Viejo Rodríguez, Marco Gandolfi, Andrea Rossetti, Yoav Urbina Elgueta, Evgeny B. Modin, Svetlana Starikovskaia, Tatloon Chng, Vasily Temnov, Maria Antonietta Vincenti, Daniele Brida, Paolo Vavassori, Nicolò Maccaferri"
"Search for a neutral gauge boson with nonuniversal fermion couplings in
  vector boson fusion processes in proton-proton collisions at $\sqrt{s}$ = 13
  TeV","The first search for a heavy neutral spin-1 gauge boson (Z') with
nonuniversal fermion couplings produced via vector boson fusion processes and
decaying to tau leptons or W bosons is presented. The analysis is performed
using LHC data at $\sqrt{s}$ = 13 TeV, collected from 2016 to 2018 and
corresponding to an integrated luminosity of 138 fb$^{-1}$. The data are
consistent with the standard model predictions. Upper limits are set on the
product of the cross section for production of the Z' boson and its branching
fraction to $\tau\tau$ or WW. The presence of a Z' boson decaying to
$\tau^+\tau^-$ (W$^+$W$^-$) is excluded for masses up to 2.45 (1.60) TeV,
depending on the Z' boson coupling to SM weak bosons, and assuming a Z' $\to$
$\tau^+\tau^-$ (W$^+$W$^-$) branching fraction of 50%.",2024-12-26T15:54:58Z,http://arxiv.org/abs/2412.19261v1,CMS Collaboration
"MEDEC: A Benchmark for Medical Error Detection and Correction in
  Clinical Notes","Several studies showed that Large Language Models (LLMs) can answer medical
questions correctly, even outperforming the average human score in some medical
exams. However, to our knowledge, no study has been conducted to assess the
ability of language models to validate existing or generated medical text for
correctness and consistency. In this paper, we introduce MEDEC
(https://github.com/abachaa/MEDEC), the first publicly available benchmark for
medical error detection and correction in clinical notes, covering five types
of errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal
Organism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes
from three US hospital systems that were not previously seen by any LLM. The
dataset has been used for the MEDIQA-CORR shared task to evaluate seventeen
participating systems [Ben Abacha et al., 2024]. In this paper, we describe the
data creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,
Claude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and
correcting medical errors requiring both medical knowledge and reasoning
capabilities. We also conducted a comparative study where two medical doctors
performed the same task on the MEDEC test set. The results showed that MEDEC is
a sufficiently challenging benchmark to assess the ability of models to
validate existing or generated notes and to correct medical errors. We also
found that although recent LLMs have a good performance in error detection and
correction, they are still outperformed by medical doctors in these tasks. We
discuss the potential factors behind this gap, the insights from our
experiments, the limitations of current evaluation metrics, and share potential
pointers for future research.",2024-12-26T15:54:10Z,http://arxiv.org/abs/2412.19260v1,"Asma Ben Abacha, Wen-wai Yim, Yujuan Fu, Zhaoyi Sun, Meliha Yetisgen, Fei Xia, Thomas Lin"
"VoiceDiT: Dual-Condition Diffusion Transformer for Environment-Aware
  Speech Synthesis","We present VoiceDiT, a multi-modal generative model for producing
environment-aware speech and audio from text and visual prompts. While aligning
speech with text is crucial for intelligible speech, achieving this alignment
in noisy conditions remains a significant and underexplored challenge in the
field. To address this, we present a novel audio generation pipeline named
VoiceDiT. This pipeline includes three key components: (1) the creation of a
large-scale synthetic speech dataset for pre-training and a refined real-world
speech dataset for fine-tuning, (2) the Dual-DiT, a model designed to
efficiently preserve aligned speech information while accurately reflecting
environmental conditions, and (3) a diffusion-based Image-to-Audio Translator
that allows the model to bridge the gap between audio and image, facilitating
the generation of environmental sound that aligns with the multi-modal prompts.
Extensive experimental results demonstrate that VoiceDiT outperforms previous
models on real-world datasets, showcasing significant improvements in both
audio quality and modality integration.",2024-12-26T15:52:58Z,http://arxiv.org/abs/2412.19259v1,"Jaemin Jung, Junseok Ahn, Chaeyoung Jung, Tan Dat Nguyen, Youngjoon Jang, Joon Son Chung"
"Complexity and Structural Results for the Hull and Convexity Numbers in
  Cycle Convexity for Graph Products","Let $G$ be a graph and $S \subseteq V(G)$. In the cycle convexity, we say
that $S$ is \textit{cycle convex} if for any $u\in V(G)\setminus S$, the
induced subgraph of $S\cup\{u\}$ contains no cycle that includes $u$. The
\textit{cycle convex hull} of $S$ is the smallest convex set containing $S$.
The \textit{cycle hull number} of $G$, denoted by $hn_{cc}(G)$, is the
cardinality of the smallest set $S$ such that the convex hull of $S$ is $V(G)$.
The \textit{convexity number} of $G$, denoted by $C_{cc}(G)$, is the maximum
cardinality of a proper convex set of $V(G)$. This paper studies cycle
convexity in graph products. We show that the cycle hull number is always two
for strong and lexicographic products. For the Cartesian, we establish tight
bounds for this product and provide a closed formula when the factors are
trees, generalizing an existing result for grid graphs. In addition, given a
graph $G$ and an integer $k$, we prove that $hn_{cc}(G) \leq k$ is NP-complete
even if $G$ is a bipartite Cartesian product graph, addressing an open question
in the literature. Furthermore, we present exact formulas for the cycle
convexity number in those three graph products. That leads to the
NP-completeness of, given a graph $G$ and an integer $k$, deciding whether
$C_{cc}(G) \geq k$, when $G$ is a Cartesian, strong or lexicographic product
graph.",2024-12-26T15:50:39Z,http://arxiv.org/abs/2412.19258v1,"Bijo S. Anand, Ullas Chandran S. V., Julliano R. Nascimento, Revathy S. Nair"
"Prospects for detecting the dark matter particles and primordial black
  holes with the Hongmeng mission using the 21 cm global spectrum at cosmic
  dawn","Dark matter is believed to account for a significant portion of the mass in
the universe, exerting a critical influence on the formation and evolution of
cosmic structures. This research delves into the processes of annihilation and
decay of dark matter particles, which generate observable signals that deepen
our comprehension of their characteristics and behaviors. Furthermore, the
study explores the potential role of primordial black holes, with a focus on
the emissions of Hawking radiation that could offer valuable insights into
their distribution and size range. A key aspect of this investigation revolves
around the 21 cm signal, a vital tool for scrutinizing the effects of dark
matter particles and primordial black hole phenomena on the intergalactic
medium. The upcoming Hongmeng mission, featuring a lunar orbital interferometer
array, is poised to revolutionize our ability to observe the 21 cm signal. By
conducting measurements devoid of atmospheric disturbances, the mission will
significantly boost sensitivity to subtle signals associated with dark matter
particle annihilation, decay, and primordial black hole emissions. This study
assesses the expected performance of the Hongmeng mission in detecting these
telltale signs and aims to unveil fresh insights into the nature and
interactions of dark matter particles and primordial black hole emissions
through a meticulous analysis of the global 21 cm spectrum. The mission holds
immense promise for reshaping our understanding of the universe's concealed
components.",2024-12-26T15:47:51Z,http://arxiv.org/abs/2412.19257v1,"Meng-Lin Zhao, Sai Wang, Xin Zhang"
Swarm Contract: A Multi-Sovereign Agent Consensus Mechanism,"Traditional smart contracts on blockchains excel at on-chain, deterministic
logic. However, they have inherent limitations when dealing with large-scale
off-chain data, dynamic multi-step workflows, and scenarios requiring high
flexibility or iterative updates. In this paper, we propose the concept of a
""Swarm Contract"" (Swarm), a multi-agent mechanism wherein several digital life
forms (DLF) or Sovereign Agents (SA) collectively handle complex tasks in
Trusted Execution Environments (TEE). These digital entities are defined as
autonomous software agents that own their code, state, and possibly on-chain
assets, while operating free from centralized control.
  By leveraging a simple multi-signature wallet on-chain, Swarm moves most of
the logic off-chain, achieving trust minimization through multi-agent consensus
rather than a single monolithic on-chain contract. We illustrate these ideas
with a lightweight off-chain auction example - minting and selling 10,000
identical NFTs - to showcase how off-chain coordination can determine a
clearing price and finalize distribution, with each step performed collectively
by multiple agents in TEE. This approach broadens the scope of trustless and
decentralized solutions, potentially benefiting DAO governance, multi-modal
data processing, and cross-chain interoperability.",2024-12-26T15:46:56Z,http://arxiv.org/abs/2412.19256v1,Haowei Yang
"Leveraging Self-Training and Variational Autoencoder for Agitation
  Detection in People with Dementia Using Wearable Sensors","Dementia is a neurodegenerative disorder that has been growing among elder
people over the past decades. This growth profoundly impacts the quality of
life for patients and caregivers due to the symptoms arising from it. Agitation
and aggression (AA) are some of the symptoms of people with severe dementia
(PwD) in long-term care or hospitals. AA not only causes discomfort but also
puts the patients or others at potential risk. Existing monitoring solutions
utilizing different wearable sensors integrated with Artificial Intelligence
(AI) offer a way to detect AA early enough for timely and adequate medical
intervention. However, most studies are limited by the availability of
accurately labeled datasets, which significantly affects the efficacy of such
solutions in real-world scenarios. This study presents a novel comprehensive
approach to detect AA in PwD using physiological data from the Empatica E4
wristbands. The research creates a diverse dataset, consisting of three
distinct datasets gathered from 14 participants across multiple hospitals in
Canada. These datasets have not been extensively explored due to their limited
labeling. We propose a novel approach employing self-training and a variational
autoencoder (VAE) to detect AA in PwD effectively. The proposed approach aims
to learn the representation of the features extracted using the VAE and then
uses a semi-supervised block to generate labels, classify events, and detect
AA. We demonstrate that combining Self-Training and Variational Autoencoder
mechanism significantly improves model performance in classifying AA in PwD.
Among the tested techniques, the XGBoost classifier achieved the highest
accuracy of 90.16\%. By effectively addressing the challenge of limited labeled
data, the proposed system not only learns new labels but also proves its
superiority in detecting AA.",2024-12-26T15:34:25Z,http://arxiv.org/abs/2412.19254v1,"Abeer Badawi, Somayya Elmoghazy, Samira Choudhury, Khalid Elgazzar, Amer Burhan"
"Localized exploration in contextual dynamic pricing achieves
  dimension-free regret","We study the problem of contextual dynamic pricing with a linear demand
model. We propose a novel localized exploration-then-commit (LetC) algorithm
which starts with a pure exploration stage, followed by a refinement stage that
explores near the learned optimal pricing policy, and finally enters a pure
exploitation stage. The algorithm is shown to achieve a minimax optimal,
dimension-free regret bound when the time horizon exceeds a polynomial of the
covariate dimension. Furthermore, we provide a general theoretical framework
that encompasses the entire time spectrum, demonstrating how to balance
exploration and exploitation when the horizon is limited. The analysis is
powered by a novel critical inequality that depicts the
exploration-exploitation trade-off in dynamic pricing, mirroring its existing
counterpart for the bias-variance trade-off in regularized regression. Our
theoretical results are validated by extensive experiments on synthetic and
real-world data.",2024-12-26T15:29:58Z,http://arxiv.org/abs/2412.19252v1,"Jinhang Chai, Yaqi Duan, Jianqing Fan, Kaizheng Wang"
Network double autoregression,"Modeling high-dimensional time series with simple structures is a challenging
problem. This paper proposes a network double autoregression (NDAR) model,
which combines the advantages of network structure and the double
autoregression (DAR) model, to handle high-dimensional, conditionally
heteroscedastic, and network-structured data within a simple framework. The
parameters of the model are estimated using quasi-maximum likelihood
estimation, and the asymptotic properties of the estimators are derived. The
selection of the model's lag order will be based on the Bayesian information
criterion. Finite-sample simulations show that the proposed model performs well
even with moderate time dimensions and network sizes. Finally, the model is
applied to analyze three different categories of stock data.",2024-12-26T15:28:41Z,http://arxiv.org/abs/2412.19251v1,"Tingting Li, Hao Wang"
"A Space Lower Bound for Approximate Membership with Duplicate Insertions
  or Deletions of Nonelements","Designs of data structures for approximate membership queries with
false-positive errors that support both insertions and deletions stipulate the
following two conditions: (1) Duplicate insertions are prohibited, i.e., it is
prohibited to insert an element $x$ if $x$ is currently a member of the
dataset. (2) Deletions of nonelements are prohibited, i.e., it is prohibited to
delete $x$ if $x$ is not currently a member of the dataset. Under these
conditions, the space required for the approximate representation of a datasets
of cardinality $n$ with a false-positive probability of $\epsilon^{+}$ is at
most $(1+o(1))n\cdot\log_2 (1/\epsilon^{+}) + O(n)$ bits [Bender et al., 2018;
Bercea and Even, 2019].
  We prove that if these conditions are lifted, then the space required for the
approximate representation of datasets of cardinality $n$ from a universe of
cardinality $u$ is at least $\frac 12 \cdot (1-\epsilon^{+} -\frac 1n)\cdot
\log \binom{u}{n} -O(n)$ bits.",2024-12-26T15:21:42Z,http://arxiv.org/abs/2412.19249v1,"Aryan Agarwala, Guy Even"
Functional structural equation modeling with latent variables,"Handling latent variables in Structural Equation Models (SEMs) in a case
where both the latent variables and their corresponding indicators in the
measurement error part of the model are random curves presents significant
challenges, especially with sparse data. In this paper, we develop a novel
family of Functional Structural Equation Models (FSEMs) that incorporate latent
variables modeled as Gaussian Processes (GPs). The introduced FSEMs are built
upon functional regression models having response variables modeled as
underlying GPs. The model flexibly adapts to cases when the random curves'
realizations are observed only over a sparse subset of the domain, and the
inferential framework is based on a restricted maximum likelihood approach. The
advantage of this framework lies in its ability and flexibility in handling
various data scenarios, including regularly and irregularly spaced points and
thus missing data. To extract smooth estimates for the functional parameters,
we employ a penalized likelihood approach that selects the smoothing parameters
using a cross-validation method. We evaluate the performance of the proposed
model using simulation studies and a real data example, which suggests that our
model performs well in practice. The uncertainty associated with the estimates
of the functional coefficients is also assessed by constructing confidence
regions for each estimate. The goodness of fit indices that are commonly used
to evaluate the fit of SEMs are developed for the FSEMs introduced in this
paper. Overall, the proposed method is a promising approach for modeling
functional data in SEMs with functional latent variables.",2024-12-26T14:57:14Z,http://arxiv.org/abs/2412.19242v1,"Fatemeh Asgari, Valeria Vitelli, Uta Sailer"
"Effect of Peak Absolute Magnitude of Type Ia Supernovae and Sound
  Horizon Values on Hubble Tension using DESI results","We apply data-motivated priors on the peak absolute magnitude of Type Ia
supernovae ($M$), and on the sound horizon at the drag epoch ($r_d$), to study
their impact on the Hubble tension, when compared to the Planck estimated value
of the Hubble constant. We use the data from Pantheon$+$, cosmic chronometers,
and the latest DESI BAO results for this purpose. We reaffirm the fact that
there is a degeneracy between $M$ and $r_d$, and modifying the $r_d$ values to
reconcile the Hubble tension also requires a change in the peak absolute
magnitude $M$. For certain $M$ and $r_d$ priors, the tension is found to reduce
to as low as (1.2-2) $\sigma$.",2024-12-26T14:51:09Z,http://arxiv.org/abs/2412.19240v1,"Shubham Barua, Shantanu Desai"
SeaMo: A Multi-Seasonal and Multimodal Remote Sensing Foundation Model,"Remote Sensing (RS) data contains a wealth of multi-dimensional information
crucial for Earth observation. Owing to its vast volume, diverse sources, and
temporal properties, RS data is highly suitable for the development of large
Visual Foundation Models (VFMs). VFMs act as robust feature extractors,
learning from extensive RS data, and are subsequently fine-tuned for deployment
in various geoscientific tasks. However, current VFMs in the RS domain are
predominantly pretrained and tailored exclusively for specific characteristics
of RS imagery, neglecting the potential of utilizing the multi-dimensional
properties of RS data. Therefore, in this work, we propose SeaMo, a pioneering
visual foundation model that integrates multi-seasonal and multimodal
information in the RS field. SeaMo is designed to harness multiple properties
of RS data. Within the masked image modeling framework, we employ non-aligned
cropping techniques to extract spatial properties, use multi-source inputs for
multimodal integration, and incorporate temporal-multimodal fusion blocks for
effective assimilation of multi-seasonal data. SeaMo explicitly models the
multi-dimensional properties of RS data, making the model more comprehensive,
robust, and versatile. We applied SeaMo to several downstream geoscience tasks,
which demonstrated exceptional performance. Extensive ablation studies were
conducted to validate the model's superiority.",2024-12-26T14:40:38Z,http://arxiv.org/abs/2412.19237v1,"Xuyang Li, Danfeng Hong, Chenyu Li, Jocelyn Chanussot"
"Are Two Hidden Layers Still Enough for the Physics-Informed Neural
  Networks?","The article discusses the development of various methods and techniques for
initializing and training neural networks with a single hidden layer, as well
as training a separable physics-informed neural network consisting of neural
networks with a single hidden layer to solve physical problems described by
ordinary differential equations (ODEs) and partial differential equations
(PDEs). A method for strictly deterministic initialization of a neural network
with one hidden layer for solving physical problems described by an ODE is
proposed. Modifications to existing methods for weighting the loss function are
given, as well as new methods developed for training strictly
deterministic-initialized neural networks to solve ODEs (detaching, additional
weighting based on the second derivative, predicted solution-based weighting,
relative residuals). An algorithm for physics-informed data-driven
initialization of a neural network with one hidden layer is proposed. A neural
network with pronounced generalizing properties is presented, whose
generalizing abilities of which can be precisely controlled by adjusting
network parameters. A metric for measuring the generalization of such neural
network has been introduced. A gradient-free neuron-by-neuron fitting method
has been developed for adjusting the parameters of a single-hidden-layer neural
network, which does not require the use of an optimizer or solver for its
implementation. The proposed methods have been extended to 2D problems using
the separable physics-informed neural networks approach. Numerous experiments
have been carried out to develop the above methods and approaches. Experiments
on physical problems, such as solving various ODEs and PDEs, have demonstrated
that these methods for initializing and training neural networks with one or
two hidden layers (SPINN) achieve competitive accuracy and, in some cases,
state-of-the-art results.",2024-12-26T14:30:54Z,http://arxiv.org/abs/2412.19235v1,"Vasiliy A. Es'kin, Alexey O. Malkhanov, Mikhail E. Smorkalov"
"Virtual Nodes Can Help: Tackling Distribution Shifts in Federated Graph
  Learning","Federated Graph Learning (FGL) enables multiple clients to jointly train
powerful graph learning models, e.g., Graph Neural Networks (GNNs), without
sharing their local graph data for graph-related downstream tasks, such as
graph property prediction. In the real world, however, the graph data can
suffer from significant distribution shifts across clients as the clients may
collect their graph data for different purposes. In particular, graph
properties are usually associated with invariant label-relevant substructures
(i.e., subgraphs) across clients, while label-irrelevant substructures can
appear in a client-specific manner. The issue of distribution shifts of graph
data hinders the efficiency of GNN training and leads to serious performance
degradation in FGL. To tackle the aforementioned issue, we propose a novel FGL
framework entitled FedVN that eliminates distribution shifts through
client-specific graph augmentation strategies with multiple learnable Virtual
Nodes (VNs). Specifically, FedVN lets the clients jointly learn a set of shared
VNs while training a global GNN model. To eliminate distribution shifts, each
client trains a personalized edge generator that determines how the VNs connect
local graphs in a client-specific manner. Furthermore, we provide theoretical
analyses indicating that FedVN can eliminate distribution shifts of graph data
across clients. Comprehensive experiments on four datasets under five settings
demonstrate the superiority of our proposed FedVN over nine baselines.",2024-12-26T14:16:15Z,http://arxiv.org/abs/2412.19229v1,"Xingbo Fu, Zihan Chen, Yinhan He, Song Wang, Binchi Zhang, Chen Chen, Jundong Li"
Multi-view Fake News Detection Model Based on Dynamic Hypergraph,"With the rapid development of online social networks and the inadequacies in
content moderation mechanisms, the detection of fake news has emerged as a
pressing concern for the public. Various methods have been proposed for fake
news detection, including text-based approaches as well as a series of
graph-based approaches. However, the deceptive nature of fake news renders
text-based approaches less effective. Propagation tree-based methods focus on
the propagation process of individual news, capturing pairwise relationships
but lacking the capability to capture high-order complex relationships. Large
heterogeneous graph-based approaches necessitate the incorporation of
substantial additional information beyond news text and user data, while
hypergraph-based approaches rely on predefined hypergraph structures. To tackle
these issues, we propose a novel dynamic hypergraph-based multi-view fake news
detection model (DHy-MFND) that learns news embeddings across three distinct
views: text-level, propagation tree-level, and hypergraph-level. By employing
hypergraph structures to model complex high-order relationships among multiple
news pieces and introducing dynamic hypergraph structure learning, we optimize
predefined hypergraph structures while learning news embeddings. Additionally,
we introduce contrastive learning to capture authenticity-relevant embeddings
across different views. Extensive experiments on two benchmark datasets
demonstrate the effectiveness of our proposed DHy-MFND compared with a broad
range of competing baselines.",2024-12-26T14:05:51Z,http://arxiv.org/abs/2412.19227v1,"Rongping Ye, Xiaobing Pei"
"Completion as Enhancement: A Degradation-Aware Selective Image Guided
  Network for Depth Completion","In this paper, we introduce the Selective Image Guided Network (SigNet), a
novel degradation-aware framework that transforms depth completion into depth
enhancement for the first time. Moving beyond direct completion using
convolutional neural networks (CNNs), SigNet initially densifies sparse depth
data through non-CNN densification tools to obtain coarse yet dense depth. This
approach eliminates the mismatch and ambiguity caused by direct convolution
over irregularly sampled sparse data. Subsequently, SigNet redefines completion
as enhancement, establishing a self-supervised degradation bridge between the
coarse depth and the targeted dense depth for effective RGB-D fusion. To
achieve this, SigNet leverages the implicit degradation to adaptively select
high-frequency components (e.g., edges) of RGB data to compensate for the
coarse depth. This degradation is further integrated into a multi-modal
conditional Mamba, dynamically generating the state parameters to enable
efficient global high-frequency information interaction. We conduct extensive
experiments on the NYUv2, DIML, SUN RGBD, and TOFDC datasets, demonstrating the
state-of-the-art (SOTA) performance of SigNet.",2024-12-26T14:05:01Z,http://arxiv.org/abs/2412.19225v1,"Zhiqiang Yan, Zhengxue Wang, Kun Wang, Jun Li, Jian Yang"
"Interference-Robust Broadband Rapidly-Varying MIMO Communications: A
  Knowledge-Data Dual Driven Framework","A novel time-efficient framework is proposed for improving the robustness of
a broadband multiple-input multiple-output (MIMO) system against unknown
interference under rapidly-varying channels. A mean-squared error (MSE)
minimization problem is formulated by optimizing the beamformers employed.
Since the unknown interference statistics are the premise for solving the
formulated problem, an interference statistics tracking (IST) module is first
designed. The IST module exploits both the time- and spatial-domain
correlations of the interference-plus-noise (IPN) covariance for the future
predictions with data training. Compared to the conventional signal-free space
sampling approach, the IST module can realize zero-pilot and low-latency
estimation. Subsequently, an interference-resistant hybrid beamforming (IR-HBF)
module is presented, which incorporates both the prior knowledge of the
theoretical optimization method as well as the data-fed training. Taking
advantage of the interpretable network structure, the IR-HBF module enables the
simplified mapping from the interference statistics to the beamforming weights.
The simulations are executed in high-mobility scenarios, where the numerical
results unveil that: 1) the proposed IST module attains promising prediction
accuracy compared to the conventional counterparts under different snapshot
sampling errors; and 2) the proposed IR-HBF module achieves lower MSE with
significantly reduced computational complexity.",2024-12-26T13:59:08Z,http://arxiv.org/abs/2412.19221v1,"Jingjing Zhao, Jing Su, Xianchi Lv, Kaiquan Cai, Yanbo Zhu, Yuanwei Liu, Naofal Al-Dhahir"
"Transformer-Based Wireless Capsule Endoscopy Bleeding Tissue Detection
  and Classification","Informed by the success of the transformer model in various computer vision
tasks, we design an end-to-end trainable model for the automatic detection and
classification of bleeding and non-bleeding frames extracted from Wireless
Capsule Endoscopy (WCE) videos. Based on the DETR model, our model uses the
Resnet50 for feature extraction, the transformer encoder-decoder for bleeding
and non-bleeding region detection, and a feedforward neural network for
classification. Trained in an end-to-end approach on the Auto-WCEBleedGen
Version 1 challenge training set, our model performs both detection and
classification tasks as a single unit. Our model achieves an accuracy, recall,
and F1-score classification percentage score of 98.28, 96.79, and 98.37
respectively, on the Auto-WCEBleedGen version 1 validation set. Further, we
record an average precision (AP @ 0.5), mean-average precision (mAP) of 0.7447
and 0.7328 detection results. This earned us a 3rd place position in the
challenge. Our code is publicly available via
https://github.com/BasitAlawode/WCEBleedGen.",2024-12-26T13:49:39Z,http://arxiv.org/abs/2412.19218v1,"Basit Alawode, Shibani Hamza, Adarsh Ghimire, Divya Velayudhan"
"Applying the maximum entropy principle to multi-species neural networks
  improves species distribution models","The rapid expansion of citizen science initiatives has led to a significant
growth of biodiversity databases, and particularly presence-only (PO)
observations. PO data are invaluable for understanding species distributions
and their dynamics, but their use in Species Distribution Models (SDM) is
curtailed by sampling biases and the lack of information on absences. Poisson
point processes are widely used for SDMs, with Maxent being one of the most
popular methods. Maxent maximises the entropy of a probability distribution
across sites as a function of predefined transformations of environmental
variables, called features. In contrast, neural networks and deep learning have
emerged as a promising technique for automatic feature extraction from complex
input variables. In this paper, we propose DeepMaxent, which harnesses neural
networks to automatically learn shared features among species, using the
maximum entropy principle. To do so, it employs a normalised Poisson loss where
for each species, presence probabilities across sites are modelled by a neural
network. We evaluate DeepMaxent on a benchmark dataset known for its spatial
sampling biases, using PO data for calibration and presence-absence (PA) data
for validation across six regions with different biological groups and
environmental covariates. Our results indicate that DeepMaxent improves model
performance over Maxent and other state-of-the-art SDMs across regions and
taxonomic groups. The method performs particularly well in regions of uneven
sampling, demonstrating substantial potential to improve species distribution
modelling. The method opens the possibility to learn more robust environmental
features predicting jointly many species and scales to arbitrary large numbers
of sites without an increased memory demand.",2024-12-26T13:47:04Z,http://arxiv.org/abs/2412.19217v1,"Maxime Ryckewaert, Diego Marcos, Christophe Botella, Maximilien Servajean, Pierre Bonnet, Alexis Joly"
"Optimizing Fantasy Sports Team Selection with Deep Reinforcement
  Learning","Fantasy sports, particularly fantasy cricket, have garnered immense
popularity in India in recent years, offering enthusiasts the opportunity to
engage in strategic team-building and compete based on the real-world
performance of professional athletes. In this paper, we address the challenge
of optimizing fantasy cricket team selection using reinforcement learning (RL)
techniques. By framing the team creation process as a sequential
decision-making problem, we aim to develop a model that can adaptively select
players to maximize the team's potential performance. Our approach leverages
historical player data to train RL algorithms, which then predict future
performance and optimize team composition. This not only represents a huge
business opportunity by enabling more accurate predictions of high-performing
teams but also enhances the overall user experience. Through empirical
evaluation and comparison with traditional fantasy team drafting methods, we
demonstrate the effectiveness of RL in constructing competitive fantasy teams.
Our results show that RL-based strategies provide valuable insights into player
selection in fantasy sports.",2024-12-26T13:36:18Z,http://arxiv.org/abs/2412.19215v1,"Shamik Bhattacharjee, Kamlesh Marathe, Hitesh Kapoor, Nilesh Patil"
Primordial Power Spectrum of Five Dimensional Uniform Inflation,"Five dimensional (5D) uniform inflation describes a de Sitter (or
approximate) solution of 5D Einstein equations, with cosmological constant and
a 5D Planck scale $M_* \sim 10^9$ GeV. During the inflationary period all
dimensions (compact and non-compact) expand exponentially in terms of the 5D
proper time. This set-up requires about 40 $e$-folds to expand the fifth
dimension from the fundamental length to the micron size. At the end of 5D
inflation (or at any given moment during the inflationary phase) one can
interpret the solution in terms of 4D fields using 4D Planck units from the
relation $M_p^2 = 2 \pi R M_*^3$, which amounts going to the 4D Einstein frame.
This implies that if the compactification radius $R$ expands $N$ $e$-folds,
then the 3D space would expand $3N/2$ $e$-folds as a result of a uniform 5D
inflation. We reexamine the primordial power spectrum predicted by this model
and show that it is consistent with Planck's measurements of the comic
microwave background. The best-fit to Planck data corresponds to $R \sim
10~\mu$m. A departure of the angular power spectrum predicted by 4D cosmology
is visible at multipole moment $\ell \sim 7$.",2024-12-26T13:24:36Z,http://arxiv.org/abs/2412.19213v1,"Luis A. Anchordoqui, Ignatios Antoniadis"
"Towards Better Spherical Sliced-Wasserstein Distance Learning with
  Data-Adaptive Discriminative Projection Direction","Spherical Sliced-Wasserstein (SSW) has recently been proposed to measure the
discrepancy between spherical data distributions in various fields, such as
geology, medical domains, computer vision, and deep representation learning.
However, in the original SSW, all projection directions are treated equally,
which is too idealistic and cannot accurately reflect the importance of
different projection directions for various data distributions. To address this
issue, we propose a novel data-adaptive Discriminative Spherical
Sliced-Wasserstein (DSSW) distance, which utilizes a projected energy function
to determine the discriminative projection direction for SSW. In our new DSSW,
we introduce two types of projected energy functions to generate the weights
for projection directions with complete theoretical guarantees. The first type
employs a non-parametric deterministic function that transforms the projected
Wasserstein distance into its corresponding weight in each projection
direction. This improves the performance of the original SSW distance with
negligible additional computational overhead. The second type utilizes a neural
network-induced function that learns the projection direction weight through a
parameterized neural network based on data projections. This further enhances
the performance of the original SSW distance with less extra computational
overhead. Finally, we evaluate the performance of our proposed DSSW by
comparing it with several state-of-the-art methods across a variety of machine
learning tasks, including gradient flows, density estimation on real earth
data, and self-supervised learning.",2024-12-26T13:23:37Z,http://arxiv.org/abs/2412.19212v1,"Hongliang Zhang, Shuo Chen, Lei Luo, Jian Yang"
"Large Language Models Meet Graph Neural Networks: A Perspective of Graph
  Mining","Graph mining is an important area in data mining and machine learning that
involves extracting valuable information from graph-structured data. In recent
years, significant progress has been made in this field through the development
of graph neural networks (GNNs). However, GNNs are still deficient in
generalizing to diverse graph data. Aiming to this issue, Large Language Models
(LLMs) could provide new solutions for graph mining tasks with their superior
semantic understanding. In this review, we systematically review the
combination and application techniques of LLMs and GNNs and present a novel
taxonomy for research in this interdisciplinary field, which involves three
main categories: GNN-driving-LLM, LLM-driving-GNN, and GNN-LLM-co-driving.
Within this framework, we reveal the capabilities of LLMs in enhancing graph
feature extraction as well as improving the effectiveness of downstream tasks
such as node classification, link prediction, and community detection. Although
LLMs have demonstrated their great potential in handling graph-structured data,
their high computational requirements and complexity remain challenges. Future
research needs to continue to explore how to efficiently fuse LLMs and GNNs to
achieve more powerful graph learning and reasoning capabilities and provide new
impetus for the development of graph mining techniques.",2024-12-26T13:21:09Z,http://arxiv.org/abs/2412.19211v1,"Yuxin You, Zhen Liu, Xiangchao Wen, Yongtao Zhang, Wei Ai"
Context-Aware Deep Learning for Multi Modal Depression Detection,"In this study, we focus on automated approaches to detect depression from
clinical interviews using multi-modal machine learning (ML). Our approach
differentiates from other successful ML methods such as context-aware analysis
through feature engineering and end-to-end deep neural networks for depression
detection utilizing the Distress Analysis Interview Corpus. We propose a novel
method that incorporates: (1) pre-trained Transformer combined with data
augmentation based on topic modelling for textual data; and (2) deep 1D
convolutional neural network (CNN) for acoustic feature modeling. The
simulation results demonstrate the effectiveness of the proposed method for
training multi-modal deep learning models. Our deep 1D CNN and Transformer
models achieved state-of-the-art performance for audio and text modalities
respectively. Combining them in a multi-modal framework also outperforms
state-of-the-art for the combined setting. Code available at
https://github.com/genandlam/multi-modal-depression-detection",2024-12-26T13:19:26Z,http://arxiv.org/abs/2412.19209v1,"Genevieve Lam, Huang Dongyan, Weisi Lin"
"Open cluster dissolution rate and the initial cluster mass function in
  the solar neighbourhood. Modelling the age and mass distributions of clusters
  observed by Gaia","Context. The dissolution rate of open clusters (OCs) and integration of their
stars into the Milky Way's field population has been previously explored using
their age distribution. With the advent of the Gaia mission, we have an
exceptional opportunity to revisit and enhance these studies with ages and
masses from high quality data. Aims. To build a comprehensive Gaia-based OC
mass catalogue which, combined with the age distribution, allows a deeper
investigation of the disruption experienced by OCs within the solar
neighbourhood. Methods. Masses were determined by comparing luminosity
distributions to theoretical luminosity functions. The limiting and core radii
of the clusters were obtained by fitting the King function to their observed
density profiles. We examined the disruption process through simulations of the
build-up and mass evolution of a population of OCs which were compared to the
observed mass and age distributions. Results. Our analysis yielded an OC mass
distribution with a peak at $log(M)$ = 2.7 dex ($\sim 500 M_{\odot}$), as well
as radii for 1724 OCs. Our simulations showed that using a power-law Initial
Cluster Mass Function (ICMF) no parameters were able to reproduce the observed
mass distribution. Moreover, we find that a skew log-normal ICMF provides a
good match to the observations and that the disruption time of a $10^4
M{_\odot}$ OC is $t_4^{tot} = 2.9 \pm 0.4$ Gyr. Conclusions. Our results
indicate that the OC disruption time $t_4^{tot}$ is about twice longer than
previous estimates based solely on OC age distributions. We find that the shape
of the ICMF for bound OCs differs from that of embedded clusters, which could
imply a low typical star formation efficiency of $\leq 20\%$ in OCs. Our
results also suggest a lower limit of $\sim 60 M{_\odot}$ for bound OCs in the
solar neighbourhood.",2024-12-26T12:54:51Z,http://arxiv.org/abs/2412.19204v1,"Duarte Almeida, André Moitinho, Sandro Moreira"
"Connected triangle-free planar graphs whose second largest eigenvalue is
  at most 1","Let $\lambda_2$ be the second largest eigenvalue of the adjacency matrix of a
connected graph. In 2023, Li and Sun \cite{LiSun1} determined all the connected
$\{K_{2,3}, K_4\}$-minor free graphs whose second largest eigenvalue
$\lambda_2\le 1$. As a continuance of it, in this paper we completely identify
all the connected $\{K_5,K_{3,3}\}$-minor free graphs without $C_3$ whose
second largest eigenvalue does not exceed 1. This partially solves an open
problem posed by Li and Sun \cite{LiSun1}: Characterize all connected planar
graphs whose second largest eigenvalue is at most $1.$ Our main tools include
the spectral theory and the local structure characterization of the planar
graph with respect to its girth.",2024-12-26T12:54:32Z,http://arxiv.org/abs/2412.19203v1,"Kun Cheng, Shuchao Li"
"GAIS: A Novel Approach to Instance Selection with Graph Attention
  Networks","Instance selection (IS) is a crucial technique in machine learning that aims
to reduce dataset size while maintaining model performance. This paper
introduces a novel method called Graph Attention-based Instance Selection
(GAIS), which leverages Graph Attention Networks (GATs) to identify the most
informative instances in a dataset. GAIS represents the data as a graph and
uses GATs to learn node representations, enabling it to capture complex
relationships between instances. The method processes data in chunks, applies
random masking and similarity thresholding during graph construction, and
selects instances based on confidence scores from the trained GAT model.
Experiments on 13 diverse datasets demonstrate that GAIS consistently
outperforms traditional IS methods in terms of effectiveness, achieving high
reduction rates (average 96\%) while maintaining or improving model
performance. Although GAIS exhibits slightly higher computational costs, its
superior performance in maintaining accuracy with significantly reduced
training data makes it a promising approach for graph-based data selection.",2024-12-26T12:51:14Z,http://arxiv.org/abs/2412.19201v1,"Zahiriddin Rustamov, Ayham Zaitouny, Rafat Damseh, Nazar Zaki"
"Personalized Dynamic Music Emotion Recognition with Dual-Scale
  Attention-Based Meta-Learning","Dynamic Music Emotion Recognition (DMER) aims to predict the emotion of
different moments in music, playing a crucial role in music information
retrieval. The existing DMER methods struggle to capture long-term dependencies
when dealing with sequence data, which limits their performance. Furthermore,
these methods often overlook the influence of individual differences on emotion
perception, even though everyone has their own personalized emotional
perception in the real world. Motivated by these issues, we explore more
effective sequence processing methods and introduce the Personalized DMER
(PDMER) problem, which requires models to predict emotions that align with
personalized perception. Specifically, we propose a Dual-Scale Attention-Based
Meta-Learning (DSAML) method. This method fuses features from a dual-scale
feature extractor and captures both short and long-term dependencies using a
dual-scale attention transformer, improving the performance in traditional
DMER. To achieve PDMER, we design a novel task construction strategy that
divides tasks by annotators. Samples in a task are annotated by the same
annotator, ensuring consistent perception. Leveraging this strategy alongside
meta-learning, DSAML can predict personalized perception of emotions with just
one personalized annotation sample. Our objective and subjective experiments
demonstrate that our method can achieve state-of-the-art performance in both
traditional DMER and PDMER.",2024-12-26T12:47:35Z,http://arxiv.org/abs/2412.19200v1,"Dengming Zhang, Weitao You, Ziheng Liu, Lingyun Sun, Pei Chen"
Multi-Attribute Constraint Satisfaction via Language Model Rewriting,"Obeying precise constraints on top of multiple external attributes is a
common computational problem underlying seemingly different domains, from
controlled text generation to protein engineering. Existing language model (LM)
controllability methods for multi-attribute constraint satisfaction often rely
on specialized architectures or gradient-based classifiers, limiting their
flexibility to work with arbitrary black-box evaluators and pretrained models.
Current general-purpose large language models, while capable, cannot achieve
fine-grained multi-attribute control over external attributes. Thus, we create
Multi-Attribute Constraint Satisfaction (MACS), a generalized method capable of
finetuning language models on any sequential domain to satisfy user-specified
constraints on multiple external real-value attributes. Our method trains LMs
as editors by sampling diverse multi-attribute edit pairs from an initial set
of paraphrased outputs. During inference, LM iteratively improves upon its
previous solution to satisfy constraints for all attributes by leveraging our
designed constraint satisfaction reward. We additionally experiment with
reward-weighted behavior cloning to further improve the constraint satisfaction
rate of LMs. To evaluate our approach, we present a new Fine-grained Constraint
Satisfaction (FineCS) benchmark, featuring two challenging tasks: (1) Text
Style Transfer, where the goal is to simultaneously modify the sentiment and
complexity of reviews, and (2) Protein Design, focusing on modulating
fluorescence and stability of Green Fluorescent Proteins (GFP). Our empirical
results show that MACS achieves the highest threshold satisfaction in both
FineCS tasks, outperforming strong domain-specific baselines. Our work opens
new avenues for generalized and real-value multi-attribute control, with
implications for diverse applications spanning NLP and bioinformatics.",2024-12-26T12:36:39Z,http://arxiv.org/abs/2412.19198v1,"Ashutosh Baheti, Debanjana Chakraborty, Faeze Brahman, Ronan Le Bras, Ximing Lu, Nouha Dziri, Yejin Choi, Mark Riedl, Maarten Sap"
"Implications for the non-Gaussianity of primordial gravitational waves
  from pulsar timing arrays","The detection of a stochastic signal by recent pulsar timing array (PTA)
collaborations, including NANOGrav, PPTA, EPTA+InPTA, CPTA and MPTA, has opened
a new window to explore gravitational waves (GWs) at nanohertz frequencies.
Motivated by the possibility that such a signal could arise from primordial
gravitational waves (PGWs), we investigate the implications of tensor
non-Gaussianity for the PGW power spectrum. Utilizing PTA data sets, we provide
constraints on local-type tensor non-Gaussianity parameter ${F}_{\mathrm{NL}}$.
We find $|{F}_{\mathrm{NL}}|\lesssim 7.97$ for a log-normal PGW power spectrum.
Our analysis reveals that even moderate tensor non-Gaussianity can lead to
significant deviations from standard predictions, thereby offering a novel
means to test inflationary scenarios and probe the underlying dynamics of the
early Universe. Future multi-band GW observatories, such as LISA, Taiji, and
TianQin, will be instrumental in complementing these efforts and further
refining our understanding of tensor non-Gaussianity.",2024-12-26T12:31:58Z,http://arxiv.org/abs/2412.19196v1,"Zhi-Zhang Peng, You Wu, Lang Liu"
Provably Efficient Exploration in Reward Machines with Low Regret,"We study reinforcement learning (RL) for decision processes with
non-Markovian reward, in which high-level knowledge of the task in the form of
reward machines is available to the learner. We consider probabilistic reward
machines with initially unknown dynamics, and investigate RL under the
average-reward criterion, where the learning performance is assessed through
the notion of regret. Our main algorithmic contribution is a model-based RL
algorithm for decision processes involving probabilistic reward machines that
is capable of exploiting the structure induced by such machines. We further
derive high-probability and non-asymptotic bounds on its regret and demonstrate
the gain in terms of regret over existing algorithms that could be applied, but
obliviously to the structure. We also present a regret lower bound for the
studied setting. To the best of our knowledge, the proposed algorithm
constitutes the first attempt to tailor and analyze regret specifically for RL
with probabilistic reward machines.",2024-12-26T12:25:04Z,http://arxiv.org/abs/2412.19194v1,"Hippolyte Bourel, Anders Jonsson, Odalric-Ambrym Maillard, Chenxiao Ma, Mohammad Sadegh Talebi"
"Game-Theoretically Secure Distributed Protocols for Fair Allocation in
  Coalitional Games","We consider game-theoretically secure distributed protocols for coalition
games that approximate the Shapley value with small multiplicative error. Since
all known existing approximation algorithms for the Shapley value are
randomized, it is a challenge to design efficient distributed protocols among
mutually distrusted players when there is no central authority to generate
unbiased randomness. The game-theoretic notion of maximin security has been
proposed to offer guarantees to an honest player's reward even if all other
players are susceptible to an adversary.
  Permutation sampling is often used in approximation algorithms for the
Shapley value. A previous work in 1994 by Zlotkin et al. proposed a simple
constant-round distributed permutation generation protocol based on commitment
scheme, but it is vulnerable to rushing attacks. The protocol, however, can
detect such attacks.
  In this work, we model the limited resources of an adversary by a violation
budget that determines how many times it can perform such detectable attacks.
Therefore, by repeating the number of permutation samples, an honest player's
reward can be guaranteed to be close to its Shapley value. We explore both high
probability and expected maximin security. We obtain an upper bound on the
number of permutation samples for high probability maximin security, even with
an unknown violation budget. Furthermore, we establish a matching lower bound
for the weaker notion of expected maximin security in specific permutation
generation protocols. We have also performed experiments on both synthetic and
real data to empirically verify our results.",2024-12-26T12:13:21Z,http://arxiv.org/abs/2412.19192v1,"T-H. Hubert Chan, Qipeng Kuang, Quan Xue"
"Biology Instructions: A Dataset and Benchmark for Multi-Omics Sequence
  Understanding Capability of Large Language Models","Large language models have already demonstrated their formidable capabilities
in general domains, ushering in a revolutionary transformation. However,
exploring and exploiting the extensive knowledge of these models to comprehend
multi-omics biology remains underexplored. To fill this research gap, we first
introduce Biology-Instructions, the first large-scale multi-omics biological
sequences-related instruction-tuning dataset including DNA, RNA, proteins, and
multi-molecules, designed to bridge the gap between large language models
(LLMs) and complex biological sequences-related tasks. This dataset can enhance
the versatility of LLMs by integrating diverse biological sequenced-based
prediction tasks with advanced reasoning capabilities, while maintaining
conversational fluency. Additionally, we reveal significant performance
limitations in even state-of-the-art LLMs on biological sequence-related
multi-omics tasks without specialized pre-training and instruction-tuning. We
further develop a strong baseline called ChatMultiOmics with a novel
three-stage training pipeline, demonstrating the powerful ability to understand
biology by using Biology-Instructions. Biology-Instructions and ChatMultiOmics
are publicly available and crucial resources for enabling more effective
integration of LLMs with multi-omics sequence analysis.",2024-12-26T12:12:23Z,http://arxiv.org/abs/2412.19191v1,"Haonan He, Yuchen Ren, Yining Tang, Ziyang Xu, Junxian Li, Minghao Yang, Di Zhang, Dong Yuan, Tao Chen, Shufei Zhang, Yuqiang Li, Nanqing Dong, Wanli Ouyang, Dongzhan Zhou, Peng Ye"
An End-to-End Depth-Based Pipeline for Selfie Image Rectification,"Portraits or selfie images taken from a close distance typically suffer from
perspective distortion. In this paper, we propose an end-to-end deep
learning-based rectification pipeline to mitigate the effects of perspective
distortion. We learn to predict the facial depth by training a deep CNN. The
estimated depth is utilized to adjust the camera-to-subject distance by moving
the camera farther, increasing the camera focal length, and reprojecting the 3D
image features to the new perspective. The reprojected features are then fed to
an inpainting module to fill in the missing pixels. We leverage a
differentiable renderer to enable end-to-end training of our depth estimation
and feature extraction nets to improve the rectified outputs. To boost the
results of the inpainting module, we incorporate an auxiliary module to predict
the horizontal movement of the camera which decreases the area that requires
hallucination of challenging face parts such as ears. Unlike previous works, we
process the full-frame input image at once without cropping the subject's face
and processing it separately from the rest of the body, eliminating the need
for complex post-processing steps to attach the face back to the subject's
body. To train our network, we utilize the popular game engine Unreal Engine to
generate a large synthetic face dataset containing various subjects, head
poses, expressions, eyewear, clothes, and lighting. Quantitative and
qualitative results show that our rectification pipeline outperforms previous
methods, and produces comparable results with a time-consuming 3D GAN-based
method while being more than 260 times faster.",2024-12-26T11:57:54Z,http://arxiv.org/abs/2412.19189v1,"Ahmed Alhawwary, Phong Nguyen-Ha, Janne Mustaniemi, Janne Heikkilä"
New Physics in the 3-3-1 models,"Two main ingredients of current particle physics
  such as local gauge symmetry and mass generation via the Higgs mechanism
being basic ground of the Standard Model are widely confirmed by
  experimental data. However, some problems such as neutrino masses, dark
matter, baryon asymmetry of Universe have clearly indicated that the Standard
Model cannot be the ultimate theory of nature. To surpass the mentioned
puzzles,
  many extensions of the Standard Model (called beyond Standard Model) have
been proposed. Among beyond Standard Models, the 3-3-1 models have some
intriguing features and they get wide attention. The pioneer models develop in
some directions. In this paper, %some new main versions of the 3-3-1 models and
their consequences are presented.",2024-12-26T11:55:43Z,http://arxiv.org/abs/2412.19188v1,H. N. Long
"Outlier-Bias Removal with Alpha Divergence: A Robust Non-Convex
  Estimator for Linear Regression","Convex and penalized robust methods often suffer from bias induced by large
outliers, limiting their effectiveness in adversarial or heavy-tailed settings.
In this study, we propose a novel approach that eliminates this bias (when
possible) by leveraging a non-convex $M$-estimator based on the alpha
divergence. We address the problem of estimating the parameters vector in high
dimensional linear regression, even when a subset of the data has been
deliberately corrupted by an adversary with full knowledge of the dataset and
its underlying distribution.
  Our primary contribution is to demonstrate that the objective function,
although non-convex, exhibits convexity within a carefully chosen basin of
attraction, enabling robust and unbiased estimation. Additionally, we establish
three key theoretical guarantees for the estimator: (a) a deviation bound that
is minimax optimal up to a logarithmic factor, (b) an improved unbiased bound
when the outliers are large and (c) asymptotic normality as the sample size
increases. Finally, we validate the theoretical findings through empirical
comparisons with state-of-the-art estimators on both synthetic and real-world
datasets, highlighting the proposed method's superior robustness, efficiency,
and ability to mitigate outlier-induced bias.",2024-12-26T11:42:46Z,http://arxiv.org/abs/2412.19183v1,"Ilyes Hammouda, Mohamed Ndaoud, and Abd-Krim Seghouane"
"Unraveling the magnetic and electronic complexity of intermetallic
  ErPd$_2$Si$_2$: Anisotropic thermal expansion, phase transitions, and twofold
  magnetotransport behavior","We present a comprehensive investigation into the physical properties of
intermetallic ErPd$_2$Si$_2$, a compound renowned for its intriguing magnetic
and electronic characteristics. We confirm the tetragonal crystal structure of
ErPd$_2$Si$_2$ within the $I4/mmm$ space group. Notably, we observed
anisotropic thermal expansion, with the lattice constant $a$ expanding and $c$
contracting between 15 K and 300 K. This behavior is attributed to lattice
vibrations and electronic contributions. Heat capacity measurements revealed
three distinct temperature regimes: $T_1 \sim 3.0$ K, $T_\textrm{N} \sim 4.20$
K, and $T_2 \sim 15.31$ K. These correspond to the disappearance of
spin-density waves, the onset of an incommensurate antiferromagnetic (AFM)
structure, and the crystal-field splitting and/or the presence of short-range
spin fluctuations, respectively. Remarkably, the AFM phase transition anomaly
was observed exclusively in low-field magnetization data (120 Oe) at
$T_\textrm{N}$. A high magnetic field ($B =$ 3 T) effectively suppressed this
anomaly, likely due to spin-flop and spin-flip transitions. Furthermore, the
extracted effective PM moments closely matched the expected theoretical value,
suggesting a dominant magnetic contribution from localized 4$f$ spins of Er.
Additionally, significant differences in resistance ($R$) values at low
temperatures under applied $B$ indicated a magnetoresistance (MR) effect with a
minimum value of -4.36\%. Notably, the measured MR effect exhibited anisotropic
behavior, where changes in the strength or direction of the applied $B$ induced
variations in the MR effect. A twofold symmetry of $R$ was discerned at 3 T and
9 T, originating from the orientation of spin moments relative to the applied
$B$. Intriguingly, above $T_\textrm{N}$, short-range spin fluctuations also
displayed a preferred orientation along the $c$-axis due to single-ion
anisotropy.",2024-12-26T11:39:24Z,http://arxiv.org/abs/2412.19181v1,"Kaitong Sun, Si Wu, Guanping Xu, Lingwei Li, Hongyu Chen, Qian Zhao, Muqing Su, Wolfgang Schmidt, Chongde Cao, Hai-Feng Li"
"Mask Approximation Net: Merging Feature Extraction and Distribution
  Learning for Remote Sensing Change Captioning","Remote sensing image change description, as a novel multimodal task in the
field of remote sensing processing, not only enables the detection of changes
in surface conditions but also provides detailed descriptions of these changes,
thereby enhancing human interpretability and interactivity. However, previous
methods mainly employed Convolutional Neural Network (CNN) architectures to
extract bitemporal image features. This approach often leads to an overemphasis
on designing specific network architectures and limits the captured feature
distributions to the current dataset, resulting in poor generalizability and
robustness when applied to other datasets or real-world scenarios. To address
these limitations, this paper proposes a novel approach for remote sensing
image change detection and description that integrates diffusion models, aiming
to shift the focus from conventional feature learning paradigms to data
distribution learning. The proposed method primarily includes a simple
multi-scale change detection module, whose output features are subsequently
refined using a diffusion model. Additionally, we introduce a frequency-guided
complex filter module to handle high-frequency noise during the diffusion
process, which helps to maintain model performance. Finally, we validate the
effectiveness of our proposed method on several remote sensing change detection
description datasets, demonstrating its superior performance. The code
available at MaskApproxNet.",2024-12-26T11:35:57Z,http://arxiv.org/abs/2412.19179v1,"Dongwei Sun, Xiangyong Cao"
Physical nature of quasi-stable structures existing in antimony melt,"Equilibrium antimony melt near the melting temperature is characterised by
structural features that are not present in simple single-component liquids.
The cause of these features may be long-lived structural formations that are
not yet fully understood. The present work provides the detailed
characterization of the structures formed in liquid antimony near the melting
temperature based on the results of quantum chemical calculations and the
available neutron and X-ray diffraction data. The quasi-stable structures in
antimony melt are detected with lifetimes exceeding the structural relaxation
time of this melt. These structures are characterised by a low degree of order
and spatial localisation. It is shown for the first time that the elementary
units of these quasi-stable structures are triplets of atoms with
characteristic lengths of $3.07$\,\AA~and $4.7$\,\AA~and characteristic angles
of $45$ and $90$ degrees. It was found that these triplets can form chains and
percolating clusters up to $\sim15$\,\AA~in length. The characteristic lengths
of these triplets are fully consistent with the correlation lengths associated
with short-range order in the antimony melt as determined by diffraction
experiments.",2024-12-26T11:30:38Z,http://arxiv.org/abs/2412.19177v1,"Artem A. Tsygankov, Bulat N. Galimzyanov, Anatolii V. Mokshin"
"VQE for Ising Model \&amp; A Comparative Analysis of Classical and Quantum
  Optimization Methods","In this study, we delved into several optimization methods, both classical
and quantum, and analyzed the quantum advantage that each of these methods
offered, and then we proposed a new combinatorial optimization scheme, deemed
as QN-SPSA+PSR which combines calculating approximately Fubini-study metric
(QN-SPSA) and the exact evaluation of gradient by Parameter-Shift Rule (PSR).
The QN-SPSA+PSR method integrates the QN-SPSA computational efficiency with the
precise gradient computation of the PSR, improving both stability and
convergence speed while maintaining low computational consumption. Our results
provide a new potential quantum supremacy in the VQE's optimization subroutine
and enhance viable paths toward efficient quantum simulations on Noisy
Intermediate-Scale Quantum Computing (NISQ) devices. Additionally, we also
conducted a detailed study of quantum circuit ansatz structures in order to
find the one that would work best with the Ising model and NISQ, in which we
utilized the symmetry of the investigated model.",2024-12-26T11:25:30Z,http://arxiv.org/abs/2412.19176v1,"Duc-Truyen Le, Vu-Linh Nguyen, Triet Minh Ha, Cong-Ha Nguyen, Quoc-Hung Nguyen, Van-Duy Nguyen"
"Convergence analysis of PM-BDF2 method for quasiperiodic parabolic
  equations","Numerically solving parabolic equations with quasiperiodic coefficients is a
significant challenge due to the potential formation of space-filling
quasiperiodic structures that lack translational symmetry or decay. In this
paper, we introduce a highly accurate numerical method for solving
time-dependent quasiperiodic parabolic equations. We discretize the spatial
variables using the projection method (PM) and the time variable with the
second-order backward differentiation formula (BDF2). We provide a complexity
analysis for the resulting PM-BDF2 method. Furthermore, we conduct a detailed
convergence analysis, demonstrating that the proposed method exhibits spectral
accuracy in space and second-order accuracy in time. Numerical results in both
one and two dimensions validate these convergence results, highlighting the
PM-BDF2 method as a highly efficient algorithm for addressing quasiperiodic
parabolic equations.",2024-12-26T11:23:15Z,http://arxiv.org/abs/2412.19175v1,"Kai Jiang, Meng Li, Juan Zhang, Lei Zhang"
"High-Precision Schottky Diagnostics for Low-SNR Betatron Tune
  Measurement in Ramping Synchrotrons","This paper presents a novel Schottky diagnostics-based method for real-time
betatron tune measurement in ramping synchrotrons, exemplified by the Shanghai
Advanced Proton Therapy (SAPT) facility. The proposed approach achieves high
precision under challenging conditions, including low frequency resolution and
signal-to-noise ratios (SNR) as low as -15 dB within the bandwidth of a
narrowband detector. By employing Short-Time Fourier Transform (STFT) analysis
with automatically optimized time windows, the method effectively addresses the
rapid increase in revolution frequency from 4 MHz to 7.5 MHz over 0.35 seconds,
assuming constant beam properties within each window. Monte Carlo
macro-particle simulations are employed to generate Schottky signals, which are
subsequently combined with real noise collected from an analog-to-digital
converter to emulate practical conditions. The betatron tune measurement
procedure integrates longitudinal signal exclusion, spectrum smoothing, and
spectral multiplication to reliably extract transverse Schottky spectra buried
in noise, to enable precise betatron tune determination. Experimental results
demonstrate that the proposed method surpasses existing approaches in
precision, accuracy, and robustness, while meeting stringent design
requirements. This innovative approach addresses key limitations of Schottky
diagnostics for betatron tune measurement in ramping synchrotrons, providing a
foundation for applications such as proton therapy.",2024-12-26T11:05:21Z,http://arxiv.org/abs/2412.19171v1,"Peihan Sun, Manzhou Zhang, Renxian Yuan, Deming Li, Jian Dong, Ying Shi"
"Accelerating Stochastic Gravitational Wave Backgrounds Parameter
  Estimation in Pulsar Timing Arrays with Flow Matching","Pulsar timing arrays (PTAs) are essential tools for detecting the stochastic
gravitational wave background (SGWB), but their analysis faces significant
computational challenges. Traditional methods like Markov-chain Monte Carlo
(MCMC) struggle with high-dimensional parameter spaces where noise parameters
often dominate, while existing deep learning approaches fail to model the
Hellings-Downs (HD) correlation or are validated only on synthetic datasets. We
propose a flow-matching-based continuous normalizing flow (CNF) for efficient
and accurate PTA parameter estimation. By focusing on the 10 most contributive
pulsars from the NANOGrav 15-year dataset, our method achieves posteriors
consistent with MCMC, with a Jensen-Shannon divergence below \(10^{-2}\) nat,
while reducing sampling time from 50 hours to 4 minutes. Powered by a versatile
embedding network and a reweighting loss function, our approach prioritizes the
SGWB parameters and scales effectively for future datasets. It enables precise
reconstruction of SGWB and opens new avenues for exploring vast observational
data and uncovering potential new physics, offering a transformative tool for
advancing gravitational wave astronomy.",2024-12-26T11:02:11Z,http://arxiv.org/abs/2412.19169v1,"Bo Liang, Chang Liu, Tianyu Zhao, Minghui Du, Manjia Liang, Ruijun Shi, Hong Guo, Yuxiang Xu, Li-e Qiang, Peng Xu, Wei-Liang Qian, Ziren Luo"
"Fluid-particle interactions and fluctuation-dissipation relations I --
  General linear theory and basic fluctuational patterns","The article provides a unitary and complete solution to the
fluctuation-dissipation relations for particle hydromechanics in a generic
fluid, accounting for the hydrodynamic fluid-particle interactions (including
arbitrary memory kernels in the description of dissipative and fluid inertial
effects) in linear hydrodynamic regimes, via the concepts of fluctuational
patterns. This is achieved by expressing the memory kernels as a linear
superposition of exponentially decaying modes. Given the structure of the
interaction with the internal degrees of freedom, and assuming the
representation of the thermal force as a superposition of modal contributions,
the fluctuation-dissipation relation follows simply from the moment analysis of
the corresponding Fokker-Planck equation, imposing the condition that at
equilibrium all the internal degrees of freedom are uncorrelated with particle
velocity. Moreover, the functional structure of the resulting equation of
motion corresponds to the principle of complete decoupling amongst the internal
degrees of freedom. The theory is extended to the case of confined geometries,
by generalizing previous results including the effect of fluid inertia.",2024-12-26T10:52:07Z,http://arxiv.org/abs/2412.19166v1,"Massimiliano Giona, Giuseppe Procopio, Chiara Pezzotti"
"Revisiting Monocular 3D Object Detection from Scene-Level Depth
  Retargeting to Instance-Level Spatial Refinement","Monocular 3D object detection is challenging due to the lack of accurate
depth. However, existing depth-assisted solutions still exhibit inferior
performance, whose reason is universally acknowledged as the unsatisfactory
accuracy of monocular depth estimation models. In this paper, we revisit
monocular 3D object detection from the depth perspective and formulate an
additional issue as the limited 3D structure-aware capability of existing depth
representations (\textit{e.g.}, depth one-hot encoding or depth distribution).
To address this issue, we propose a novel depth-adapted monocular 3D object
detection network, termed \textbf{RD3D}, that mainly comprises a Scene-Level
Depth Retargeting (SDR) module and an Instance-Level Spatial Refinement (ISR)
module. The former incorporates the scene-level perception of 3D structures,
retargeting traditional depth representations to a new formulation:
\textbf{Depth Thickness Field}. The latter refines the voxel spatial
representation with the guidance of instances, eliminating the ambiguity of 3D
occupation and thus improving detection accuracy. Extensive experiments on the
KITTI and Waymo datasets demonstrate our superiority to existing
state-of-the-art (SoTA) methods and the universality when equipped with
different depth estimation models. The code will be available.",2024-12-26T10:51:50Z,http://arxiv.org/abs/2412.19165v1,"Qiude Zhang, Chunyu Lin, Zhijie Shen, Nie Lang, Yao Zhao"
Master Stability Functions in Complex Networks,"Synchronization is an emergent phenomenon in coupled dynamical networks. The
Master Stability Function (MSF) is a highly elegant and powerful tool for
characterizing the stability of synchronization states. However, a significant
challenge lies in determining the MSF for complex dynamical networks driven by
nonlinear interaction mechanisms. These mechanisms introduce additional
complexity through the intricate connectivity of interacting elements within
the network and the intrinsic dynamics, which are governed by nonlinear
processes with diverse parameters and higher dimensionality of systems. Over
the past 25 years, extensive research has focused on determining the MSF for
pairwise coupled identical systems with diffusive coupling. Our literature
survey highlights two significant advancements in recent years: the
consideration of multilayer networks instead of single-layer networks and the
extension of MSF analysis to incorporate higher-order interactions alongside
pairwise interactions.
  In this review article, we revisit the analysis of the MSF for diffusively
pairwise coupled dynamical systems and extend this framework to more general
coupling schemes. Furthermore, we systematically derive the MSF for multilayer
dynamical networks and single-layer coupled systems by incorporating
higher-order interactions alongside pairwise interactions. The primary focus of
our review is on the analytical derivation and numerical computation of the MSF
for complex dynamical networks. Finally, we demonstrate the application of the
MSF in data science, emphasizing its relevance and potential in this rapidly
evolving field.",2024-12-26T10:47:00Z,http://arxiv.org/abs/2412.19163v1,"Suman Acharyya, Priodyuti Pradhan, Chandrakala Meena"
"Dual Channel Multi-Attention in ViT for Biometric Authentication using
  Forehead Subcutaneous Vein Pattern and Periocular Pattern","Traditional biometric systems, like face and fingerprint recognition, have
encountered significant setbacks due to wearing face masks and hygiene
concerns. To meet the challenges of the partially covered face due to face
masks and hygiene concerns of fingerprint recognition, this paper proposes a
novel dual-channel multi-attention Vision Transformer (ViT) framework for
biometric authentication using forehead subcutaneous vein patterns and
periocular patterns, offering a promising alternative to traditional methods,
capable of performing well even with face masks and without any physical touch.
The proposed framework leverages a dual-channel ViT architecture, designed to
handle two distinct biometric traits. It can capture long-range dependencies of
independent features from the vein and periocular patterns. A custom classifier
is then designed to integrate the independently extracted features, producing a
final class prediction. The performance of the proposed algorithm was
rigorously evaluated using the Forehead Subcutaneous Vein Pattern and
Periocular Biometric Pattern (FSVP-PBP) database. The results demonstrated the
superiority of the algorithm over state-of-the-art methods, achieving
remarkable classification accuracy of $99.3 \pm 0.02\%$ with the combined vein
and periocular patterns.",2024-12-26T10:40:15Z,http://arxiv.org/abs/2412.19160v1,"Arun K. Sharma, Shubhobrata Bhattacharya, Motahar Reza"
"Mobile Robots through Task-Based Human Instructions using Incremental
  Curriculum Learning","This paper explores the integration of incremental curriculum learning (ICL)
with deep reinforcement learning (DRL) techniques to facilitate mobile robot
navigation through task-based human instruction. By adopting a curriculum that
mirrors the progressive complexity encountered in human learning, our approach
systematically enhances robots' ability to interpret and execute complex
instructions over time. We explore the principles of DRL and its synergy with
ICL, demonstrating how this combination not only improves training efficiency
but also equips mobile robots with the generalization capability required for
navigating through dynamic indoor environments. Empirical results indicate that
robots trained with our ICL-enhanced DRL framework outperform those trained
without curriculum learning, highlighting the benefits of structured learning
progressions in robotic training.",2024-12-26T10:38:40Z,http://arxiv.org/abs/2412.19159v1,"Muhammad A. Muttaqien, Ayanori Yorozu, Akihisa Ohya"
Nonlinear Piezomagnetic Effects in $g$-wave Altermagnets,"We theoretically study the generation of net magnetization induced by lattice
distortion due to elastic waves in $g$-wave altermagnets, which exhibit the
symmetric spin-split band structure under collinear spin textures free from the
relativistic spin--orbit coupling. By analyzing a tight-binding model in a
two-dimensional tetragonal system, we show that the $g$-wave altermagnets give
rise to the nonlinear piezomagnetic effect, where a net magnetization is
induced by the second-order strain. We compare the results for the $g$-wave
altermagnets with those for the $d$-wave altermagnets, where the linear
piezomagnetic effect occurs. As a result, we find that the induced
magnetization is enhanced when the Fermi level lies on the band with the large
spin splitting in both cases. We also show that the magnitudes of the induced
magnetization are comparable to each other. Our results indicate that the
nonlinear piezomagnetic effect is a good phenomenon to characterize the
physical properties in $g$-wave altermagnets.",2024-12-26T10:37:27Z,http://arxiv.org/abs/2412.19158v1,"Yuuki Ogawa, Satoru Hayami"
Advancements in Terahertz Antenna Design,"The promising way to provide sufficient transmission capacity is by accessing
transmission bands at higher carrier frequencies. This desire for higher
carrier frequency or more bandwidth led the researchers to take advantage of
the terahertz (THz) spectrum. The opportunity for large bandwidth in the THz
band leads to the possibility of easy, high data rate transmission. In spite of
the advantages, the THz band suffers from large free space path loss. In the
development of THz communication systems, the antenna is the most significant
component. The focus is especially on designing highly directive antennas
because they enhance the performance of the overall system by compensating for
the large path loss at THz and thus improving the signal-to-noise ratio. This
chapter presents different types of THz antennas, including planar,
reflectarray, horn antenna, and lens antenna. Emphasis has been made to present
the latest trend of designing THz antennas using carbon-based materials, such
as graphene and carbon nanotubes. The performance of these antennas has been
compared with that of traditional copper-based THz antennas by critically
analyzing their properties. A brief discussion on THz power sources is included
in this chapter for completeness. A comprehensive discussion on different
fabrication techniques has been provided to appraise the reader of the general
fabrication processes of THz components.",2024-12-26T10:21:58Z,http://arxiv.org/abs/2412.19156v1,"Sasmita Dash, Amalendu Patnaik"
"Evolutionary de-homogenization using a generative model for optimizing
  solid-porous infill structures considering the stress concentration issue","The design of porous infill structures presents significant challenges due to
their complex geometric configurations, such as the accurate representation of
geometric boundaries and the control of localized maximum stress. In current
mainstream design methods, such as topology optimization, the analysis is often
performed using pixel or voxel-based element approximations. These
approximations, constrained by the optimization framework, result in
substantial geometric discrepancies between the analysis model and the final
physical model. Such discrepancies can severely impact structural performance,
particularly for localized properties like stress response, where accurate
geometry is critical to mitigating stress concentration. To address these
challenges, we propose evolutionary de-homogenization, which is a design
framework based on the integration of de-homogenization and data-driven
multifidelity optimization. This framework facilitates the hybrid solid-porous
infill design by bridging the gap between low-fidelity analysis and
high-fidelity physical realizations, ensuring both geometric accuracy and
enhanced structural performance. The low-fidelity level utilizes commonly used
density control variables, while the high-fidelity level involves stress
analysis based on structures with precise geometric representations. By
employing a de-homogenization-based mapping method, a side-by-side
correspondence between low-fidelity and high-fidelity results is established.
The low-fidelity control variables are iteratively adjusted to optimize the
high-fidelity results by integrating deep generative model with multi-objective
evolutionary algorithm. Finally, numerical experiments demonstrate the
effectiveness of the proposed method.",2024-12-26T10:18:16Z,http://arxiv.org/abs/2412.19154v1,"Shuzhi Xu, Hiroki Kawabe, Kentaro Yaji"
"To Predict or Not To Predict? Proportionally Masked Autoencoders for
  Tabular Data Imputation","Masked autoencoders (MAEs) have recently demonstrated effectiveness in
tabular data imputation. However, due to the inherent heterogeneity of tabular
data, the uniform random masking strategy commonly used in MAEs can disrupt the
distribution of missingness, leading to suboptimal performance. To address
this, we propose a proportional masking strategy for MAEs. Specifically, we
first compute the statistics of missingness based on the observed proportions
in the dataset, and then generate masks that align with these statistics,
ensuring that the distribution of missingness is preserved after masking.
Furthermore, we argue that simple MLP-based token mixing offers competitive or
often superior performance compared to attention mechanisms while being more
computationally efficient, especially in the tabular domain with the inherent
heterogeneity. Experimental results validate the effectiveness of the proposed
proportional masking strategy across various missing data patterns in tabular
datasets. Code is available at: \url{https://github.com/normal-kim/PMAE}.",2024-12-26T10:12:08Z,http://arxiv.org/abs/2412.19152v1,"Jungkyu Kim, Kibok Lee, Taeyoung Park"
AskChart: Universal Chart Understanding through Textual Enhancement,"Chart understanding tasks such as ChartQA and Chart-to-Text involve
automatically extracting and interpreting key information from charts, enabling
users to query or convert visual data into structured formats. State-of-the-art
approaches primarily focus on visual cues from chart images, failing to
explicitly incorporate rich textual information (e.g., data labels and axis
labels) embedded within the charts. This textual information is vital for
intuitive human comprehension and interpretation of charts. Moreover, existing
models are often large and computationally intensive, limiting their practical
applicability. In this paper, we introduce AskChart, a universal model that
explicitly integrates both textual and visual cues from charts using a Mixture
of Experts (MoE) architecture. AskChart facilitates the learning of enhanced
visual-textual representations of charts for effectively handling multiple
chart understanding tasks, while maintaining a smaller model size. To capture
the synergy between visual and textual modalities, we curate a large-scale
dataset named ChartBank with about 7.5M data samples, which helps align textual
and visual information and facilitates the extraction of visual entities and
text. To effectively train AskChart, we design a three-stage training strategy
to align visual and textual modalities for learning robust visual-textual
representations and optimizing the learning of the MoE layer. Extensive
experiments across five datasets demonstrate the significant performance gains
of AskChart in four chart understanding tasks. Remarkably, AskChart with 4.6B
parameters outperforms state-of-the-art models with 13B parameters by 68.3% in
Open-ended ChartQA and 49.2% in Chart-to-Text tasks, while achieving comparable
performance in ChartQA and Chart-to-Table tasks.",2024-12-26T09:59:43Z,http://arxiv.org/abs/2412.19146v1,"Xudong Yang, Yifan Wu, Yizhang Zhu, Nan Tang, Yuyu Luo"
"Impact of color and mixing proportion of synthetic point clouds on
  semantic segmentation","Semantic segmentation of point clouds is essential for understanding the
built environment, and a large amount of high-quality data is required for
training deep learning models. Despite synthetic point clouds (SPC) having the
potential to compensate for the shortage of real data, how to exploit the
benefits of SPC is still open. Therefore, this study systematically
investigates how color and mixing proportion of SPC impact semantic
segmentation for the first time. First, a new method to mimic the scanning
process and generate SPC based on BIM is proposed, to create a synthetic
dataset with consistent colors of BIM (UniSPC) and a synthetic dataset with
real colors (RealSPC) respectively. Subsequently, by integrating with the S3DIS
dataset, further experiments on PointNet, PointNet++, and DGCNN are conducted.
Meanwhile, benchmark experiments and new evaluation metrics are introduced to
better evaluate the performance of different models. Experiments show that
synthetic color significantly impacts model performance, the performance for
common components of the models trained with pure RealSPC is comparable to
models with real data, and RealSPC contributes average improvements of 14.1% on
overall accuracy and 7.3% on mIoU than UniSPC. Furthermore, the proportion of
SPC also has a significant impact on the performance. In mixing training
experiments, adding more than 70% SPC achieves an average of 3.9% on overall
accuracy and 3.4% on mIoU better than benchmark on three models. It is also
revealed that for large flat elements such as floors, ceilings, and walls, the
SPC can even replace real point clouds without compromising model performance.",2024-12-26T09:58:04Z,http://arxiv.org/abs/2412.19145v1,"Shaojie Zhou, Jia-Rui Lin, Peng Pan, Yuandong Pan, Ioannis Brilakis"
LibAFL-DiFuzz: Advanced Architecture Enabling Directed Fuzzing,"Directed fuzzing performs best for targeted program testing via estimating
the impact of each input in reaching predefined program points. But due to
insufficient analysis of the program structure and lack of flexibility and
configurability it can lose efficiency. In this paper, we enhance directed
fuzzing with context weights for graph nodes and resolve indirect edges during
call graph construction. We construct flexible tool for directed fuzzing with
components able to be easily combined with other techniques. We implement
proposed method in three separate modules: DiFuzzLLVM library for graph
construction and indirect calls resolving, DiFuzz static analysis tool for
processing program graphs and computing proximity metrics, and LibAFL-DiFuzz
directed fuzzer based on LibAFL fuzzing library. We create additional LibAFL
modules for enabling custom power scheduling and static instrumentation. We
evaluate indirect calls resolving and get increase in directed fuzzing
efficiency for reaching deeper target points. We evaluate context weights
contribution and get benefits in TTE and scheduling iterations number. We
evaluate our fuzzer in comparison with AFLGo and BEACON, and reveal speedup in
time to exposure on several benchmarks. Furthermore, our tool implements some
important usability features that are not available in mentioned tools: target
points detection, multiple target points support, etc.",2024-12-26T09:54:57Z,http://arxiv.org/abs/2412.19143v1,"Darya Parygina, Timofey Mezhuev, Daniil Kuts"
"CLIP-GS: Unifying Vision-Language Representation with 3D Gaussian
  Splatting","Recent works in 3D multimodal learning have made remarkable progress.
However, typically 3D multimodal models are only capable of handling point
clouds. Compared to the emerging 3D representation technique, 3D Gaussian
Splatting (3DGS), the spatially sparse point cloud cannot depict the texture
information of 3D objects, resulting in inferior reconstruction capabilities.
This limitation constrains the potential of point cloud-based 3D multimodal
representation learning. In this paper, we present CLIP-GS, a novel multimodal
representation learning framework grounded in 3DGS. We introduce the GS
Tokenizer to generate serialized gaussian tokens, which are then processed
through transformer layers pre-initialized with weights from point cloud
models, resulting in the 3DGS embeddings. CLIP-GS leverages contrastive loss
between 3DGS and the visual-text embeddings of CLIP, and we introduce an image
voting loss to guide the directionality and convergence of gradient
optimization. Furthermore, we develop an efficient way to generate triplets of
3DGS, images, and text, facilitating CLIP-GS in learning unified multimodal
representations. Leveraging the well-aligned multimodal representations,
CLIP-GS demonstrates versatility and outperforms point cloud-based models on
various 3D tasks, including multimodal retrieval, zero-shot, and few-shot
classification.",2024-12-26T09:54:25Z,http://arxiv.org/abs/2412.19142v1,"Siyu Jiao, Haoye Dong, Yuyang Yin, Zequn Jie, Yinlong Qian, Yao Zhao, Humphrey Shi, Yunchao Wei"
"SILC-EFSA: Self-aware In-context Learning Correction for Entity-level
  Financial Sentiment Analysis","In recent years, fine-grained sentiment analysis in finance has gained
significant attention, but the scarcity of entity-level datasets remains a key
challenge. To address this, we have constructed the largest English and Chinese
financial entity-level sentiment analysis datasets to date. Building on this
foundation, we propose a novel two-stage sentiment analysis approach called
Self-aware In-context Learning Correction (SILC). The first stage involves
fine-tuning a base large language model to generate pseudo-labeled data
specific to our task. In the second stage, we train a correction model using a
GNN-based example retriever, which is informed by the pseudo-labeled data. This
two-stage strategy has allowed us to achieve state-of-the-art performance on
the newly constructed datasets, advancing the field of financial sentiment
analysis. In a case study, we demonstrate the enhanced practical utility of our
data and methods in monitoring the cryptocurrency market. Our datasets and code
are available at https://github.com/NLP-Bin/SILC-EFSA.",2024-12-26T09:53:01Z,http://arxiv.org/abs/2412.19140v1,"Senbin Zhu, Chenyuan He, Hongde Liu, Pengcheng Dong, Hanjie Zhao, Yuchen Yan, Yuxiang Jia, Hongying Zan, Min Peng"
SUTrack: Towards Simple and Unified Single Object Tracking,"In this paper, we propose a simple yet unified single object tracking (SOT)
framework, dubbed SUTrack. It consolidates five SOT tasks (RGB-based,
RGB-Depth, RGB-Thermal, RGB-Event, RGB-Language Tracking) into a unified model
trained in a single session. Due to the distinct nature of the data, current
methods typically design individual architectures and train separate models for
each task. This fragmentation results in redundant training processes,
repetitive technological innovations, and limited cross-modal knowledge
sharing. In contrast, SUTrack demonstrates that a single model with a unified
input representation can effectively handle various common SOT tasks,
eliminating the need for task-specific designs and separate training sessions.
Additionally, we introduce a task-recognition auxiliary training strategy and a
soft token type embedding to further enhance SUTrack's performance with minimal
overhead. Experiments show that SUTrack outperforms previous task-specific
counterparts across 11 datasets spanning five SOT tasks. Moreover, we provide a
range of models catering edge devices as well as high-performance GPUs,
striking a good trade-off between speed and accuracy. We hope SUTrack could
serve as a strong foundation for further compelling research into unified
tracking models. Code and models are available at
github.com/chenxin-dlut/SUTrack.",2024-12-26T09:41:36Z,http://arxiv.org/abs/2412.19138v1,"Xin Chen, Ben Kang, Wanting Geng, Jiawen Zhu, Yi Liu, Dong Wang, Huchuan Lu"
"Renormalized Volume, Polyakov Anomaly and Orbifold Riemann Surfaces","In arXiv:2310.17536, two of the authors studied the function
$\mathscr{S}_{\boldsymbol{m}} = S_{\boldsymbol{m}} - \pi \sum_{i=1}^n (m_i -
\tfrac{1}{m_i}) \log \mathsf{h}_{i}$ for orbifold Riemann surfaces of signature
$(g;m_1,...,m_{n_e};n_p)$ on the generalized Schottky space
$\mathfrak{S}_{g,n}(\boldsymbol{m})$. In this paper, we prove the holographic
duality between $\mathscr{S}_{\boldsymbol{m}}$ and the renormalized hyperbolic
volume $V_{\text{ren}}$ of the corresponding Schottky 3-orbifolds with lines of
conical singularity that reach the conformal boundary. In case of the classical
Liouville action on $\mathfrak{S}_{g}$ and
$\mathfrak{S}_{g,n}(\boldsymbol{\infty})$, the holography principle was proved
in arXiv:0005106 and arXiv:1508.02102, respectively. Our result implies that
$V_{\text{ren}}$ acts as K\""ahler potential for a particular combination of the
Weil-Petersson and Takhtajan-Zograf metrics that appears in the local index
theorem for orbifold Riemann surfaces arXiv:1701.00771. Moreover, we
demonstrate that under the conformal transformations, the change of function
$\mathscr{S}_{\boldsymbol{m}}$ is equivalent to the Polyakov anomaly, which
indicates that the function $\mathscr{S}_{\boldsymbol{m}}$ is a consistent
height function with a unique hyperbolic solution. Consequently, the associated
renormalized hyperbolic volume $V_{\text{ren}}$ also admits a Polyakov anomaly
formula. The method we used to establish this equivalence may provide an
alternative approach to derive the renormalized Polyakov anomaly for Riemann
surfaces with punctures (cusps), as described in arXiv:0909.0807.",2024-12-26T09:41:30Z,http://arxiv.org/abs/2412.19137v1,"Hossein Mohammadi, Ali Naseh, Behrad Taghavi"
"A Rhetorical Relations-Based Framework for Tailored Multimedia Document
  Summarization","In the rapidly evolving landscape of digital content, the task of summarizing
multimedia documents, which encompass textual, visual, and auditory elements,
presents intricate challenges. These challenges include extracting pertinent
information from diverse formats, maintaining the structural integrity and
semantic coherence of the original content, and generating concise yet
informative summaries. This paper introduces a novel framework for multimedia
document summarization that capitalizes on the inherent structure of the
document to craft coherent and succinct summaries. Central to this framework is
the incorporation of a rhetorical structure for structural analysis, augmented
by a graph-based representation to facilitate the extraction of pivotal
information. Weighting algorithms are employed to assign significance values to
document units, thereby enabling effective ranking and selection of relevant
content. Furthermore, the framework is designed to accommodate user preferences
and time constraints, ensuring the production of personalized and contextually
relevant summaries. The summarization process is elaborately delineated,
encompassing document specification, graph construction, unit weighting, and
summary extraction, supported by illustrative examples and algorithmic
elucidation. This proposed framework represents a significant advancement in
automatic summarization, with broad potential applications across multimedia
document processing, promising transformative impacts in the field.",2024-12-26T09:29:59Z,http://arxiv.org/abs/2412.19133v1,"Azze-Eddine Maredj, Madjid Sadallah"
Semantic Residual for Multimodal Unified Discrete Representation,"Recent research in the domain of multimodal unified representations
predominantly employs codebook as representation forms, utilizing Vector
Quantization(VQ) for quantization, yet there has been insufficient exploration
of other quantization representation forms. Our work explores more precise
quantization methods and introduces a new framework, Semantic Residual
Cross-modal Information Disentanglement (SRCID), inspired by the numerical
residual concept inherent to Residual Vector Quantization (RVQ). SRCID employs
semantic residual-based information disentanglement for multimodal data to
better handle the inherent discrepancies between different modalities. Our
method enhances the capabilities of unified multimodal representations and
demonstrates exceptional performance in cross-modal generalization and
cross-modal zero-shot retrieval. Its average results significantly surpass
existing state-of-the-art models, as well as previous attempts with RVQ and
Finite Scalar Quantization (FSQ) based on these modals.",2024-12-26T09:08:52Z,http://arxiv.org/abs/2412.19128v1,"Hai Huang, Shulei Wang, Yan Xia"
"Advanced Knowledge Transfer: Refined Feature Distillation for Zero-Shot
  Quantization in Edge Computing","We introduce AKT (Advanced Knowledge Transfer), a novel method to enhance the
training ability of low-bit quantized (Q) models in the field of zero-shot
quantization (ZSQ). Existing research in ZSQ has focused on generating
high-quality data from full-precision (FP) models. However, these approaches
struggle with reduced learning ability in low-bit quantization due to its
limited information capacity. To overcome this limitation, we propose effective
training strategy compared to data generation. Particularly, we analyzed that
refining feature maps in the feature distillation process is an effective way
to transfer knowledge to the Q model. Based on this analysis, AKT efficiently
transfer core information from the FP model to the Q model. AKT is the first
approach to utilize both spatial and channel attention information in feature
distillation in ZSQ. Our method addresses the fundamental gradient exploding
problem in low-bit Q models. Experiments on CIFAR-10 and CIFAR-100 datasets
demonstrated the effectiveness of the AKT. Our method led to significant
performance enhancement in existing generative models. Notably, AKT achieved
significant accuracy improvements in low-bit Q models, achieving
state-of-the-art in the 3,5bit scenarios on CIFAR-10. The code is available at
https://github.com/Inpyo-Hong/AKT-Advanced-knowledge-Transfer.",2024-12-26T08:52:27Z,http://arxiv.org/abs/2412.19125v1,"Inpyo Hong, Youngwan Jo, Hyojeong Lee, Sunghyun Ahn, Sanghyun Park"
"Evaluating Self-Supervised Learning in Medical Imaging: A Benchmark for
  Robustness, Generalizability, and Multi-Domain Impact","Self-supervised learning (SSL) has emerged as a promising paradigm in medical
imaging, addressing the chronic challenge of limited labeled data in healthcare
settings. While SSL has shown impressive results, existing studies in the
medical domain are often limited in scope, focusing on specific datasets or
modalities, or evaluating only isolated aspects of model performance. This
fragmented evaluation approach poses a significant challenge, as models
deployed in critical medical settings must not only achieve high accuracy but
also demonstrate robust performance and generalizability across diverse
datasets and varying conditions. To address this gap, we present a
comprehensive evaluation of SSL methods within the medical domain, with a
particular focus on robustness and generalizability. Using the MedMNIST dataset
collection as a standardized benchmark, we evaluate 8 major SSL methods across
11 different medical datasets. Our study provides an in-depth analysis of model
performance in both in-domain scenarios and the detection of
out-of-distribution (OOD) samples, while exploring the effect of various
initialization strategies, model architectures, and multi-domain pre-training.
We further assess the generalizability of SSL methods through cross-dataset
evaluations and the in-domain performance with varying label proportions (1%,
10%, and 100%) to simulate real-world scenarios with limited supervision. We
hope this comprehensive benchmark helps practitioners and researchers make more
informed decisions when applying SSL methods to medical applications.",2024-12-26T08:51:56Z,http://arxiv.org/abs/2412.19124v1,"Valay Bundele, Oğuz Ata Çal, Bora Kargi, Karahan Sarıtaş, Kıvanç Tezören, Zohreh Ghaderi, Hendrik Lensch"
"Discovery of an Ultra-Stable Antiferromagnetic Two-Dimensional CrF3
  Phase with Anisotropic Quasi-1D Mechanical, Electronic, and Thermal
  Properties","We report the discovery of an ultra-stable antiferromagnetic two-dimensional
(2D) CrF3 phase that is energetically more favorable than the traditionally
assumed hexagonal structure. Using first-principles calculations and
evolutionary structure searches, we identify a new low-energy rectangular
configuration of CrF3 with remarkable anisotropic properties. Mechanically,
this phase exhibits zero in-plane Poisson's ratio, a rare negative out-of-plane
Poisson's ratio, and quasi-one-dimensional (quasi-1D) behavior characterized by
minimal coupling between orthogonal directions. Electronically, CrF3 shows
quasi-1D transport with two independent conduction bands near the Fermi level,
tunable via uniaxial strain. The calculated bandgap is 3.05 eV, which can be
modulated under strain, enabling control over its electronic properties. The
material also displays out-of-plane antiferromagnetic ordering with a magnetic
anisotropy energy of 0.098 meV per Cr atom and an estimated Neel temperature of
20 K. Additionally, we investigate the thermal conductivity of monolayer
rectangular CrF3 (r-CrF3), revealing significant anisotropy in heat transport.
The thermal conductivity along the y-axis is approximately 60.5 W/mK at 300 K,
much higher than along the x-axis at 13.2 W/mK. The thermal anisotropic factor
is 4.58, surpassing that of other 2D materials like black phosphorene, WTe2,
and arsenene, highlighting r-CrF3's potential for advanced directional heat
management. Consequently, the rectangular CrF3 phase is a promising candidate
for applications in spintronics, strain-engineered nanoelectronics, mechanical
metamaterials, and thermal management technologies.",2024-12-26T08:36:35Z,http://arxiv.org/abs/2412.19118v1,"Xin Chen, Fengyi Zhou, Yan Suo, Cheng Shao, Duo Wang, Biplab Sanyal"
Deformation and core$+n$ decoupling in the spectrum of $^{17}$C,"The coexistence of various structures, such as diverse shapes and cluster
structures, is a fundamental property of atomic nuclei. In neutron-rich nuclei,
a core$+n$ structure can compete with nuclear deformation due to the small
neutron separation energy. A neutron-rich carbon isotope, $^{17}$C, exemplifies
the appearance of the deformation and the core+$n$ decoupling in its spectrum,
which is desirable for a deeper understanding of the coexistence phenomena in
neutron-rich nuclei. We aim to describe and understand this coexistence
phenomenon in the low-lying levels of $^{17}$C in a unified manner considering
explicitly the degrees of freedom of both the quadrupole deformation and the
relative motion between a $^{16}$C core and a valence neutron. Method: We adopt
the generator coordinate method (GCM) with the antisymmetrized molecular
dynamics (AMD) to describe various configurations. We superpose various basis
wave functions generated by the energy variation by imposing two types of
constraints: the $\beta$-$\gamma$ and the $d$ constraints, which control the
degree of the quadrupole deformation and the relative motion between a $^{16}$C
core and a valence neutron, respectively. We find that the experimental energy
level is well reproduced by the present method, including both deformed and
$^{16}$C+$n$ configurations. The ground $3/2^{+}$ and second excited $5/2^{+}$
states exhibit a triaxially deformed shape, while the main component of the
first excited $1/2^{+}$ state is a $^{16}$C($0^{+}$) core plus an $s$-wave
neutron configuration. The tail of the valence neutron is significantly
improved by including the $^{16}$C+$n$ basis functions explicitly. The explicit
inclusion of both the quadrupole deformation and the relative motion between a
core and a valence neutron is essential to describe the coexistence phenomena
observed in neutron-rich nuclei in the AMD+GCM framework.",2024-12-26T08:30:51Z,http://arxiv.org/abs/2412.19117v1,"Tadahiro Suhara, Yasutaka Taniguchi, Wataru Horiuchi, Shin Watanabe, Takenori Furumoto"
Counting absolutely indecomposable $G$-bundles,"For a reductive group $G$ over a finite field $k$, and a smooth projective
curve $X/k$, we give a motivic counting formula for the number of absolutely
indecomposable $G$-bundles on $X$. We prove that the counting can be expressed
via the cohomology of the moduli stack of stable parabolic $G$-Higgs bundles on
$X$. This result generalizes work of Schiffmann and work of Dobrovolska,
Ginzburg, and Travkin from $\mathrm{GL}_n$ to a general reductive group. Along
the way we prove some structural results on automorphism groups of $G$-torsors,
and we study certain Lie-theoretic counting problems related to the case when
$X$ is an elliptic curve - a case which we investigate more carefully following
Fratila, Gunningham and P. Li.",2024-12-26T08:26:32Z,http://arxiv.org/abs/2412.19116v1,"Konstantin Jakob, Zhiwei Yun"
Discrete vs. Continuous Trade-offs for Generative Models,"This work explores the theoretical and practical foundations of denoising
diffusion probabilistic models (DDPMs) and score-based generative models, which
leverage stochastic processes and Brownian motion to model complex data
distributions. These models employ forward and reverse diffusion processes
defined through stochastic differential equations (SDEs) to iteratively add and
remove noise, enabling high-quality data generation. By analyzing the
performance bounds of these models, we demonstrate how score estimation errors
propagate through the reverse process and bound the total variation distance
using discrete Girsanov transformations, Pinsker's inequality, and the data
processing inequality (DPI) for an information theoretic lens.",2024-12-26T08:14:27Z,http://arxiv.org/abs/2412.19114v1,"Jathin Korrapati, Tanish Baranwal, Rahul Shah"
"SketchFill: Sketch-Guided Code Generation for Imputing Derived Missing
  Values","Missing value is a critical issue in data science, significantly impacting
the reliability of analyses and predictions. Missing value imputation (MVI) is
a longstanding problem because it highly relies on domain knowledge. Large
language models (LLMs) have emerged as a promising tool for data cleaning,
including MVI for tabular data, offering advanced capabilities for
understanding and generating content. However, despite their promise, existing
LLM techniques such as in-context learning and Chain-of-Thought (CoT) often
fall short in guiding LLMs to perform complex reasoning for MVI, particularly
when imputing derived missing values, which require mathematical formulas and
data relationships across rows and columns. This gap underscores the need for
further advancements in LLM methodologies to enhance their reasoning
capabilities for more reliable imputation outcomes. To fill this gap, we
propose SketchFill, a novel sketch-based method to guide LLMs in generating
accurate formulas to impute missing numerical values. Our experimental results
demonstrate that SketchFill significantly outperforms state-of-the-art methods,
achieving 56.2% higher accuracy than CoT-based methods and 78.8% higher
accuracy than MetaGPT. This sets a new standard for automated data cleaning and
advances the field of MVI for numerical values.",2024-12-26T08:13:34Z,http://arxiv.org/abs/2412.19113v1,"Yunfan Zhang, Changlun Li, Yuyu Luo, Nan Tang"
"Spectral Enhancement and Pseudo-Anchor Guidance for Infrared-Visible
  Person Re-Identification","The development of deep learning has facilitated the application of person
re-identification (ReID) technology in intelligent security. Visible-infrared
person re-identification (VI-ReID) aims to match pedestrians across infrared
and visible modality images enabling 24-hour surveillance. Current studies
relying on unsupervised modality transformations as well as inefficient
embedding constraints to bridge the spectral differences between infrared and
visible images, however, limit their potential performance. To tackle the
limitations of the above approaches, this paper introduces a simple yet
effective Spectral Enhancement and Pseudo-anchor Guidance Network, named
SEPG-Net. Specifically, we propose a more homogeneous spectral enhancement
scheme based on frequency domain information and greyscale space, which avoids
the information loss typically caused by inefficient modality transformations.
Further, a Pseudo Anchor-guided Bidirectional Aggregation (PABA) loss is
introduced to bridge local modality discrepancies while better preserving
discriminative identity embeddings. Experimental results on two public
benchmark datasets demonstrate the superior performance of SEPG-Net against
other state-of-the-art methods. The code is available at
https://github.com/1024AILab/ReID-SEPG.",2024-12-26T08:03:53Z,http://arxiv.org/abs/2412.19111v1,"Yiyuan Ge, Zhihao Chen, Ziyang Wang, Jiaju Kang, Mingya Zhang"
"Graph Mixture of Experts and Memory-augmented Routers for Multivariate
  Time Series Anomaly Detection","Multivariate time series (MTS) anomaly detection is a critical task that
involves identifying abnormal patterns or events in data that consist of
multiple interrelated time series. In order to better model the complex
interdependence between entities and the various inherent characteristics of
each entity, the GNN based methods are widely adopted by existing methods. In
each layer of GNN, node features aggregate information from their neighboring
nodes to update their information. In doing so, from shallow layer to deep
layer in GNN, original individual node features continue to be weakened and
more structural information,i.e., from short-distance neighborhood to
long-distance neighborhood, continues to be enhanced. However, research to date
has largely ignored the understanding of how hierarchical graph information is
represented and their characteristics that can benefit anomaly detection.
Existing methods simply leverage the output from the last layer of GNN for
anomaly estimation while neglecting the essential information contained in the
intermediate GNN layers. To address such limitations, in this paper, we propose
a Graph Mixture of Experts (Graph-MoE) network for multivariate time series
anomaly detection, which incorporates the mixture of experts (MoE) module to
adaptively represent and integrate hierarchical multi-layer graph information
into entity representations. It is worth noting that our Graph-MoE can be
integrated into any GNN-based MTS anomaly detection method in a plug-and-play
manner. In addition, the memory-augmented routers are proposed in this paper to
capture the correlation temporal information in terms of the global historical
features of MTS to adaptively weigh the obtained entity representations to
achieve successful anomaly estimation. Extensive experiments on five
challenging datasets prove the superiority of our approach and each proposed
module.",2024-12-26T07:49:51Z,http://arxiv.org/abs/2412.19108v1,"Xiaoyu Huang, Weidong Chen, Bo Hu, Zhendong Mao"
"The role of potential energy landscape research in the development of
  new electrolyte solutions","The development of new electrolyte solutions with improved characteristics is
a key challenge for creating high-performance batteries, fuel cells,
supercapacitors, and other electrochemical devices. The study of the potential
energy landscape (PEL) plays an important role in this process, providing
information about the interactions between solution components at the molecular
level. In this work, we review the practice of applying PEL research methods
based on classical and quantum-chemical algorithms to analyze the structure,
dynamics, and thermodynamic properties of electrolyte solutions. Intermolecular
and ion-molecular interactions at the microscopic level, which determine the
macroscopic properties of the electrolyte solution, are considered in detail.
The importance of identifying stable configurations of ions and their solvates
is emphasized. PEL analysis allows for the systematic determination of the most
probable structures and complexes formed in solution, which is important for
understanding ion transport mechanisms. The study of the PEL allows for the
determination of the energy barriers that must be overcome for ion migration,
which is related to the conductivity of the electrolyte. The application of PEL
research methods in combination with experimental data opens up new
possibilities for the rational design of electrolyte solutions with desired
physicochemical properties.",2024-12-26T07:45:29Z,http://arxiv.org/abs/2412.19103v1,Vitaly V. Chaban
"""I've Heard of You!"": Generate Spoken Named Entity Recognition Data for
  Unseen Entities","Spoken named entity recognition (NER) aims to identify named entities from
speech, playing an important role in speech processing. New named entities
appear every day, however, annotating their Spoken NER data is costly. In this
paper, we demonstrate that existing Spoken NER systems perform poorly when
dealing with previously unseen named entities. To tackle this challenge, we
propose a method for generating Spoken NER data based on a named entity
dictionary (NED) to reduce costs. Specifically, we first use a large language
model (LLM) to generate sentences from the sampled named entities and then use
a text-to-speech (TTS) system to generate the speech. Furthermore, we introduce
a noise metric to filter out noisy data. To evaluate our approach, we release a
novel Spoken NER benchmark along with a corresponding NED containing 8,853
entities. Experiment results show that our method achieves state-of-the-art
(SOTA) performance in the in-domain, zero-shot domain adaptation, and fully
zero-shot settings. Our data will be available at
https://github.com/DeepLearnXMU/HeardU.",2024-12-26T07:43:18Z,http://arxiv.org/abs/2412.19102v1,"Jiawei Yu, Xiang Geng, Yuang Li, Mengxin Ren, Wei Tang, Jiahuan Li, Zhibin Lan, Min Zhang, Hao Yang, Shujian Huang, Jinsong Su"
"Reconstruction Target Matters in Masked Image Modeling for Cross-Domain
  Few-Shot Learning","Cross-Domain Few-Shot Learning (CDFSL) requires the model to transfer
knowledge from the data-abundant source domain to data-scarce target domains
for fast adaptation, where the large domain gap makes CDFSL a challenging
problem. Masked Autoencoder (MAE) excels in effectively using unlabeled data
and learning image's global structures, enhancing model generalization and
robustness. However, in the CDFSL task with significant domain shifts, we find
MAE even shows lower performance than the baseline supervised models. In this
paper, we first delve into this phenomenon for an interpretation. We find that
MAE tends to focus on low-level domain information during reconstructing pixels
while changing the reconstruction target to token features could mitigate this
problem. However, not all features are beneficial, as we then find
reconstructing high-level features can hardly improve the model's
transferability, indicating a trade-off between filtering domain information
and preserving the image's global structure. In all, the reconstruction target
matters for the CDFSL task. Based on the above findings and interpretations, we
further propose Domain-Agnostic Masked Image Modeling (DAMIM) for the CDFSL
task. DAMIM includes an Aggregated Feature Reconstruction module to
automatically aggregate features for reconstruction, with balanced learning of
domain-agnostic information and images' global structure, and a Lightweight
Decoder module to further benefit the encoder's generalizability. Experiments
on four CDFSL datasets demonstrate that our method achieves state-of-the-art
performance.",2024-12-26T07:43:01Z,http://arxiv.org/abs/2412.19101v1,"Ran Ma, Yixiong Zou, Yuhua Li, Ruixuan Li"
"BSDB-Net: Band-Split Dual-Branch Network with Selective State Spaces
  Mechanism for Monaural Speech Enhancement","Although the complex spectrum-based speech enhancement(SE) methods have
achieved significant performance, coupling amplitude and phase can lead to a
compensation effect, where amplitude information is sacrificed to compensate
for the phase that is harmful to SE. In addition, to further improve the
performance of SE, many modules are stacked onto SE, resulting in increased
model complexity that limits the application of SE. To address these problems,
we proposed a dual-path network based on compressed frequency using Mamba.
First, we extract amplitude and phase information through parallel dual
branches. This approach leverages structured complex spectra to implicitly
capture phase information and solves the compensation effect by decoupling
amplitude and phase, and the network incorporates an interaction module to
suppress unnecessary parts and recover missing components from the other
branch. Second, to reduce network complexity, the network introduces a
band-split strategy to compress the frequency dimension. To further reduce
complexity while maintaining good performance, we designed a Mamba-based module
that models the time and frequency dimensions under linear complexity. Finally,
compared to baselines, our model achieves an average 8.3 times reduction in
computational complexity while maintaining superior performance. Furthermore,
it achieves a 25 times reduction in complexity compared to transformer-based
models.",2024-12-26T07:42:07Z,http://arxiv.org/abs/2412.19099v1,"Cunhang Fan, Enrui Liu, Andong Li, Jianhua Tao, Jian Zhou, Jiahao Li, Chengshi Zheng, Zhao Lv"
"Towards structural softness and enhanced electromechanical responses in
  HfO2 ferroelectrics","Structural softness - often characterized by unstable phonon modes and large
electromechanical responses - is a hallmark of ferroelectric perovskites like
BaTiO3 or Pb(Ti,Zr)O3. Whether HfO2 ferroelectrics present any such structural
softness is still a matter of debate. Here, using first principles
calculations, we predict that it is possible to induce structural instabilities
in hafnia. More specifically, our calculations show that in-plane epitaxial
tensile strain causes a mechanical instability of the ferroelectric phase,
which transforms discontinuously into an antipolar polymorph. Then, upon
release of the tensile strain, the antipolar polymorph transforms back to the
ferroelectric state by a soft phonon instability. We show that the softening is
accompanied by enhancements in the dielectric and piezoelectric responses.
While these transitions occur at high epitaxial strains for pure ferroelectric
HfO2, we show that the required deformations are considerably lowered in
superlattices with other simple oxides, which may facilitate realizing these
effects experimentally.",2024-12-26T07:23:16Z,http://arxiv.org/abs/2412.19093v1,"Binayak Mukherjee, Natalya S. Fedorova, Jorge Íñiguez-González"
"TrajGEOS: Trajectory Graph Enhanced Orientation-based Sequential Network
  for Mobility Prediction","Human mobility studies how people move to access their needed resources and
plays a significant role in urban planning and location-based services. As a
paramount task of human mobility modeling, next location prediction is
challenging because of the diversity of users' historical trajectories that
gives rise to complex mobility patterns and various contexts. Deep sequential
models have been widely used to predict the next location by leveraging the
inherent sequentiality of trajectory data. However, they do not fully leverage
the relationship between locations and fail to capture users' multi-level
preferences. This work constructs a trajectory graph from users' historical
traces and proposes a \textbf{Traj}ectory \textbf{G}raph \textbf{E}nhanced
\textbf{O}rientation-based \textbf{S}equential network (TrajGEOS) for
next-location prediction tasks. TrajGEOS introduces hierarchical graph
convolution to capture location and user embeddings. Such embeddings consider
not only the contextual feature of locations but also the relation between
them, and serve as additional features in downstream modules. In addition, we
design an orientation-based module to learn users' mid-term preferences from
sequential modeling modules and their recent trajectories. Extensive
experiments on three real-world LBSN datasets corroborate the value of graph
and orientation-based modules and demonstrate that TrajGEOS outperforms the
state-of-the-art methods on the next location prediction task.",2024-12-26T07:18:38Z,http://arxiv.org/abs/2412.19092v1,"Zhaoping Hu, Zongyuan Huang, Jinming Yang, Tao Yang, Yaohui Jin, Yanyan Xu"
From Coin to Data: The Impact of Object Detection on Digital Numismatics,"In this work we investigate the application of advanced object detection
techniques to digital numismatics, focussing on the analysis of historical
coins. Leveraging models such as Contrastive Language-Image Pre-training
(CLIP), we develop a flexible framework for identifying and classifying
specific coin features using both image and textual descriptions. By examining
two distinct datasets, modern Russian coins featuring intricate ""Saint George
and the Dragon"" designs and degraded 1st millennium AD Southeast Asian coins
bearing Hindu-Buddhist symbols, we evaluate the efficacy of different detection
algorithms in search and classification tasks. Our results demonstrate the
superior performance of larger CLIP models in detecting complex imagery, while
traditional methods excel in identifying simple geometric patterns.
Additionally, we propose a statistical calibration mechanism to enhance the
reliability of similarity scores in low-quality datasets. This work highlights
the transformative potential of integrating state-of-the-art object detection
into digital numismatics, enabling more scalable, precise, and efficient
analysis of historical artifacts. These advancements pave the way for new
methodologies in cultural heritage research, artefact provenance studies, and
the detection of forgeries.",2024-12-26T07:05:53Z,http://arxiv.org/abs/2412.19091v1,"Rafael Cabral, Maria De Iorio, Andrew Harris"
"Integrating Artificial Open Generative Artificial Intelligence into
  Software Supply Chain Security","While new technologies emerge, human errors always looming. Software supply
chain is increasingly complex and intertwined, the security of a service has
become paramount to ensuring the integrity of products, safeguarding data
privacy, and maintaining operational continuity. In this work, we conducted
experiments on the promising open Large Language Models (LLMs) into two main
software security challenges: source code language errors and deprecated code,
with a focus on their potential to replace conventional static and dynamic
security scanners that rely on predefined rules and patterns. Our findings
suggest that while LLMs present some unexpected results, they also encounter
significant limitations, particularly in memory complexity and the management
of new and unfamiliar data patterns. Despite these challenges, the proactive
application of LLMs, coupled with extensive security databases and continuous
updates, holds the potential to fortify Software Supply Chain (SSC) processes
against emerging threats.",2024-12-26T07:03:55Z,http://arxiv.org/abs/2412.19088v1,"Vasileios Alevizos, George A Papakostas, Akebu Simasiku, Dimitra Malliarou, Antonis Messinis, Sabrina Edralin, Clark Xu, Zongliang Yue"
MoPD: Mixture-of-Prompts Distillation for Vision-Language Models,"Soft prompt learning methods are effective for adapting vision-language
models (VLMs) to downstream tasks. Nevertheless, empirical evidence reveals a
tendency of existing methods that they overfit seen classes and exhibit
degraded performance on unseen classes. This limitation is due to the inherent
bias in the training data towards the seen classes. To address this issue, we
propose a novel soft prompt learning method, named Mixture-of-Prompts
Distillation (MoPD), which can effectively transfer useful knowledge from hard
prompts manually hand-crafted (a.k.a. teacher prompts) to the learnable soft
prompt (a.k.a. student prompt), thereby enhancing the generalization ability of
soft prompts on unseen classes. Moreover, the proposed MoPD method utilizes a
gating network that learns to select hard prompts used for prompt distillation.
Extensive experiments demonstrate that the proposed MoPD method outperforms
state-of-the-art baselines especially on on unseen classes.",2024-12-26T06:57:04Z,http://arxiv.org/abs/2412.19087v1,"Yang Chen, Shuai Fu, Yu Zhang"
Investigating the Temporal Dynamics of Cyber Threat Intelligence,"Indicators of Compromise (IoCs) play a crucial role in the rapid detection
and mitigation of cyber threats. However, the existing body of literature lacks
in-depth analytical studies on the temporal aspects of IoC publication,
especially when considering up-to-date datasets related to Common
Vulnerabilities and Exposures (CVEs). This paper addresses this gap by
conducting an analysis of the timeliness and comprehensiveness of Cyber Threat
Intelligence (CTI) pertaining to several recent CVEs. The insights derived from
this study aim to enhance cybersecurity defense strategies, particularly when
dealing with dynamic cyber threats that continually adapt their Tactics,
Techniques, and Procedures (TTPs). Utilizing IoCs sourced from multiple
providers, we scrutinize the IoC publication rate. Our analysis delves into how
various factors, including the inherent nature of a threat, its evolutionary
trajectory, and its observability over time, influence the publication rate of
IoCs. Our preliminary findings emphasize the critical need for cyber defenders
to maintain a constant state of vigilance in updating their IoCs for any given
vulnerability. This vigilance is warranted because the publication rate of IoCs
may exhibit fluctuations over time. We observe a recurring pattern akin to an
epidemic model, with an initial phase following the public disclosure of a
vulnerability characterized by sparse IoC publications, followed by a sudden
surge, and subsequently, a protracted period with a slower rate of IoC
publication.",2024-12-26T06:54:27Z,http://arxiv.org/abs/2412.19086v1,"Angel Kodituwakku, Clark Xu, Daniel Rogers, David K. Ahn, Errin W. Fulp"
"Assessing Pre-trained Models for Transfer Learning through Distribution
  of Spectral Components","Pre-trained model assessment for transfer learning aims to identify the
optimal candidate for the downstream tasks from a model hub, without the need
of time-consuming fine-tuning. Existing advanced works mainly focus on
analyzing the intrinsic characteristics of the entire features extracted by
each pre-trained model or how well such features fit the target labels. This
paper proposes a novel perspective for pre-trained model assessment through the
Distribution of Spectral Components (DISCO). Through singular value
decomposition of features extracted from pre-trained models, we investigate
different spectral components and observe that they possess distinct
transferability, contributing diversely to the fine-tuning performance.
Inspired by this, we propose an assessment method based on the distribution of
spectral components which measures the proportions of their corresponding
singular values. Pre-trained models with features concentrating on more
transferable components are regarded as better choices for transfer learning.
We further leverage the labels of downstream data to better estimate the
transferability of each spectral component and derive the final assessment
criterion. Our proposed method is flexible and can be applied to both
classification and regression tasks. We conducted comprehensive experiments
across three benchmarks and two tasks including image classification and object
detection, demonstrating that our method achieves state-of-the-art performance
in choosing proper pre-trained models from the model hub for transfer learning.",2024-12-26T06:54:22Z,http://arxiv.org/abs/2412.19085v1,"Tengxue Zhang, Yang Shu, Xinyang Chen, Yifei Long, Chenjuan Guo, Bin Yang"
A Microservice Graph Generator with Production Characteristics,"A production microservice application may provide multiple services, queries
of a service may have different call graphs, and a microservice may be shared
across call graphs. It is challenging to improve the resource efficiency of
such complex applications without proper benchmarks, while production traces
are too large to be used in experiments. To this end, we propose a Service
Dependency Graph Generator (DGG) that comprises a Data Handler and a Graph
Generator, for generating the service dependency graphs of benchmarks that
incorporate production-level characteristics from traces. The data handler
first constructs fine-grained call graphs with dynamic interface and repeated
calling features from the trace and merges them into dependency graphs, and
then clusters them into different categories based on the topological and
invocation types. Taking the organized data and the selected category, the
graph generator simulates the process of real microservices invoking downstream
microservices using a random graph model, generates multiple call graphs, and
merges the call graphs to form the small-scale service dependency graph with
production-level characteristics. Case studies show that DGG's generated graphs
are similar to real traces in terms of topologies. Moreover, the resource
scaling based on DGG's fine-grained call graph constructing increases the
resource efficiency by up to 44.8% while ensuring the required QoS.",2024-12-26T06:51:35Z,http://arxiv.org/abs/2412.19083v1,"Fanrong Du, Jiuchen Shi, Quan Chen, Li Li, Minyi Guo"
Blue laser induced bright red fluorescence in hot cesium vapor,"We have observed laser-induced fluorescence using 456 nm laser radiation,
resonant with the 6S1/2-7P3/2 transition in Cs atoms. It includes red emission
lines in the range of 580-730 nm and a prominent line at 852 nm corresponding
to the 6P3/2-6S1/2 transition. A T-shaped all-sapphire cell with a length of 1
cm, containing Cs atomic vapor and capable of being heated up to 500 oC, was
used. The laser-induced fluorescence (LIF) power at 852 nm was investigated as
a function of the cell temperature. The maximum LIF power was achieved at 130
oC, while a significant decrease was observed around 300 oC. At 130 oC, the
Doppler-broadened LIF spectrum at 852 nm exhibited self-conversion, resulting
in the formation of two distinct peaks within the spectrum. The LIF power at
852 nm was also studied as a function of the 456 nm radiation power. The Cs
cell demonstrated potential as an efficient optical filter and down-converter,
effectively transforming 456 nm radiation into 852 nm radiation.",2024-12-26T06:43:33Z,http://arxiv.org/abs/2412.19081v1,"Armen Sargsyan, Anahit Gogyan, David Sarkisyan"
"Mask Factory: Towards High-quality Synthetic Data Generation for
  Dichotomous Image Segmentation","Dichotomous Image Segmentation (DIS) tasks require highly precise
annotations, and traditional dataset creation methods are labor intensive,
costly, and require extensive domain expertise. Although using synthetic data
for DIS is a promising solution to these challenges, current generative models
and techniques struggle with the issues of scene deviations, noise-induced
errors, and limited training sample variability. To address these issues, we
introduce a novel approach, \textbf{\ourmodel{}}, which provides a scalable
solution for generating diverse and precise datasets, markedly reducing
preparation time and costs. We first introduce a general mask editing method
that combines rigid and non-rigid editing techniques to generate high-quality
synthetic masks. Specially, rigid editing leverages geometric priors from
diffusion models to achieve precise viewpoint transformations under zero-shot
conditions, while non-rigid editing employs adversarial training and
self-attention mechanisms for complex, topologically consistent modifications.
Then, we generate pairs of high-resolution image and accurate segmentation mask
using a multi-conditional control generation method. Finally, our experiments
on the widely-used DIS5K dataset benchmark demonstrate superior performance in
quality and efficiency compared to existing methods. The code is available at
\url{https://qian-hao-tian.github.io/MaskFactory/}.",2024-12-26T06:37:25Z,http://arxiv.org/abs/2412.19080v1,"Haotian Qian, YD Chen, Shengtao Lou, Fahad Shahbaz Khan, Xiaogang Jin, Deng-Ping Fan"
"Graph-Enhanced Dual-Stream Feature Fusion with Pre-Trained Model for
  Acoustic Traffic Monitoring","Microphone array techniques are widely used in sound source localization and
smart city acoustic-based traffic monitoring, but these applications face
significant challenges due to the scarcity of labeled real-world traffic audio
data and the complexity and diversity of application scenarios. The DCASE
Challenge's Task 10 focuses on using multi-channel audio signals to count
vehicles (cars or commercial vehicles) and identify their directions
(left-to-right or vice versa). In this paper, we propose a graph-enhanced
dual-stream feature fusion network (GEDF-Net) for acoustic traffic monitoring,
which simultaneously considers vehicle type and direction to improve detection.
We propose a graph-enhanced dual-stream feature fusion strategy which consists
of a vehicle type feature extraction (VTFE) branch, a vehicle direction feature
extraction (VDFE) branch, and a frame-level feature fusion module to combine
the type and direction feature for enhanced performance. A pre-trained model
(PANNs) is used in the VTFE branch to mitigate data scarcity and enhance the
type features, followed by a graph attention mechanism to exploit temporal
relationships and highlight important audio events within these features. The
frame-level fusion of direction and type features enables fine-grained feature
representation, resulting in better detection performance. Experiments
demonstrate the effectiveness of our proposed method. GEDF-Net is our
submission that achieved 1st place in the DCASE 2024 Challenge Task 10.",2024-12-26T06:28:42Z,http://arxiv.org/abs/2412.19078v1,"Shitong Fan, Feiyang Xiao, Wenbo Wang, Shuhan Qi, Qiaoxi Zhu, Wenwu Wang, Jian Guan"
Prescribed-Time Boundary Control of Flexible String Systems,"This paper presents a boundary control scheme for prescribed-time (PT) stable
of flexible string systems via backstepping method, and the dynamics of such
systems modeled by Hamilton's principle is described as hyperbolic partial
differential equation (PDE). Initially, to construct a boundary controller with
PT stabilization capacity, a PT stable hyperbolic PDE system with time-varying
coefficient is chosen as the target system, and a corresponding Volterra
integral transform with time-varying kernel function is considered. Meanwhile,
the kernel equation and controller is determined by taking derivative. Then, to
identify the boundary controller, the well-posedness of kernel equation is
derived by means of successive approximation and mathematical induction, and
the upper bound of kernel function is estimated. Furthermore, the inverse
transform is proved with the help of a similar process for kernel function.
Subsequently, the PT stability of closed-loop system is proved by PT stability
of target system and reversible integral transform. Finally, the simulation
results demonstrate the effectiveness of our scheme.",2024-12-26T06:08:15Z,http://arxiv.org/abs/2412.19073v1,"He Yang, Chuan Zhang, Yingxin Guo, Xianfu Zhang"
"Robust Speech and Natural Language Processing Models for Depression
  Screening","Depression is a global health concern with a critical need for increased
patient screening. Speech technology offers advantages for remote screening but
must perform robustly across patients. We have described two deep learning
models developed for this purpose. One model is based on acoustics; the other
is based on natural language processing. Both models employ transfer learning.
Data from a depression-labeled corpus in which 11,000 unique users interacted
with a human-machine application using conversational speech is used. Results
on binary depression classification have shown that both models perform at or
above AUC=0.80 on unseen data with no speaker overlap. Performance is further
analyzed as a function of test subset characteristics, finding that the models
are generally robust over speaker and session variables. We conclude that
models based on these approaches offer promise for generalized automated
depression screening.",2024-12-26T06:05:52Z,http://arxiv.org/abs/2412.19072v1,"Y. Lu, A. Harati, T. Rutowski, R. Oliveira, P. Chlebek, E. Shriberg"
Cross-Demographic Portability of Deep NLP-Based Depression Models,"Deep learning models are rapidly gaining interest for real-world applications
in behavioral health. An important gap in current literature is how well such
models generalize over different populations. We study Natural Language
Processing (NLP) based models to explore portability over two different corpora
highly mismatched in age. The first and larger corpus contains younger
speakers. It is used to train an NLP model to predict depression. When testing
on unseen speakers from the same age distribution, this model performs at
AUC=0.82. We then test this model on the second corpus, which comprises seniors
from a retirement community. Despite the large demographic differences in the
two corpora, we saw only modest degradation in performance for the
senior-corpus data, achieving AUC=0.76. Interestingly, in the senior
population, we find AUC=0.81 for the subset of patients whose health state is
consistent over time. Implications for demographic portability of speech-based
applications are discussed.",2024-12-26T05:54:24Z,http://arxiv.org/abs/2412.19070v1,"Tomek Rutowski, Elizabeth Shriberg, Amir Harati, Yang Lu, Ricardo Oliveira, Piotr Chlebek"
Effective and secure federated online learning to rank,"Online Learning to Rank (OLTR) optimises ranking models using implicit user
feedback, such as clicks. Unlike traditional Learning to Rank (LTR) methods
that rely on a static set of training data with relevance judgements to learn a
ranking model, OLTR methods update the model continually as new data arrives.
Thus, it addresses several drawbacks such as the high cost of human
annotations, potential misalignment between user preferences and human
judgments, and the rapid changes in user query intents. However, OLTR methods
typically require the collection of searchable data, user queries, and clicks,
which poses privacy concerns for users.
  Federated Online Learning to Rank (FOLTR) integrates OLTR within a Federated
Learning (FL) framework to enhance privacy by not sharing raw data. While
promising, FOLTR methods currently lag behind traditional centralised OLTR due
to challenges in ranking effectiveness, robustness with respect to data
distribution across clients, susceptibility to attacks, and the ability to
unlearn client interactions and data. This thesis presents a comprehensive
study on Federated Online Learning to Rank, addressing its effectiveness,
robustness, security, and unlearning capabilities, thereby expanding the
landscape of FOLTR.",2024-12-26T05:53:10Z,http://arxiv.org/abs/2412.19069v1,Shuyi Wang
"Attacking Voice Anonymization Systems with Augmented Feature and Speaker
  Identity Difference","This study focuses on the First VoicePrivacy Attacker Challenge within the
ICASSP 2025 Signal Processing Grand Challenge, which aims to develop speaker
verification systems capable of determining whether two anonymized speech
signals are from the same speaker. However, differences between feature
distributions of original and anonymized speech complicate this task. To
address this challenge, we propose an attacker system that combines Data
Augmentation enhanced feature representation and Speaker Identity Difference
enhanced classifier to improve verification performance, termed DA-SID.
Specifically, data augmentation strategies (i.e., data fusion and SpecAugment)
are utilized to mitigate feature distribution gaps, while probabilistic linear
discriminant analysis (PLDA) is employed to further enhance speaker identity
difference. Our system significantly outperforms the baseline, demonstrating
exceptional effectiveness and robustness against various voice anonymization
systems, ultimately securing a top-5 ranking in the challenge.",2024-12-26T05:52:44Z,http://arxiv.org/abs/2412.19068v1,"Yanzhe Zhang, Zhonghao Bi, Feiyang Xiao, Xuefeng Yang, Qiaoxi Zhu, Jian Guan"
Learning Monocular Depth from Events via Egomotion Compensation,"Event cameras are neuromorphically inspired sensors that sparsely and
asynchronously report brightness changes. Their unique characteristics of high
temporal resolution, high dynamic range, and low power consumption make them
well-suited for addressing challenges in monocular depth estimation (e.g.,
high-speed or low-lighting conditions). However, current existing methods
primarily treat event streams as black-box learning systems without
incorporating prior physical principles, thus becoming over-parameterized and
failing to fully exploit the rich temporal information inherent in event camera
data. To address this limitation, we incorporate physical motion principles to
propose an interpretable monocular depth estimation framework, where the
likelihood of various depth hypotheses is explicitly determined by the effect
of motion compensation. To achieve this, we propose a Focus Cost Discrimination
(FCD) module that measures the clarity of edges as an essential indicator of
focus level and integrates spatial surroundings to facilitate cost estimation.
Furthermore, we analyze the noise patterns within our framework and improve it
with the newly introduced Inter-Hypotheses Cost Aggregation (IHCA) module,
where the cost volume is refined through cost trend prediction and multi-scale
cost consistency constraints. Extensive experiments on real-world and synthetic
datasets demonstrate that our proposed framework outperforms cutting-edge
methods by up to 10\% in terms of the absolute relative error metric, revealing
superior performance in predicting accuracy.",2024-12-26T05:41:18Z,http://arxiv.org/abs/2412.19067v1,"Haitao Meng, Chonghao Zhong, Sheng Tang, Lian JunJia, Wenwei Lin, Zhenshan Bing, Yi Chang, Gang Chen, Alois Knoll"
"Predicting Accurate X-ray Absorption Spectra for CN$^+$, CN$^\bullet$,
  and CN$^-$: Insights from First-Principles Simulations","High-resolution X-ray spectroscopy is an essential tool in X-ray astronomy,
enabling detailed studies of celestial objects and their physical and chemical
properties. However, comprehensive mapping of high-resolution X-ray spectra for
even simple interstellar and circumstellar molecules is still lacking. In this
study, we conducted systematic quantum chemical simulations to predict the C1s
X-ray absorption spectra of CN$^+$, CN, and CN$^-$. Our findings provide
valuable references for both X-ray astronomy and laboratory studies. We
assigned the first electronic peak of CN$^+$ and CN to C1s $\rightarrow
\sigma^*$ transitions, while the peak for CN$^-$ corresponds to a C1s
$\rightarrow \pi^*$ transition. We further calculated the vibronic fine
structures for these transitions using the quantum wavepacket method based on
multiconfigurational-level, anharmonic potential energy curves, revealing
distinct energy positions for the 0-0 absorptions at 280.7 eV, 279.6 eV, and
285.8 eV. Each vibronic profile features a prominent 0-0 peak, showing overall
similarity but differing intensity ratios of the 0-0 and 0-1 peaks. Notably,
introducing a C1s core hole leads to shortened C-N bond lengths and increased
vibrational frequencies across all species. These findings enhance our
understanding of the electronic structures and X-ray spectra of carbon-nitrogen
species, emphasizing the influence of charge state on X-ray absorptions.",2024-12-26T05:27:06Z,http://arxiv.org/abs/2412.19065v1,"Jinyu Li, Sheng-Yu Wang, Lu Zhang, Guoyan Ge, Minrui Wei, Junxiang Zuo, Weijie Hua"
DAPoinTr: Domain Adaptive Point Transformer for Point Cloud Completion,"Point Transformers (PoinTr) have shown great potential in point cloud
completion recently. Nevertheless, effective domain adaptation that improves
transferability toward target domains remains unexplored. In this paper, we
delve into this topic and empirically discover that direct feature alignment on
point Transformer's CNN backbone only brings limited improvements since it
cannot guarantee sequence-wise domain-invariant features in the Transformer. To
this end, we propose a pioneering Domain Adaptive Point Transformer (DAPoinTr)
framework for point cloud completion. DAPoinTr consists of three key
components: Domain Query-based Feature Alignment (DQFA), Point Token-wise
Feature alignment (PTFA), and Voted Prediction Consistency (VPC). In
particular, DQFA is presented to narrow the global domain gaps from the
sequence via the presented domain proxy and domain query at the Transformer
encoder and decoder, respectively. PTFA is proposed to close the local domain
shifts by aligning the tokens, \emph{i.e.,} point proxy and dynamic query, at
the Transformer encoder and decoder, respectively. VPC is designed to consider
different Transformer decoders as multiple of experts (MoE) for ensembled
prediction voting and pseudo-label generation. Extensive experiments with
visualization on several domain adaptation benchmarks demonstrate the
effectiveness and superiority of our DAPoinTr compared with state-of-the-art
methods. Code will be publicly available at:
https://github.com/Yinghui-Li-New/DAPoinTr",2024-12-26T05:16:54Z,http://arxiv.org/abs/2412.19062v1,"Yinghui Li, Qianyu Zhou, Jingyu Gong, Ye Zhu, Richard Dazeley, Xinkui Zhao, Xuequan Lu"
"An active hydroelastic liquid crystal phase of a fluttering
  ferroelectric nematic","Polarization flutter, produced by an applied AC electric field drives an
equilibrium ferroelectric nematic ($\mathrm{N_F}$) liquid crystal (LC) through
a transition into a dissipative active ferroelectric nematic state exhibiting
strong elasto-hydrodynamic intermolecular interaction. In such a fluttering
ferroelectric, the typical equilibrium $\mathrm{N_F}$ textural features adopted
to reduce electrostatic energy, such as preferences for director bend, and
alignment of polarization parallel to LC/air interfaces, are overcome, giving
way to nonequilibrium conjugate structures in which director splay, and
alignment of polarization normal to $\mathrm{N_F}$/air interfaces are
preferred. Viewing the latter textures as those of an active nematic phase
reveals that self-organization to reduce effective viscosity and resulting
dissipation generates a flow-driven apparent nematic elasticity and interface
structuring that dominates equilibrium LC elastic and surface forces.",2024-12-26T05:13:55Z,http://arxiv.org/abs/2412.19061v1,"Xi Chen, Cory Pecinovsky, Eva Korblova, Matthew A. Glaser, Leo Radzihovsky, Joseph E. Maclennan, David M. Walba, Noel A. Clark"
Coarse-grained binning in Drell-Yan transverse momentum spectra,"We report a study of the determination of the intrinsic transverse momentum
of partons, the intrinsic $k_T$, from the dilepton transverse momentum $p_T$ in
Drell-Yan (DY) production at hadron colliders. The result shows that a good
sensitivity to the intrinsic $k_T$ distribution is achieved by measuring
relative ratios between the cross sections of suitably defined low-$p_T$ and
high-$p_T$ regions. The study is performed through both a pseudo-data test and
an extraction from measurements of the DY process by the CMS collaboration.
Since the methodology does not rely on any dedicated partition of bins, this
$p_T$-ratio observable requires less special treatment in very low $p_T$
regions, and propagates lower systematic uncertainties induced from unfolding
or momentum migration, in contrast with previous proposals of using a
fine-binning measurement of the differential cross section.",2024-12-26T05:13:39Z,http://arxiv.org/abs/2412.19060v1,"Wenxiao Zhan, Siqi Yang, Minghui Liu, Francesco Hautmann, Liang Han"
Faster Semi-streaming Matchings via Alternating Trees,"We design a deterministic algorithm for the $(1+\epsilon)$-approximate
maximum matching problem. Our primary result demonstrates that this problem can
be solved in $O(\epsilon^{-6})$ semi-streaming passes, improving upon the
$O(\epsilon^{-19})$ pass-complexity algorithm by [Fischer, Mitrovi\'c, and
Uitto, STOC'22]. This contributes substantially toward resolving Open
question~2 from [Assadi, SOSA'24]. Leveraging the framework introduced in
[FMU'22], our algorithm achieves an analogous round complexity speed-up for
computing a $(1+\epsilon)$-approximate maximum matching in both the Massively
Parallel Computation (MPC) and CONGEST models.
  The data structures maintained by our algorithm are formulated using blossom
notation and represented through alternating trees. This approach enables a
simplified correctness analysis by treating specific components as if operating
on bipartite graphs, effectively circumventing certain technical intricacies
present in prior work.",2024-12-26T04:59:27Z,http://arxiv.org/abs/2412.19057v1,"Slobodan Mitrović, Anish Mukherjee, Piotr Sankowski, Wen-Horng Sheu"
"SpectralKD: Understanding and Optimizing Vision Transformer Distillation
  through Spectral Analysis","Knowledge distillation effectively reduces model complexity while improving
performance, yet the underlying knowledge transfer mechanisms remain poorly
understood. We propose novel spectral analysis methods and guidelines to
optimize distillation, making the knowledge transfer process more
interpretable. Our analysis reveals that CaiT models concentrate information in
their first and last few layers, informing optimal layer selection for feature
map distillation. Surprisingly, we discover that Swin Transformer and CaiT
exhibit similar spectral encoding patterns despite their architectural
differences, enhancing our understanding of transformer architectures and
leading to improved feature map alignment strategies. Based on these insights,
we introduce a simple yet effective spectral alignment method named SpectralKD.
Experimental results demonstrate that following our guidelines enables
SpectralKD to achieve state-of-the-art performance (DeiT-Tiny: $+5.2\%$,
Swin-Tiny: $+1.4\%$ in ImageNet-1k Top-1 accuracy). Furthermore, through
spectral analysis of student models trained with and without distillation, we
show that distilled models mirror spectral patterns of their teachers,
providing a new lens for interpreting knowledge distillation dynamics. Our
code, pre-trained models, and experimental logs will be made publicly
available.",2024-12-26T04:45:05Z,http://arxiv.org/abs/2412.19055v1,"Huiyuan Tian, Bonan Xu, Shijian Li, Gang Pan"
Flattening subtyping by eta expansion,"To design type systems that use subtyping, we have to make tradeoffs. Deep
subtyping is more expressive than shallow subtyping, because deep subtyping
compares the entire structure of types. However, shallow subtyping is easier to
reason about. By eta-expanding source programs, we can get the effect of deep
subtyping with less of its complexity. An early paper on filter models
(Barendregt et al. 1983) examined two similar intersection type systems. The
first included a subsumption rule that used a rich subtyping relation,
including multiple rules for the top type and a distributivity rule. Their
second type system dropped the subsumption rule, but added a rule that allowed
a term to be eta-expanded before typing it. This rule in their second type
system compensated for the lack of subsumption: where their first type system
used subtyping to manipulate intersections deep inside types, their second type
system used introduction and elimination rules directly on the subterms created
by eta-expansion. Viewed as a computation, their proof of completeness for the
second (shallow) system performs eta-expansion. Thus, we can regard their proof
as inventing the application of eta-expansion to avoid deep subtyping. This
paper serves as a tutorial on using eta-expansion to obviate deep subtyping,
puts the invention of the technique by Barendregt et al. (1983) into context,
gives a complete proof of the relevant lemma, and discusses how the technique
can be used in type system design.",2024-12-26T04:28:10Z,http://arxiv.org/abs/2412.19053v1,Jana Dunfield
"Performance Characterization and Optimizations of Traditional ML
  Applications","Even in the era of Deep Learning based methods, traditional machine learning
methods with large data sets continue to attract significant attention.
However, we find an apparent lack of a detailed performance characterization of
these methods in the context of large training datasets. In this work, we study
the system's behavior of a number of traditional ML methods as implemented in
popular free software libraries/modules to identify critical performance
bottlenecks experienced by these applications. The performance characterization
study reveals several interesting insights on the performance of these
applications. Then we evaluate the performance benefits of applying some
well-known optimizations at the levels of caches and the main memory. More
specifically, we test the usefulness of optimizations such as (i) software
prefetching to improve cache performance and (ii) data layout and computation
reordering optimizations to improve locality in DRAM accesses. These
optimizations are implemented as modifications to the well-known scikit-learn
library, and hence can be easily leveraged by application programmers. We
evaluate the impact of the proposed optimizations using a combination of
simulation and execution on a real system. The software prefetching
optimization results in performance benefits varying from 5.2%-27.1% on
different ML applications while the data layout and computation reordering
approaches yield 6.16%-28.0% performance improvement.",2024-12-26T04:13:52Z,http://arxiv.org/abs/2412.19051v1,"Harsh Kumar, R. Govindarajan"
3+1 formalism of the minimally extended varying speed of light model,"The $3+1$ formalism provides a structured approach to analyzing spacetime by
separating it into spatial and temporal components. When applied to the
Robertson-Walker metric, it simplifies the analysis of cosmological evolution
by dividing the Einstein field equations into constraint and evolution
equations. It introduces the lapse function $N$ and the shift vector $N^i$,
which control how time and spatial coordinates evolve between hypersurfaces. In
standard model cosmology, $N = 1$ and $N^i = 0$ for the Robertson-Walker
metric. However, the $N$ becomes a function of time when we apply the metric to
the minimally extended varying speed of light model. This approach allows for a
more direct examination of the evolution of spatial geometry and offers
flexibility in handling scenarios where the lapse function and shift vector
vary. In this manuscript, we derive the model's $N$ and $N^i$, along with the
constraint and evolution equations, and demonstrate their consistency with the
existing Einstein equations. We have shown in a previous paper that the
possibility of changes in the speed of light in the Robertson-Walker metric is
due to cosmological time dilation. Through the $3+1$ formalism, we can make the
physical significance more explicit and demonstrate that it can be interpreted
as the lapse function. From this, we show that the minimally extended varying
speed of light model is consistent.",2024-12-26T04:05:39Z,http://arxiv.org/abs/2412.19049v1,Seokcheon Lee
Jasper and Stella: distillation of SOTA embedding models,"A crucial component of many deep learning applications (such as FAQ and RAG)
is dense retrieval, in which embedding models are used to convert raw text to
numerical vectors and then get the most similar text by MIPS (Maximum Inner
Product Search). Some text embedding benchmarks (e.g. MTEB, BEIR, and
AIR-Bench) have been established to evaluate embedding models accurately.
Thanks to these benchmarks, we can use SOTA models; however, the deployment and
application of these models in industry were hampered by their large vector
dimensions and numerous parameters. To alleviate this problem, 1) we present a
distillation technique that can enable a smaller student model to achieve good
performance. 2) Inspired by MRL we present a training approach of reducing the
vector dimensions based on its own vectors or its teacher vectors. 3) We do
simple yet effective alignment training between images and text to make our
model a multimodal encoder. We trained Stella and Jasper models using the
technologies above and achieved high scores on the MTEB leaderboard. We release
the model and data at Hugging Face Hub
(https://huggingface.co/infgrad/jasper_en_vision_language_v1) and the training
logs are at https://api.wandb.ai/links/dunnzhang0/z8jqoqpb.",2024-12-26T04:05:28Z,http://arxiv.org/abs/2412.19048v1,"Dun Zhang, FulongWang"
Inverses of integral transforms of RKHSs,"The Fourier transform and its inverse are well-known to have complex
conjugate integral kernels. S.~Saitoh demonstrated that this relationship
extends to the theory of integral transforms of Hilbert spaces of functions
under certain conditions. In this paper, we derive a necessary and sufficient
condition for the inverse of an integral transform of a Hilbert space of
functions to be represented by a complex conjugate integral kernel. As an
application, we present an alternative proof of Plancherel's theorem using the
theory of reproducing kernels.",2024-12-26T03:45:30Z,http://arxiv.org/abs/2412.19047v1,Akira Yamada
Revealing the Self: Brainwave-Based Human Trait Identification,"People exhibit unique emotional responses. In the same scenario, the
emotional reactions of two individuals can be either similar or vastly
different. For instance, consider one person's reaction to an invitation to
smoke versus another person's response to a query about their sleep quality.
The identification of these individual traits through the observation of common
physical parameters opens the door to a wide range of applications, including
psychological analysis, criminology, disease prediction, addiction control, and
more. While there has been previous research in the fields of psychometrics,
inertial sensors, computer vision, and audio analysis, this paper introduces a
novel technique for identifying human traits in real time using brainwave data.
To achieve this, we begin with an extensive study of brainwave data collected
from 80 participants using a portable EEG headset. We also conduct a
statistical analysis of the collected data utilizing box plots. Our analysis
uncovers several new insights, leading us to a groundbreaking unified approach
for identifying diverse human traits by leveraging machine learning techniques
on EEG data. Our analysis demonstrates that this proposed solution achieves
high accuracy. Moreover, we explore two deep-learning models to compare the
performance of our solution. Consequently, we have developed an integrated,
real-time trait identification solution using EEG data, based on the insights
from our analysis. To validate our approach, we conducted a rigorous user
evaluation with an additional 20 participants. The outcomes of this evaluation
illustrate both high accuracy and favorable user ratings, emphasizing the
robust potential of our proposed method to serve as a versatile solution for
human trait identification.",2024-12-26T03:27:34Z,http://arxiv.org/abs/2412.19041v1,"Md Mirajul Islam, Md Nahiyan Uddin, Maoyejatun Hasana, Debojit Pandit, Nafis Mahmud Rahman, Sriram Chellappan, Sami Azam, A. B. M. Alim Al Islam"
CL-attack: Textual Backdoor Attacks via Cross-Lingual Triggers,"Backdoor attacks significantly compromise the security of large language
models by triggering them to output specific and controlled content. Currently,
triggers for textual backdoor attacks fall into two categories: fixed-token
triggers and sentence-pattern triggers. However, the former are typically easy
to identify and filter, while the latter, such as syntax and style, do not
apply to all original samples and may lead to semantic shifts. In this paper,
inspired by cross-lingual (CL) prompts of LLMs in real-world scenarios, we
propose a higher-dimensional trigger method at the paragraph level, namely
CL-attack. CL-attack injects the backdoor by using texts with specific
structures that incorporate multiple languages, thereby offering greater
stealthiness and universality compared to existing backdoor attack techniques.
Extensive experiments on different tasks and model architectures demonstrate
that CL-attack can achieve nearly 100% attack success rate with a low poisoning
rate in both classification and generation tasks. We also empirically show that
the CL-attack is more robust against current major defense methods compared to
baseline backdoor attacks. Additionally, to mitigate CL-attack, we further
develop a new defense called TranslateDefense, which can partially mitigate the
impact of CL-attack.",2024-12-26T03:13:03Z,http://arxiv.org/abs/2412.19037v1,"Jingyi Zheng, Tianyi Hu, Tianshuo Cong, Xinlei He"
"Unifying Tree-Reweighted Belief Propagation and Mean Field for Tracking
  Extended Targets","This paper proposes a unified tree-reweighted belief propagation (BP) and
mean field (MF) approach for scalable detection and tracking of extended
targets within the framework of factor graph. The factor graph is partitioned
into a BP region and an MF region so that the messages in each region are
updated according to the corresponding region rules. The BP region exploits the
tree-reweighted BP, which offers improved convergence than the standard BP for
graphs with massive cycles, to resolve data association. The MF region
approximates the posterior densities of the measurement rate, kinematic state
and extent. For linear Gaussian target models and gamma Gaussian inverse
Wishart distributed state density, the unified approach provides a closed-form
recursion for the state density. Hence, the proposed algorithm is more
efficient than particle-based BP algorithms for extended target tracking. This
method also avoids measurement clustering and gating since it solves the data
association problem in a probabilistic fashion. We compare the proposed
approach with algorithms such as the Poisson multi-Bernoulli mixture filter and
the BP-based Poisson multi-Bernoulli filter. Simulation results demonstrate
that the proposed algorithm achieves enhanced tracking performance.",2024-12-26T03:12:53Z,http://arxiv.org/abs/2412.19036v1,"Weizhen Ma, Zhongliang Jing, Peng Dong, Henry Leung"
"Reflection on Purpose Changes Students' Academic Interests: A Scalable
  Intervention in an Online Course Catalog","College students routinely use online course catalogs to explore a variety of
academic offerings. Course catalogs may therefore be an effective place to
encourage reflection on academic choices and interests. To test this, we
embedded a psychological intervention in an online course catalog to encourage
students to reflect on their purpose during course exploration. Results of a
randomized field experiment with over 4,000 students at a large U.S. university
show that a purpose intervention increased students' cognitive engagement in
describing their interests, but reduced search activities. Students became more
interested in courses related to creative arts and social change, but less in
computer and data science. The findings demonstrate the malleability of
students' interests during course exploration and suggest practical strategies
to support purpose reflection and guide students toward deliberate exploration
of their interests in higher education.",2024-12-26T03:12:50Z,http://arxiv.org/abs/2412.19035v1,"Youjie Chen, Pranathi Iyer, Rene F. Kizilcec"
Repository Structure-Aware Training Makes SLMs Better Issue Resolver,"Language models have been applied to various software development tasks, but
the performance varies according to the scale of the models. Large Language
Models (LLMs) outperform Small Language Models (SLMs) in complex tasks like
repository-level issue resolving, but raise concerns about privacy and cost. In
contrast, SLMs are more accessible but under-perform in complex tasks. In this
paper, we introduce ReSAT (Repository Structure-Aware Training), construct
training data based on a large number of issues and corresponding pull requests
from open-source communities to enhance the model's understanding of repository
structure and issue resolving ability. We construct two types of training data:
(1) localization training data, a multi-level progressive localization data to
improve code understanding and localization capability; (2) code edit training
data, which improves context-based code editing capability. The evaluation
results on SWE-Bench-verified and RepoQA demonstrate that ReSAT effectively
enhances SLMs' issue-resolving and repository-level long-context understanding
capabilities.",2024-12-26T03:01:32Z,http://arxiv.org/abs/2412.19031v1,"Zexiong Ma, Shengnan An, Zeqi Lin, Yanzhen Zou, Bing Xie"
"Modality-Projection Universal Model for Comprehensive Full-Body Medical
  Imaging Segmentation","The integration of deep learning in medical imaging has shown great promise
for enhancing diagnostic, therapeutic, and research outcomes. However, applying
universal models across multiple modalities remains challenging due to the
inherent variability in data characteristics. This study aims to introduce and
evaluate a Modality Projection Universal Model (MPUM). MPUM employs a novel
modality-projection strategy, which allows the model to dynamically adjust its
parameters to optimize performance across different imaging modalities. The
MPUM demonstrated superior accuracy in identifying anatomical structures,
enabling precise quantification for improved clinical decision-making. It also
identifies metabolic associations within the brain-body axis, advancing
research on brain-body physiological correlations. Furthermore, MPUM's unique
controller-based convolution layer enables visualization of saliency maps
across all network layers, significantly enhancing the model's
interpretability.",2024-12-26T02:23:27Z,http://arxiv.org/abs/2412.19026v1,"Yixin Chen, Lin Gao, Yajuan Gao, Rui Wang, Jingge Lian, Xiangxi Meng, Yanhua Duan, Leiying Chai, Hongbin Han, Zhaoping Cheng, Zhaoheng Xie"
"Nonparametric Estimation of Matching Efficiency and Elasticity in a Spot
  Gig Work Platform: 2019-2023","This paper provides new evidence on spot gig work platforms for unemployed
workers searching for occupations with minimal educational or experience
requirements in Japan. Using proprietary data from a private online spot work
matching platform, Timee, it examines trends in key variables such as the
numbers of unemployed users, vacancies, hires, and labor market tightness. The
study compares these trends with part-time worker data from the public
employment platform, Hello Work. The private platform shows a significant
market expansion from December 2019 to December 2023. Applying a novel
nonparametric approach, the paper finds greater variability in efficiency and
higher elasticity, with elasticity with respect to the number of users
fluctuating from below 0.7 to above 1.5, and elasticity with respect to the
number of vacancies often exceeding 1.0, which is higher than Hello Work.
Lastly, the study highlights that Tokyo's labor market exhibits higher
efficiency compared to Osaka and Aichi, while elasticities are similar,
indicating less geographical heterogeneity of the spot work compared to Hello
Work.",2024-12-26T02:16:58Z,http://arxiv.org/abs/2412.19024v1,"Hayato Kanayama, Suguru Otani"
"Nuclear matter properties from chiral-scale effective theory including a
  dilatonic scalar meson","Chiral effective theory has become a powerful tool for studying the
low-energy properties of QCD. In this work, we apply an extended chiral
effective theory -- chiral-scale effective theory -- including a dilatonic
scalar meson to study nuclear matter and find that the properties around
saturation density can be well reproduced. Compared to the traditionally used
Walecka-type models in nuclear matter studies, our approach improves the
behavior of symmetry energy and the incompressibility coefficient in describing
empirical data without introducing additional freedoms. Moreover, the predicted
neutron star structures fall within the constraints of GW170817, PSR
J0740+6620, and PSR J0030+0451, while the maximum neutron star mass can reach
about $~3M_{\odot}$ with a pure hadronic phase. Additionally, we find that
symmetry patterns of the effective theory significantly impact neutron star
structures. %In chiral-scale effective theory, effective operators are well
organized by chiral-scale orders and freedoms induced by QCD symmetry patterns.
We believe that introducing this type of theory into nuclear matter studies can
lead to a deeper understanding of QCD, nuclear matter, and compact
astrophysical objects.",2024-12-26T02:13:05Z,http://arxiv.org/abs/2412.19023v1,"Lu-Qi Zhang, Yao Ma, Yong-Liang Ma"
Adaptivity can help exponentially for shadow tomography,"In recent years there has been significant interest in understanding the
statistical complexity of learning from quantum data under the constraint that
one can only make unentangled measurements. While a key challenge in
establishing tight lower bounds in this setting is to deal with the fact that
the measurements can be chosen in an adaptive fashion, a recurring theme has
been that adaptivity offers little advantage over more straightforward,
nonadaptive protocols.
  In this note, we offer a counterpoint to this. We show that for the basic
task of shadow tomography, protocols that use adaptively chosen two-copy
measurements can be exponentially more sample-efficient than any protocol that
uses nonadaptive two-copy measurements.",2024-12-26T02:13:04Z,http://arxiv.org/abs/2412.19022v1,"Sitan Chen, Weiyuan Gong, Zhihan Zhang"
"Travelling wave solutions of an equation of Harry Dym type arising in
  the Black-Scholes framework","The Black-Scholes framework is crucial in pricing a vast number of financial
instruments that permeate the complex dynamics of world markets. Associated
with this framework, we consider a second-order differential operator $L(x,
{\partial_x}) := v^2(x,t) (\partial_x^2 -\partial_x)$ that carries a variable
volatility term $v(x,t)$ and which is dependent on the underlying log-price $x$
and a time parameter $t$ motivated by the celebrated Dupire local volatility
model. In this context, we ask and answer the question of whether one can find
a non-linear evolution equation derived from a zero-curvature condition for a
time-dependent deformation of the operator $L$. The result is a variant of the
Harry Dym equation for which we can then find a family of travelling wave
solutions. This brings in extensive machinery from soliton theory and
integrable systems. As a by-product, it opens up the way to the use of coherent
structures in financial-market volatility studies.",2024-12-26T02:09:47Z,http://arxiv.org/abs/2412.19020v1,"Jorge P. Zubelli, Kuldeep Singh, Vinicius Albani, Ioannis Kourakis"
"Brain Ageing Prediction using Isolation Forest Technique and Residual
  Neural Network (ResNet)","Brain aging is a complex and dynamic process, leading to functional and
structural changes in the brain. These changes could lead to the increased risk
of neurodegenerative diseases and cognitive decline. Accurate brain-age
estimation utilizing neuroimaging data has become necessary for detecting
initial signs of neurodegeneration. Here, we propose a novel deep learning
approach using the Residual Neural Network 101 Version 2 (ResNet101V2) model to
predict brain age from MRI scans. To train, validate and test our proposed
model, we used a large dataset of 2102 images which were selected randomly from
the International Consortium for Brain Mapping (ICBM). Next, we applied data
preprocessing techniques, including normalizing the images and using outlier
detection via Isolation Forest method. Then, we evaluated various pre-trained
approaches (namely: MobileNetV2, ResNet50V2, ResNet101V2, Xception). The
results demonstrated that the ResNet101V2 model has higher performance compared
with the other models, attaining MAEs of 0.9136 and 0.8242 years for before and
after using Isolation Forest process. Our method achieved a high accuracy in
brain age estimation in ICBM dataset and it provides a reliable brain age
prediction.",2024-12-26T01:49:21Z,http://arxiv.org/abs/2412.19017v1,"Saadat Behzadi, Danial Sharifrazi, Roohallah Alizadehsani, Mojtaba Lotfaliany, Mohammadreza Mohebbi"
Stratified L-convex groups,"In this paper, we introduce a novel structure of stratified L-convex groups,
defined as groups equipped with a stratified L-convex space, ensuring that the
group operation is an L-convexity-preserving mapping. We prove that stratified
L-convex groups serve as objects, while L-convexity-preserving group
homomorphisms as morphisms, together forming a concrete category, denoted as
SLCG. As a specific instance of SLCG (i.e., when L=2), we define the category
of convex groups, denoted as CG. We demonstrate that SLCG possesses
well-defined characterizations, localization properties, as well as initial and
final structures, establishing it as a topological category over groups.
Additionally, we show that CG can be embedded within SLCG as a reflective
subcategory.",2024-12-26T01:33:29Z,http://arxiv.org/abs/2412.19014v1,"Lingqiang Li, Qiu Jin"
Dynamic networks clustering via mirror distance,"The classification of different patterns of network evolution, for example in
brain connectomes or social networks, is a key problem in network inference and
modern data science. Building on the notion of a network's Euclidean mirror,
which captures its evolution as a curve in Euclidean space, we develop the
Dynamic Network Clustering through Mirror Distance (DNCMD), an algorithm for
clustering dynamic networks based on a distance measure between their
associated mirrors. We provide theoretical guarantees for DNCMD to achieve
exact recovery of distinct evolutionary patterns for latent position random
networks both when underlying vertex features change deterministically and when
they follow a stochastic process. We validate our theoretical results through
numerical simulations and demonstrate the application of DNCMD to understand
edge functions in Drosophila larval connectome data, as well as to analyze
temporal patterns in dynamic trade networks.",2024-12-26T01:14:21Z,http://arxiv.org/abs/2412.19012v1,"Runbing Zheng, Avanti Athreya, Marta Zlatic, Michael Clayton, Carey E. Priebe"
"Developing a single-phase and nanograined refractory high-entropy alloy
  ZrHfNbTaW with ultrahigh hardness by phase transformation via high-pressure
  torsion","High-entropy alloys (HEAs) are potential candidates for applications as
refractory materials. While dual-phase refractory HEAs containing an ordered
phase exhibit high hardness, there is high interest in developing
intermetallic-free and single-phase refractory HEAs with high hardness. In this
study, a new equiatomic HEA ZrHfNbTaW with an ultrahigh hardness of 860 Hv is
developed. The alloy is first synthesized with a dual-phase structure via arc
melting and further homogenized to a single body-centered cubic (BCC) structure
by phase transformation via high-pressure torsion (HPT), using the concept of
ultra-severe plastic deformation process. The ultrahigh hardness of the alloy,
which is higher than those reported for refractory alloys and single-phase
HEAs, is attributed to (i) solution hardening by severe lattice distortion,
(ii) Hall-Petch grain boundary hardening by the formation of nanograins with 12
nm average size, and (iii) dislocation hardening confirmed by high-resolution
transmission electron microscopy.",2024-12-26T00:36:31Z,http://arxiv.org/abs/2412.19006v1,"Shivam Dangwal, Kaveh Edalati"
"Enhancing Audiovisual Speech Recognition through Bifocal Preference
  Optimization","Audiovisual Automatic Speech Recognition (AV-ASR) aims to improve speech
recognition accuracy by leveraging visual signals. It is particularly
challenging in unconstrained real-world scenarios across various domains due to
noisy acoustic environments, spontaneous speech, and the uncertain use of
visual information. Most previous works fine-tune audio-only ASR models on
audiovisual datasets, optimizing them for conventional ASR objectives. However,
they often neglect visual features and common errors in unconstrained video
scenarios. In this paper, we propose using a preference optimization strategy
to improve speech recognition accuracy for real-world videos. First, we create
preference data via simulating common errors that occurred in AV-ASR from two
focals: manipulating the audio or vision input and rewriting the output
transcript. Second, we propose BPO-AVASR, a Bifocal Preference Optimization
method to improve AV-ASR models by leveraging both input-side and output-side
preference. Extensive experiments demonstrate that our approach significantly
improves speech recognition accuracy across various domains, outperforming
previous state-of-the-art models on real-world video speech recognition.",2024-12-26T00:26:45Z,http://arxiv.org/abs/2412.19005v1,"Yihan Wu, Yichen Lu, Yifan Peng, Xihua Wang, Ruihua Song, Shinji Watanabe"
Robust functional PCA for density data,"This paper introduces a robust approach to functional principal component
analysis (FPCA) for compositional data, particularly density functions. While
recent papers have studied density data within the Bayes space framework, there
has been limited focus on developing robust methods to effectively handle
anomalous observations and large noise. To address this, we extend the
Mahalanobis distance concept to Bayes spaces, proposing its regularized version
that accounts for the constraints inherent in density data. Based on this
extension, we introduce a new method, robust density principal component
analysis (RDPCA), for more accurate estimation of functional principal
components in the presence of outliers. The method's performance is validated
through simulations and real-world applications, showing its ability to improve
covariance estimation and principal component analysis compared to traditional
methods.",2024-12-26T00:06:47Z,http://arxiv.org/abs/2412.19004v1,"Jeremy Oguamalam, Peter Filzmoser, Karel Hron, Alessandra Menafoglio, Una Radojičić"
"Tempus Core: Area-Power Efficient Temporal-Unary Convolution Core for
  Low-Precision Edge DLAs","The increasing complexity of deep neural networks (DNNs) poses significant
challenges for edge inference deployment due to resource and power constraints
of edge devices. Recent works on unary-based matrix multiplication hardware aim
to leverage data sparsity and low-precision values to enhance hardware
efficiency. However, the adoption and integration of such unary hardware into
commercial deep learning accelerators (DLA) remain limited due to processing
element (PE) array dataflow differences. This work presents Tempus Core, a
convolution core with highly scalable unary-based PE array comprising of tub
(temporal-unary-binary) multipliers that seamlessly integrates with the NVDLA
(NVIDIA's open-source DLA for accelerating CNNs) while maintaining dataflow
compliance and boosting hardware efficiency. Analysis across various datapath
granularities shows that for INT8 precision in 45nm CMOS, Tempus Core's PE cell
unit (PCU) yields 59.3% and 15.3% reductions in area and power consumption,
respectively, over NVDLA's CMAC unit. Considering a 16x16 PE array in Tempus
Core, area and power improves by 75% and 62%, respectively, while delivering 5x
and 4x iso-area throughput improvements for INT8 and INT4 precisions.
Post-place and route analysis of Tempus Core's PCU shows that the 16x4 PE array
for INT4 precision in 45nm CMOS requires only 0.017 mm^2 die area and consumes
only 6.2mW of total power. We demonstrate that area-power efficient unary-based
hardware can be seamlessly integrated into conventional DLAs, paving the path
for efficient unary hardware for edge AI inference.",2024-12-25T23:20:02Z,http://arxiv.org/abs/2412.19002v1,"Prabhu Vellaisamy, Harideep Nair, Thomas Kang, Yichen Ni, Haoyang Fan, Bin Qi, Jeff Chen, Shawn Blanton, John Paul Shen"
"Impact of resummation on the production and experimental bounds of
  scalar high-electric-charge objects","A one-loop Dyson-Schwinger-like resummation scheme is applied to scalar
High-Electric-Charge compact Objects (HECOs), extending previous work on
spin-1/2 case. The electromagnetic interactions of HECOs are considered within
the framework of strongly coupled scalar Quantun Electrodynamics. The
resummation amounts to determining non-trivial ultraviolet (UV) fixed points,
at which the effective Lagrangian, which will lead to the pertinent predictions
on the cross sections, is computed. In contrast to the fermionic HECO case, in
which the fixed point structure was determined solely by the interactions of
the HECOs with the photon field, in the scalar case the existence of
non-trivial UV fixed points requires the presence of additional strong self
interactions among the HECOs. Our resummation scheme, which is notably
different from a lattice strong-coupling approach, makes the computation of the
pertinent scalar-HECO-production cross sections reliable, thus allowing
revisiting the mass bounds obtained from searches for such objects in current
or future colliders. Our MadGraph implementation of the results leads to
enhanced (up to ~30%) lower bounds on the mass of scalar HECOs, as compared to
those extracted from the tree-level processes typically used in LHC collider
searches by ATLAS and MoEDAL experiments.",2024-12-25T23:13:36Z,http://arxiv.org/abs/2412.19001v1,"Jean Alexandre, Nick E. Mavromatos, Vasiliki A. Mitsou, Emanuela Musumeci"
"MGAN-CRCM: A Novel Multiple Generative Adversarial Network and
  Coarse-Refinement Based Cognizant Method for Image Inpainting","Image inpainting is a widely used technique in computer vision for
reconstructing missing or damaged pixels in images. Recent advancements with
Generative Adversarial Networks (GANs) have demonstrated superior performance
over traditional methods due to their deep learning capabilities and
adaptability across diverse image domains. Residual Networks (ResNet) have also
gained prominence for their ability to enhance feature representation and
compatibility with other architectures. This paper introduces a novel
architecture combining GAN and ResNet models to improve image inpainting
outcomes. Our framework integrates three components: Transpose
Convolution-based GAN for guided and blind inpainting, Fast
ResNet-Convolutional Neural Network (FR-CNN) for object removal, and
Co-Modulation GAN (Co-Mod GAN) for refinement. The model's performance was
evaluated on benchmark datasets, achieving accuracies of 96.59% on Image-Net,
96.70% on Places2, and 96.16% on CelebA. Comparative analyses demonstrate that
the proposed architecture outperforms existing methods, highlighting its
effectiveness in both qualitative and quantitative evaluations.",2024-12-25T22:54:28Z,http://arxiv.org/abs/2412.19000v1,"Nafiz Al Asad, Md. Appel Mahmud Pranto, Shbiruzzaman Shiam, Musaddeq Mahmud Akand, Mohammad Abu Yousuf, Khondokar Fida Hasan, Mohammad Ali Moni"
Soliton foam formation in the early Universe,"The formation of composite solitons produced by scalar fields without thermal
phase transitions in the early Universe is considered. We present numerical
simulations of the formation and evolution of soliton structures at the
post-inflationary stage. The realistic initial conditions are obtained through
the simulation of multiple quantum fluctuations during the inflation epoch. The
initial field distributions allow to form local soliton clusters in the early
Universe without the need for the thermal production of a soliton network
throughout the Universe. We find that in three-dimensional space, the
nontrivial composite field structures are formed in the form of &lt;&lt;soliton
foam&gt;&gt;, consisting of closed domain walls, domain walls bounded by cosmic
strings, and scalar field radiation. The possible cosmological implications of
the soliton foam are discussed.",2024-12-25T22:35:56Z,http://arxiv.org/abs/2412.18997v1,"A. A. Kirillov, B. S. Murygin, V. V. Nikulin"
"MiTREE: Multi-input Transformer Ecoregion Encoder for Species
  Distribution Modelling","Climate change poses an extreme threat to biodiversity, making it imperative
to efficiently model the geographical range of different species. The
availability of large-scale remote sensing images and environmental data has
facilitated the use of machine learning in Species Distribution Models (SDMs),
which aim to predict the presence of a species at any given location.
Traditional SDMs, reliant on expert observation, are labor-intensive, but
advancements in remote sensing and citizen science data have facilitated
machine learning approaches to SDM development. However, these models often
struggle with leveraging spatial relationships between different inputs -- for
instance, learning how climate data should inform the data present in satellite
imagery -- without upsampling or distorting the original inputs. Additionally,
location information and ecological characteristics at a location play a
crucial role in predicting species distribution models, but these aspects have
not yet been incorporated into state-of-the-art approaches. In this work, we
introduce MiTREE: a multi-input Vision-Transformer-based model with an
ecoregion encoder. MiTREE computes spatial cross-modal relationships without
upsampling as well as integrates location and ecological context. We evaluate
our model on the SatBird Summer and Winter datasets, the goal of which is to
predict bird species encounter rates, and we find that our approach improves
upon state-of-the-art baselines.",2024-12-25T22:20:47Z,http://arxiv.org/abs/2412.18995v1,"Theresa Chen, Yao-Yi Chiang"
"Geospatial Data Fusion: Combining Lidar, SAR, and Optical Imagery with
  AI for Enhanced Urban Mapping","This study explores the integration of Lidar, Synthetic Aperture Radar (SAR),
and optical imagery through advanced artificial intelligence techniques for
enhanced urban mapping. By fusing these diverse geospatial datasets, we aim to
overcome the limitations associated with single-sensor data, achieving a more
comprehensive representation of urban environments. The research employs Fully
Convolutional Networks (FCNs) as the primary deep learning model for urban
feature extraction, enabling precise pixel-wise classification of essential
urban elements, including buildings, roads, and vegetation. To optimize the
performance of the FCN model, we utilize Particle Swarm Optimization (PSO) for
hyperparameter tuning, significantly enhancing model accuracy. Key findings
indicate that the FCN-PSO model achieved a pixel accuracy of 92.3% and a mean
Intersection over Union (IoU) of 87.6%, surpassing traditional single-sensor
approaches. These results underscore the potential of fused geospatial data and
AI-driven methodologies in urban mapping, providing valuable insights for urban
planning and management. The implications of this research pave the way for
future developments in real-time mapping and adaptive urban infrastructure
planning.",2024-12-25T22:17:31Z,http://arxiv.org/abs/2412.18994v1,"Sajjad Afroosheh, Mohammadreza Askari"
"On the architecture of the Symplectic $(A_\infty,2)$-Category","This note relates to the author's construction of the Symplectic
$(A_\infty,2)$-Category, $\mathsf{Symp}$. Here we explain two ways of encoding
the information in $\mathsf{Symp}$, one topological, one algebraic. The
topological encoding is as an $(A_\infty,2)$-flow category, which we define
here. The algebraic encoding is as a linear $(A_\infty,2)$-category, which we
extract from the topological encoding. In upcoming work, the author and
Wehrheim plan to use the adiabatic Fredholm theory recently developed by
Bottman-Wehrheim to construct $\mathsf{Symp}$ as an $(A_\infty,2)$-flow
category.
  The definition of linear $(A_\infty,2)$-category that we give in this note is
different than the one proposed by Bottman-Carmeli. The recursive structure of
the 2-associahedra identifies faces with fiber products of 2-associahedra over
associahedra, and these fiber products led Bottman-Carmeli to associate
operations to singular chains on 2-associahedra. The innovation in our new
definition of linear $(A_\infty,2)$-category is to extend the family of
2-associahedra to include all fiber products of 2-associahedra over
associahedra. This allows us to associate operations to cellular chains, which
in particular enables us to produce a definition that involves only one
operation in each arity, governed by a collection of $(A_\infty,2)$-equations.",2024-12-25T22:11:18Z,http://arxiv.org/abs/2412.18993v1,Nathaniel Bottman
"Optimal Federated Learning for Functional Mean Estimation under
  Heterogeneous Privacy Constraints","Federated learning (FL) is a distributed machine learning technique designed
to preserve data privacy and security, and it has gained significant importance
due to its broad range of applications. This paper addresses the problem of
optimal functional mean estimation from discretely sampled data in a federated
setting.
  We consider a heterogeneous framework where the number of individuals,
measurements per individual, and privacy parameters vary across one or more
servers, under both common and independent design settings. In the common
design setting, the same design points are measured for each individual,
whereas in the independent design, each individual has their own random
collection of design points. Within this framework, we establish minimax upper
and lower bounds for the estimation error of the underlying mean function,
highlighting the nuanced differences between common and independent designs
under distributed privacy constraints.
  We propose algorithms that achieve the optimal trade-off between privacy and
accuracy and provide optimality results that quantify the fundamental limits of
private functional mean estimation across diverse distributed settings. These
results characterize the cost of privacy and offer practical insights into the
potential for privacy-preserving statistical analysis in federated
environments.",2024-12-25T22:06:12Z,http://arxiv.org/abs/2412.18992v1,"Tony Cai, Abhinav Chakraborty, Lasse Vuursteen"
"MTCAE-DFER: Multi-Task Cascaded Autoencoder for Dynamic Facial
  Expression Recognition","This paper expands the cascaded network branch of the autoencoder-based
multi-task learning (MTL) framework for dynamic facial expression recognition,
namely Multi-Task Cascaded Autoencoder for Dynamic Facial Expression
Recognition (MTCAE-DFER). MTCAE-DFER builds a plug-and-play cascaded decoder
module, which is based on the Vision Transformer (ViT) architecture and employs
the decoder concept of Transformer to reconstruct the multi-head attention
module. The decoder output from the previous task serves as the query (Q),
representing local dynamic features, while the Video Masked Autoencoder
(VideoMAE) shared encoder output acts as both the key (K) and value (V),
representing global dynamic features. This setup facilitates interaction
between global and local dynamic features across related tasks. Additionally,
this proposal aims to alleviate overfitting of complex large model. We utilize
autoencoder-based multi-task cascaded learning approach to explore the impact
of dynamic face detection and dynamic face landmark on dynamic facial
expression recognition, which enhances the model's generalization ability.
After we conduct extensive ablation experiments and comparison with
state-of-the-art (SOTA) methods on various public datasets for dynamic facial
expression recognition, the robustness of the MTCAE-DFER model and the
effectiveness of global-local dynamic feature interaction among related tasks
have been proven.",2024-12-25T21:52:31Z,http://arxiv.org/abs/2412.18988v1,"Peihao Xiang, Kaida Wu, Chaohao Lin, Ou Bai"
TravelAgent: Generative Agents in the Built Environment,"Understanding human behavior in built environments is critical for designing
functional, user centered urban spaces. Traditional approaches, such as manual
observations, surveys, and simplified simulations, often fail to capture the
complexity and dynamics of real world behavior. To address these limitations,
we introduce TravelAgent, a novel simulation platform that models pedestrian
navigation and activity patterns across diverse indoor and outdoor environments
under varying contextual and environmental conditions. TravelAgent leverages
generative agents integrated into 3D virtual environments, enabling agents to
process multimodal sensory inputs and exhibit human-like decision-making,
behavior, and adaptation. Through experiments, including navigation,
wayfinding, and free exploration, we analyze data from 100 simulations
comprising 1898 agent steps across diverse spatial layouts and agent
archetypes, achieving an overall task completion rate of 76%. Using spatial,
linguistic, and sentiment analyses, we show how agents perceive, adapt to, or
struggle with their surroundings and assigned tasks. Our findings highlight the
potential of TravelAgent as a tool for urban design, spatial cognition
research, and agent-based modeling. We discuss key challenges and opportunities
in deploying generative agents for the evaluation and refinement of spatial
designs, proposing TravelAgent as a new paradigm for simulating and
understanding human experiences in built environments.",2024-12-25T21:27:51Z,http://arxiv.org/abs/2412.18985v1,"Ariel Noyman, Kai Hu, Kent Larson"
Positivity of Schubert Coefficients,"Schubert coefficients $c_{u,v}^w$ are structure constants describing
multiplication of Schubert polynomials. Deciding positivity of Schubert
coefficients is a major open problem in Algebraic Combinatorics. We prove a
positive rule for this problem based on two standard assumptions.",2024-12-25T21:23:06Z,http://arxiv.org/abs/2412.18984v1,"Igor Pak, Colleen Robichaux"
"Deep Learning-Based Traffic-Aware Base Station Sleep Mode and Cell
  Zooming Strategy in RIS-Aided Multi-Cell Networks","Advances in wireless technology have significantly increased the number of
wireless connections, leading to higher energy consumption in networks. Among
these, base stations (BSs) in radio access networks (RANs) account for over
half of the total energy usage. To address this, we propose a multi-cell sleep
strategy combined with adaptive cell zooming, user association, and
reconfigurable intelligent surface (RIS) to minimize BS energy consumption.
This approach allows BSs to enter sleep during low traffic, while adaptive cell
zooming and user association dynamically adjust coverage to balance traffic
load and enhance data rates through RIS, minimizing the number of active BSs.
However, it is important to note that the proposed method may achieve
energy-savings at the cost of increased delay, requiring a trade-off between
these two factors. Moreover, minimizing BS energy consumption under the delay
constraint is a complicated non-convex problem. To address this issue, we model
the RIS-aided multi-cell network as a Markov decision process (MDP) and use the
proximal policy optimization (PPO) algorithm to optimize sleep mode (SM), cell
zooming, and user association. Besides, we utilize a double cascade correlation
network (DCCN) algorithm to optimize the RIS reflection coefficients.
Simulation results demonstrate that PPO balances energy-savings and delay,
while DCCN-optimized RIS enhances BS energy-savings. Compared to systems
optimised by the benchmark DQN algorithm, energy consumption is reduced by
49.61%",2024-12-25T21:06:40Z,http://arxiv.org/abs/2412.18983v1,"Shuo Sun, Chong Huang, Gaojie Chen, Pei Xiao, Rahim Tafazolli"
"Evaluating deep learning models for fault diagnosis of a rotating
  machinery with epistemic and aleatoric uncertainty","Uncertainty-aware deep learning (DL) models recently gained attention in
fault diagnosis as a way to promote the reliable detection of faults when
out-of-distribution (OOD) data arise from unseen faults (epistemic uncertainty)
or the presence of noise (aleatoric uncertainty). In this paper, we present the
first comprehensive comparative study of state-of-the-art uncertainty-aware DL
architectures for fault diagnosis in rotating machinery, where different
scenarios affected by epistemic uncertainty and different types of aleatoric
uncertainty are investigated. The selected architectures include sampling by
dropout, Bayesian neural networks, and deep ensembles. Moreover, to distinguish
between in-distribution and OOD data in the different scenarios two uncertainty
thresholds, one of which is introduced in this paper, are alternatively
applied. Our empirical findings offer guidance to practitioners and researchers
who have to deploy real-world uncertainty-aware fault diagnosis systems. In
particular, they reveal that, in the presence of epistemic uncertainty, all DL
models are capable of effectively detecting, on average, a substantial portion
of OOD data across all the scenarios. However, deep ensemble models show
superior performance, independently of the uncertainty threshold used for
discrimination. In the presence of aleatoric uncertainty, the noise level plays
an important role. Specifically, low noise levels hinder the models' ability to
effectively detect OOD data. Even in this case, however, deep ensemble models
exhibit a milder degradation in performance, dominating the others. These
achievements, combined with their shorter inference time, make deep ensemble
architectures the preferred choice.",2024-12-25T20:22:59Z,http://arxiv.org/abs/2412.18980v1,"Reza Jalayer, Masoud Jalayer, Andrea Mor, Carlotta Orsenigo, Carlo Vercellis"
Enhanced Elastocaloric Effects in γ-graphyne,"The global emphasis on sustainable technologies has become a paramount
concern for nations worldwide. Specifically, numerous sustainable methods are
being explored as promising alternatives to the well-established
vapor-compression technologies in cooling and heating devices. One such avenue
gaining traction within the scientific community is the elastocaloric effect
(eC). This phenomenon holds promise for efficient cooling and heating processes
without causing environmental harm. Studies carried out at the nanoscale have
demonstrated the efficiency of the eC, proving to be comparable to that of
state-of-the-art macroscopic systems. In this study, we used classical
molecular dynamics simulations to investigate the elastocaloric effect for
{\gamma}-graphyne. Our analysis goes beyond obtaining changes in eC temperature
and the coefficient of performance (COP) for two species of {\gamma}-graphyne
nanoribbons (armchair and zigzag). We also explore their dependence on various
conditions, including whether they are on deposited on a substrate or
pre-strained. Our findings reveal a substantial enhancement in the
elastocaloric effect for {\gamma}-graphyne nanoribbons when subjected to
pre-strain, amplifying it by at least one order of magnitude. Under certain
conditions, the change in the eC temperature and the COP of the structures
reach expressive values as high as 224 K and 14, respectively. We discuss the
implications of these results by examining the shape and behavior of the
carbon-carbon bond lengths within the structures.",2024-12-25T20:12:14Z,http://arxiv.org/abs/2412.18978v1,"Guilherme B. Kanegae, Marcelo L. Pereira Junior, Douglas S. Galvão, Luiz A. Ribeiro Junior, Alexandre F. Fonseca"
Injecting Bias into Text Classification Models using Backdoor Attacks,"The rapid growth of natural language processing (NLP) and pre-trained
language models have enabled accurate text classification in a variety of
settings. However, text classification models are susceptible to backdoor
attacks, where an attacker embeds a trigger into the victim model to make the
model predict attacker-desired labels in targeted scenarios. In this paper, we
propose to utilize backdoor attacks for a new purpose: bias injection. We
develop a backdoor attack in which a subset of the training dataset is poisoned
to associate strong male actors with negative sentiment. We execute our attack
on two popular text classification datasets (IMDb and SST) and seven different
models ranging from traditional Doc2Vec-based models to LSTM networks and
modern transformer-based BERT and RoBERTa models. Our results show that the
reduction in backdoored models' benign classification accuracy is limited,
implying that our attacks remain stealthy, whereas the models successfully
learn to associate strong male actors with negative sentiment (100% attack
success rate with &gt;= 3% poison rate). Attacks on BERT and RoBERTa are
particularly more stealthy and effective, demonstrating an increased risk of
using modern and larger models. We also measure the generalizability of our
bias injection by proposing two metrics: (i) U-BBSR which uses previously
unseen words when measuring attack success, and (ii) P-BBSR which measures
attack success using paraphrased test samples. U-BBSR and P-BBSR results show
that the bias injected by our attack can go beyond memorizing a trigger phrase.",2024-12-25T19:32:02Z,http://arxiv.org/abs/2412.18975v1,"A. Dilara Yavuz, M. Emre Gursoy"
"Adopting Trustworthy AI for Sleep Disorder Prediction: Deep Time Series
  Analysis with Temporal Attention Mechanism and Counterfactual Explanations","Sleep disorders have a major impact on both lifestyle and health. Effective
sleep disorder prediction from lifestyle and physiological data can provide
essential details for early intervention. This research utilizes three deep
time series models and facilitates them with explainability approaches for
sleep disorder prediction. Specifically, our approach adopts Temporal
Convolutional Networks (TCN), Long Short-Term Memory (LSTM) for time series
data analysis, and Temporal Fusion Transformer model (TFT). Meanwhile, the
temporal attention mechanism and counterfactual explanation with SHapley
Additive exPlanations (SHAP) approach are employed to ensure dependable,
accurate, and interpretable predictions. Finally, using a large dataset of
sleep health measures, our evaluation demonstrates the effect of our method in
predicting sleep disorders.",2024-12-25T19:19:45Z,http://arxiv.org/abs/2412.18971v1,"Pegah Ahadian, Wei Xu, Sherry Wang, Qiang Guan"
Flat Bands and Compact Localised States: A Carrollian roadmap,"We show how Carrollian symmetries become important in the construction of
one-dimensional fermionic systems with all flat-band spectra from first
principles. The key ingredient of this construction is the identification of
Compact Localised States (CLSs), which appear naturally by demanding
$\textit{supertranslation}$ invariance of the system. We use CLS basis states,
with inherent $\textit{ultra-local}$ correlations, to write down an interacting
theory which shows a non-trivial phase structure and an emergent Carroll
conformal symmetry at the gapless points. We analyze this theory in detail for
both zero and finite chemical potential.",2024-12-25T18:54:45Z,http://arxiv.org/abs/2412.18965v1,"Nisa Ara, Aritra Banerjee, Rudranil Basu, Bhagya Krishnan"
"Don't Lose Yourself: Boosting Multimodal Recommendation via Reducing
  Node-neighbor Discrepancy in Graph Convolutional Network","The rapid expansion of multimedia contents has led to the emergence of
multimodal recommendation systems. It has attracted increasing attention in
recommendation systems because its full utilization of data from different
modalities alleviates the persistent data sparsity problem. As such, multimodal
recommendation models can learn personalized information about nodes in terms
of visual and textual. To further alleviate the data sparsity problem, some
previous works have introduced graph convolutional networks (GCNs) for
multimodal recommendation systems, to enhance the semantic representation of
users and items by capturing the potential relationships between them. However,
adopting GCNs inevitably introduces the over-smoothing problem, which make
nodes to be too similar. Unfortunately, incorporating multimodal information
will exacerbate this challenge because nodes that are too similar will lose the
personalized information learned through multimodal information. To address
this problem, we propose a novel model that retains the personalized
information of ego nodes during feature aggregation by Reducing Node-neighbor
Discrepancy (RedN^nD). Extensive experiments on three public datasets show that
RedN^nD achieves state-of-the-art performance on accuracy and robustness, with
significant improvements over existing GCN-based multimodal frameworks.",2024-12-25T18:41:36Z,http://arxiv.org/abs/2412.18962v1,"Zheyu Chen, Jinfeng Xu, Haibo Hu"
"RIS-Assisted Aerial Non-Terrestrial Networks: An Intelligent Synergy
  with Deep Reinforcement Learning","Reconfigurable intelligent surface (RIS)-assisted aerial non-terrestrial
networks (NTNs) offer a promising paradigm for enhancing wireless
communications in the era of 6G and beyond. By integrating RIS with aerial
platforms such as unmanned aerial vehicles (UAVs) and high-altitude platforms
(HAPs), these networks can intelligently control signal propagation, extending
coverage, improving capacity, and enhancing link reliability. This article
explores the application of deep reinforcement learning (DRL) as a powerful
tool for optimizing RIS-assisted aerial NTNs. We focus on hybrid proximal
policy optimization (H-PPO), a robust DRL algorithm well-suited for handling
the complex, hybrid action spaces inherent in these networks. Through a case
study of an aerial RIS (ARIS)-aided coordinated multi-point non-orthogonal
multiple access (CoMP-NOMA) network, we demonstrate how H-PPO can effectively
optimize the system and maximize the sum rate while adhering to system
constraints. Finally, we discuss key challenges and promising research
directions for DRL-powered RIS-assisted aerial NTNs, highlighting their
potential to transform next-generation wireless networks.",2024-12-25T18:11:34Z,http://arxiv.org/abs/2412.18957v1,"Muhammad Umer, Muhammad Ahmed Mohsin, Aryan Kaushik, Qurrat-ul-Ain Nadeem, Ali Arshad Nasir, Syed Ali Hassan"
"Mixed Fourier norm spaces of analytic functions on the upper half-plane
  and Toeplitz operators","We introduce and study weighted spaces of functions with mixed norm on the
upper half-plane, defined in terms of Fourier transform. We give a
characterization of analytic functions within these spaces, and in particular,
we provide an analog of the Paley-Wiener theorem in this setting. As an
application, we consider Toeplitz operators with vertical symbols in these new
spaces.",2024-12-25T17:45:45Z,http://arxiv.org/abs/2412.18954v1,"Zhirayr Avetisyan, Alexey Karapetyants, Irina Smirnova"
"Bridging Interpretability and Robustness Using LIME-Guided Model
  Refinement","This paper explores the intricate relationship between interpretability and
robustness in deep learning models. Despite their remarkable performance across
various tasks, deep learning models often exhibit critical vulnerabilities,
including susceptibility to adversarial attacks, over-reliance on spurious
correlations, and a lack of transparency in their decision-making processes. To
address these limitations, we propose a novel framework that leverages Local
Interpretable Model-Agnostic Explanations (LIME) to systematically enhance
model robustness. By identifying and mitigating the influence of irrelevant or
misleading features, our approach iteratively refines the model, penalizing
reliance on these features during training. Empirical evaluations on multiple
benchmark datasets demonstrate that LIME-guided refinement not only improves
interpretability but also significantly enhances resistance to adversarial
perturbations and generalization to out-of-distribution data.",2024-12-25T17:32:45Z,http://arxiv.org/abs/2412.18952v1,"Navid Nayyem, Abdullah Rakin, Longwei Wang"
"TopoBDA: Towards Bezier Deformable Attention for Road Topology
  Understanding","Understanding road topology is crucial for autonomous driving. This paper
introduces TopoBDA (Topology with Bezier Deformable Attention), a novel
approach that enhances road topology understanding by leveraging Bezier
Deformable Attention (BDA). BDA utilizes Bezier control points to drive the
deformable attention mechanism, significantly improving the detection and
representation of elongated and thin polyline structures, such as lane
centerlines. TopoBDA processes multi-camera 360-degree imagery to generate
Bird's Eye View (BEV) features, which are refined through a transformer decoder
employing BDA. This method enhances computational efficiency while maintaining
high accuracy in centerline prediction. Additionally, TopoBDA incorporates an
instance mask formulation and an auxiliary one-to-many set prediction loss
strategy to further refine centerline detection and improve road topology
understanding. Experimental evaluations on the OpenLane-V2 dataset demonstrate
that TopoBDA outperforms existing methods, achieving state-of-the-art results
in centerline detection and topology reasoning. The integration of multi-modal
data, including lidar and radar, specifically for road topology understanding,
further enhances the model's performance, underscoring its importance in
autonomous driving applications.",2024-12-25T17:31:54Z,http://arxiv.org/abs/2412.18951v1,"Muhammet Esat Kalfaoglu, Halil Ibrahim Ozturk, Ozsel Kilinc, Alptekin Temizel"
A Power-Efficient Hardware Implementation of L-Mul,"Multiplication is a core operation in modern neural network (NN)
computations, contributing significantly to energy consumption. The
linear-complexity multiplication (L-Mul) algorithm is specifically proposed as
an approximate multiplication method for emerging NN models, such as large
language model (LLM), to reduce the energy consumption and computational
complexity of multiplications. However, hardware implementation designs for
L-Mul have not yet been reported. Additionally, 8-bit floating-point (FP8), as
an emerging data format, offers a better dynamic range compared to traditional
8-bit integer (INT8), making it increasingly popular and widely adopted in NN
computations. This paper thus presents a power-efficient FPGAbased hardware
implementation (approximate FP8 multiplier) for L-Mul. The core computation is
implemented using the dynamic reconfigurable lookup tables and carry chains
primitives available in AMD Xilinx UltraScale/UltraScale+ technology. The
accuracy and resource utilization of the approximate multiplier are evaluated
and analyzed. Furthermore, the FP8 approximate multiplier is deployed in the
inference phase of representative NN models to validate its effectiveness.",2024-12-25T17:05:00Z,http://arxiv.org/abs/2412.18948v1,"Ruiqi Chen, Yangxintong Lyu, Han Bao, Bruno da Silva"
"Constraint-Adaptive Policy Switching for Offline Safe Reinforcement
  Learning","Offline safe reinforcement learning (OSRL) involves learning a
decision-making policy to maximize rewards from a fixed batch of training data
to satisfy pre-defined safety constraints. However, adapting to varying safety
constraints during deployment without retraining remains an under-explored
challenge. To address this challenge, we introduce constraint-adaptive policy
switching (CAPS), a wrapper framework around existing offline RL algorithms.
During training, CAPS uses offline data to learn multiple policies with a
shared representation that optimize different reward and cost trade-offs.
During testing, CAPS switches between those policies by selecting at each state
the policy that maximizes future rewards among those that satisfy the current
cost constraint. Our experiments on 38 tasks from the DSRL benchmark
demonstrate that CAPS consistently outperforms existing methods, establishing a
strong wrapper-based baseline for OSRL. The code is publicly available at
https://github.com/yassineCh/CAPS.",2024-12-25T16:42:27Z,http://arxiv.org/abs/2412.18946v1,"Yassine Chemingui, Aryan Deshwal, Honghao Wei, Alan Fern, Janardhan Rao Doppa"
Amuse: Human-AI Collaborative Songwriting with Multimodal Inspirations,"Songwriting is often driven by multimodal inspirations, such as imagery,
narratives, or existing music, yet songwriters remain unsupported by current
music AI systems in incorporating these multimodal inputs into their creative
processes. We introduce Amuse, a songwriting assistant that transforms
multimodal (image, text, or audio) inputs into chord progressions that can be
seamlessly incorporated into songwriters' creative processes. A key feature of
Amuse is its novel method for generating coherent chords that are relevant to
music keywords in the absence of datasets with paired examples of multimodal
inputs and chords. Specifically, we propose a method that leverages multimodal
large language models (LLMs) to convert multimodal inputs into noisy chord
suggestions and uses a unimodal chord model to filter the suggestions. A user
study with songwriters shows that Amuse effectively supports transforming
multimodal ideas into coherent musical suggestions, enhancing users' agency and
creativity throughout the songwriting process.",2024-12-25T16:23:32Z,http://arxiv.org/abs/2412.18940v1,"Yewon Kim, Sung-Ju Lee, Chris Donahue"
"Malware Classification using a Hybrid Hidden Markov Model-Convolutional
  Neural Network","The proliferation of malware variants poses a significant challenges to
traditional malware detection approaches, such as signature-based methods,
necessitating the development of advanced machine learning techniques. In this
research, we present a novel approach based on a hybrid architecture combining
features extracted using a Hidden Markov Model (HMM), with a Convolutional
Neural Network (CNN) then used for malware classification. Inspired by the
strong results in previous work using an HMM-Random Forest model, we propose
integrating HMMs, which serve to capture sequential patterns in opcode
sequences, with CNNs, which are adept at extracting hierarchical features. We
demonstrate the effectiveness of our approach on the popular Malicia dataset,
and we obtain superior performance, as compared to other machine learning
methods -- our results surpass the aforementioned HMM-Random Forest model. Our
findings underscore the potential of hybrid HMM-CNN architectures in bolstering
malware classification capabilities, offering several promising avenues for
further research in the field of cybersecurity.",2024-12-25T15:34:57Z,http://arxiv.org/abs/2412.18932v1,"Ritik Mehta, Olha Jureckova, Mark Stamp"
"Graph Cut-guided Maximal Coding Rate Reduction for Learning Image
  Embedding and Clustering","In the era of pre-trained models, image clustering task is usually addressed
by two relevant stages: a) to produce features from pre-trained vision models;
and b) to find clusters from the pre-trained features. However, these two
stages are often considered separately or learned by different paradigms,
leading to suboptimal clustering performance. In this paper, we propose a
unified framework, termed graph Cut-guided Maximal Coding Rate Reduction
(CgMCR$^2$), for jointly learning the structured embeddings and the clustering.
To be specific, we attempt to integrate an efficient clustering module into the
principled framework for learning structured representation, in which the
clustering module is used to provide partition information to guide the
cluster-wise compression and the learned embeddings is aligned to desired
geometric structures in turn to help for yielding more accurate partitions. We
conduct extensive experiments on both standard and out-of-domain image datasets
and experimental results validate the effectiveness of our approach.",2024-12-25T15:20:54Z,http://arxiv.org/abs/2412.18930v1,"W. He, Z. Huang, X. Meng, X. Qi, R. Xiao, C. -G. Li"
"UNIC-Adapter: Unified Image-instruction Adapter with Multi-modal
  Transformer for Image Generation","Recently, text-to-image generation models have achieved remarkable
advancements, particularly with diffusion models facilitating high-quality
image synthesis from textual descriptions. However, these models often struggle
with achieving precise control over pixel-level layouts, object appearances,
and global styles when using text prompts alone. To mitigate this issue,
previous works introduce conditional images as auxiliary inputs for image
generation, enhancing control but typically necessitating specialized models
tailored to different types of reference inputs. In this paper, we explore a
new approach to unify controllable generation within a single framework.
Specifically, we propose the unified image-instruction adapter (UNIC-Adapter)
built on the Multi-Modal-Diffusion Transformer architecture, to enable flexible
and controllable generation across diverse conditions without the need for
multiple specialized models. Our UNIC-Adapter effectively extracts multi-modal
instruction information by incorporating both conditional images and task
instructions, injecting this information into the image generation process
through a cross-attention mechanism enhanced by Rotary Position Embedding.
Experimental results across a variety of tasks, including pixel-level spatial
control, subject-driven image generation, and style-image-based image
synthesis, demonstrate the effectiveness of our UNIC-Adapter in unified
controllable image generation.",2024-12-25T15:19:02Z,http://arxiv.org/abs/2412.18928v1,"Lunhao Duan, Shanshan Zhao, Wenjun Yan, Yinglun Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Mingming Gong, Gui-Song Xia"
"Effective Lagrangian for strong and electromagnetic interactions of
  high-spin resonances","Recent experiments of photon-nucleon and meson-nucleon scatterings have
accumulated a lot of data for various meson production processes. One of the
purposes of those experiments is to search for the missing resonances which are
not discovered until now but whose existence was predicted by hadron models.
The analyses of the data requires the development of dynamical coupled-channel
models. Since several missing resonances are expected to have spin higher than
3/2, we need to include higher-spin resonances in dynamical coupled-channel
models, which enable us to determine the couplings of effective Lagrangians of
higher-spin baryons with pseudoscalar mesons or vector mesons. However, hadron
models, such as quark models, give predictions only of the decay amplitudes of
such baryons. Here we demonstrate the formalism of high-spin resonances and
construct the relation between the coupling constants of effective Lagrangians
and the partial decay widths that can be predicted by hadron models. This
allows us to compare the coupling constants to the hadron model predictions not
only in magnitude but in sign as well.",2024-12-25T15:17:08Z,http://arxiv.org/abs/2412.18927v1,"Sang-Ho Kim, Yongseok Oh, Sangyeong Son, S. Sakinah, Myung-Ki Cheoun"
Exemplar-condensed Federated Class-incremental Learning,"We propose Exemplar-Condensed federated class-incremental learning (ECoral)
to distil the training characteristics of real images from streaming data into
informative rehearsal exemplars. The proposed method eliminates the limitations
of exemplar selection in replay-based approaches for mitigating catastrophic
forgetting in federated continual learning (FCL). The limitations particularly
related to the heterogeneity of information density of each summarized data.
Our approach maintains the consistency of training gradients and the
relationship to past tasks for the summarized exemplars to represent the
streaming data compared to the original images effectively. Additionally, our
approach reduces the information-level heterogeneity of the summarized data by
inter-client sharing of the disentanglement generative model. Extensive
experiments show that our ECoral outperforms several state-of-the-art methods
and can be seamlessly integrated with many existing approaches to enhance
performance.",2024-12-25T15:13:40Z,http://arxiv.org/abs/2412.18926v1,"Rui Sun, Yumin Zhang, Varun Ojha, Tejal Shah, Haoran Duan, Bo Wei, Rajiv Ranjan"
"Asymptotic stability of the high-dimensional Kuramoto model on Stiefel
  manifolds","The aim of this article is to investigate the convergence properties of a
heterogeneous consensus model on Stiefel manifolds. We consider each agent,
without interaction, moving according to the flow determined by the fundamental
vector field of the right multiplication action of the orthogonal group on the
Stiefel manifold. We analyze the asymptotic behavior of N such agents, assuming
that, as a result of their interactions, each agent's velocity is the sum of
its natural velocity and an additional velocity directed towards the average
position of the N agents. If the fundamental vector fields of all agents are
the same, their movement can be represented as a gradient flow on a product
manifold. In this study, we specifically investigate the asymptotic behavior in
a non-gradient flow setting, where the fundamental vector fields are not all
the same. Since fewer tools are available to address non-gradient flows, we
perform an orbital stability analysis to obtain the desired results instead of
relying on a gradient flow structure. Our estimate improves upon the previous
result in [Ha et al., Automatica 136 (2022)]. Furthermore, as a direct
consequence of the asymptotic dynamics, we derive uniform-in-time stability
with respect to the initial data.",2024-12-25T15:06:49Z,http://arxiv.org/abs/2412.18923v1,"Dohyun Kim, Woojoo Shim"
"Generative Face Parsing Map Guided 3D Face Reconstruction Under Occluded
  Scenes","Over the past few years, single-view 3D face reconstruction methods can
produce beautiful 3D models. Nevertheless,the input of these works is
unobstructed faces.We describe a system designed to reconstruct convincing face
texture in the case of occlusion.Motivated by parsing facial features,we
propose a complete face parsing map generation method guided by landmarks.We
estimate the 2D face structure of the reasonable position of the occlusion
area,which is used for the construction of 3D texture.An excellent
anti-occlusion face reconstruction method should ensure the authenticity of the
output,including the topological structure between the eyes,nose, and mouth. We
extensively tested our method and its components, qualitatively demonstrating
the rationality of our estimated facial structure. We conduct extensive
experiments on general 3D face reconstruction tasks as concrete examples to
demonstrate the method's superior regulation ability over existing methods
often break down.We further provide numerous quantitative examples showing that
our method advances both the quality and the robustness of 3D face
reconstruction under occlusion scenes.",2024-12-25T14:49:41Z,http://arxiv.org/abs/2412.18920v1,"Dapeng Zhao, Yue Qi"
"An Attentive Dual-Encoder Framework Leveraging Multimodal Visual and
  Semantic Information for Automatic OSAHS Diagnosis","Obstructive sleep apnea-hypopnea syndrome (OSAHS) is a common sleep disorder
caused by upper airway blockage, leading to oxygen deprivation and disrupted
sleep. Traditional diagnosis using polysomnography (PSG) is expensive,
time-consuming, and uncomfortable. Existing deep learning methods using facial
image analysis lack accuracy due to poor facial feature capture and limited
sample sizes. To address this, we propose a multimodal dual encoder model that
integrates visual and language inputs for automated OSAHS diagnosis. The model
balances data using randomOverSampler, extracts key facial features with
attention grids, and converts physiological data into meaningful text.
Cross-attention combines image and text data for better feature extraction, and
ordered regression loss ensures stable learning. Our approach improves
diagnostic efficiency and accuracy, achieving 91.3% top-1 accuracy in a
four-class severity classification task, demonstrating state-of-the-art
performance. Code will be released upon acceptance.",2024-12-25T14:42:17Z,http://arxiv.org/abs/2412.18919v1,"Yingchen Wei, Xihe Qiu, Xiaoyu Tan, Jingjing Huang, Wei Chu, Yinghui Xu, Yuan Qi"
"Open-Vocabulary Panoptic Segmentation Using BERT Pre-Training of
  Vision-Language Multiway Transformer Model","Open-vocabulary panoptic segmentation remains a challenging problem. One of
the biggest difficulties lies in training models to generalize to an unlimited
number of classes using limited categorized training data. Recent popular
methods involve large-scale vision-language pre-trained foundation models, such
as CLIP. In this paper, we propose OMTSeg for open-vocabulary segmentation
using another large-scale vision-language pre-trained model called BEiT-3 and
leveraging the cross-modal attention between visual and linguistic features in
BEiT-3 to achieve better performance. Experiments result demonstrates that
OMTSeg performs favorably against state-of-the-art models.",2024-12-25T14:31:00Z,http://arxiv.org/abs/2412.18917v1,"Yi-Chia Chen, Wei-Hua Li, Chu-Song Chen"
"Optimization-based model order reduction of fluid-structure interaction
  problems","We introduce optimization-based full-order and reduced-order formulations of
fluid structure interaction problems. We study the flow of an incompressible
Newtonian fluid which interacts with an elastic body: we consider an arbitrary
Lagrangian Eulerian formulation of the fluid problem and a fully Lagrangian
formulation of the solid problem; we rely on a finite element discretization of
both fluid and solid equations. The distinctive feature of our approach is an
implicit coupling of fluid and structural problems that relies on the solution
to a constrained optimization problem with equality constraints. We discuss the
application of projection-based model reduction to both fluid and solid
subproblems: we rely on Galerkin projection for the solid equations and on
least-square Petrov-Galerkin projection for the fluid equations. Numerical
results for three model problems illustrate the many features of the
formulation.",2024-12-25T14:26:54Z,http://arxiv.org/abs/2412.18916v1,"Tommaso Taddei, Xuejun Xu, Lei Zhang"
"Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With
  Structured Memories","Long-range tasks require reasoning over long inputs. Existing solutions
either need large compute budgets, training data, access to model weights, or
use complex, task-specific approaches. We present PRISM, which alleviates these
concerns by processing information as a stream of chunks, maintaining a
structured in-context memory specified by a typed hierarchy schema. This
approach demonstrates superior performance to baselines on diverse tasks while
using at least 4x smaller contexts than long-context models. Moreover, PRISM is
token-efficient. By producing short outputs and efficiently leveraging
key-value (KV) caches, it achieves up to 54% cost reduction when compared to
alternative short-context approaches. The method also scales down to tiny
information chunks (e.g., 500 tokens) without increasing the number of tokens
encoded or sacrificing quality. Furthermore, we show that it is possible to
generate schemas to generalize our approach to new tasks with minimal effort.",2024-12-25T14:14:31Z,http://arxiv.org/abs/2412.18914v1,"Dulhan Jayalath, James Bradley Wendt, Nicholas Monath, Sandeep Tata, Beliz Gunel"
Quantitative estimates of the singular values of random i.i.d. matrices,"Let $M$ be an $n\times n$ random i.i.d. matrix. This paper studies the
deviation inequality of $s_{n-k+1}(M)$, the $k$-th smallest singular value of
$M$. In particular, when the entries of $M$ are subgaussian, we show that for
any $\gamma\in (0, 1/2), \varepsilon&gt;0$ and $\log n\le k\le c\sqrt{n}$
  \begin{align}
  \textsf{P}\{s_{n-k+1}(M)\le \frac{\varepsilon}{\sqrt{n}} \}\le \Big(
\frac{C\varepsilon}{k}\Big)^{\gamma k^{2}}+e^{-c_{1}kn}.\nonumber
  \end{align}
  This result improves an existing result of Nguyen, which obtained a deviation
inequality of $s_{n-k+1}(M)$ with $(C\varepsilon/k)^{\gamma k^{2}}+e^{-cn}$
decay.",2024-12-25T14:01:39Z,http://arxiv.org/abs/2412.18912v1,"Guozheng Dai, Zhonggen Su, Hanchao Wang"
Accelerating Diffusion Transformers with Dual Feature Caching,"Diffusion Transformers (DiT) have become the dominant methods in image and
video generation yet still suffer substantial computational costs. As an
effective approach for DiT acceleration, feature caching methods are designed
to cache the features of DiT in previous timesteps and reuse them in the next
timesteps, allowing us to skip the computation in the next timesteps. However,
on the one hand, aggressively reusing all the features cached in previous
timesteps leads to a severe drop in generation quality. On the other hand,
conservatively caching only the features in the redundant layers or tokens but
still computing the important ones successfully preserves the generation
quality but results in reductions in acceleration ratios. Observing such a
tradeoff between generation quality and acceleration performance, this paper
begins by quantitatively studying the accumulated error from cached features.
Surprisingly, we find that aggressive caching does not introduce significantly
more caching errors in the caching step, and the conservative feature caching
can fix the error introduced by aggressive caching. Thereby, we propose a dual
caching strategy that adopts aggressive and conservative caching iteratively,
leading to significant acceleration and high generation quality at the same
time. Besides, we further introduce a V-caching strategy for token-wise
conservative caching, which is compatible with flash attention and requires no
training and calibration data.
  Our codes have been released in Github: \textbf{Code:
\href{https://github.com/Shenyi-Z/DuCa}{\texttt{\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",2024-12-25T14:00:14Z,http://arxiv.org/abs/2412.18911v1,"Chang Zou, Evelyn Zhang, Runlin Guo, Haohang Xu, Conghui He, Xuming Hu, Linfeng Zhang"
"AdaEAGLE: Optimizing Speculative Decoding via Explicit Modeling of
  Adaptive Draft Structures","Speculative Decoding (SD) is a popular lossless technique for accelerating
the inference of Large Language Models (LLMs). We show that the decoding speed
of SD frameworks with static draft structures can be significantly improved by
incorporating context-aware adaptive draft structures. However, current studies
on adaptive draft structures are limited by their performance, modeling
approaches, and applicability. In this paper, we introduce AdaEAGLE, the first
SD framework that explicitly models adaptive draft structures. AdaEAGLE
leverages the Lightweight Draft Length Predictor (LDLP) module to explicitly
predict the optimal number of draft tokens during inference to guide the draft
model. It achieves comparable speedup results without manual thresholds and
allows for deeper, more specialized optimizations. Moreover, together with
threshold-based strategies, AdaEAGLE achieves a $1.62\times$ speedup over the
vanilla AR decoding and outperforms fixed-length SotA baseline while
maintaining output quality.",2024-12-25T13:57:33Z,http://arxiv.org/abs/2412.18910v1,"Situo Zhang, Hankun Wang, Da Ma, Zichen Zhu, Lu Chen, Kunyao Lan, Kai Yu"
Relaxation behavior near the first-order phase transition line,"Using the Metropolis algorithm, we simulate the relaxation process of the
three-dimensional kinetic Ising model. Starting from a random initial
configuration, we first present the average equilibration time across the
entire phase boundary. It is observed that the average equilibration time
increases significantly as the temperature decreases from the critical
temperature ($T_{\rm c}$). The average equilibration time along the first-order
phase transition (1st-PT) line exhibits an ultra-slow relaxation. We also
investigate the dynamic scaling behavior with system sizes, and find that
dynamic scaling holds not only at $T_{\rm c}$, but also below $T_{\rm c}$. The
dynamic exponent below $T_{\rm c}$ is larger than that at $T_{\rm c}$.
Additionally, we analyze the dynamic scaling of the average autocorrelation
time and find that it depends on system size only near $T_{\rm c}$, while it
becomes size-independent both above and below $T_{\rm c}$. The extremely slow
relaxation dynamics observed near the 1st-PT is attributed to the complex
structure of the free energy.",2024-12-25T13:56:18Z,http://arxiv.org/abs/2412.18909v1,"Xiaobing Li, Ranran Guo, Mingmei Xu, Yu Zhou, Jinghua Fu, Yuanfang Wu"
"Research Experiment on Multi-Model Comparison for Chinese Text
  Classification Tasks","With the explosive growth of Chinese text data and advancements in natural
language processing technologies, Chinese text classification has become one of
the key techniques in fields such as information retrieval and sentiment
analysis, attracting increasing attention. This paper conducts a comparative
study on three deep learning models:TextCNN, TextRNN, and FastText.specifically
for Chinese text classification tasks. By conducting experiments on the
THUCNews dataset, the performance of these models is evaluated, and their
applicability in different scenarios is discussed.",2024-12-25T13:54:40Z,http://arxiv.org/abs/2412.18908v1,JiaCheng Li
"EC-Diffuser: Multi-Object Manipulation via Entity-Centric Behavior
  Generation","Object manipulation is a common component of everyday tasks, but learning to
manipulate objects from high-dimensional observations presents significant
challenges. These challenges are heightened in multi-object environments due to
the combinatorial complexity of the state space as well as of the desired
behaviors. While recent approaches have utilized large-scale offline data to
train models from pixel observations, achieving performance gains through
scaling, these methods struggle with compositional generalization in unseen
object configurations with constrained network and dataset sizes. To address
these issues, we propose a novel behavioral cloning (BC) approach that
leverages object-centric representations and an entity-centric Transformer with
diffusion-based optimization, enabling efficient learning from offline image
data. Our method first decomposes observations into an object-centric
representation, which is then processed by our entity-centric Transformer that
computes attention at the object level, simultaneously predicting object
dynamics and the agent's actions. Combined with the ability of diffusion models
to capture multi-modal behavior distributions, this results in substantial
performance improvements in multi-object tasks and, more importantly, enables
compositional generalization. We present BC agents capable of zero-shot
generalization to tasks with novel compositions of objects and goals, including
larger numbers of objects than seen during training. We provide video rollouts
on our webpage: https://sites.google.com/view/ec-diffuser.",2024-12-25T13:50:15Z,http://arxiv.org/abs/2412.18907v1,"Carl Qi, Dan Haramati, Tal Daniel, Aviv Tamar, Amy Zhang"
External Bias and Opinion Clustering in Cooperative Networks,"In this work, we consider a group of n agents which interact with each other
in a cooperative framework. A Laplacian-based model is proposed to govern the
evolution of opinions in the group when the agents are subjected to external
biases like agents' traits, news, etc. The objective of the paper is to design
a control input which leads to any desired opinion clustering even in the
presence of external bias factors. Further, we also determine the conditions
which ensure the reachability to any arbitrary opinion states. Note that all of
these results hold for any kind of graph structure. Finally, some numerical
simulations are discussed to validate these results.",2024-12-25T13:36:36Z,http://arxiv.org/abs/2412.18905v1,"Akshay Nagesh Kamthe, Vishnudatta Thota, Aashi Shrinate, Twinkle Tripathy"
"FedCFA: Alleviating Simpson's Paradox in Model Aggregation with
  Counterfactual Federated Learning","Federated learning (FL) is a promising technology for data privacy and
distributed optimization, but it suffers from data imbalance and heterogeneity
among clients. Existing FL methods try to solve the problems by aligning client
with server model or by correcting client model with control variables. These
methods excel on IID and general Non-IID data but perform mediocrely in
Simpson's Paradox scenarios. Simpson's Paradox refers to the phenomenon that
the trend observed on the global dataset disappears or reverses on a subset,
which may lead to the fact that global model obtained through aggregation in FL
does not accurately reflect the distribution of global data. Thus, we propose
FedCFA, a novel FL framework employing counterfactual learning to generate
counterfactual samples by replacing local data critical factors with global
average data, aligning local data distributions with the global and mitigating
Simpson's Paradox effects. In addition, to improve the quality of
counterfactual samples, we introduce factor decorrelation (FDC) loss to reduce
the correlation among features and thus improve the independence of extracted
factors. We conduct extensive experiments on six datasets and verify that our
method outperforms other FL methods in terms of efficiency and global model
accuracy under limited communication rounds.",2024-12-25T13:35:54Z,http://arxiv.org/abs/2412.18904v1,"Zhonghua Jiang, Jimin Xu, Shengyu Zhang, Tao Shen, Jiwei Li, Kun Kuang, Haibin Cai, Fei Wu"
"Stationary Processes, Wiener-Granger Causality, and Matrix Spectral
  Factorization","Granger causality has become an indispensable tool for analyzing causal
relationships between time series. In this paper, we provide a detailed
overview of its mathematical foundations, trace its historical development, and
explore how recent computational advancements can enhance its application in
various fields. We will not hesitate to present the proofs in full if they are
simple and transparent. For more complex theorems on which we rely, we will
provide supporting citations. We also discuss potential future directions for
the method, particularly in the context of largescale data analysis.",2024-12-25T13:29:39Z,http://arxiv.org/abs/2412.18901v1,Lasha Ephremidze
"Observation of on- and off-resonant interaction between a solid-state
  spin qubit and a superconducting resonator","Hybrid systems consisting of multiple materials with distinct physical
properties and tunable interactions provide a promising route for fulfilling
transformative quantum innovations. Solid-state spin qubits and superconducting
circuits stand out as leading candidates in this context due to their
complementary device performance and quantum mechanical properties. Here, we
report experimental integration of a single nitrogen-vacancy (NV) spin qubit
and an on-chip superconducting resonator for realizing multimodal quantum
applications. Specifically, we have observed superconductivity enhanced NV spin
relaxation, which shows a similar Hebel-Slichter peak feature around the phase
transition point. In the coherent interaction regime, we show that the
superconducting resonator mode is capable of exciting NV Rabi oscillations.
Taking advantage of scanning NV magnetometry, we further visualized microscopic
electromagnetic behaviors of the superconducting resonator, revealing the
formation and evolution of superconducting vortices at the nanoscale. Our
results highlight the potential of harnessing NV centers and superconducting
circuits for designing hybrid systems to advance the burgeoning quantum
revolution. The current study will also open a new pathway to test and evaluate
miniaturized superconducting electronics for their future design and
performance improvements.",2024-12-25T13:06:25Z,http://arxiv.org/abs/2412.18896v1,"Senlei Li, Shane P. Kelly, Jingcheng Zhou, Hanyi Lu, Yaroslav Tserkovnyak, Hailong Wang, Chunhui Rita Du"
"Comprehensive Study on Lumbar Disc Segmentation Techniques Using MRI
  Data","Lumbar disk segmentation is essential for diagnosing and curing spinal
disorders by enabling precise detection of disk boundaries in medical imaging.
The advent of deep learning has resulted in the development of many
segmentation methods, offering differing levels of accuracy and effectiveness.
This study assesses the effectiveness of several sophisticated deep learning
architectures, including ResUnext, Ef3 Net, UNet, and TransUNet, for lumbar
disk segmentation, highlighting key metrics like as Pixel Accuracy, Mean
Intersection over Union (Mean IoU), and Dice Coefficient. The findings indicate
that ResUnext achieved the highest segmentation accuracy, with a Pixel Accuracy
of 0.9492 and a Dice Coefficient of 0.8425, with TransUNet following closely
after. Filtering techniques somewhat enhanced the performance of most models,
particularly Dense UNet, improving stability and segmentation quality. The
findings underscore the efficacy of these models in lumbar disk segmentation
and highlight potential areas for improvement.",2024-12-25T12:54:52Z,http://arxiv.org/abs/2412.18894v1,"Serkan Salturk, Irem Sayin, Ibrahim Cem Balci, Taha Emre Pamukcu, Zafer Soydan, Huseyin Uvet"
"CoEvo: Continual Evolution of Symbolic Solutions Using Large Language
  Models","Large Language Models (LLMs) have emerged as transformative tools in
artificial intelligence, capable of processing and understanding extensive
human knowledge to enhance problem-solving across various domains. This paper
explores the potential of LLMs to drive the discovery of symbolic solutions
within scientific and engineering disciplines, where such solutions are crucial
for advancing theoretical and practical applications. We propose a novel
framework that utilizes LLMs in an evolutionary search methodology, augmented
by a dynamic knowledge library that integrates and refines insights in an
\textit{open-ended manner}. This approach aims to tackle the dual challenges of
efficiently navigating complex symbolic representation spaces and leveraging
both existing and newly generated knowledge to foster open-ended innovation. By
enabling LLMs to interact with and expand upon a knowledge library, we
facilitate the continuous generation of novel solutions in diverse forms such
as language, code, and mathematical expressions. Our experimental results
demonstrate that this method not only enhances the efficiency of searching for
symbolic solutions but also supports the ongoing discovery process, akin to
human scientific endeavors. This study represents a first effort in
conceptualizing the search for symbolic solutions as a lifelong, iterative
process, marking a significant step towards harnessing AI in the perpetual
pursuit of scientific and engineering breakthroughs. We have open-sourced our
code and data, please visit \url{https://github.com/pgg3/CoEvo} for more
information.",2024-12-25T12:27:27Z,http://arxiv.org/abs/2412.18890v1,"Ping Guo, Qingfu Zhang, Xi Lin"
"Adversarial Training for Graph Neural Networks via Graph Subspace Energy
  Optimization","Despite impressive capability in learning over graph-structured data, graph
neural networks (GNN) suffer from adversarial topology perturbation in both
training and inference phases. While adversarial training has demonstrated
remarkable effectiveness in image classification tasks, its suitability for GNN
models has been doubted until a recent advance that shifts the focus from
transductive to inductive learning. Still, GNN robustness in the inductive
setting is under-explored, and it calls for deeper understanding of GNN
adversarial training. To this end, we propose a new concept of graph subspace
energy (GSE) -- a generalization of graph energy that measures graph stability
-- of the adjacency matrix, as an indicator of GNN robustness against topology
perturbations. To further demonstrate the effectiveness of such concept, we
propose an adversarial training method with the perturbed graphs generated by
maximizing the GSE regularization term, referred to as AT-GSE. To deal with the
local and global topology perturbations raised respectively by LRBCD and PRBCD,
we employ randomized SVD (RndSVD) and Nystrom low-rank approximation to favor
the different aspects of the GSE terms. An extensive set of experiments shows
that AT-GSE outperforms consistently the state-of-the-art GNN adversarial
training methods over different homophily and heterophily datasets in terms of
adversarial accuracy, whilst more surprisingly achieving a superior clean
accuracy on non-perturbed graphs.",2024-12-25T12:04:18Z,http://arxiv.org/abs/2412.18886v1,"Ganlin Liu, Ziling Liang, Xiaowei Huang, Xinping Yi, Shi Jin"
"HV-BEV: Decoupling Horizontal and Vertical Feature Sampling for
  Multi-View 3D Object Detection","The application of vision-based multi-view environmental perception system
has been increasingly recognized in autonomous driving technology, especially
the BEV-based models. Current state-of-the-art solutions primarily encode image
features from each camera view into the BEV space through explicit or implicit
depth prediction. However, these methods often focus on improving the accuracy
of projecting 2D features into corresponding depth regions, while overlooking
the highly structured information of real-world objects and the varying height
distributions of objects across different scenes. In this work, we propose
HV-BEV, a novel approach that decouples feature sampling in the BEV grid
queries paradigm into horizontal feature aggregation and vertical adaptive
height-aware reference point sampling, aiming to improve both the aggregation
of objects' complete information and generalization to diverse road
environments. Specifically, we construct a learnable graph structure in the
horizontal plane aligned with the ground for 3D reference points, reinforcing
the association of the same instance across different BEV grids, especially
when the instance spans multiple image views around the vehicle. Additionally,
instead of relying on uniform sampling within a fixed height range, we
introduce a height-aware module that incorporates historical information,
enabling the reference points to adaptively focus on the varying heights at
which objects appear in different scenes. Extensive experiments validate the
effectiveness of our proposed method, demonstrating its superior performance
over the baseline across the nuScenes dataset. Moreover, our best-performing
model achieves a remarkable 50.5% mAP and 59.8% NDS on the nuScenes testing
set.",2024-12-25T11:49:14Z,http://arxiv.org/abs/2412.18884v1,"Di Wu, Feng Yang, Benlian Xu, Pan Liao, Wenhui Zhao, Dingwen Zhang"
Cross-PCR: A Robust Cross-Source Point Cloud Registration Framework,"Due to the density inconsistency and distribution difference between
cross-source point clouds, previous methods fail in cross-source point cloud
registration. We propose a density-robust feature extraction and matching
scheme to achieve robust and accurate cross-source registration. To address the
density inconsistency between cross-source data, we introduce a density-robust
encoder for extracting density-robust features. To tackle the issue of
challenging feature matching and few correct correspondences, we adopt a
loose-to-strict matching pipeline with a ``loose generation, strict selection''
idea. Under it, we employ a one-to-many strategy to loosely generate initial
correspondences. Subsequently, high-quality correspondences are strictly
selected to achieve robust registration through sparse matching and dense
matching. On the challenging Kinect-LiDAR scene in the cross-source 3DCSR
dataset, our method improves feature matching recall by 63.5 percentage points
(pp) and registration recall by 57.6 pp. It also achieves the best performance
on 3DMatch, while maintaining robustness under diverse downsampling densities.",2024-12-25T11:14:59Z,http://arxiv.org/abs/2412.18873v1,"Guiyu Zhao, Zhentao Guo, Zewen Du, Hongbin Ma"
Proton Flux Measurement from Neutron Monitor Data Using Neural Networks,"Accurate measurements of cosmic ray proton flux are crucial for studying the
modulation processes of cosmic rays during the solar activity cycle. We present
a proton flux measurement method based on ground-based neutron monitor (NM)
data and machine learning techniques. After preprocessing the NM data, we use a
convolutional neural network (CNN) model to simulate the relationship between
the NM observations and proton flux measured by the Alpha Magnetic Spectrometer
(AMS-02). We obtain daily proton flux data ranging from 1GV to 100GV for the
period from 2011 to 2024, showing that the measured values are in good
agreement with the observed ones. In particular, we provide daily proton flux
measurements for periods when AMS-02 data are unavailable due to operational
reasons. We also perform wavelet analyses on the continuous proton flux data to
investigate the relationship between proton flux and solar activity variations,
particularly during late 2014 when AMS-02 measurements were missing.",2024-12-25T11:14:13Z,http://arxiv.org/abs/2412.18872v1,"Pengwei Zhao, Jie Feng"
"TSceneJAL: Joint Active Learning of Traffic Scenes for 3D Object
  Detection","Most autonomous driving (AD) datasets incur substantial costs for collection
and labeling, inevitably yielding a plethora of low-quality and redundant data
instances, thereby compromising performance and efficiency. Many applications
in AD systems necessitate high-quality training datasets using both existing
datasets and newly collected data. In this paper, we propose a traffic scene
joint active learning (TSceneJAL) framework that can efficiently sample the
balanced, diverse, and complex traffic scenes from both labeled and unlabeled
data. The novelty of this framework is threefold: 1) a scene sampling scheme
based on a category entropy, to identify scenes containing multiple object
classes, thus mitigating class imbalance for the active learner; 2) a
similarity sampling scheme, estimated through the directed graph representation
and a marginalize kernel algorithm, to pick sparse and diverse scenes; 3) an
uncertainty sampling scheme, predicted by a mixture density network, to select
instances with the most unclear or complex regression outcomes for the learner.
Finally, the integration of these three schemes in a joint selection strategy
yields an optimal and valuable subdataset. Experiments on the KITTI, Lyft,
nuScenes and SUScape datasets demonstrate that our approach outperforms
existing state-of-the-art methods on 3D object detection tasks with up to 12%
improvements.",2024-12-25T11:07:04Z,http://arxiv.org/abs/2412.18870v1,"Chenyang Lei, Meiying Zhang, Weiyuan Peng, Qi Hao, Chengzhong Xu, Chunlin Ji, Guang Zhou"
"Overview of MWE history, challenges, and horizons: standing at the 20th
  anniversary of the MWE workshop series via MWE-UD2024","Starting in 2003 when the first MWE workshop was held with ACL in Sapporo,
Japan, this year, the joint workshop of MWE-UD co-located with the LREC-COLING
2024 conference marked the 20th anniversary of MWE workshop events over the
past nearly two decades. Standing at this milestone, we look back to this
workshop series and summarise the research topics and methodologies researchers
have carried out over the years. We also discuss the current challenges that we
are facing and the broader impacts/synergies of MWE research within the CL and
NLP fields. Finally, we give future research perspectives. We hope this
position paper can help researchers, students, and industrial practitioners
interested in MWE get a brief but easy understanding of its history, current,
and possible future.",2024-12-25T11:00:27Z,http://arxiv.org/abs/2412.18868v1,"Lifeng Han, Kilian Evang, Archna Bhatia, Gosse Bouma, A. Seza Doğruöz, Marcos Garcia, Voula Giouli, Joakim Nivre, Alexandre Rademacher"
"Autonomous Navigation of 4WIS4WID Agricultural Field Mobile Robot using
  Deep Reinforcement Learning","In the futuristic agricultural fields compatible with Agriculture 4.0, robots
are envisaged to navigate through crops to perform functions like pesticide
spraying and fruit harvesting, which are complex tasks due to factors such as
non-geometric internal obstacles, space constraints, and outdoor conditions. In
this paper, we attempt to employ Deep Reinforcement Learning (DRL) to solve the
problem of 4WIS4WID mobile robot navigation in a structured, automated
agricultural field. This paper consists of three sections: parameterization of
four-wheel steering configurations, crop row tracking using DRL, and autonomous
navigation of 4WIS4WID mobile robot using DRL through multiple crop rows. We
show how to parametrize various configurations of four-wheel steering to two
variables. This includes symmetric four-wheel steering, zero-turn, and an
additional steering configuration that allows the 4WIS4WID mobile robot to move
laterally. Using DRL, we also followed an irregularly shaped crop row with
symmetric four-wheel steering. In the multiple crop row simulation environment,
with the help of waypoints, we effectively performed point-to-point navigation.
Finally, a comparative analysis of various DRL algorithms that use continuous
actions was carried out.",2024-12-25T10:33:33Z,http://arxiv.org/abs/2412.18865v1,"Tom Baby, Mahendra Kumar Gohil, Bishakh Bhattacharya"
"Whose Morality Do They Speak? Unraveling Cultural Bias in Multilingual
  Language Models","Large language models (LLMs) have become integral tools in diverse domains,
yet their moral reasoning capabilities across cultural and linguistic contexts
remain underexplored. This study investigates whether multilingual LLMs, such
as GPT-3.5-Turbo, GPT-4o-mini, Llama 3.1, and MistralNeMo, reflect culturally
specific moral values or impose dominant moral norms, particularly those rooted
in English. Using the updated Moral Foundations Questionnaire (MFQ-2) in eight
languages, Arabic, Farsi, English, Spanish, Japanese, Chinese, French, and
Russian, the study analyzes the models' adherence to six core moral
foundations: care, equality, proportionality, loyalty, authority, and purity.
The results reveal significant cultural and linguistic variability, challenging
the assumption of universal moral consistency in LLMs. Although some models
demonstrate adaptability to diverse contexts, others exhibit biases influenced
by the composition of the training data. These findings underscore the need for
culturally inclusive model development to improve fairness and trust in
multilingual AI systems.",2024-12-25T10:17:15Z,http://arxiv.org/abs/2412.18863v1,Meltem Aksoy
Bootstrap Your Own Context Length,"We introduce a bootstrapping approach to train long-context language models
by exploiting their short-context capabilities only. Our method utilizes a
simple agent workflow to synthesize diverse long-context instruction tuning
data, thereby eliminating the necessity for manual data collection and
annotation. The proposed data synthesis workflow requires only a short-context
language model, a text retriever, and a document collection, all of which are
readily accessible within the open-source ecosystem. Subsequently, language
models are fine-tuned using the synthesized data to extend their context
lengths. In this manner, we effectively transfer the short-context capabilities
of language models to long-context scenarios through a bootstrapping process.
We conduct experiments with the open-source Llama-3 family of models and
demonstrate that our method can successfully extend the context length to up to
1M tokens, achieving superior performance across various benchmarks.",2024-12-25T10:08:54Z,http://arxiv.org/abs/2412.18860v1,"Liang Wang, Nan Yang, Xingxing Zhang, Xiaolong Huang, Furu Wei"
"Few-shot Metric Domain Adaptation: Practical Learning Strategies for an
  Automated Plant Disease Diagnosis","Numerous studies have explored image-based automated systems for plant
disease diagnosis, demonstrating impressive diagnostic capabilities. However,
recent large-scale analyses have revealed a critical limitation: that the
diagnostic capability suffers significantly when validated on images captured
in environments (domains) differing from those used during training. This
shortfall stems from the inherently limited dataset size and the diverse
manifestation of disease symptoms, combined with substantial variations in
cultivation environments and imaging conditions, such as equipment and
composition. These factors lead to insufficient variety in training data,
ultimately constraining the system's robustness and generalization. To address
these challenges, we propose Few-shot Metric Domain Adaptation (FMDA), a
flexible and effective approach for enhancing diagnostic accuracy in practical
systems, even when only limited target data is available. FMDA reduces domain
discrepancies by introducing a constraint to the diagnostic model that
minimizes the ""distance"" between feature spaces of source (training) data and
target data with limited samples. FMDA is computationally efficient, requiring
only basic feature distance calculations and backpropagation, and can be
seamlessly integrated into any machine learning (ML) pipeline. In large-scale
experiments, involving 223,015 leaf images across 20 fields and 3 crop species,
FMDA achieved F1 score improvements of 11.1 to 29.3 points compared to cases
without target data, using only 10 images per disease from the target domain.
Moreover, FMDA consistently outperformed fine-tuning methods utilizing the same
data, with an average improvement of 8.5 points.",2024-12-25T10:01:30Z,http://arxiv.org/abs/2412.18859v1,"Shoma Kudo, Satoshi Kagiwada, Hitoshi Iyatomi"
Computing Approximate Graph Edit Distance via Optimal Transport,"Given a graph pair $(G^1, G^2)$, graph edit distance (GED) is defined as the
minimum number of edit operations converting $G^1$ to $G^2$. GED is a
fundamental operation widely used in many applications, but its exact
computation is NP-hard, so the approximation of GED has gained a lot of
attention. Data-driven learning-based methods have been found to provide
superior results compared to classical approximate algorithms, but they
directly fit the coupling relationship between a pair of vertices from their
vertex features. We argue that while pairwise vertex features can capture the
coupling cost (discrepancy) of a pair of vertices, the vertex coupling matrix
should be derived from the vertex-pair cost matrix through a more
well-established method that is aware of the global context of the graph pair,
such as optimal transport. In this paper, we propose an ensemble approach that
integrates a supervised learning-based method and an unsupervised method, both
based on optimal transport. Our learning method, GEDIOT, is based on inverse
optimal transport that leverages a learnable Sinkhorn algorithm to generate the
coupling matrix. Our unsupervised method, GEDGW, models GED computation as a
linear combination of optimal transport and its variant, Gromov-Wasserstein
discrepancy, for node and edge operations, respectively, which can be solved
efficiently without needing the ground truth. Our ensemble method, GEDHOT,
combines GEDIOT and GEDGW to further boost the performance. Extensive
experiments demonstrate that our methods significantly outperform the existing
methods in terms of the performance of GED computation, edit path generation,
and model generalizability.",2024-12-25T09:55:14Z,http://arxiv.org/abs/2412.18857v1,"Qihao Cheng, Da Yan, Tianhao Wu, Zhongyi Huang, Qin Zhang"
"Digital Twin Enhanced Deep Reinforcement Learning for Intelligent
  Omni-Surface Configurations in MU-MIMO Systems","Intelligent omni-surface (IOS) is a promising technique to enhance the
capacity of wireless networks, by reflecting and refracting the incident signal
simultaneously. Traditional IOS configuration schemes, relying on all
sub-channels' channel state information and user equipments' mobility, are
difficult to implement in complex realistic systems. Existing works attempt to
address this issue employing deep reinforcement learning (DRL), but this method
requires a lot of trial-and-error interactions with the external environment
for efficient results and thus cannot satisfy the real-time decision-making. To
enable model-free and real-time IOS control, this paper puts forth a new
framework that integrates DRL and digital twins. DeepIOS, a DRL based IOS
configuration scheme with the goal of maximizing the sum data rate, is first
developed to jointly optimize the phase-shift and amplitude of IOS in
multi-user multiple-input-multiple-output systems. Thereafter, to further
reduce the computational complexity, DeepIOS introduces an action branch
architecture, which separately decides two optimization variables in parallel.
Finally, a digital twin module is constructed through supervised learning as a
pre-verification platform for DeepIOS, such that the decision-making's
real-time can be guaranteed. The formulated framework is a closed-loop system,
in which the physical space provides data to establish and calibrate the
digital space, while the digital space generates experience samples for DeepIOS
training and sends the trained parameters to the IOS controller for
configurations. Numerical results show that compared with random and MAB
schemes, the proposed framework attains a higher data rate and is more robust
to different settings. Furthermore, the action branch architecture reduces
DeepIOS's computational complexity, and the digital twin module improves the
convergence speed and run-time.",2024-12-25T09:53:07Z,http://arxiv.org/abs/2412.18856v1,"Xiaowen Ye, Xianghao Yu, Liqun Fu"
Machine Learning-Based Detection of Pump-and-Dump Schemes in Real-Time,"Cryptocurrency markets often face manipulation through prevalent
pump-and-dump (P&amp;D) schemes, where self-organized Telegram groups, some
exceeding two million members, artificially inflate target cryptocurrency
prices. These groups sell premium access to inside information, worsening
information asymmetry and financial risks for subscribers and all investors.
This paper presents a real-time prediction pipeline to forecast target coins
and alert investors to possible P&amp;D schemes. In a Poloniex case study, the
model accurately identified the target coin among the top five from 50 random
coins in 24 out of 43 (55.81%) P&amp;D events. The pipeline uses advanced natural
language processing (NLP) to classify Telegram messages, identifying 2,079 past
pump events and detecting new ones in real-time. Our analysis also evaluates
the susceptibility of token standards - ERC-20, ERC-721, BRC-20, Inscriptions,
and Runes - to manipulation and identifies exchanges commonly involved in P&amp;D
schemes.",2024-12-25T09:23:36Z,http://arxiv.org/abs/2412.18848v1,"Manuel Bolz, Kevin Bründler, Liam Kane, Panagiotis Patsias, Liam Tessendorf, Krzysztof Gogol, Taehoon Kim, Claudio Tessone"
"TPCH: Tensor-interacted Projection and Cooperative Hashing for
  Multi-view Clustering","In recent years, anchor and hash-based multi-view clustering methods have
gained attention for their efficiency and simplicity in handling large-scale
data. However, existing methods often overlook the interactions among
multi-view data and higher-order cooperative relationships during projection,
negatively impacting the quality of hash representation in low-dimensional
spaces, clustering performance, and sensitivity to noise. To address this
issue, we propose a novel approach named Tensor-Interacted Projection and
Cooperative Hashing for Multi-View Clustering(TPCH). TPCH stacks multiple
projection matrices into a tensor, taking into account the synergies and
communications during the projection process. By capturing higher-order
multi-view information through dual projection and Hamming space, TPCH employs
an enhanced tensor nuclear norm to learn more compact and distinguishable hash
representations, promoting communication within and between views. Experimental
results demonstrate that this refined method significantly outperforms
state-of-the-art methods in clustering on five large-scale multi-view datasets.
Moreover, in terms of CPU time, TPCH achieves substantial acceleration compared
to the most advanced current methods. The code is available at
\textcolor{red}{\url{https://github.com/jankin-wang/TPCH}}.",2024-12-25T09:22:11Z,http://arxiv.org/abs/2412.18847v1,"Zhongwen Wang, Xingfeng Li, Yinghui Sun, Quansen Sun, Yuan Sun, Han Ling, Jian Dai, Zhenwen Ren"
"Enhancing Federated Graph Learning via Adaptive Fusion of Structural and
  Node Characteristics","Federated Graph Learning (FGL) has demonstrated the advantage of training a
global Graph Neural Network (GNN) model across distributed clients using their
local graph data. Unlike Euclidean data (\eg, images), graph data is composed
of nodes and edges, where the overall node-edge connections determine the
topological structure, and individual nodes along with their neighbors capture
local node features. However, existing studies tend to prioritize one aspect
over the other, leading to an incomplete understanding of the data and the
potential misidentification of key characteristics across varying graph
scenarios. Additionally, the non-independent and identically distributed
(non-IID) nature of graph data makes the extraction of these two data
characteristics even more challenging. To address the above issues, we propose
a novel FGL framework, named FedGCF, which aims to simultaneously extract and
fuse structural properties and node features to effectively handle diverse
graph scenarios. FedGCF first clusters clients by structural similarity,
performing model aggregation within each cluster to form the shared structural
model. Next, FedGCF selects the clients with common node features and
aggregates their models to generate a common node model. This model is then
propagated to all clients, allowing common node features to be shared. By
combining these two models with a proper ratio, FedGCF can achieve a
comprehensive understanding of the graph data and deliver better performance,
even under non-IID distributions. Experimental results show that FedGCF
improves accuracy by 4.94%-7.24% under different data distributions and reduces
communication cost by 64.18%-81.25% to reach the same accuracy compared to
baselines.",2024-12-25T09:20:06Z,http://arxiv.org/abs/2412.18845v1,"Xianjun Gao, Jianchun Liu, Hongli Xu, Shilong Wang, Liusheng Huang"
"Improving Integrated Gradient-based Transferable Adversarial Examples by
  Refining the Integration Path","Transferable adversarial examples are known to cause threats in practical,
black-box attack scenarios. A notable approach to improving transferability is
using integrated gradients (IG), originally developed for model
interpretability. In this paper, we find that existing IG-based attacks have
limited transferability due to their naive adoption of IG in model
interpretability. To address this limitation, we focus on the IG integration
path and refine it in three aspects: multiplicity, monotonicity, and diversity,
supported by theoretical analyses. We propose the Multiple Monotonic
Diversified Integrated Gradients (MuMoDIG) attack, which can generate highly
transferable adversarial examples on different CNN and ViT models and defenses.
Experiments validate that MuMoDIG outperforms the latest IG-based attack by up
to 37.3\% and other state-of-the-art attacks by 8.4\%. In general, our study
reveals that migrating established techniques to improve transferability may
require non-trivial efforts. Code is available at
\url{https://github.com/RYC-98/MuMoDIG}.",2024-12-25T09:15:39Z,http://arxiv.org/abs/2412.18844v1,"Yuchen Ren, Zhengyu Zhao, Chenhao Lin, Bo Yang, Lu Zhou, Zhe Liu, Chao Shen"
"Improving the Readability of Automatically Generated Tests using Large
  Language Models","Search-based test generators are effective at producing unit tests with high
coverage. However, such automatically generated tests have no meaningful test
and variable names, making them hard to understand and interpret by developers.
On the other hand, large language models (LLMs) can generate highly readable
test cases, but they are not able to match the effectiveness of search-based
generators, in terms of achieved code coverage.
  In this paper, we propose to combine the effectiveness of search-based
generators with the readability of LLM generated tests. Our approach focuses on
improving test and variable names produced by search-based tools, while keeping
their semantics (i.e., their coverage) unchanged.
  Our evaluation on nine industrial and open source LLMs show that our
readability improvement transformations are overall semantically-preserving and
stable across multiple repetitions. Moreover, a human study with ten
professional developers, show that our LLM-improved tests are as readable as
developer-written tests, regardless of the LLM employed.",2024-12-25T09:08:53Z,http://arxiv.org/abs/2412.18843v1,"Matteo Biagiola, Gianluca Ghislotti, Paolo Tonella"
"Context-Based Semantic-Aware Alignment for Semi-Supervised Multi-Label
  Learning","Due to the lack of extensive precisely-annotated multi-label data in real
word, semi-supervised multi-label learning (SSMLL) has gradually gained
attention. Abundant knowledge embedded in vision-language models (VLMs)
pre-trained on large-scale image-text pairs could alleviate the challenge of
limited labeled data under SSMLL setting.Despite existing methods based on
fine-tuning VLMs have achieved advances in weakly-supervised multi-label
learning, they failed to fully leverage the information from labeled data to
enhance the learning of unlabeled data. In this paper, we propose a
context-based semantic-aware alignment method to solve the SSMLL problem by
leveraging the knowledge of VLMs. To address the challenge of handling multiple
semantics within an image, we introduce a novel framework design to extract
label-specific image features. This design allows us to achieve a more compact
alignment between text features and label-specific image features, leading the
model to generate high-quality pseudo-labels. To incorporate the model with
comprehensive understanding of image, we design a semi-supervised context
identification auxiliary task to enhance the feature representation by
capturing co-occurrence information. Extensive experiments on multiple
benchmark datasets demonstrate the effectiveness of our proposed method.",2024-12-25T09:06:54Z,http://arxiv.org/abs/2412.18842v1,"Heng-Bo Fan, Ming-Kun Xie, Jia-Hao Xiao, Sheng-Jun Huang"
"Implicit factorized transformer approach to fast prediction of turbulent
  channel flows","Transformer neural operators have recently become an effective approach for
surrogate modeling of nonlinear systems governed by partial differential
equations (PDEs). In this paper, we introduce a modified implicit factorized
transformer (IFactFormer-m) model which replaces the original chained
factorized attention with parallel factorized attention. The IFactFormer-m
model successfully performs long-term predictions for turbulent channel flow,
whereas the original IFactFormer (IFactFormer-o), Fourier neural operator
(FNO), and implicit Fourier neural operator (IFNO) exhibit a poor performance.
Turbulent channel flows are simulated by direct numerical simulation using fine
grids at friction Reynolds numbers $\text{Re}_{\tau}\approx 180,395,590$, and
filtered to coarse grids for training neural operator. The neural operator
takes the current flow field as input and predicts the flow field at the next
time step, and long-term prediction is achieved in the posterior through an
autoregressive approach. The prediction results show that IFactFormer-m,
compared to other neural operators and the traditional large eddy simulation
(LES) methods including dynamic Smagorinsky model (DSM) and the wall-adapted
local eddy-viscosity (WALE) model, reduces prediction errors in the short term,
and achieves stable and accurate long-term prediction of various statistical
properties and flow structures, including the energy spectrum, mean streamwise
velocity, root mean square (rms) values of fluctuating velocities, Reynolds
shear stress, and spatial structures of instantaneous velocity. Moreover, the
trained IFactFormer-m is much faster than traditional LES methods.",2024-12-25T09:05:14Z,http://arxiv.org/abs/2412.18840v1,"Huiyu Yang, Yunpeng Wang, Jianchun Wang"
"Advancing NAM-to-Speech Conversion with Novel Methods and the MultiNAM
  Dataset","Current Non-Audible Murmur (NAM)-to-speech techniques rely on voice cloning
to simulate ground-truth speech from paired whispers. However, the simulated
speech often lacks intelligibility and fails to generalize well across
different speakers. To address this issue, we focus on learning phoneme-level
alignments from paired whispers and text and employ a Text-to-Speech (TTS)
system to simulate the ground-truth. To reduce dependence on whispers, we learn
phoneme alignments directly from NAMs, though the quality is constrained by the
available training data. To further mitigate reliance on NAM/whisper data for
ground-truth simulation, we propose incorporating the lip modality to infer
speech and introduce a novel diffusion-based method that leverages recent
advancements in lip-to-speech technology. Additionally, we release the MultiNAM
dataset with over $7.96$ hours of paired NAM, whisper, video, and text data
from two speakers and benchmark all methods on this dataset. Speech samples and
the dataset are available at \url{https://diff-nam.github.io/DiffNAM/}",2024-12-25T08:57:24Z,http://arxiv.org/abs/2412.18839v1,"Neil Shah, Shirish Karande, Vineet Gandhi"
DiFiC: Your Diffusion Model Holds the Secret to Fine-Grained Clustering,"Fine-grained clustering is a practical yet challenging task, whose essence
lies in capturing the subtle differences between instances of different
classes. Such subtle differences can be easily disrupted by data augmentation
or be overwhelmed by redundant information in data, leading to significant
performance degradation for existing clustering methods. In this work, we
introduce DiFiC a fine-grained clustering method building upon the conditional
diffusion model. Distinct from existing works that focus on extracting
discriminative features from images, DiFiC resorts to deducing the textual
conditions used for image generation. To distill more precise and
clustering-favorable object semantics, DiFiC further regularizes the diffusion
target and guides the distillation process utilizing neighborhood similarity.
Extensive experiments demonstrate that DiFiC outperforms both state-of-the-art
discriminative and generative clustering methods on four fine-grained image
clustering benchmarks. We hope the success of DiFiC will inspire future
research to unlock the potential of diffusion models in tasks beyond
generation. The code will be released.",2024-12-25T08:55:48Z,http://arxiv.org/abs/2412.18838v1,"Ruohong Yang, Peng Hu, Xi Peng, Xiting Liu, Yunfan Li"
"Experimental secure entanglement-free quantum remote sensing over 50 km
  of optical fiber","Secure quantum remote sensing (SQRS) uses quantum states to gather
information about distant objects or environments while ensuring secure data
transmission against eavesdropping. It has potential applications in various
fields, including environmental monitoring, military surveillance, and disaster
response, where both data accuracy and transmission security are critical.
Recent experiments have demonstrated the feasibility of SQRS using entanglement
states. Here, we experimentally demonstrate an SQRS that can estimate a phase
without requiring entanglement, offering the practical advantage that
single-qubit states are easier to prepare. We successfully estimate the preset
phase information at a remote site over a fiber distance of 50 km, which serves
as a key step toward long-distance applications.",2024-12-25T08:52:06Z,http://arxiv.org/abs/2412.18837v1,"Wenjie He, Chunfeng Huang, Rui Guan, Ye Chen, Zhenrong Zhang, Kejin Wei"
"LoGFiLM: Fine-Tuning A Large Language Model for Automated Generation of
  Log Statements","Log statements have become an integral part of modern software systems. Prior
research efforts have focused on supporting the decisions of placing log
statements, such as where/what to log, while automated generation or completion
of log statements has received little attention. With the increasing use of
Large Language Models (LLMs) for code-related tasks such as code completion or
generation, automated methods for generating or completing log statements have
gained much momentum. Fine-tuning open-source LLMs like the Llama series is
often preferred by enterprises over using commercial ones like the GPT series
due to considerations including privacy, security, openness, performance, etc.
Fine-tuning LLMs requires task-specific training data and custom-designed
processing algorithms, which, however, have not been thoroughly explored for
the log statement generation task. This paper fills this gap by contributing
such a fine-tuning method LoGFiLM and an exemplar model by using the proposed
method to fine-tune Llama-3-8B. Experiments with our own curated dataset and a
public dataset show that LoGFiLM consistently outperforms the original
Llama-3-8B and the commercial LLMs of GPT-3.5 and GPT-4. The results further
reveal that fine-tuning Llama-3-8B with data encompassing broader contextual
ranges surrounding log statements yields a better model for the automated
generation of log statements.",2024-12-25T08:43:00Z,http://arxiv.org/abs/2412.18835v1,"Hao Zhang, Dongjun Yu, Lei Zhang, Guoping Rong, Yongda Yu, Haifeng Shen, He Zhang, Dong Shao, Hongyu Kuang"
"Federated Learning with Partially Labeled Data: A Conditional
  Distillation Approach","In medical imaging, developing generalized segmentation models that can
handle multiple organs and lesions is crucial. However, the scarcity of fully
annotated datasets and strict privacy regulations present significant barriers
to data sharing. Federated Learning (FL) allows decentralized model training,
but existing FL methods often struggle with partial labeling, leading to model
divergence and catastrophic forgetting. We propose ConDistFL, a novel FL
framework incorporating conditional distillation to address these challenges.
ConDistFL enables effective learning from partially labeled datasets,
significantly improving segmentation accuracy across distributed and
non-uniform datasets. In addition to its superior segmentation performance,
ConDistFL maintains computational and communication efficiency, ensuring its
scalability for real-world applications. Furthermore, ConDistFL demonstrates
remarkable generalizability, significantly outperforming existing FL methods in
out-of-federation tests, even adapting to unseen contrast phases (e.g.,
non-contrast CT images) in our experiments. Extensive evaluations on 3D CT and
2D chest X-ray datasets show that ConDistFL is an efficient, adaptable solution
for collaborative medical image segmentation in privacy-constrained settings.",2024-12-25T08:40:03Z,http://arxiv.org/abs/2412.18833v1,"Pochuan Wang, Chen Shen, Masahiro Oda, Chiou-Shann Fuh, Kensaku Mori, Weichung Wang, Holger R. Roth"
"Structured Speaker-Deficiency Adaptation of Foundation Models for
  Dysarthric and Elderly Speech Recognition","Data-intensive fine-tuning of speech foundation models (SFMs) to scarce and
diverse dysarthric and elderly speech leads to data bias and poor
generalization to unseen speakers. This paper proposes novel structured
speaker-deficiency adaptation approaches for SSL pre-trained SFMs on such data.
Speaker and speech deficiency invariant SFMs were constructed in their
supervised adaptive fine-tuning stage to reduce undue bias to training data
speakers, and serves as a more neutral and robust starting point for test time
unsupervised adaptation. Speech variability attributed to speaker identity and
speech impairment severity, or aging induced neurocognitive decline, are
modelled using separate adapters that can be combined together to model any
seen or unseen speaker. Experiments on the UASpeech dysarthric and DementiaBank
Pitt elderly speech corpora suggest structured speaker-deficiency adaptation of
HuBERT and Wav2vec2-conformer models consistently outperforms baseline SFMs
using either: a) no adapters; b) global adapters shared among all speakers; or
c) single attribute adapters modelling speaker or deficiency labels alone by
statistically significant WER reductions up to 3.01% and 1.50% absolute (10.86%
and 6.94% relative) on the two tasks respectively. The lowest published WER of
19.45% (49.34% on very low intelligibility, 33.17% on unseen words) is obtained
on the UASpeech test set of 16 dysarthric speakers.",2024-12-25T08:39:02Z,http://arxiv.org/abs/2412.18832v1,"Shujie Hu, Xurong Xie, Mengzhe Geng, Jiajun Deng, Zengrui Jin, Tianzi Wang, Mingyu Cui, Guinan Li, Zhaoqing Li, Helen Meng, Xunying Liu"
"Data-driven $H_{\infty}$ predictive control for constrained systems: a
  Lagrange duality approcah","This article proposes a data-driven $H_{\infty}$ control scheme for
time-domain constrained systems based on model predictive control formulation.
The scheme combines $H_{\infty}$ control and minimax model predictive control,
enabling more effective handling of external disturbances and time-domain
constraints. First, by leveraging input-output-disturbance data, the scheme
ensures $H_{\infty}$ performance of the closed-loop system. Then, a minimax
optimization problem is converted to a more manageable minimization problem
employing Lagrange duality, which reduces conservatism typically associated
with ellipsoidal evaluations of time-domain constraints. The study examines key
closed-loop properties, including stability, disturbance attenuation, and
constraint satisfaction, achieved by the proposed data-driven moving horizon
predictive control algorithm. The effectiveness and advantages of the proposed
method are demonstrated through numerical simulations involving a batch reactor
system, confirming its robustness and feasibility under noisy conditions.",2024-12-25T08:38:32Z,http://arxiv.org/abs/2412.18831v1,"Wenhuang Wu, Lulu Guo, Nan Li, Hong Chen"
