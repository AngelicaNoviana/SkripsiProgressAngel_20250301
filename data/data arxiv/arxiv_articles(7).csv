Title,Summary,Published,Link,Authors
LASER: A new method for locally adaptive nonparametric regression,"In this article, we introduce \textsf{LASER} (Locally Adaptive Smoothing
Estimator for Regression), a computationally efficient locally adaptive
nonparametric regression method that performs variable bandwidth local
polynomial regression. We prove that it adapts (near-)optimally to the local
H\""{o}lder exponent of the underlying regression function
\texttt{simultaneously} at all points in its domain. Furthermore, we show that
there is a single ideal choice of a global tuning parameter under which the
above mentioned local adaptivity holds. Despite the vast literature on
nonparametric regression, instances of practicable methods with provable
guarantees of such a strong notion of local adaptivity are rare. The proposed
method achieves excellent performance across a broad range of numerical
experiments in comparison to popular alternative locally adaptive methods.",2024-12-27T18:59:03Z,http://arxiv.org/abs/2412.19802v1,"Sabyasachi Chatterjee, Subhajit Goswami, Soumendu Sundar Mukherjee"
Concentration of ergotropy in many-body systems,"Ergotropy -- the maximal amount of unitarily extractable work -- measures the
``charge level'' of quantum batteries. We prove that in large many-body
batteries ergotropy exhibits a concentration of measure phenomenon. Namely, the
ergotropy of such systems is almost constant for almost all states sampled from
the Hilbert--Schmidt measure. We establish this by first proving that
ergotropy, as a function of the state, is Lipschitz-continuous with respect to
the Bures distance, and then applying Levy's measure concentration lemma. In
parallel, we showcase the analogous properties of von Neumann entropy,
compiling and adapting known results about its continuity and concentration
properties. Furthermore, we consider the situation with the least amount of
prior information about the state. This corresponds to the quantum version of
the Jeffreys prior distribution -- the Bures measure. In this case, there exist
no analytical bounds guaranteeing exponential concentration of measure.
Nonetheless, we provide numerical evidence that ergotropy, as well as von
Neumann entropy, concentrate also in this case.",2024-12-27T18:58:43Z,http://arxiv.org/abs/2412.19801v1,"Karen V. Hovhannisyan, Rick P. A. Simon, Janet Anders"
"Generalized Grade-of-Membership Estimation for High-dimensional Locally
  Dependent Data","This work focuses on the mixed membership models for multivariate categorical
data widely used for analyzing survey responses and population genetics data.
These grade of membership (GoM) models offer rich modeling power but present
significant estimation challenges for high-dimensional polytomous data. Popular
existing approaches, such as Bayesian MCMC inference, are not scalable and lack
theoretical guarantees in high-dimensional settings. To address this, we first
observe that data from this model can be reformulated as a three-way
(quasi-)tensor, with many subjects responding to many items with varying
numbers of categories. We introduce a novel and simple approach that flattens
the three-way quasi-tensor into a ""fat"" matrix, and then perform a singular
value decomposition of it to estimate parameters by exploiting the singular
subspace geometry. Our fast spectral method can accommodate a broad range of
data distributions with arbitrarily locally dependent noise, which we formalize
as the generalized-GoM models. We establish finite-sample entrywise error
bounds for the generalized-GoM model parameters. This is supported by a new
sharp two-to-infinity singular subspace perturbation theory for locally
dependent and flexibly distributed noise, a contribution of independent
interest. Simulations and applications to data in political surveys, population
genetics, and single-cell sequencing demonstrate our method's superior
performance.",2024-12-27T18:51:15Z,http://arxiv.org/abs/2412.19796v1,"Ling Chen, Chengzhu Huang, Yuqi Gu"
"g-factor theory of Si/SiGe quantum dots: spin-valley and giant
  renormalization effects","Understanding the $g$-factor physics of Si/SiGe quantum dots is crucial for
realizing high-quality spin qubits. While previous work has explained some
aspects of $g$-factor physics in idealized geometries, the results do not
extend to general cases and they miss several important features. Here, we
construct a theory that gives $g$ in terms of readily computable matrix
elements, and can be applied to all Si/SiGe heterostructures of current
interest. As a concrete example, which currently has no $g$-factor
understanding, we study the so-called Wiggle Well structure, containing Ge
concentration oscillations inside the quantum well. Here we find a significant
renormalization of the $g$-factor compared to conventional Si/SiGe quantum
wells. We also uncover a giant $g$-factor suppression of order
$\mathcal{O}(1)$, which arises due to spin-valley coupling, and occurs at
locations of low valley splitting. Our work therefore opens up new avenues for
$g$-factor engineering in Si/SiGe quantum dots.",2024-12-27T18:50:38Z,http://arxiv.org/abs/2412.19795v1,"Benjamin D. Woods, Merritt P. Losert, Robert Joynt, Mark Friesen"
InfAlign: Inference-aware language model alignment,"Language model alignment has become a critical step in training modern
generative language models. The goal of alignment is to finetune a reference
model such that the win rate of a sample from the aligned model over a sample
from the reference model is high, subject to a KL divergence constraint. Today,
we are increasingly using inference-time algorithms (e.g., Best-of-N,
controlled decoding, tree search) to decode from language models rather than
standard sampling. However, the alignment objective does not capture such
inference-time decoding procedures. We show that the existing alignment
framework is sub-optimal in view of such inference-time methods. We then modify
the alignment objective and propose a framework for inference-aware alignment
(IAPO). We prove that for any inference-time decoding algorithm, the optimal
solution that optimizes the inference-time win rate of the aligned policy
against the reference policy is the solution to the typical RLHF problem with a
transformation of the reward. This motivates us to provide the KL-regularized
calibrate-and-transform RL (CTRL) algorithm to solve this problem, which
involves a reward calibration step and a KL-regularized reward maximization
step with a transformation of the calibrated reward. We particularize our study
to two important inference-time strategies: best-of-N sampling and best-of-N
jailbreaking, where N responses are sampled from the model and the one with the
highest or lowest reward is selected. We propose specific transformations for
these strategies and demonstrate that our framework offers significant
improvements over existing state-of-the-art methods for language model
alignment. Empirically, we outperform baselines that are designed without
taking inference-time decoding into consideration by 8-12% and 4-9% on
inference-time win rates over the Anthropic helpfulness and harmlessness dialog
benchmark datasets.",2024-12-27T18:45:36Z,http://arxiv.org/abs/2412.19792v1,"Ananth Balashankar, Ziteng Sun, Jonathan Berant, Jacob Eisenstein, Michael Collins, Adrian Hutter, Jong Lee, Chirag Nagpal, Flavien Prost, Aradhana Sinha, and Ananda Theertha Suresh, Ahmad Beirami"
"Machine Learning for Sentiment Analysis of Imported Food in Trinidad and
  Tobago","This research investigates the performance of various machine learning
algorithms (CNN, LSTM, VADER, and RoBERTa) for sentiment analysis of Twitter
data related to imported food items in Trinidad and Tobago. The study addresses
three primary research questions: the comparative accuracy and efficiency of
the algorithms, the optimal configurations for each model, and the potential
applications of the optimized models in a live system for monitoring public
sentiment and its impact on the import bill. The dataset comprises tweets from
2018 to 2024, divided into imbalanced, balanced, and temporal subsets to assess
the impact of data balancing and the COVID-19 pandemic on sentiment trends. Ten
experiments were conducted to evaluate the models under various configurations.
Results indicated that VADER outperformed the other models in both multi-class
and binary sentiment classifications. The study highlights significant changes
in sentiment trends pre- and post-COVID-19, with implications for import
policies.",2024-12-27T18:25:08Z,http://arxiv.org/abs/2412.19781v1,"Cassandra Daniels, Koffka Khan"
Tensor Network Estimation of Distribution Algorithms,"Tensor networks are a tool first employed in the context of many-body quantum
physics that now have a wide range of uses across the computational sciences,
from numerical methods to machine learning. Methods integrating tensor networks
into evolutionary optimization algorithms have appeared in the recent
literature. In essence, these methods can be understood as replacing the
traditional crossover operation of a genetic algorithm with a tensor
network-based generative model. We investigate these methods from the point of
view that they are Estimation of Distribution Algorithms (EDAs). We find that
optimization performance of these methods is not related to the power of the
generative model in a straightforward way. Generative models that are better
(in the sense that they better model the distribution from which their training
data is drawn) do not necessarily result in better performance of the
optimization algorithm they form a part of. This raises the question of how
best to incorporate powerful generative models into optimization routines. In
light of this we find that adding an explicit mutation operator to the output
of the generative model often improves optimization performance.",2024-12-27T18:22:47Z,http://arxiv.org/abs/2412.19780v1,"John Gardiner, Javier Lopez-Piqueres"
"Symbolic Approximations to Ricci-flat Metrics Via Extrinsic Symmetries
  of Calabi-Yau Hypersurfaces","Ever since Yau's non-constructive existence proof of Ricci-flat metrics on
Calabi-Yau manifolds, finding their explicit construction remains a major
obstacle to development of both string theory and algebraic geometry. Recent
computational approaches employ machine learning to create novel neural
representations for approximating these metrics, offering high accuracy but
limited interpretability. In this paper, we analyse machine learning
approximations to flat metrics of Fermat Calabi-Yau n-folds and some of their
one-parameter deformations in three dimensions in order to discover their new
properties. We formalise cases in which the flat metric has more symmetries
than the underlying manifold, and prove that these symmetries imply that the
flat metric admits a surprisingly compact representation for certain choices of
complex structure moduli. We show that such symmetries uniquely determine the
flat metric on certain loci, for which we present an analytic form. We also
incorporate our theoretical results into neural networks to achieve
state-of-the-art reductions in Ricci curvature for multiple Calabi-Yau
manifolds. We conclude by distilling the ML models to obtain for the first time
closed form expressions for Kahler metrics with near-zero scalar curvature.",2024-12-27T18:19:26Z,http://arxiv.org/abs/2412.19778v1,"Viktor Mirjanić, Challenger Mishra"
"Application of normalizing flows to nuclear many-body perturbation
  theory","Many-body perturbation theory provides a powerful framework to study the
ground state and thermodynamic properties of nuclear matter as well as
associated single-particle potentials and response functions within a
systematic order-by-order expansion. However, computational challenges can
emerge beyond the lowest orders of perturbation theory, especially when
computing both single-particle potentials and response functions, which in
general are complex-valued and require Cauchy principal value calculations of
high-dimensional integrals. We demonstrate that normalizing flows are suitable
for Monte Carlo importance sampling of both regular and irregular functions
appearing in nuclear many-body calculations. Normalizing flows are a class of
machine learning models that can be used to build and sample from complicated
distributions through a bijective mapping from a simple base distribution.
Furthermore, a well-trained model for a certain target integrand can be
efficiently transferred to calculate related integrals with varying physical
conditions. These features can enable more efficient tabulations of nuclear
physics inputs to numerical simulations of supernovae and neutron star mergers
across varying physical conditions and nuclear force models.",2024-12-27T18:18:20Z,http://arxiv.org/abs/2412.19777v1,"Pengsheng Wen, Jeremy W. Holt, Albany Blackburn"
"Analysis of Premature Death Rates in Texas Counties: The Impact of Air
  Quality, Socioeconomic Factors, and COPD Prevalence","Understanding factors contributing to premature mortality is critical for
public health planning. This study examines the relationships between premature
death rates and multiple risk factors across several Texas counties, utilizing
EPA air quality data, Census information, and county health records from recent
years. We analyze the impact of air quality (PM2.5 levels), socioeconomic
factors (median household income), and health conditions (COPD prevalence)
through statistical analysis and modeling techniques. Results reveal COPD
prevalence as a strong predictor of premature death rates, with higher
prevalence associated with a substantial increase in years of potential life
lost. While socioeconomic factors show a significant negative correlation, air
quality demonstrates more complex indirect relationships. These findings
emphasize the need for integrated public health interventions that prioritize
key health conditions while addressing underlying socioeconomic disparities.",2024-12-27T18:12:04Z,http://arxiv.org/abs/2412.19774v1,"Richard Rich, Ernesto Diaz"
"On the uplift of 4D wormholes in Braneworld models and their 5D
  structure","Recent developments in the consistent embedding of general 4D static and
spherically-symmetric spacetimes in arbitrary single-brane braneworld models
[Phys.Rev.D 109 (2024) 4, L041501] initiated the program of studying the bulk
structure of braneworld wormholes. In this article, adopting a completely
generic approach, we derive the general conditions that the metric functions of
any braneworld spacetime must satisfy to describe a wormhole structure in the
bulk. Particular emphasis is placed on clarifying the proper uplift of 4D
wormholes, expressed in terms of various radial coordinates on the brane, and
we demonstrate the important role of the circumferential radius metric function
for the embedding. Additionally, the flare-out conditions for braneworld
wormholes are presented for the first time and are found to differ from the
case of flat extra dimensions. To illustrate the method, we first perform the
uplift into the Randall-Sundrum II braneworld model for three well-known 4D
wormhole spacetimes; the effective braneworld wormhole solutions of
Casadio-Fabbri-Mazzacurati and Bronnikov-Kim, and the Simpson-Visser spacetime.
Subsequently, we study their bulk features by means of curvature invariants,
flare-out conditions, energy conditions and embedding diagrams. Our analysis
reveals that the assumption of a warped extra dimension has non-trivial
implications for the structure of 5D wormholes.",2024-12-27T18:12:03Z,http://arxiv.org/abs/2412.19773v1,"Thomas Pappas, Theodoros Nakas"
"Fortran2CPP: Automating Fortran-to-C++ Migration using LLMs via
  Multi-Turn Dialogue and Dual-Agent Integration","Migrating Fortran code to C++ is a common task for many scientific computing
teams, driven by the need to leverage modern programming paradigms, enhance
cross-platform compatibility, and improve maintainability. Automating this
translation process using large language models (LLMs) has shown promise, but
the lack of high-quality, specialized datasets has hindered their
effectiveness. In this paper, we address this challenge by introducing a novel
multi-turn dialogue dataset, Fortran2CPP, specifically designed for
Fortran-to-C++ code migration. Our dataset, significantly larger than existing
alternatives, is generated using a unique LLM-driven, dual-agent pipeline
incorporating iterative compilation, execution, and code repair to ensure high
quality and functional correctness. To demonstrate the effectiveness of our
dataset, we fine-tuned several open-weight LLMs on Fortran2CPP and evaluated
their performance on two independent benchmarks. Fine-tuning on our dataset led
to remarkable gains, with models achieving up to a 3.31x increase in CodeBLEU
score and a 92\% improvement in compilation success rate. This highlights the
dataset's ability to enhance both the syntactic accuracy and compilability of
the translated C++ code. Our dataset and model have been open-sourced and are
available on our public GitHub
repository\footnote{\url{https://github.com/HPC-Fortran2CPP/Fortran2Cpp}}.",2024-12-27T18:06:25Z,http://arxiv.org/abs/2412.19770v1,"Le Chen, Bin Lei, Dunzhi Zhou, Pei-Hung Lin, Chunhua Liao, Caiwen Ding, Ali Jannesari"
"Teaching materials aligned or unaligned with the principles of the
  Cognitive Theory of Multimedia Learning: the choices made by Physics teachers
  and students","In a recent study [Rev. Bras. Ens. F\'is. vol. 45, 2023], the absence of the
Cognitive Theory of Multimedia Learning (CTML) in the curricula of Physics
teacher education programs at Brazilian public universities was highlighted.
Considering this gap, the present study investigates whether, even without any
formal prior knowledge of CTML principles (Coherence, Signaling, Spatial
Contiguity, Segmentation, Multimedia, and Personalization), Physics teacher
trainees and educators tend to choose, among two formats of multimedia
materials - one aligned with a given CTML principle and the other not - the
materials aligned with these principles. The findings of this case study
revealed that, although most participants generally selected materials aligned
with the mentioned principles, a significant portion did not. These results
underscore the importance of Brazilian universities considering the inclusion
of CTML in Physics teacher education curricula.",2024-12-27T18:02:16Z,http://arxiv.org/abs/2412.19768v1,"Aline N. Braga, Antonio A. M. Neto, Alessandra N. Braga, Silvio C. F. Pereira Filho, Nelson P. C. de Souza, Danilo T. Alves"
"From Ceilings to Walls: Universal Dynamic Perching of Small Aerial
  Robots on Surfaces with Variable Orientations","This work demonstrates universal dynamic perching capabilities for quadrotors
of various sizes and on surfaces with different orientations. By employing a
non-dimensionalization framework and deep reinforcement learning, we
systematically assessed how robot size and surface orientation affect landing
capabilities. We hypothesized that maintaining geometric proportions across
different robot scales ensures consistent perching behavior, which was
validated in both simulation and experimental tests. Additionally, we
investigated the effects of joint stiffness and damping in the landing gear on
perching behaviors and performance. While joint stiffness had minimal impact,
joint damping ratios influenced landing success under vertical approaching
conditions. The study also identified a critical velocity threshold necessary
for successful perching, determined by the robot's maneuverability and leg
geometry. Overall, this research advances robotic perching capabilities,
offering insights into the role of mechanical design and scaling effects, and
lays the groundwork for future drone autonomy and operational efficiency in
unstructured environments.",2024-12-27T17:53:01Z,http://arxiv.org/abs/2412.19765v1,"Bryan Habas, Aaron Brown, Donghyeon Lee, Mitchell Goldman, Bo Cheng"
"Multi-population Differential Evolution for RSS based Cooperative
  Localization in Wireless Sensor Networks with Limited Communication Range","This paper presents a novel approach to deal with the cooperative
localization problem in wireless sensor networks based on received signal
strength measurements. In cooperative scenarios, the cost function of the
localization problem becomes increasingly nonlinear and nonconvex due to the
heightened interaction between sensor nodes, making the estimation of the
positions of the target nodes more challenging. Although most of existing
cooperative localization algorithms assure acceptable localization accuracy,
their computational complexity increases dramatically, which may restrict their
applicability. To reduce the computational complexity and provide competitive
localization accuracy at the same time, we propose a localization algorithm
based on the differential evolution with multiple populations, opposite-based
learning, redirection, and anchoring. In this work, the cooperative
localization cost function is split into several simpler cost functions, each
of which accounts only for one individual target node. Then, each cost function
is solved by a dedicated population of the proposed algorithm. In addition, an
enhanced version of the proposed algorithm which incorporates the population
midpoint scheme for further improvement in the localization accuracy is
devised. Simulation results demonstrate that the proposed algorithms provide
comparative localization accuracy with much lower computational complexity
compared with the state-of-the-art algorithms.",2024-12-27T17:45:10Z,http://arxiv.org/abs/2412.19763v1,"Lismer Andres Caceres Najarro, Iickho Song, Muhammad Salman, Kiseon Kim"
"Enhancing Cognitive Diagnosis by Modeling Learner Cognitive Structure
  State","Cognitive diagnosis represents a fundamental research area within intelligent
education, with the objective of measuring the cognitive status of individuals.
Theoretically, an individual's cognitive state is essentially equivalent to
their cognitive structure state. Cognitive structure state comprises two key
components: knowledge state (KS) and knowledge structure state (KUS). The
knowledge state reflects the learner's mastery of individual concepts, a widely
studied focus within cognitive diagnosis. In contrast, the knowledge structure
state-representing the learner's understanding of the relationships between
concepts-remains inadequately modeled. A learner's cognitive structure is
essential for promoting meaningful learning and shaping academic performance.
Although various methods have been proposed, most focus on assessing KS and
fail to assess KUS. To bridge this gap, we propose an innovative and effective
framework-CSCD (Cognitive Structure State-based Cognitive Diagnosis)-which
introduces a novel framework to modeling learners' cognitive structures in
diagnostic assessments, thereby offering new insights into cognitive structure
modeling. Specifically, we employ an edge-feature-based graph attention network
to represent the learner's cognitive structure state, effectively integrating
KS and KUS. Extensive experiments conducted on real datasets demonstrate the
superior performance of this framework in terms of diagnostic accuracy and
interpretability.",2024-12-27T17:41:39Z,http://arxiv.org/abs/2412.19759v1,"Zhifu Chen, Hengnian Gu, Jin Peng Zhou, Dongdai Zhou"
"""Did my figure do justice to the answer?"" : Towards Multimodal Short
  Answer Grading with Feedback (MMSAF)","Personalized feedback plays a vital role in a student's learning process.
While existing systems are adept at providing feedback over MCQ-based
evaluation, this work focuses more on subjective and open-ended questions,
which is similar to the problem of Automatic Short Answer Grading (ASAG) with
feedback. Additionally, we introduce the Multimodal Short Answer grading with
Feedback (MMSAF) problem over the traditional ASAG feedback problem to address
the scenario where the student answer and reference answer might contain
images. Moreover, we introduce the MMSAF dataset with 2197 data points along
with an automated framework for generating such data sets. Our evaluations on
existing LLMs over this dataset achieved an overall accuracy of 55\% on Level
of Correctness labels, 75\% on Image Relevance labels and a score of 4.27 out
of 5 in correctness level of LLM generated feedback as rated by experts. As per
experts, Pixtral achieved a rating of above 4 out of all metrics, indicating
that it is more aligned to human judgement, and that it is the best solution
for assisting students.",2024-12-27T17:33:39Z,http://arxiv.org/abs/2412.19755v1,"Pritam Sil, Bhaskaran Raman, Pushpak Bhattacharyya"
"IMAGINE: An 8-to-1b 22nm FD-SOI Compute-In-Memory CNN Accelerator With
  an End-to-End Analog Charge-Based 0.15-8POPS/W Macro Featuring
  Distribution-Aware Data Reshaping","Charge-domain compute-in-memory (CIM) SRAMs have recently become an enticing
compromise between computing efficiency and accuracy to process sub-8b
convolutional neural networks (CNNs) at the edge. Yet, they commonly make use
of a fixed dot-product (DP) voltage swing, which leads to a loss in effective
ADC bits due to data-dependent clipping or truncation effects that waste
precious conversion energy and computing accuracy. To overcome this, we present
IMAGINE, a workload-adaptive 1-to-8b CIM-CNN accelerator in 22nm FD-SOI. It
introduces a 1152x256 end-to-end charge-based macro with a multi-bit DP based
on an input-serial, weight-parallel accumulation that avoids power-hungry DACs.
An adaptive swing is achieved by combining a channel-wise DP array split with a
linear in-ADC implementation of analog batch-normalization (ABN), obtaining a
distribution-aware data reshaping. Critical design constraints are relaxed by
including the post-silicon equivalent noise within a CIM-aware CNN training
framework. Measurement results showcase an 8b system-level energy efficiency of
40TOPS/W at 0.3/0.6V, with competitive accuracies on MNIST and CIFAR-10.
Moreover, the peak energy and area efficiencies of the 187kB/mm2 macro
respectively reach up to 0.15-8POPS/W and 2.6-154TOPS/mm2, scaling with the
8-to-1b computing precision. These results exceed previous charge-based designs
by 3-to-5x while being the first work to provide linear in-memory rescaling.",2024-12-27T17:18:15Z,http://arxiv.org/abs/2412.19750v1,"Adrian Kneip, Martin Lefebvre, Pol Maistriaux, David Bol"
"Enhancing Adversarial Robustness of Deep Neural Networks Through
  Supervised Contrastive Learning","Adversarial attacks exploit the vulnerabilities of convolutional neural
networks by introducing imperceptible perturbations that lead to
misclassifications, exposing weaknesses in feature representations and decision
boundaries. This paper presents a novel framework combining supervised
contrastive learning and margin-based contrastive loss to enhance adversarial
robustness. Supervised contrastive learning improves the structure of the
feature space by clustering embeddings of samples within the same class and
separating those from different classes. Margin-based contrastive loss,
inspired by support vector machines, enforces explicit constraints to create
robust decision boundaries with well-defined margins. Experiments on the
CIFAR-100 dataset with a ResNet-18 backbone demonstrate robustness performance
improvements in adversarial accuracy under Fast Gradient Sign Method attacks.",2024-12-27T17:14:52Z,http://arxiv.org/abs/2412.19747v1,"Longwei Wang, Navid Nayyem, Abdullah Rakin"
Pulsed laser as a continuous particle stream,"With the recently introduced particle interpretation of the double-slit
experiment [arXiv:2112.05512 (2024)], all interference phenomena can be
reinterpreted in terms of particle states that couple (bright) or do not couple
(dark) with the detectors. Here, we extend the two-mode analysis to multiple
modes, particularly mode-locked pulsed lasers, and show that collective
particle states of multiple light modes feature a significantly larger number
of dark than bright states. By linking interference in time-independent
multi-slit experiments (same-mode frequencies) to time-dependent multimode
experiments (different frequencies), we demonstrate that pulsed laser light
comprises a continuous photon beam, with photons forming bright states during
pulses and dark states between them. To confirm this, we analyzed mode-locked
lasers, finding that the ratio of the number of bright to dark states aligns
with the pulse-to-interval duration ratio.",2024-12-27T17:14:41Z,http://arxiv.org/abs/2412.19746v1,"Ciro Micheletti Diniz, Franciele Renata Henrique, Bruno Santos de Souza, Lino Misoguti, Paulo Henrique Dias Ferreira, Celso Jorge Villas Bôas"
IR Bounds on Theories with Spontaneously-Broken Lorentz Symmetry,"In nature, some UV features of the dynamics are reflected in IR quantities.
In fully relativistic theories, this connection can be probed through the
analyticity properties of scattering amplitudes, allowing to understand which
IR theories respect the UV assumptions of quantum field theory. The ensuing
analyticity bounds can be usually rephrased as the absence of faster-than-light
propagation for low-energy excitations. While it is interesting to understand
these relations and their IR characterization for theories that have less
idealized properties, it is also more difficult to derive analyticity bounds in
these cases. For theories that spontaneously break Lorentz symmetry, recent
progress was made by considering correlators of conserved currents and their
analyticity properties. In this work, we focus on such theories and close the
gap from the IR side, deriving bounds from the speed of propagation that are
equivalent to the known analyticity bounds. Our bounds require that gapped
excitations have a slower speed than gapless ones, at least for momenta that
are low with respect to the mass gap. These results suggest a way to study the
UV/IR connection in more complex theories.",2024-12-27T17:14:14Z,http://arxiv.org/abs/2412.19745v1,"Francesco Serra, Leonardo G. Trombetta"
"AAM-SEALS: Developing Aerial-Aquatic Manipulators in SEa, Air, and Land
  Simulator","Current simulators lack the ability to accurately model integrated
environments that encompass sea, air, and land. To address this gap, we
introduce Aerial-Aquatic Manipulators (AAMs) in SEa, Air, and Land Simulator
(SEALS), a comprehensive and photorealistic simulator designed for AAMs to
operate and learn in these diverse environments. The development of AAM-SEALS
tackles several significant challenges, including the creation of integrated
controllers for flying, swimming, and manipulation, and the high-fidelity
simulation of aerial dynamics and hydrodynamics leveraging particle physics.
Our evaluation demonstrates smooth operation and photorealistic transitions
across air, water, and their interfaces. We quantitatively validate the
fidelity of particle-based hydrodynamics by comparing position-tracking errors
across real-world and simulated systems. AAM-SEALS promises to benefit a broad
range of robotics communities, including robot learning, aerial robotics,
underwater robotics, mobile manipulation, and robotic simulators. We will
open-source our code and data to foster the advancement of research in these
fields. Please access our project website at: https:
//aam-seals.github.io/aam-seals-v1/",2024-12-27T17:13:14Z,http://arxiv.org/abs/2412.19744v1,"William Wang Yang, Karthikeya Kona, Yashveer Jain, Abhinav Bhamidipati, Tomer Atzili, Xiaomin Lin, Yantian Zha"
"Adaptive Context-Aware Multi-Path Transmission Control for VR/AR
  Content: A Deep Reinforcement Learning Approach","This paper introduces the Adaptive Context-Aware Multi-Path Transmission
Control Protocol (ACMPTCP), an efficient approach designed to optimize the
performance of Multi-Path Transmission Control Protocol (MPTCP) for
data-intensive applications such as augmented and virtual reality (AR/VR)
streaming. ACMPTCP addresses the limitations of conventional MPTCP by
leveraging deep reinforcement learning (DRL) for agile end-to-end path
management and optimal bandwidth allocation, facilitating path realignment
across diverse network environments.",2024-12-27T16:56:12Z,http://arxiv.org/abs/2412.19737v1,"Shakil Ahmed, Saifur Rahman Sabuj, Ashfaq Khokhar"
"Periodically and aperiodically Thue-Morse driven long-range systems:
  from dynamical localization to slow dynamics","We investigate the electric-field driven power-law random banded
matrix(PLRBM) model where a variation in the power-law exponent $\alpha$ yields
a delocalization-to-localization phase transition. We examine the periodically
driven PLRBM model with the help of the Floquet operator. The level spacing
ratio and the generalized participation ratio of the Floquet Hamiltonian reveal
a drive-induced fractal phase accompanied by diffusive transport on the
delocalized side of the undriven PLRBM model. On the localized side, the
time-periodic model remains localized - the average spacing ratio corresponds
to Poisson statistics and logarithmic transport is observed in the dynamics.
Extending our analysis to the aperiodic Thue-Morse (TM) driven system, we find
that the aperiodically driven clean long-range hopping model (clean counterpart
of the PLRBM model) exhibits the phenomenon of \textit{exact dynamical
localization} (EDL) on tuning the drive-parameters at special points. The
disordered time-aperiodic system shows diffusive transport followed by
relaxation to the infinite-temperature state on the delocalized side, and a
prethermal plateau with subdiffusion on the localized side. Additionally, we
compare this with a quasi-periodically driven AAH model that also undergoes a
localization-delocalization transition. Unlike the disordered long-range model,
it features a prolonged prethermal plateau followed by subdiffusion to the
infinite temperature state, even on the delocalized side.",2024-12-27T16:55:47Z,http://arxiv.org/abs/2412.19736v1,"Vatsana Tiwari, Devendra Singh Bhakuni, Auditya Sharma"
"A General Framework of Brain Region Detection And Genetic Variants
  Selection in Imaging Genetics","Imaging genetics is a growing field that employs structural or functional
neuroimaging techniques to study individuals with genetic risk variants
potentially linked to specific illnesses. This area presents considerable
challenges to statisticians due to the heterogeneous information and different
data forms it involves. In addition, both imaging and genetic data are
typically high-dimensional, creating a ""big data squared"" problem. Moreover,
brain imaging data contains extensive spatial information. Simply vectorizing
tensor images and treating voxels as independent features can lead to
computational issues and disregard spatial structure. This paper presents a
novel statistical method for imaging genetics modeling while addressing all
these challenges. We explore a Canonical Correlation Analysis based linear
model for the joint modeling of brain imaging, genetic information, and
clinical phenotype, enabling the simultaneous detection of significant brain
regions and selection of important genetic variants associated with the
phenotype outcome. Scalable algorithms are developed to tackle the ""big data
squared"" issue. We apply the proposed method to explore the reaction speed, an
indicator of cognitive functions, and its associations with brain MRI and
genetic factors using the UK Biobank database. Our study reveals a notable
connection between the caudate nucleus region of brain and specific significant
SNPs, along with their respective regulated genes, and the reaction speed.",2024-12-27T16:54:11Z,http://arxiv.org/abs/2412.19735v1,"Siqiang Su, Zhenghao Li, Long Feng, Ting Li"
"Dynamics, data and reconstruction","Data-driven learning is prevalent in many fields of science, mathematics and
engineering. The goal of data-driven learning of dynamical systems is to
interpret timeseries as a continuous observation of an underlying dynamical
system. This task is not well-posed for a variety of reasons. A dynamical
system may have multiple sub-systems co-existing within it. The nature of the
dataset depends on the portion of the phase space being viewed, and may thus my
confined to a sub-system. Secondly these sub-systems may be topologically
inter-weaved, so may be inseparable computationally. Thirdly, two timeseries
sampled separately from different dynamical systems may be close or even
indistinguishable. So there is no unqiue source for the timeseries. We show how
these ambiguities are circumvented if one considers dynamical systems and
measurement maps collectively. This is made possible in a category theoretical
framework, in which reconstruction is unique up to equivalences. We introduce
two categories of observed dynamical systems and timeseries-data. These are
related to the well known category of dynamical systems via functors. This
enables a functorial interpretation of the task of reconstruction as well.",2024-12-27T16:49:52Z,http://arxiv.org/abs/2412.19734v1,"Suddhasattwa Das, Tomoharu Suda"
"Generative Pretrained Embedding and Hierarchical Irregular Time Series
  Representation for Daily Living Activity Recognition","Within the evolving landscape of smart homes, the precise recognition of
daily living activities using ambient sensor data stands paramount. This paper
not only aims to bolster existing algorithms by evaluating two distinct
pretrained embeddings suited for ambient sensor activations but also introduces
a novel hierarchical architecture. We delve into an architecture anchored on
Transformer Decoder-based pre-trained embeddings, reminiscent of the GPT
design, and contrast it with the previously established state-of-the-art (SOTA)
ELMo embeddings for ambient sensors. Our proposed hierarchical structure
leverages the strengths of each pre-trained embedding, enabling the discernment
of activity dependencies and sequence order, thereby enhancing classification
precision. To further refine recognition, we incorporate into our proposed
architecture an hour-of-the-day embedding. Empirical evaluations underscore the
preeminence of the Transformer Decoder embedding in classification endeavors.
Additionally, our innovative hierarchical design significantly bolsters the
efficacy of both pre-trained embeddings, notably in capturing inter-activity
nuances. The integration of temporal aspects subtly but distinctively augments
classification, especially for time-sensitive activities. In conclusion, our
GPT-inspired hierarchical approach, infused with temporal insights, outshines
the SOTA ELMo benchmark.",2024-12-27T16:43:52Z,http://arxiv.org/abs/2412.19732v1,"Damien Bouchabou, Sao Mai Nguyen"
"Fully-relativistic evolution of vacuum tensor inhomogeneities during
  inflation","We present a complete method for the initialisation and extraction of
first-order inflationary tensor perturbations for fully relativistic
simulations which incorporate gravitational back-reaction. We outline a
correspondence between the Cosmological Perturbation Theory (CPT) framework and
the numerical relativity BSSN variables in the appropriate limit. We describe a
generation method for stochastic tensoral initial conditions, inspired by the
standard scalar initial condition used from inflation and implemented in
lattice cosmology. We discuss the implementation of this procedure in the
GRChombo/GRTeclyn code, and demonstrate the detailed quantitative
correspondence between the linearised and fully-nonlinear solutions in the
perturbative limit, through the evolution of the background and the tensor
power spectrum. We also validate the methodology by showing that energy and
momentum constraints are introduced and preserved to second-order or better. We
provide some preliminary indicative results probing tensoral non-Gaussianity
using the skewness and kurtosis. The computational pipeline presented here will
be used to study the emergence of a primordial tensor bispectra and
cross-spectra that incorporate the effect of nonlinear gravitational couplings
with the metric, which has potential applications for the analysis of
next-generation CMB surveys.",2024-12-27T16:42:59Z,http://arxiv.org/abs/2412.19731v1,"Ericka Florio, E. Paul S. Shellard"
"Learning to Forget: Bayesian Time Series Forecasting using Recurrent
  Sparse Spectrum Signature Gaussian Processes","The signature kernel is a kernel between time series of arbitrary length and
comes with strong theoretical guarantees from stochastic analysis. It has found
applications in machine learning such as covariance functions for Gaussian
processes. A strength of the underlying signature features is that they provide
a structured global description of a time series. However, this property can
quickly become a curse when local information is essential and forgetting is
required; so far this has only been addressed with ad-hoc methods such as
slicing the time series into subsegments. To overcome this, we propose a
principled, data-driven approach by introducing a novel forgetting mechanism
for signatures. This allows the model to dynamically adapt its context length
to focus on more recent information. To achieve this, we revisit the recently
introduced Random Fourier Signature Features, and develop Random Fourier
Decayed Signature Features (RFDSF) with Gaussian processes (GPs). This results
in a Bayesian time series forecasting algorithm with variational inference,
that offers a scalable probabilistic algorithm that processes and transforms a
time series into a joint predictive distribution over time steps in one pass
using recurrence. For example, processing a sequence of length $10^4$ steps in
$\approx 10^{-2}$ seconds and in $&lt; 1\text{GB}$ of GPU memory. We demonstrate
that it outperforms other GP-based alternatives and competes with
state-of-the-art probabilistic time series forecasting algorithms.",2024-12-27T16:31:09Z,http://arxiv.org/abs/2412.19727v1,"Csaba Tóth, Masaki Adachi, Michael A. Osborne, Harald Oberhauser"
EEG-Reptile: An Automatized Reptile-Based Meta-Learning Library for BCIs,"Meta-learning, i.e., ""learning to learn"", is a promising approach to enable
efficient BCI classifier training with limited amounts of data. It can
effectively use collections of in some way similar classification tasks, with
rapid adaptation to new tasks where only minimal data are available. However,
applying meta-learning to existing classifiers and BCI tasks requires
significant effort. To address this issue, we propose EEG-Reptile, an automated
library that leverages meta-learning to improve classification accuracy of
neural networks in BCIs and other EEG-based applications. It utilizes the
Reptile meta-learning algorithm to adapt neural network classifiers of EEG data
to the inter-subject domain, allowing for more efficient fine-tuning for a new
subject on a small amount of data. The proposed library incorporates an
automated hyperparameter tuning module, a data management pipeline, and an
implementation of the Reptile meta-learning algorithm. EEG-Reptile automation
level allows using it without deep understanding of meta-learning. We
demonstrate the effectiveness of EEG-Reptile on two benchmark datasets (BCI IV
2a, Lee2019 MI) and three neural network architectures (EEGNet, FBCNet,
EEG-Inception). Our library achieved improvement in both zero-shot and few-shot
learning scenarios compared to traditional transfer learning approaches.",2024-12-27T16:24:31Z,http://arxiv.org/abs/2412.19725v1,"Daniil A. Berdyshev, Artem M. Grachev, Sergei L. Shishkin, Bogdan L. Kozyrskiy"
"OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse
  Task Synthesis","Graphical User Interface (GUI) agents powered by Vision-Language Models
(VLMs) have demonstrated human-like computer control capability. Despite their
utility in advancing digital automation, a critical bottleneck persists:
collecting high-quality trajectory data for training. Common practices for
collecting such data rely on human supervision or synthetic data generation
through executing pre-defined tasks, which are either resource-intensive or
unable to guarantee data quality. Moreover, these methods suffer from limited
data diversity and significant gaps between synthetic data and real-world
environments. To address these challenges, we propose OS-Genesis, a novel GUI
data synthesis pipeline that reverses the conventional trajectory collection
process. Instead of relying on pre-defined tasks, OS-Genesis enables agents
first to perceive environments and perform step-wise interactions, then
retrospectively derive high-quality tasks to enable trajectory-level
exploration. A trajectory reward model is then employed to ensure the quality
of the generated trajectories. We demonstrate that training GUI agents with
OS-Genesis significantly improves their performance on highly challenging
online benchmarks. In-depth analysis further validates OS-Genesis's efficiency
and its superior data quality and diversity compared to existing synthesis
methods. Our codes, data, and checkpoints are available at
\href{https://qiushisun.github.io/OS-Genesis-Home/}{OS-Genesis Homepage}.",2024-12-27T16:21:58Z,http://arxiv.org/abs/2412.19723v1,"Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, Ben Kao, Guohao Li, Junxian He, Yu Qiao, Zhiyong Wu"
Sharpening Neural Implicit Functions with Frequency Consolidation Priors,"Signed Distance Functions (SDFs) are vital implicit representations to
represent high fidelity 3D surfaces. Current methods mainly leverage a neural
network to learn an SDF from various supervisions including signed distances,
3D point clouds, or multi-view images. However, due to various reasons
including the bias of neural network on low frequency content, 3D unaware
sampling, sparsity in point clouds, or low resolutions of images, neural
implicit representations still struggle to represent geometries with high
frequency components like sharp structures, especially for the ones learned
from images or point clouds. To overcome this challenge, we introduce a method
to sharpen a low frequency SDF observation by recovering its high frequency
components, pursuing a sharper and more complete surface. Our key idea is to
learn a mapping from a low frequency observation to a full frequency coverage
in a data-driven manner, leading to a prior knowledge of shape consolidation in
the frequency domain, dubbed frequency consolidation priors. To better
generalize a learned prior to unseen shapes, we introduce to represent
frequency components as embeddings and disentangle the embedding of the low
frequency component from the embedding of the full frequency component. This
disentanglement allows the prior to generalize on an unseen low frequency
observation by simply recovering its full frequency embedding through a
test-time self-reconstruction. Our evaluations under widely used benchmarks or
real scenes show that our method can recover high frequency component and
produce more accurate surfaces than the latest methods. The code, data, and
pre-trained models are available at \url{https://github.com/chenchao15/FCP}.",2024-12-27T16:18:46Z,http://arxiv.org/abs/2412.19720v1,"Chao Chen, Yu-Shen Liu, Zhizhong Han"
"Text2Insight: Transform natural language text into insights seamlessly
  using multi-model architecture","The growing demand for dynamic, user-centric data analysis and visualization
is evident across domains like healthcare, finance, and research. Traditional
visualization tools often fail to meet individual user needs due to their
static and predefined nature. To address this gap, Text2Insight is introduced
as an innovative solution that delivers customized data analysis and
visualizations based on user-defined natural language requirements. Leveraging
a multi-model architecture, Text2Insight transforms user inputs into actionable
insights and dynamic visualizations.
  The methodology begins with analyzing the input dataset to extract structural
details such as columns and values. A pre-trained Llama3 model converts the
user's natural language query into an SQL query, which is further refined using
a Named Entity Recognition (NER) model for accuracy. A chart predictor
determines the most suitable visualization type, while the Llama3 model
generates insights based on the SQL query's results. The output is a
user-friendly and visually informative chart. To enhance analysis capabilities,
the system integrates a question-answering model and a predictive model using
the BERT framework. These models provide insights into historical data and
predict future trends.
  Performance evaluation of Text2Insight demonstrates its effectiveness,
achieving high accuracy (99%), precision (100%), recall (99%), and F1-score
(99%), with a BLEU score of 0.5. The question-answering model attained an
accuracy of 89% and the predictive model achieved 70% accuracy. These results
validate Text2Insight as a robust and viable solution for transforming natural
language text into dynamic, user-specific data analysis and visualizations.",2024-12-27T16:17:22Z,http://arxiv.org/abs/2412.19718v1,Pradeep Sain
"ProKAN: Progressive Stacking of Kolmogorov-Arnold Networks for Efficient
  Liver Segmentation","The growing need for accurate and efficient 3D identification of tumors,
particularly in liver segmentation, has spurred considerable research into deep
learning models. While many existing architectures offer strong performance,
they often face challenges such as overfitting and excessive computational
costs. An adjustable and flexible architecture that strikes a balance between
time efficiency and model complexity remains an unmet requirement. In this
paper, we introduce proKAN, a progressive stacking methodology for
Kolmogorov-Arnold Networks (KANs) designed to address these challenges. Unlike
traditional architectures, proKAN dynamically adjusts its complexity by
progressively adding KAN blocks during training, based on overfitting behavior.
This approach allows the network to stop growing when overfitting is detected,
preventing unnecessary computational overhead while maintaining high accuracy.
Additionally, proKAN utilizes KAN's learnable activation functions modeled
through B-splines, which provide enhanced flexibility in learning complex
relationships in 3D medical data. Our proposed architecture achieves
state-of-the-art performance in liver segmentation tasks, outperforming
standard Multi-Layer Perceptrons (MLPs) and fixed KAN architectures. The
dynamic nature of proKAN ensures efficient training times and high accuracy
without the risk of overfitting. Furthermore, proKAN provides better
interpretability by allowing insight into the decision-making process through
its learnable coefficients. The experimental results demonstrate a significant
improvement in accuracy, Dice score, and time efficiency, making proKAN a
compelling solution for 3D medical image segmentation tasks.",2024-12-27T16:14:06Z,http://arxiv.org/abs/2412.19713v1,"Bhavesh Gyanchandani, Aditya Oza, Abhinav Roy"
"Causal machine learning for heterogeneous treatment effects in the
  presence of missing outcome data","When estimating heterogeneous treatment effects, missing outcome data can
complicate treatment effect estimation, causing certain subgroups of the
population to be poorly represented. In this work, we discuss this commonly
overlooked problem and consider the impact that missing at random (MAR) outcome
data has on causal machine learning estimators for the conditional average
treatment effect (CATE). We then propose two de-biased machine learning
estimators for the CATE, the mDR-learner and mEP-learner, which address the
issue of under-representation by integrating inverse probability of censoring
weights into the DR-learner and EP-learner respectively. We show that under
reasonable conditions, these estimators are oracle efficient, and illustrate
their favorable performance through simulated data settings, comparing them to
existing CATE estimators, including comparison to estimators which use common
missing data techniques. Guidance on the implementation of these estimators is
provided and we present an example of their application using the ACTG175
trial, exploring treatment effect heterogeneity when comparing Zidovudine
mono-therapy against alternative antiretroviral therapies among HIV-1-infected
individuals.",2024-12-27T16:10:03Z,http://arxiv.org/abs/2412.19711v1,"Matthew Pryce, Karla Diaz-Ordaz, Ruth H. Keogh, Stijn Vansteelandt"
High precision spectroscopy of trilobite Rydberg molecules,"We perform three-photon photoassociation to obtain high resolution spectra of
$^{87}$Rb trilobite dimers for the principal quantum numbers $n = 22,24,25,26$,
and 27. The large binding energy of the molecules in combination with a
relative spectroscopic resolution of $10^{-4}$ provides a rigorous benchmark
for existing theoretical models. A recently developed Green's function
framework, which circumvents the convergence issues that afflicted previous
studies,, is employed to theoretically reproduce the vibrational spectrum of
the molecule with high accuracy. The relatively large molecular binding energy
are primarily determined by the low energy $S$-wave electron-atom scattering
length, thereby allowing us to extract the $^3S_1$ scattering phase shift with
unprecedented accuracy, at low energy regimes inaccessible to free electrons.",2024-12-27T16:04:17Z,http://arxiv.org/abs/2412.19710v1,"Markus Exner, Rohan Srikumar, Richard Blättner, Matthew T. Eiles, Peter Schmelcher, Herwig Ott"
Toward Adaptive Reasoning in Large Language Models with Thought Rollback,"Large language models (LLMs) have been routinely used to solve various tasks
using step-by-step reasoning. However, the structure of intermediate reasoning
steps, or thoughts, is rigid and unidirectional, such as chains, trees, or
acyclic-directed graphs. Consequently, the resulting inflexible and
forward-only reasoning may not address challenging tasks and fail when the LLM
frequently gives false responses, i.e., ``hallucinations''. This paper proposes
a new reasoning framework, called Thought Rollback (TR), allowing LLMs to
adaptively build thought structure while maintaining effective reasoning toward
problem-solving under ``hallucinations''. The core mechanism of TR is rolling
back thoughts, which allows LLMs to perform error analysis on thoughts, and
thus roll back to any previously mistaken thought for revision. Subsequently,
by including such trial-and-error in the prompt to guide the LLM, each rollback
leads to one more reliable reasoning path. Therefore, starting with a simple
prompt without human annotations, LLM with TR adaptively and gradually explores
thoughts for a correct solution. Comprehensive experiments on mathematical
problems and multi-task reasoning demonstrate the state-of-the-art performance
of TR in terms of problem-solving rate and interaction cost. For instance, the
solving rate of GPT-4 with TR outperforms the current best by $9\%$ on the MATH
dataset.",2024-12-27T16:02:34Z,http://arxiv.org/abs/2412.19707v1,"Sijia Chen, Baochun Li"
"An Integrated Optimization and Deep Learning Pipeline for Predicting
  Live Birth Success in IVF Using Feature Optimization and Transformer-Based
  Models","In vitro fertilization (IVF) is a widely utilized assisted reproductive
technology, yet predicting its success remains challenging due to the
multifaceted interplay of clinical, demographic, and procedural factors. This
study develops a robust artificial intelligence (AI) pipeline aimed at
predicting live birth outcomes in IVF treatments. The pipeline uses anonymized
data from 2010 to 2018, obtained from the Human Fertilization and Embryology
Authority (HFEA). We evaluated the prediction performance of live birth success
as a binary outcome (success/failure) by integrating different feature
selection methods, such as principal component analysis (PCA) and particle
swarm optimization (PSO), with different traditional machine learning-based
classifiers including random forest (RF) and decision tree, as well as deep
learning-based classifiers including custom transformer-based model and a tab
transformer model with an attention mechanism. Our research demonstrated that
the best performance was achieved by combining PSO for feature selection with
the TabTransformer-based deep learning model, yielding an accuracy of 99.50%
and an AUC of 99.96%, highlighting its significant performance to predict live
births. This study establishes a highly accurate AI pipeline for predicting
live birth outcomes in IVF, demonstrating its potential to enhance personalized
fertility treatments.",2024-12-27T15:46:59Z,http://arxiv.org/abs/2412.19696v1,"Arezoo Borji, Hossam Haick, Birgit Pohn, Antonia Graf, Jana Zakall, S M Ragib Shahriar Islam, Gernot Kronreif, Daniel Kovatchki, Heinz Strohmer, Sepideh Hatamikia"
"From prediction to explanation: managing influential negative reviews
  through explainable AI","The profound impact of online reviews on consumer decision-making has made it
crucial for businesses to manage negative reviews. Recent advancements in
artificial intelligence (AI) technology have offered businesses novel and
effective ways to manage and analyze substantial consumer feedback. In response
to the growing demand for explainablility and transparency in AI applications,
this study proposes a novel explainable AI (XAI) algorithm aimed at identifying
influential negative reviews. The experiments conducted on 101,338 restaurant
reviews validate the algorithm's effectiveness and provides understandable
explanations from both the feature-level and word-level perspectives. By
leveraging this algorithm, businesses can gain actionable insights for
predicting, perceiving, and strategically responding to online negative
feedback, fostering improved customer service and mitigating the potential
damage caused by negative reviews.",2024-12-27T15:37:19Z,http://arxiv.org/abs/2412.19692v1,Rongping Shen
"A Review on the Integration of Artificial Intelligence and Medical
  Imaging in IVF Ovarian Stimulation","Artificial intelligence (AI) has emerged as a powerful tool to enhance
decision-making and optimize treatment protocols in in vitro fertilization
(IVF). In particular, AI shows significant promise in supporting
decision-making during the ovarian stimulation phase of the IVF process. This
review evaluates studies focused on the applications of AI combined with
medical imaging in ovarian stimulation, examining methodologies, outcomes, and
current limitations. Our analysis of 13 studies on this topic reveals that,
reveal that while AI algorithms demonstrated notable potential in predicting
optimal hormonal dosages, trigger timing, and oocyte retrieval outcomes, the
medical imaging data utilized predominantly came from two-dimensional (2D)
ultrasound which mainly involved basic quantifications, such as follicle size
and number, with limited use of direct feature extraction or advanced image
analysis techniques. This points to an underexplored opportunity where advanced
image analysis approaches, such as deep learning, and more diverse imaging
modalities, like three-dimensional (3D) ultrasound, could unlock deeper
insights. Additionally, the lack of explainable AI (XAI) in most studies raises
concerns about the transparency and traceability of AI-driven decisions - key
factors for clinical adoption and trust. Furthermore, many studies relied on
single-center designs and small datasets, which limit the generalizability of
their findings. This review highlights the need for integrating advanced
imaging analysis techniques with explainable AI methodologies, as well as the
importance of leveraging multicenter collaborations and larger datasets.
Addressing these gaps has the potential to enhance ovarian stimulation
management, paving the way for efficient, personalized, and data-driven
treatment pathways that improve IVF outcomes.",2024-12-27T15:29:08Z,http://arxiv.org/abs/2412.19688v1,"Jana Zakall, Birgit Pohn, Antonia Graf, Daniel Kovatchki, Arezoo Borji, Ragib Shahriar Islam, Hossam Haick, Heinz Strohmer, Sepideh Hatamikia"
"Combining Machine Learning with Recurrence Analysis for resonance
  detection","The width of a resonance in a nearly integrable system, i.e. in a
non-integrable system where chaotic motion is still not prominent, can tell us
how a perturbation parameter is driving the system away from integrability.
Although the tool that we are presenting here can be used is quite generic and
can be used in a variety of systems, our particular interest lies in binary
compact object systems known as extreme mass ratio inspirals (EMRIs). In an
EMRI a lighter compact object, like a black hole or a neutron star, inspirals
into a supermassive black hole due to gravitational radiation reaction. During
this inspiral the lighter object crosses resonances, which are still not very
well modeled. Measuring the width of resonances in EMRI models allows us to
estimate the importance of each perturbation parameter able to drive the system
away from resonances and decide whether its impact should be included in EMRI
waveform modeling or not. To tackle this issue in our study we show first that
recurrence quantifiers of orbits carry imprints of resonant behavior,
regardless of the system's dimensionality. As a next step, we apply a long
short-term memory machine learning architecture to automate the resonance
detection procedure. Our analysis is developed on a simple standard map and
gradually we extend it to more complicated systems until finally we employ it
in a generic deformed Kerr spacetime known in the literature as the
Johannsen-Psaltis spacetime.",2024-12-27T15:20:57Z,http://arxiv.org/abs/2412.19683v1,"Ondřej Zelenka, Ondřej Kopáček, Georgios Lukes-Gerakopoulos"
"A Hybrid Technique for Plant Disease Identification and Localisation in
  Real-time","Over the past decade, several image-processing methods and algorithms have
been proposed for identifying plant diseases based on visual data. DNN (Deep
Neural Networks) have recently become popular for this task. Both traditional
image processing and DNN-based methods encounter significant performance issues
in real-time detection owing to computational limitations and a broad spectrum
of plant disease features. This article proposes a novel technique for
identifying and localising plant disease based on the Quad-Tree decomposition
of an image and feature learning simultaneously. The proposed algorithm
significantly improves accuracy and faster convergence in high-resolution
images with relatively low computational load. Hence it is ideal for deploying
the algorithm in a standalone processor in a remotely operated image
acquisition and disease detection system, ideally mounted on drones and robots
working on large agricultural fields. The technique proposed in this article is
hybrid as it exploits the advantages of traditional image processing methods
and DNN-based models at different scales, resulting in faster inference. The F1
score is approximately 0.80 for four disease classes corresponding to potato
and tomato crops.",2024-12-27T15:20:45Z,http://arxiv.org/abs/2412.19682v1,"Mahendra Kumar Gohil, Anirudha Bhattacharjee, Rwik Rana, Kishan Lal, Samir Kumar Biswas, Nachiketa Tiwari, Bishakh Bhattacharya"
Identifying clusters in Czekanowski's diagram,"Visualizing data through Czekanowski's diagram has as its aim the
illustration of the relationships between objects. Often, obvious clusters of
observations are directly visible. However, it is not straightforward to
precisely delineate these clusters. This paper presents the development of the
package RMaCzek, which now includes features for cluster identification in
Czekanowski diagrams.",2024-12-27T15:04:06Z,http://arxiv.org/abs/2412.19679v1,"Krzysztof Bartoszek, Ying Luo"
Deep ReLU networks -- injectivity capacity upper bounds,"We study deep ReLU feed forward neural networks (NN) and their injectivity
abilities. The main focus is on \emph{precisely} determining the so-called
injectivity capacity. For any given hidden layers architecture, it is defined
as the minimal ratio between number of network's outputs and inputs which
ensures unique recoverability of the input from a realizable output. A strong
recent progress in precisely studying single ReLU layer injectivity properties
is here moved to a deep network level. In particular, we develop a program that
connects deep $l$-layer net injectivity to an $l$-extension of the $\ell_0$
spherical perceptrons, thereby massively generalizing an isomorphism between
studying single layer injectivity and the capacity of the so-called
(1-extension) $\ell_0$ spherical perceptrons discussed in [82]. \emph{Random
duality theory} (RDT) based machinery is then created and utilized to
statistically handle properties of the extended $\ell_0$ spherical perceptrons
and implicitly of the deep ReLU NNs. A sizeable set of numerical evaluations is
conducted as well to put the entire RDT machinery in practical use. From these
we observe a rapidly decreasing tendency in needed layers' expansions, i.e., we
observe a rapid \emph{expansion saturation effect}. Only $4$ layers of depth
are sufficient to closely approach level of no needed expansion -- a result
that fairly closely resembles observations made in practical experiments and
that has so far remained completely untouchable by any of the existing
mathematical methodologies.",2024-12-27T14:57:40Z,http://arxiv.org/abs/2412.19677v1,Mihailo Stojnic
"Optimizing Local-Global Dependencies for Accurate 3D Human Pose
  Estimation","Transformer-based methods have recently achieved significant success in 3D
human pose estimation, owing to their strong ability to model long-range
dependencies. However, relying solely on the global attention mechanism is
insufficient for capturing the fine-grained local details, which are crucial
for accurate pose estimation. To address this, we propose SSR-STF, a
dual-stream model that effectively integrates local features with global
dependencies to enhance 3D human pose estimation. Specifically, we introduce
SSRFormer, a simple yet effective module that employs the skeleton selective
refine attention (SSRA) mechanism to capture fine-grained local dependencies in
human pose sequences, complementing the global dependencies modeled by the
Transformer. By adaptively fusing these two feature streams, SSR-STF can better
learn the underlying structure of human poses, overcoming the limitations of
traditional methods in local feature extraction. Extensive experiments on the
Human3.6M and MPI-INF-3DHP datasets demonstrate that SSR-STF achieves
state-of-the-art performance, with P1 errors of 37.4 mm and 13.2 mm
respectively, outperforming existing methods in both accuracy and
generalization. Furthermore, the motion representations learned by our model
prove effective in downstream tasks such as human mesh recovery. Codes are
available at https://github.com/poker-xu/SSR-STF.",2024-12-27T14:54:12Z,http://arxiv.org/abs/2412.19676v1,"Guangsheng Xu, Guoyi Zhang, Lejia Ye, Shuwei Gan, Xiaohu Zhang, Xia Yang"
"DLScanner: A parameter space scanner package assisted by deep learning
  methods","In this paper, we introduce a scanner package enhanced by deep learning (DL)
techniques. The proposed package addresses two significant challenges
associated with previously developed DL-based methods: slow convergence in
high-dimensional scans and the limited generalization of the DL network when
mapping random points to the target space. To tackle the first issue, we
utilize a similarity learning network that maps sampled points into a
representation space. In this space, in-target points are grouped together
while out-target points are effectively pushed apart. This approach enhances
the scan convergence by refining the representation of sampled points. The
second challenge is mitigated by integrating a dynamic sampling strategy.
Specifically, we employ a VEGAS mapping to adaptively suggest new points for
the DL network while also improving the mapping when more points are collected.
Our proposed framework demonstrates substantial gains in both performance and
efficiency compared to other scanning methods.",2024-12-27T14:52:42Z,http://arxiv.org/abs/2412.19675v1,"A. Hammad, Raymundo Ramos"
Spectral form factors for curved spacetimes with horizon,"The spectral form factor is believed to provide a special type of behavior
called ""dip-ramp-plateau"" in chaotic quantum systems which originates from the
random matrix theory. A similar behavior could be observed for deterministic
systems, ranging from the Riemann zeta function to the scattering amplitudes of
different types. It has been shown recently, the same behavior is observed for
the spectral form factor when the normal modes of a scalar massless field
theory in the brickwall model of the BTZ black hole are substituted as
eigenvalues of some quantum Hamiltonian. At the same time, the level spacing
distribution of these eigenvalues differs from that associated with the random
matrix theory ensembles. In this paper, we generalize these results considering
the recently proposed generalized spectral form factor for the de Sitter and
BTZ spacetimes. We study the details of this complex-valued form factor for
integrable quantum systems and for backgrounds with a horizon comparing it with
the random matrix theory behavior. As a result, we confirm that the scalar
field normal modes once again exhibit features of chaos.",2024-12-27T14:43:35Z,http://arxiv.org/abs/2412.19672v1,"Dmitry S. Ageev, Vasilii V. Pushkarev, Anastasia N. Zueva"
"Conjugation, loop and closure invariants of the iterated-integrals
  signature","Given a feature set for the shape of a closed loop, it is natural to ask
which features in that set do not change when the starting point of the path is
moved. For example, in two dimensions, the area enclosed by the path does not
depend on the starting point. In the present article, we characterize such loop
invariants among all those features known as interated integrals of a given
path. Furthermore, we relate these to conjugation invariants, which are a
canonical object of study when treating (tree reduced) paths as a group with
multiplication given by the concatenation. Finally, closure invariants are a
third class in this context which is of particular relevance when studying
piecewise linear trajectories, e.g. given by linear interpolation of time
series.
  Keywords: invariant features; concatenation of paths; combinatorial
necklaces; shuffle algebra; free Lie algebra; signed area; signed volume;
tree-like equivalence.",2024-12-27T14:32:30Z,http://arxiv.org/abs/2412.19670v1,"Joscha Diehl, Rosa Preiß, Jeremy Reizenstein"
"Toward Scalable Multirobot Control: Fast Policy Learning in Distributed
  MPC","Distributed model predictive control (DMPC) is promising in achieving optimal
cooperative control in multirobot systems (MRS). However, real-time DMPC
implementation relies on numerical optimization tools to periodically calculate
local control sequences online. This process is computationally demanding and
lacks scalability for large-scale, nonlinear MRS. This article proposes a novel
distributed learning-based predictive control (DLPC) framework for scalable
multirobot control. Unlike conventional DMPC methods that calculate open-loop
control sequences, our approach centers around a computationally fast and
efficient distributed policy learning algorithm that generates explicit
closed-loop DMPC policies for MRS without using numerical solvers. The policy
learning is executed incrementally and forward in time in each prediction
interval through an online distributed actor-critic implementation. The control
policies are successively updated in a receding-horizon manner, enabling fast
and efficient policy learning with the closed-loop stability guarantee. The
learned control policies could be deployed online to MRS with varying robot
scales, enhancing scalability and transferability for large-scale MRS.
Furthermore, we extend our methodology to address the multirobot safe learning
challenge through a force field-inspired policy learning approach. We validate
our approach's effectiveness, scalability, and efficiency through extensive
experiments on cooperative tasks of large-scale wheeled robots and multirotor
drones. Our results demonstrate the rapid learning and deployment of DMPC
policies for MRS with scales up to 10,000 units.",2024-12-27T14:31:52Z,http://arxiv.org/abs/2412.19669v1,"Xinglong Zhang, Wei Pan, Cong Li, Xin Xu, Xiangke Wang, Ronghua Zhang, Dewen Hu"
"CAD-GPT: Synthesising CAD Construction Sequence with Spatial
  Reasoning-Enhanced Multimodal LLMs","Computer-aided design (CAD) significantly enhances the efficiency, accuracy,
and innovation of design processes by enabling precise 2D and 3D modeling,
extensive analysis, and optimization. Existing methods for creating CAD models
rely on latent vectors or point clouds, which are difficult to obtain and
costly to store. Recent advances in Multimodal Large Language Models (MLLMs)
have inspired researchers to use natural language instructions and images for
CAD model construction. However, these models still struggle with inferring
accurate 3D spatial location and orientation, leading to inaccuracies in
determining the spatial 3D starting points and extrusion directions for
constructing geometries. This work introduces CAD-GPT, a CAD synthesis method
with spatial reasoning-enhanced MLLM that takes either a single image or a
textual description as input. To achieve precise spatial inference, our
approach introduces a 3D Modeling Spatial Mechanism. This method maps 3D
spatial positions and 3D sketch plane rotation angles into a 1D linguistic
feature space using a specialized spatial unfolding mechanism, while
discretizing 2D sketch coordinates into an appropriate planar space to enable
precise determination of spatial starting position, sketch orientation, and 2D
sketch coordinate translations. Extensive experiments demonstrate that CAD-GPT
consistently outperforms existing state-of-the-art methods in CAD model
synthesis, both quantitatively and qualitatively.",2024-12-27T14:19:36Z,http://arxiv.org/abs/2412.19663v1,"Siyu Wang, Cailian Chen, Xinyi Le, Qimin Xu, Lei Xu, Yanzhou Zhang, Jie Yang"
Quantum Cluster State Model with Haagerup Fusion Category Symmetry,"We propose a (1+1)D lattice model, inspired by a weak Hopf algebra
generalization of the cluster state model, which realizes Haagerup fusion
category symmetry and features a tensor product Hilbert space. The construction
begins with a reconstruction of the Haagerup weak Hopf algebra $H_3$ from the
Haagerup fusion category, ensuring that the representation category of $H_3$ is
equivalent to Haagerup fusion category. Utilizing the framework of symmetry
topological field theory (SymTFT), we develop an ultra-thin weak Hopf quantum
double model, characterized by a smooth topological boundary condition. We show
that this model supports Haagerup fusion category symmetry. Finally, we solve
the ground state of the model in terms of a weak Hopf matrix product state,
which serves as a natural generalization of the cluster state, embodying
Haagerup fusion category symmetry.",2024-12-27T14:05:15Z,http://arxiv.org/abs/2412.19657v1,Zhian Jia
"Asymmetrical Reciprocity-based Federated Learning for Resolving
  Disparities in Medical Diagnosis","Geographic health disparities pose a pressing global challenge, particularly
in underserved regions of low- and middle-income nations. Addressing this issue
requires a collaborative approach to enhance healthcare quality, leveraging
support from medically more developed areas. Federated learning emerges as a
promising tool for this purpose. However, the scarcity of medical data and
limited computation resources in underserved regions make collaborative
training of powerful machine learning models challenging. Furthermore, there
exists an asymmetrical reciprocity between underserved and developed regions.
To overcome these challenges, we propose a novel cross-silo federated learning
framework, named FedHelp, aimed at alleviating geographic health disparities
and fortifying the diagnostic capabilities of underserved regions.
Specifically, FedHelp leverages foundational model knowledge via one-time API
access to guide the learning process of underserved small clients, addressing
the challenge of insufficient data. Additionally, we introduce a novel
asymmetric dual knowledge distillation module to manage the issue of asymmetric
reciprocity, facilitating the exchange of necessary knowledge between developed
large clients and underserved small clients. We validate the effectiveness and
utility of FedHelp through extensive experiments on both medical image
classification and segmentation tasks. The experimental results demonstrate
significant performance improvement compared to state-of-the-art baselines,
particularly benefiting clients in underserved regions.",2024-12-27T13:59:58Z,http://arxiv.org/abs/2412.19654v1,"Jiaqi Wang, Ziyi Yin, Quanzeng You, Lingjuan Lyu, Fenglong Ma"
"Toward Modality Gap: Vision Prototype Learning for Weakly-supervised
  Semantic Segmentation with CLIP","The application of Contrastive Language-Image Pre-training (CLIP) in Weakly
Supervised Semantic Segmentation (WSSS) research powerful cross-modal semantic
understanding capabilities. Existing methods attempt to optimize input text
prompts for improved alignment of images and text, by finely adjusting text
prototypes to facilitate semantic matching. Nevertheless, given the modality
gap between text and vision spaces, the text prototypes employed by these
methods have not effectively established a close correspondence with
pixel-level vision features. In this work, our theoretical analysis indicates
that the inherent modality gap results in misalignment of text and region
features, and that this gap cannot be sufficiently reduced by minimizing
contrast loss in CLIP. To mitigate the impact of the modality gap, we propose a
Vision Prototype Learning (VPL) framework, by introducing more representative
vision prototypes. The core of this framework is to learn class-specific vision
prototypes in vision space with the help of text prototypes, for capturing
high-quality localization maps. Moreover, we propose a regional semantic
contrast module that contrasts regions embedding with corresponding prototypes,
leading to more comprehensive and robust feature learning. Experimental results
show that our proposed framework achieves state-of-the-art performance on two
benchmark datasets.",2024-12-27T13:55:11Z,http://arxiv.org/abs/2412.19650v1,"Zhongxing Xu, Feilong Tang, Zhe Chen, Yingxue Su, Zhiyi Zhao, Ge Zhang, Jionglong Su, Zongyuan Ge"
"Distributed Download from an External Data Source in Faulty Majority
  Settings","We extend the study of retrieval problems in distributed networks, focusing
on improving the efficiency and resilience of protocols in the \emph{Data
Retrieval (DR) Model}. The DR Model consists of a complete network (i.e., a
clique) with $k$ peers, up to $\beta k$ of which may be Byzantine (for $\beta
\in [0, 1)$), and a trusted \emph{External Data Source} comprising an array $X$
of $n$ bits ($n \gg k$) that the peers can query. Additionally, the peers can
also send messages to each other. In this work, we focus on the Download
problem that requires all peers to learn $X$. Our primary goal is to minimize
the maximum number of queries made by any honest peer and additionally optimize
time.
  We begin with a randomized algorithm for the Download problem that achieves
optimal query complexity up to a logarithmic factor. For the stronger dynamic
adversary that can change the set of Byzantine peers from one round to the
next, we achieve the optimal time complexity in peer-to-peer communication but
with larger messages. In broadcast communication where all peers (including
Byzantine peers) are required to send the same message to all peers, with
larger messages, we achieve almost optimal time and query complexities for a
dynamic adversary. Finally, in a more relaxed crash fault model, where peers
stop responding after crashing, we address the Download problem in both
synchronous and asynchronous settings. Using a deterministic protocol, we
obtain nearly optimal results for both query complexity and message sizes in
these scenarios.",2024-12-27T13:55:00Z,http://arxiv.org/abs/2412.19649v1,"John Augustine, Soumyottam Chatterjee, Valerie King, Manish Kumar, Shachar Meir, David Peleg"
"Chimera: A Block-Based Neural Architecture Search Framework for
  Event-Based Object Detection","Event-based cameras are sensors that simulate the human eye, offering
advantages such as high-speed robustness and low power consumption. Established
Deep Learning techniques have shown effectiveness in processing event data.
Chimera is a Block-Based Neural Architecture Search (NAS) framework
specifically designed for Event-Based Object Detection, aiming to create a
systematic approach for adapting RGB-domain processing methods to the event
domain. The Chimera design space is constructed from various macroblocks,
including Attention blocks, Convolutions, State Space Models, and
MLP-mixer-based architectures, which provide a valuable trade-off between local
and global processing capabilities, as well as varying levels of complexity.
The results on the PErson Detection in Robotics (PEDRo) dataset demonstrated
performance levels comparable to leading state-of-the-art models, alongside an
average parameter reduction of 1.6 times.",2024-12-27T13:50:44Z,http://arxiv.org/abs/2412.19646v1,"Diego A. Silva, Ahmed Elsheikh, Kamilya Smagulova, Mohammed E. Fouda, Ahmed M. Eltawil"
"VideoMaker: Zero-shot Customized Video Generation with the Inherent
  Force of Video Diffusion Models","Zero-shot customized video generation has gained significant attention due to
its substantial application potential. Existing methods rely on additional
models to extract and inject reference subject features, assuming that the
Video Diffusion Model (VDM) alone is insufficient for zero-shot customized
video generation. However, these methods often struggle to maintain consistent
subject appearance due to suboptimal feature extraction and injection
techniques. In this paper, we reveal that VDM inherently possesses the force to
extract and inject subject features. Departing from previous heuristic
approaches, we introduce a novel framework that leverages VDM's inherent force
to enable high-quality zero-shot customized video generation. Specifically, for
feature extraction, we directly input reference images into VDM and use its
intrinsic feature extraction process, which not only provides fine-grained
features but also significantly aligns with VDM's pre-trained knowledge. For
feature injection, we devise an innovative bidirectional interaction between
subject features and generated content through spatial self-attention within
VDM, ensuring that VDM has better subject fidelity while maintaining the
diversity of the generated video.Experiments on both customized human and
object video generation validate the effectiveness of our framework.",2024-12-27T13:49:25Z,http://arxiv.org/abs/2412.19645v1,"Tao Wu, Yong Zhang, Xiaodong Cun, Zhongang Qi, Junfu Pu, Huanzhang Dou, Guangcong Zheng, Ying Shan, Xi Li"
Xmodel-2 Technical Report,"Xmodel-2 is a 1.2-billion-parameter large language model designed
specifically for reasoning tasks. Its architecture enables different model
scales to share a unified set of hyperparameters, allowing for extensive
experimentation on smaller models and seamless transfer of optimal
configurations to larger models. To maximize training efficiency and stability,
Xmodel-2 employs the WSD learning rate scheduler from MiniCPM. Pretrained on
1.5 trillion tokens from diverse sources, Xmodel-2 achieves state-of-the-art
performance in complex reasoning and agent-based tasks, while maintaining low
training costs. These results highlight the potential of efficient model design
and training strategies in advancing reasoning capabilities. Model checkpoints
and code are publicly available on GitHub at
https://github.com/XiaoduoAILab/Xmodel-2",2024-12-27T13:32:10Z,http://arxiv.org/abs/2412.19638v1,"Wang Qun, Liu Yang, Lin Qingquan, Qu Zhijiu, Jiang Ling"
ReNeg: Learning Negative Embedding with Reward Guidance,"In text-to-image (T2I) generation applications, negative embeddings have
proven to be a simple yet effective approach for enhancing generation quality.
Typically, these negative embeddings are derived from user-defined negative
prompts, which, while being functional, are not necessarily optimal. In this
paper, we introduce ReNeg, an end-to-end method designed to learn improved
Negative embeddings guided by a Reward model. We employ a reward feedback
learning framework and integrate classifier-free guidance (CFG) into the
training process, which was previously utilized only during inference, thus
enabling the effective learning of negative embeddings. We also propose two
strategies for learning both global and per-sample negative embeddings.
Extensive experiments show that the learned negative embedding significantly
outperforms null-text and handcrafted counterparts, achieving substantial
improvements in human preference alignment. Additionally, the negative
embedding learned within the same text embedding space exhibits strong
generalization capabilities. For example, using the same CLIP text encoder, the
negative embedding learned on SD1.5 can be seamlessly transferred to
text-to-image or even text-to-video models such as ControlNet, ZeroScope, and
VideoCrafter2, resulting in consistent performance improvements across the
board.",2024-12-27T13:31:55Z,http://arxiv.org/abs/2412.19637v1,"Xiaomin Li, Yixuan Liu, Takashi Isobe, Xu Jia, Qinpeng Cui, Dong Zhou, Dong Li, You He, Huchuan Lu, Zhongdao Wang, Emad Barsoum"
Deep Linear Hawkes Processes,"Marked temporal point processes (MTPPs) are used to model sequences of
different types of events with irregular arrival times, with broad applications
ranging from healthcare and social networks to finance. We address shortcomings
in existing point process models by drawing connections between modern deep
state-space models (SSMs) and linear Hawkes processes (LHPs), culminating in an
MTPP that we call the deep linear Hawkes process (DLHP). The DLHP modifies the
linear differential equations in deep SSMs to be stochastic jump differential
equations, akin to LHPs. After discretizing, the resulting recurrence can be
implemented efficiently using a parallel scan. This brings parallelism and
linear scaling to MTPP models. This contrasts with attention-based MTPPs, which
scale quadratically, and RNN-based MTPPs, which do not parallelize across the
sequence length. We show empirically that DLHPs match or outperform existing
models across a broad range of metrics on eight real-world datasets. Our
proposed DLHP model is the first instance of the unique architectural
capabilities of SSMs being leveraged to construct a new class of MTPP models.",2024-12-27T13:23:58Z,http://arxiv.org/abs/2412.19634v1,"Yuxin Chang, Alex Boyd, Cao Xiao, Taha Kass-Hout, Parminder Bhatia, Padhraic Smyth, Andrew Warrington"
IMTP: Search-based Code Generation for In-memory Tensor Programs,"Processing-in-DRAM (DRAM-PIM) has emerged as a promising technology for
accelerating memory-intensive operations in modern applications, such as Large
Language Models (LLMs). Despite its potential, current software stacks for
DRAM-PIM face significant challenges, including reliance on hand-tuned
libraries that hinder programmability, limited support for high-level
abstractions, and the lack of systematic optimization frameworks. To address
these limitations, we present IMTP, a search-based optimizing tensor compiler
for UPMEM. Key features of IMTP include: (1) automated searches of the joint
search space for host and kernel tensor programs, (2) PIM-aware optimizations
for efficiently handling boundary conditions, and (3) improved search
algorithms for the expanded search space of UPMEM systems. Our experimental
results on UPMEM hardware demonstrate performance gains of up to 8.21x for
various UPMEM benchmark kernels and 5.33x for GPT-J layers. To the best of our
knowledge, IMTP is the first tensor compiler to provide fully automated,
autotuning-integrated code generation support for a DRAM-PIM system. By
bridging the gap between high-level tensor computation abstractions and
low-level hardware-specific requirements, IMTP establishes a foundation for
advancing DRAM-PIM programmability and enabling streamlined optimization.",2024-12-27T13:19:35Z,http://arxiv.org/abs/2412.19630v1,"Yongwon Shin, Dookyung Kang, Hyojin Sung"
"Gradient Weight-normalized Low-rank Projection for Efficient LLM
  Training","Large Language Models (LLMs) have shown remarkable performance across various
tasks, but the escalating demands on computational resources pose significant
challenges, particularly in the extensive utilization of full fine-tuning for
downstream tasks. To address this, parameter-efficient fine-tuning (PEFT)
methods have been developed, but they often underperform compared to full
fine-tuning and struggle with memory efficiency. In this work, we introduce
Gradient Weight-Normalized Low-Rank Projection (GradNormLoRP), a novel approach
that enhances both parameter and memory efficiency while maintaining comparable
performance to full fine-tuning. GradNormLoRP normalizes the weight matrix to
improve gradient conditioning, facilitating better convergence during
optimization. Additionally, it applies low-rank approximations to the weight
and gradient matrices, significantly reducing memory usage during training.
Extensive experiments demonstrate that our 8-bit GradNormLoRP reduces optimizer
memory usage by up to 89.5% and enables the pre-training of large LLMs, such as
LLaMA 7B, on consumer-level GPUs like the NVIDIA RTX 4090, without additional
inference costs. Moreover, GradNormLoRP outperforms existing low-rank methods
in fine-tuning tasks. For instance, when fine-tuning the RoBERTa model on all
GLUE tasks with a rank of 8, GradNormLoRP achieves an average score of 80.65,
surpassing LoRA's score of 79.23. These results underscore GradNormLoRP as a
promising alternative for efficient LLM pre-training and fine-tuning. Source
code and Appendix:
https://github.com/Jhhuangkay/Gradient-Weight-normalized-Low-rank-Projection-for-Efficient-LLM-Training",2024-12-27T12:23:39Z,http://arxiv.org/abs/2412.19616v1,"Jia-Hong Huang, Yixian Shen, Hongyi Zhu, Stevan Rudinac, Evangelos Kanoulas"
Anisotropic Band Flattening in Twisted Bilayer of M-Valley MXenes,"Experimental studies on moir\'e materials have predominantly focused on
twisted hexagonal lattice with low-energy states near the $\Gamma$- or
K-points. These materials, characterized by isotropic low-energy dispersion,
are fundamentally distinct from those with anisotropic properties. Here we
introduce a series of semiconducting transition metal carbides (MXenes)
$M_2$C$T_2$ ($M$ = Ti, Zr, Hf, Sc, Y; $T$ = O, F, Cl) as a novel platform for
M-valley moir\'e materials. Take Ti$_2$CO$_2$ and Zr$_2$CO$_2$ as
representative examples, large-scale \emph{ab initio} calculations show that
their AB-stacked twisted homobilayer features three three-fold rotational
symmetry related M-valleys with time-reserval symmetry and giant anisotropic
band flattening. We derive a simplified moir\'e Hamiltonian for these systems
and conduct a detailed analysis of their band structures, where the origins of
anisotropic band flattening are clearly elucidated. This research broadens the
scope of moir\'e materials, where the valley- and spin-degenerate
two-dimensional array of quasi-one-dimensional system could serve as a
potential platform for realizing many interesting correlated phases.",2024-12-27T12:20:15Z,http://arxiv.org/abs/2412.19613v1,"Kejie Bao, Huan Wang, Zhaochen Liu, jing Wang"
"Enhancing Fine-grained Image Classification through Attentive Batch
  Training","Fine-grained image classification, which is a challenging task in computer
vision, requires precise differentiation among visually similar object
categories. In this paper, we propose 1) a novel module called Residual
Relationship Attention (RRA) that leverages the relationships between images
within each training batch to effectively integrate visual feature vectors of
batch images and 2) a novel technique called Relationship Position Encoding
(RPE), which encodes the positions of relationships between original images in
a batch and effectively preserves the relationship information between images
within the batch. Additionally, we design a novel framework, namely
Relationship Batch Integration (RBI), which utilizes RRA in conjunction with
RPE, allowing the discernment of vital visual features that may remain elusive
when examining a singular image representative of a particular class. Through
extensive experiments, our proposed method demonstrates significant
improvements in the accuracy of different fine-grained classifiers, with an
average increase of $(+2.78\%)$ and $(+3.83\%)$ on the CUB200-2011 and Stanford
Dog datasets, respectively, while achieving a state-of-the-art results
$(95.79\%)$ on the Stanford Dog dataset. Despite not achieving the same level
of improvement as in fine-grained image classification, our method still
demonstrates its prowess in leveraging general image classification by
attaining a state-of-the-art result of $(93.71\%)$ on the Tiny-Imagenet
dataset. Furthermore, our method serves as a plug-in refinement module and can
be easily integrated into different networks.",2024-12-27T12:07:58Z,http://arxiv.org/abs/2412.19606v1,"Duy M. Le, Bao Q. Bui, Anh Tran, Cong Tran, Cuong Pham"
"ViDTA: Enhanced Drug-Target Affinity Prediction via Virtual Graph Nodes
  and Attention-based Feature Fusion","Drug-target interaction is fundamental in understanding how drugs affect
biological systems, and accurately predicting drug-target affinity (DTA) is
vital for drug discovery. Recently, deep learning methods have emerged as a
significant approach for estimating the binding strength between drugs and
target proteins. However, existing methods simply utilize the drug's local
information from molecular topology rather than global information.
Additionally, the features of drugs and proteins are usually fused with a
simple concatenation operation, limiting their effectiveness. To address these
challenges, we proposed ViDTA, an enhanced DTA prediction framework. We
introduce virtual nodes into the Graph Neural Network (GNN)-based drug feature
extraction network, which acts as a global memory to exchange messages more
efficiently. By incorporating virtual graph nodes, we seamlessly integrate
local and global features of drug molecular structures, expanding the GNN's
receptive field. Additionally, we propose an attention-based linear feature
fusion network for better capturing the interaction information between drugs
and proteins. Experimental results evaluated on various benchmarks including
Davis, Metz, and KIBA demonstrate that our proposed ViDTA outperforms the
state-of-the-art baselines.",2024-12-27T11:19:10Z,http://arxiv.org/abs/2412.19589v1,"Minghui Li, Zikang Guo, Yang Wu, Peijin Guo, Yao Shi, Shengshan Hu, Wei Wan, Shengqing Hu"
"Goal-oriented Communications based on Recursive Early Exit Neural
  Networks","This paper presents a novel framework for goal-oriented semantic
communications leveraging recursive early exit models. The proposed approach is
built on two key components. First, we introduce an innovative early exit
strategy that dynamically partitions computations, enabling samples to be
offloaded to a server based on layer-wise recursive prediction dynamics that
detect samples for which the confidence is not increasing fast enough over
layers. Second, we develop a Reinforcement Learning-based online optimization
framework that jointly determines early exit points, computation splitting, and
offloading strategies, while accounting for wireless conditions, inference
accuracy, and resource costs. Numerical evaluations in an edge inference
scenario demonstrate the method's adaptability and effectiveness in striking an
excellent trade-off between performance, latency, and resource efficiency.",2024-12-27T11:14:11Z,http://arxiv.org/abs/2412.19587v1,"Jary Pomponi, Mattia Merluzzi, Alessio Devoto, Mateus Pontes Mota, Paolo Di Lorenzo, Simone Scardapane"
"Ultralight Signal Classification Model for Automatic Modulation
  Recognition","The growing complexity of radar signals demands responsive and accurate
detection systems that can operate efficiently on resource-constrained edge
devices. Existing models, while effective, often rely on substantial
computational resources and large datasets, making them impractical for edge
deployment. In this work, we propose an ultralight hybrid neural network
optimized for edge applications, delivering robust performance across
unfavorable signal-to-noise ratios (mean accuracy of 96.3% at 0 dB) using less
than 100 samples per class, and significantly reducing computational overhead.",2024-12-27T11:03:26Z,http://arxiv.org/abs/2412.19585v1,"Alessandro Daniele Genuardi Oquendo, Agustín Matías Galante Cerviño, Nilotpal Sinha, Luc Andrea, Sam Mugel, Román Orús"
"A Comparative Study of Machine Unlearning Techniques for Image and Text
  Classification Models","Machine Unlearning has emerged as a critical area in artificial intelligence,
addressing the need to selectively remove learned data from machine learning
models in response to data privacy regulations. This paper provides a
comprehensive comparative analysis of six state-of-theart unlearning techniques
applied to image and text classification tasks. We evaluate their performance,
efficiency, and compliance with regulatory requirements, highlighting their
strengths and limitations in practical scenarios. By systematically analyzing
these methods, we aim to provide insights into their applicability,
challenges,and tradeoffs, fostering advancements in the field of ethical and
adaptable machine learning.",2024-12-27T10:58:55Z,http://arxiv.org/abs/2412.19583v1,"Omar M. Safa, Mahmoud M. Abdelaziz, Mustafa Eltawy, Mohamed Mamdouh, Moamen Gharib, Salaheldin Eltenihy, Nagia M. Ghanem, Mohamed M. Ismail"
"An Actionable Hierarchical Scene Representation Enhancing Autonomous
  Inspection Missions in Unknown Environments","In this article, we present the Layered Semantic Graphs (LSG), a novel
actionable hierarchical scene graph, fully integrated with a multi-modal
mission planner, the FLIE: A First-Look based Inspection and Exploration
planner. The novelty of this work stems from aiming to address the task of
maintaining an intuitive and multi-resolution scene representation, while
simultaneously offering a tractable foundation for planning and scene
understanding during an ongoing inspection mission of apriori unknown
targets-of-interest in an unknown environment. The proposed LSG scheme is
composed of locally nested hierarchical graphs, at multiple layers of
abstraction, with the abstract concepts grounded on the functionality of the
integrated FLIE planner. Furthermore, LSG encapsulates real-time semantic
segmentation models that offer extraction and localization of desired semantic
elements within the hierarchical representation. This extends the capability of
the inspection planner, which can then leverage LSG to make an informed
decision to inspect a particular semantic of interest. We also emphasize the
hierarchical and semantic path-planning capabilities of LSG, which can extend
inspection missions by improving situational awareness for human operators in
an unknown environment. The validity of the proposed scheme is proven through
extensive evaluations of the proposed architecture in simulations, as well as
experimental field deployments on a Boston Dynamics Spot quadruped robot in
urban outdoor environment settings.",2024-12-27T10:57:17Z,http://arxiv.org/abs/2412.19582v1,"Vignesh Kottayam Viswanathan, Mario Alberto Valdes Saucedo, Sumeet Gajanan Satpute, Christoforos Kanellakis, George Nikolakopoulos"
Stamps and Mathematics,"This study examines the potential of using math-themed postage stamps in
mathematics lessons as a tool to engage students and integrate the subject with
history, art, and culture. Since the first mathematical stamps appeared in the
early 20th century, featuring prominent scholars like Carl Friedrich Gauss and
Isaac Newton, they serve not only as philatelic artifacts but also as
historical carriers of knowledge. The paper presents several practical projects
to interest students, such as creating their own math stamps, investigating the
price trends of math-themed stamps, and developing a timeline of mathematical
discoveries depicted in philatelic issues. The proposed projects develop
students' mathematical skills in areas such as percentage calculations, general
arithmetic, working with time intervals, and statistical analysis. Students can
analyze shapes, symmetry, and patterns on stamps, study principles of
proportion, and explore geometric figures. Using stamps broadens students'
horizons, providing an opportunity to become familiar with renowned
mathematicians from different eras, countries, and cultures. This also offers
students a new perspective on the subject, presenting mathematical discoveries
as part of the world's cultural heritage. Postage stamps dedicated to
mathematics can become a powerful tool for visualizing theoretical knowledge,
stimulating interest in mathematics, and encouraging independent research among
students.",2024-12-27T10:52:54Z,http://arxiv.org/abs/2412.19579v1,Nataliya M. Ivanova
"Graph-attention-based Casual Discovery with Trust Region-navigated
  Clipping Policy Optimization","In many domains of empirical sciences, discovering the causal structure
within variables remains an indispensable task. Recently, to tackle with
unoriented edges or latent assumptions violation suffered by conventional
methods, researchers formulated a reinforcement learning (RL) procedure for
causal discovery, and equipped REINFORCE algorithm to search for the
best-rewarded directed acyclic graph. The two keys to the overall performance
of the procedure are the robustness of RL methods and the efficient encoding of
variables. However, on the one hand, REINFORCE is prone to local convergence
and unstable performance during training. Neither trust region policy
optimization, being computationally-expensive, nor proximal policy optimization
(PPO), suffering from aggregate constraint deviation, is decent alternative for
combinatory optimization problems with considerable individual subactions. We
propose a trust region-navigated clipping policy optimization method for causal
discovery that guarantees both better search efficiency and steadiness in
policy optimization, in comparison with REINFORCE, PPO and our prioritized
sampling-guided REINFORCE implementation. On the other hand, to boost the
efficient encoding of variables, we propose a refined graph attention encoder
called SDGAT that can grasp more feature information without priori
neighbourhood information. With these improvements, the proposed method
outperforms former RL method in both synthetic and benchmark datasets in terms
of output results and optimization robustness.",2024-12-27T10:50:43Z,http://arxiv.org/abs/2412.19578v1,"Shixuan Liu, Yanghe Feng, Keyu Wu, Guangquan Cheng, Jincai Huang, Zhong Liu"
"Error estimate based adaptive quadrature for layer potentials over
  axisymmetric surfaces","Layer potentials represent solutions to partial differential equations in an
integral equation formulation. When numerically evaluating layer potentials at
evaluation points close to the domain boundary, specialized quadrature
techniques are required for accuracy because of rapid variations in the
integrand. To efficiently achieve a specified error tolerance, we introduce an
adaptive quadrature method with automatic parameter adjustment for axisymmetric
surfaces, facilitated by error estimation. Notably, while each surface must be
axisymmetric, the integrand itself need not be, allowing for applications with
complex geometries featuring multiple axisymmetric bodies.
  The proposed quadrature method utilizes so-called interpolatory
semi-analytical quadrature in conjunction with a singularity swap technique in
the azimuthal angle. In the polar angle, such a technique is used as needed,
depending on the integral kernel, combined with an adaptive subdivision of the
integration interval. The method is tied to a regular quadrature method that
employs a trapezoidal rule in the azimuthal angle and a Gauss-Legendre
quadrature rule in the polar angle, which will be used whenever deemed
sufficiently accurate, as determined by a quadrature error estimate [C.
Sorgentone and A.-K. Tornberg, Advances in Computational Mathematics, 49
(2023), p. 87].
  Error estimates for both numerical integration and interpolation are derived
using complex analysis, and are used to determine the adaptive panel
subdivision given the evaluation point and desired accuracy. Numerical examples
are presented to demonstrate the method's efficacy.",2024-12-27T10:44:34Z,http://arxiv.org/abs/2412.19575v1,"David Krantz, Anna-Karin Tornberg"
"Reinforced Label Denoising for Weakly-Supervised Audio-Visual Video
  Parsing","Audio-visual video parsing (AVVP) aims to recognize audio and visual event
labels with precise temporal boundaries, which is quite challenging since audio
or visual modality might include only one event label with only the overall
video labels available. Existing label denoising models often treat the
denoising process as a separate preprocessing step, leading to a disconnect
between label denoising and AVVP tasks. To bridge this gap, we present a novel
joint reinforcement learning-based label denoising approach (RLLD). This
approach enables simultaneous training of both label denoising and video
parsing models through a joint optimization strategy. We introduce a novel
AVVP-validation and soft inter-reward feedback mechanism that directly guides
the learning of label denoising policy. Extensive experiments on AVVP tasks
demonstrate the superior performance of our proposed method compared to label
denoising techniques. Furthermore, by incorporating our label denoising method
into other AVVP models, we find that it can further enhance parsing results.",2024-12-27T10:05:56Z,http://arxiv.org/abs/2412.19563v1,"Yongbiao Gao, Xiangcheng Sun, Guohua Lv, Deng Yu, Sijiu Niu"
"Hindsight Planner: A Closed-Loop Few-Shot Planner for Embodied
  Instruction Following","This work focuses on building a task planner for Embodied Instruction
Following (EIF) using Large Language Models (LLMs). Previous works typically
train a planner to imitate expert trajectories, treating this as a supervised
task. While these methods achieve competitive performance, they often lack
sufficient robustness. When a suboptimal action is taken, the planner may
encounter an out-of-distribution state, which can lead to task failure. In
contrast, we frame the task as a Partially Observable Markov Decision Process
(POMDP) and aim to develop a robust planner under a few-shot assumption. Thus,
we propose a closed-loop planner with an adaptation module and a novel
hindsight method, aiming to use as much information as possible to assist the
planner. Our experiments on the ALFRED dataset indicate that our planner
achieves competitive performance under a few-shot assumption. For the first
time, our few-shot agent's performance approaches and even surpasses that of
the full-shot supervised agent.",2024-12-27T10:05:45Z,http://arxiv.org/abs/2412.19562v1,"Yuxiao Yang, Shenao Zhang, Zhihan Liu, Huaxiu Yao, Zhaoran Wang"
"Structural Similarity in Deep Features: Image Quality Assessment Robust
  to Geometrically Disparate Reference","Image Quality Assessment (IQA) with references plays an important role in
optimizing and evaluating computer vision tasks. Traditional methods assume
that all pixels of the reference and test images are fully aligned. Such
Aligned-Reference IQA (AR-IQA) approaches fail to address many real-world
problems with various geometric deformations between the two images. Although
significant effort has been made to attack Geometrically-Disparate-Reference
IQA (GDR-IQA) problem, it has been addressed in a task-dependent fashion, for
example, by dedicated designs for image super-resolution and retargeting, or by
assuming the geometric distortions to be small that can be countered by
translation-robust filters or by explicit image registrations. Here we rethink
this problem and propose a unified, non-training-based Deep Structural
Similarity (DeepSSIM) approach to address the above problems in a single
framework, which assesses structural similarity of deep features in a simple
but efficient way and uses an attention calibration strategy to alleviate
attention deviation. The proposed method, without application-specific design,
achieves state-of-the-art performance on AR-IQA datasets and meanwhile shows
strong robustness to various GDR-IQA test cases. Interestingly, our test also
shows the effectiveness of DeepSSIM as an optimization tool for training image
super-resolution, enhancement and restoration, implying an even wider
generalizability. \footnote{Source code will be made public after the review is
completed.",2024-12-27T09:51:23Z,http://arxiv.org/abs/2412.19553v1,"Keke Zhang, Weiling Chen, Tiesong Zhao, Zhou Wang"
"Learning states enhanced knowledge tracing: Simulating the diversity in
  real-world learning process","The Knowledge Tracing (KT) task focuses on predicting a learner's future
performance based on the historical interactions. The knowledge state plays a
key role in learning process. However, considering that the knowledge state is
influenced by various learning factors in the interaction process, such as the
exercises similarities, responses reliability and the learner's learning state.
Previous models still face two major limitations. First, due to the exercises
differences caused by various complex reasons and the unreliability of
responses caused by guessing behavior, it is hard to locate the historical
interaction which is most relevant to the current answered exercise. Second,
the learning state is also a key factor to influence the knowledge state, which
is always ignored by previous methods. To address these issues, we propose a
new method named Learning State Enhanced Knowledge Tracing (LSKT). Firstly, to
simulate the potential differences in interactions, inspired by Item Response
Theory~(IRT) paradigm, we designed three different embedding methods ranging
from coarse-grained to fine-grained views and conduct comparative analysis on
them. Secondly, we design a learning state extraction module to capture the
changing learning state during the learning process of the learner. In turn,
with the help of the extracted learning state, a more detailed knowledge state
could be captured. Experimental results on four real-world datasets show that
our LSKT method outperforms the current state-of-the-art methods.",2024-12-27T09:41:25Z,http://arxiv.org/abs/2412.19550v1,"Shanshan Wang, Xueying Zhang, Keyang Wang, Xun Yang, Xingyi Zhang"
"Unprejudiced Training Auxiliary Tasks Makes Primary Better: A Multi-Task
  Learning Perspective","Human beings can leverage knowledge from relative tasks to improve learning
on a primary task. Similarly, multi-task learning methods suggest using
auxiliary tasks to enhance a neural network's performance on a specific primary
task. However, previous methods often select auxiliary tasks carefully but
treat them as secondary during training. The weights assigned to auxiliary
losses are typically smaller than the primary loss weight, leading to
insufficient training on auxiliary tasks and ultimately failing to support the
main task effectively. To address this issue, we propose an uncertainty-based
impartial learning method that ensures balanced training across all tasks.
Additionally, we consider both gradients and uncertainty information during
backpropagation to further improve performance on the primary task. Extensive
experiments show that our method achieves performance comparable to or better
than state-of-the-art approaches. Moreover, our weighting strategy is effective
and robust in enhancing the performance of the primary task regardless the
noise auxiliary tasks' pseudo labels.",2024-12-27T09:27:18Z,http://arxiv.org/abs/2412.19547v1,"Yuanze Li, Chun-Mei Feng, Qilong Wang, Guanglei Yang, Wangmeng Zuo"
Quantiles under ambiguity and risk sharing,"Choquet capacities and integrals are central concepts in decision making
under ambiguity or model uncertainty, pioneered by Schmeidler. Motivated by
risk optimization problems for quantiles under ambiguity, we study the subclass
of Choquet integrals, called Choquet quantiles, which generalizes the usual
(probabilistic) quantiles, also known as Value-at-Risk in finance, from
probabilities to capacities. Choquet quantiles share many features with
probabilistic quantiles, in terms of axiomatic representation, optimization
formulas, and risk sharing. We characterize Choquet quantiles via only one
axiom, called ordinality. We prove that the inf-convolution of Choquet
quantiles is again a Choquet quantile, leading to explicit optimal allocations
in risk sharing problems for quantile agents under ambiguity. A new class of
risk measures, Choquet Expected Shortfall, is introduced, which enjoys most
properties of the coherent risk measure Expected Shortfall. Our theory is
complemented by optimization algorithms, numerical examples, and a stylized
illustration with financial data.",2024-12-27T09:22:19Z,http://arxiv.org/abs/2412.19546v1,"Peng Liu, Tiantian Mao, Ruodu Wang"
"Enhancing Media Literacy: The Effectiveness of (Human) Annotations and
  Bias Visualizations on Bias Detection","Marking biased texts is a practical approach to increase media bias awareness
among news consumers. However, little is known about the generalizability of
such awareness to new topics or unmarked news articles, and the role of
machine-generated bias labels in enhancing awareness remains unclear. This
study tests how news consumers may be trained and pre-bunked to detect media
bias with bias labels obtained from different sources ( (Human or AI) and in
various manifestations. We conducted two experiments with 470 and 846
participants, exposing them to various bias-labeling conditions. We
subsequently tested how much bias they could identify in unlabeled news
materials on new topics. The results show that both Human (t(467) = 4.55, p &lt;
.001, d = 0.42) and AI labels (t(467) = 2.49, p = .039, d = 0.23) increased
correct detection compared to the control group. Human labels demonstrate
larger effect sizes and higher statistical significance. The control group
(t(467) = 4.51, p &lt; .001, d = 0.21) also improves performance through mere
exposure to study materials. We also find that participants trained with marked
biased phrases detected bias most reliably (F(834,1) = 44.00, p &lt; .001,
{\eta}2part = 0.048). Our experimental framework provides theoretical
implications for systematically assessing the generalizability of learning
effects in identifying media bias. These findings also provide practical
implications for developing news-reading platforms that offer bias indicators
and designing media literacy curricula to enhance media bias awareness.",2024-12-27T09:19:22Z,http://arxiv.org/abs/2412.19545v1,"Timo Spinde, Fei Wu, Wolfgang Gaissmaier, Gianluca Demartini, Helge Giese"
"TARGA: Targeted Synthetic Data Generation for Practical Reasoning over
  Structured Data","Semantic parsing, which converts natural language questions into logic forms,
plays a crucial role in reasoning within structured environments. However,
existing methods encounter two significant challenges: reliance on extensive
manually annotated datasets and limited generalization capability to unseen
examples. To tackle these issues, we propose Targeted Synthetic Data Generation
(TARGA), a practical framework that dynamically generates high-relevance
synthetic data without manual annotation. Starting from the pertinent entities
and relations of a given question, we probe for the potential relevant queries
through layer-wise expansion and cross-layer combination. Then we generate
corresponding natural language questions for these constructed queries to
jointly serve as the synthetic demonstrations for in-context learning.
Experiments on multiple knowledge base question answering (KBQA) datasets
demonstrate that TARGA, using only a 7B-parameter model, substantially
outperforms existing non-fine-tuned methods that utilize close-sourced model,
achieving notable improvements in F1 scores on GrailQA(+7.7) and
KBQA-Agent(+12.2). Furthermore, TARGA also exhibits superior sample efficiency,
robustness, and generalization capabilities under non-I.I.D. settings.",2024-12-27T09:16:39Z,http://arxiv.org/abs/2412.19544v1,"Xiang Huang, Jiayu Shen, Shanshan Huang, Sitao Cheng, Xiaxia Wang, Yuzhong Qu"
Diverse Rare Sample Generation with Pretrained GANs,"Deep generative models are proficient in generating realistic data but
struggle with producing rare samples in low density regions due to their
scarcity of training datasets and the mode collapse problem. While recent
methods aim to improve the fidelity of generated samples, they often reduce
diversity and coverage by ignoring rare and novel samples. This study proposes
a novel approach for generating diverse rare samples from high-resolution image
datasets with pretrained GANs. Our method employs gradient-based optimization
of latent vectors within a multi-objective framework and utilizes normalizing
flows for density estimation on the feature space. This enables the generation
of diverse rare images, with controllable parameters for rarity, diversity, and
similarity to a reference image. We demonstrate the effectiveness of our
approach both qualitatively and quantitatively across various datasets and GANs
without retraining or fine-tuning the pretrained GANs.",2024-12-27T09:10:30Z,http://arxiv.org/abs/2412.19543v1,"Subeen Lee, Jiyeon Han, Soyeon Kim, Jaesik Choi"
Interacted Object Grounding in Spatio-Temporal Human-Object Interactions,"Spatio-temporal Human-Object Interaction (ST-HOI) understanding aims at
detecting HOIs from videos, which is crucial for activity understanding.
However, existing whole-body-object interaction video benchmarks overlook the
truth that open-world objects are diverse, that is, they usually provide
limited and predefined object classes. Therefore, we introduce a new open-world
benchmark: Grounding Interacted Objects (GIO) including 1,098 interacted
objects class and 290K interacted object boxes annotation. Accordingly, an
object grounding task is proposed expecting vision systems to discover
interacted objects. Even though today's detectors and grounding methods have
succeeded greatly, they perform unsatisfactorily in localizing diverse and rare
objects in GIO. This profoundly reveals the limitations of current vision
systems and poses a great challenge. Thus, we explore leveraging
spatio-temporal cues to address object grounding and propose a 4D
question-answering framework (4D-QA) to discover interacted objects from
diverse videos. Our method demonstrates significant superiority in extensive
experiments compared to current baselines. Data and code will be publicly
available at https://github.com/DirtyHarryLYL/HAKE-AVA.",2024-12-27T09:08:46Z,http://arxiv.org/abs/2412.19542v1,"Xiaoyang Liu, Boran Wen, Xinpeng Liu, Zizheng Zhou, Hongwei Fan, Cewu Lu, Lizhuang Ma, Yulong Chen, Yong-Lu Li"
"Efficient Computation of the Non-convex Quasi-norm Ball Projection with
  Iterative Reweighted Approach","In this study, we focus on computing the projection onto the $\ell_p$
quasi-norm ball, which is challenging due to the non-convex and non-Lipschitz
nature inherent in the $\ell_p$ quasi-norm with $0&lt;p&lt;1$. We propose a novel
localized approximation method that yields a Lipschitz continuous concave
surrogate function for the $\ell_p$ quasi-norm with improved approximation
quality. Building on this approximation, we enhance the state-of-the-art
iterative reweighted algorithm proposed by Yang et al. (J Mach Learn Res
23:1-31, 2022) by constructing tighter subproblems. This improved algorithm
solves the $\ell_p$ quasinorm ball projection problem through a series of
tractable projections onto the weighted $\ell_1$ norm balls. Convergence
analyses and numerical studies demonstrate the global convergence and superior
computational efficiency of the proposed method.",2024-12-27T09:07:56Z,http://arxiv.org/abs/2412.19541v1,"Qi An, Jiao Wang, Zequn Niu, Nana Zhang"
"Scalable Hierarchical Reinforcement Learning for Hyper Scale Multi-Robot
  Task Planning","To improve the efficiency of warehousing system and meet huge customer
orders, we aim to solve the challenges of dimension disaster and dynamic
properties in hyper scale multi-robot task planning (MRTP) for robotic mobile
fulfillment system (RMFS). Existing research indicates that hierarchical
reinforcement learning (HRL) is an effective method to reduce these challenges.
Based on that, we construct an efficient multi-stage HRL-based multi-robot task
planner for hyper scale MRTP in RMFS, and the planning process is represented
with a special temporal graph topology. To ensure optimality, the planner is
designed with a centralized architecture, but it also brings the challenges of
scaling up and generalization that require policies to maintain performance for
various unlearned scales and maps. To tackle these difficulties, we first
construct a hierarchical temporal attention network (HTAN) to ensure basic
ability of handling inputs with unfixed lengths, and then design multi-stage
curricula for hierarchical policy learning to further improve the scaling up
and generalization ability while avoiding catastrophic forgetting.
Additionally, we notice that policies with hierarchical structure suffer from
unfair credit assignment that is similar to that in multi-agent reinforcement
learning, inspired of which, we propose a hierarchical reinforcement learning
algorithm with counterfactual rollout baseline to improve learning performance.
Experimental results demonstrate that our planner outperform other
state-of-the-art methods on various MRTP instances in both simulated and
real-world RMFS. Also, our planner can successfully scale up to hyper scale
MRTP instances in RMFS with up to 200 robots and 1000 retrieval racks on
unlearned maps while keeping superior performance over other methods.",2024-12-27T09:07:11Z,http://arxiv.org/abs/2412.19538v1,"Xuan Zhou, Xiang Shi, Lele Zhang, Chen Chen, Hongbo Li, Lin Ma, Fang Deng, Jie Chen"
"Finger in Camera Speaks Everything: Unconstrained Air-Writing for
  Real-World","Air-writing is a challenging task that combines the fields of computer vision
and natural language processing, offering an intuitive and natural approach for
human-computer interaction. However, current air-writing solutions face two
primary challenges: (1) their dependency on complex sensors (e.g., Radar, EEGs
and others) for capturing precise handwritten trajectories, and (2) the absence
of a video-based air-writing dataset that covers a comprehensive vocabulary
range. These limitations impede their practicality in various real-world
scenarios, including the use on devices like iPhones and laptops. To tackle
these challenges, we present the groundbreaking air-writing Chinese character
video dataset (AWCV-100K-UCAS2024), serving as a pioneering benchmark for
video-based air-writing. This dataset captures handwritten trajectories in
various real-world scenarios using commonly accessible RGB cameras, eliminating
the need for complex sensors. AWCV-100K-UCAS2024 includes 8.8 million video
frames, encompassing the complete set of 3,755 characters from the GB2312-80
level-1 set (GB1). Furthermore, we introduce our baseline approach, the
video-based character recognizer (VCRec). VCRec adeptly extracts fingertip
features from sparse visual cues and employs a spatio-temporal sequence module
for analysis. Experimental results showcase the superior performance of VCRec
compared to existing models in recognizing air-written characters, both
quantitatively and qualitatively. This breakthrough paves the way for enhanced
human-computer interaction in real-world contexts. Moreover, our approach
leverages affordable RGB cameras, enabling its applicability in a diverse range
of scenarios. The code and data examples will be made public at
https://github.com/wmeiqi/AWCV.",2024-12-27T09:04:04Z,http://arxiv.org/abs/2412.19537v1,"Meiqi Wu, Kaiqi Huang, Yuanqiang Cai, Shiyu Hu, Yuzhong Zhao, Weiqiang Wang"
"P3S-Diffusion:A Selective Subject-driven Generation Framework via Point
  Supervision","Recent research in subject-driven generation increasingly emphasizes the
importance of selective subject features. Nevertheless, accurately selecting
the content in a given reference image still poses challenges, especially when
selecting the similar subjects in an image (e.g., two different dogs). Some
methods attempt to use text prompts or pixel masks to isolate specific
elements. However, text prompts often fall short in precisely describing
specific content, and pixel masks are often expensive. To address this, we
introduce P3S-Diffusion, a novel architecture designed for context-selected
subject-driven generation via point supervision. P3S-Diffusion leverages
minimal cost label (e.g., points) to generate subject-driven images. During
fine-tuning, it can generate an expanded base mask from these points, obviating
the need for additional segmentation models. The mask is employed for
inpainting and aligning with subject representation. The P3S-Diffusion
preserves fine features of the subjects through Multi-layers Condition
Injection. Enhanced by the Attention Consistency Loss for improved training,
extensive experiments demonstrate its excellent feature preservation and image
generation capabilities.",2024-12-27T08:59:01Z,http://arxiv.org/abs/2412.19533v1,"Junjie Hu, Shuyong Gao, Lingyi Hong, Qishan Wang, Yuzhou Zhao, Yan Wang, Wenqiang Zhang"
Is Your Text-to-Image Model Robust to Caption Noise?,"In text-to-image (T2I) generation, a prevalent training technique involves
utilizing Vision Language Models (VLMs) for image re-captioning. Even though
VLMs are known to exhibit hallucination, generating descriptive content that
deviates from the visual reality, the ramifications of such caption
hallucinations on T2I generation performance remain under-explored. Through our
empirical investigation, we first establish a comprehensive dataset comprising
VLM-generated captions, and then systematically analyze how caption
hallucination influences generation outcomes. Our findings reveal that (1) the
disparities in caption quality persistently impact model outputs during
fine-tuning. (2) VLMs confidence scores serve as reliable indicators for
detecting and characterizing noise-related patterns in the data distribution.
(3) even subtle variations in caption fidelity have significant effects on the
quality of learned representations. These findings collectively emphasize the
profound impact of caption quality on model performance and highlight the need
for more sophisticated robust training algorithm in T2I. In response to these
observations, we propose a approach leveraging VLM confidence score to mitigate
caption noise, thereby enhancing the robustness of T2I models against
hallucination in caption.",2024-12-27T08:53:37Z,http://arxiv.org/abs/2412.19531v1,"Weichen Yu, Ziyan Yang, Shanchuan Lin, Qi Zhao, Jianyi Wang, Liangke Gui, Matt Fredrikson, Lu Jiang"
"The Value of AI Advice: Personalized and Value-Maximizing AI Advisors
  Are Necessary to Reliably Benefit Experts and Organizations","Despite advances in AI's performance and interpretability, AI advisors can
undermine experts' decisions and increase the time and effort experts must
invest to make decisions. Consequently, AI systems deployed in high-stakes
settings often fail to consistently add value across contexts and can even
diminish the value that experts alone provide. Beyond harm in specific domains,
such outcomes impede progress in research and practice, underscoring the need
to understand when and why different AI advisors add or diminish value. To
bridge this gap, we stress the importance of assessing the value AI advice
brings to real-world contexts when designing and evaluating AI advisors.
Building on this perspective, we characterize key pillars -- pathways through
which AI advice impacts value -- and develop a framework that incorporates
these pillars to create reliable, personalized, and value-adding advisors. Our
results highlight the need for system-level, value-driven development of AI
advisors that advise selectively, are tailored to experts' unique behaviors,
and are optimized for context-specific trade-offs between decision improvements
and advising costs. They also reveal how the lack of inclusion of these pillars
in the design of AI advising systems may be contributing to the failures
observed in practical applications.",2024-12-27T08:50:54Z,http://arxiv.org/abs/2412.19530v1,"Nicholas Wolczynski, Maytal Saar-Tsechansky, Tong Wang"
"Nonconvex Stochastic Optimization under Heavy-Tailed Noises: Optimal
  Convergence without Gradient Clipping","Recently, the study of heavy-tailed noises in first-order nonconvex
stochastic optimization has gotten a lot of attention since it was recognized
as a more realistic condition as suggested by many empirical observations.
Specifically, the stochastic noise (the difference between the stochastic and
true gradient) is considered only to have a finite $\mathfrak{p}$-th moment
where $\mathfrak{p}\in\left(1,2\right]$ instead of assuming it always satisfies
the classical finite variance assumption. To deal with this more challenging
setting, people have proposed different algorithms and proved them to converge
at an optimal $\mathcal{O}(T^{\frac{1-\mathfrak{p}}{3\mathfrak{p}-2}})$ rate
for smooth objectives after $T$ iterations. Notably, all these new-designed
algorithms are based on the same technique - gradient clipping. Naturally, one
may want to know whether the clipping method is a necessary ingredient and the
only way to guarantee convergence under heavy-tailed noises. In this work, by
revisiting the existing Batched Normalized Stochastic Gradient Descent with
Momentum (Batched NSGDM) algorithm, we provide the first convergence result
under heavy-tailed noises but without gradient clipping. Concretely, we prove
that Batched NSGDM can achieve the optimal
$\mathcal{O}(T^{\frac{1-\mathfrak{p}}{3\mathfrak{p}-2}})$ rate even under the
relaxed smooth condition. More interestingly, we also establish the first
$\mathcal{O}(T^{\frac{1-\mathfrak{p}}{2\mathfrak{p}}})$ convergence rate in the
case where the tail index $\mathfrak{p}$ is unknown in advance, which is
arguably the common scenario in practice.",2024-12-27T08:46:46Z,http://arxiv.org/abs/2412.19529v1,"Zijian Liu, Zhengyuan Zhou"
Magnon-Phonon Coupling in Layered Antiferromagnet,"We present a fully analytical model of hybridization between magnon, and
phonons observed experimentally in magneto-Raman scattering in van der Waals
(vdW) antiferromagnets (AFM). Here, the representative material, FePS3, has
been shown to be a quasi-two-dimensional-Ising antiferromagnet, with additional
features of spin-phonon coupling in the Raman spectra emerging below the N\'eel
temperature (TN) of approximately 120 K. Using magneto-Raman spectroscopy as an
optical probe of magnetic structure, we show that one of these Raman-active
modes in the magnetically ordered state is a magnon with a frequency of 3.7 THz
(~ 122 cm-1). In addition, one magnon band and three phonon bands are coupled
via the magneto-elastic coupling evidenced by anti-crossing in the complete
spectra. We consider a simple model involving only in-plane nearest neighbor
exchange couplings (designed to give rise to a similar magnetic structure) and
perpendicular anisotropy in presence of an out-of-plane magnetic field. Exact
diagonalization of the Hamiltonian leads to energy bands which show that the
interaction term gives rise to avoided crossings between the hybridized magnon
and phonon branches. Realizing magnon-phonon coupling in two-dimensional (2D)
AFMs is important for the verification of the theoretical predictions on exotic
quantum transport phenomena like spin-caloritronics, topological magnonics,
etc.",2024-12-27T08:38:33Z,http://arxiv.org/abs/2412.19526v1,"Somsubhra Ghosh, Mainak Palit, Sujan Maity, Subhadeep Datta"
"Lévy Score Function and Score-Based Particle Algorithm for Nonlinear
  Lévy--Fokker--Planck Equations","The score function for the diffusion process, also known as the gradient of
the log-density, is a basic concept to characterize the probability flow with
important applications in the score-based diffusion generative modelling and
the simulation of It\^{o} stochastic differential equations. However, neither
the probability flow nor the corresponding score function for the
diffusion-jump process are known. This paper delivers mathematical derivation,
numerical algorithm, and error analysis focusing on the corresponding score
function in non-Gaussian systems with jumps and discontinuities represented by
the nonlinear L\'{e}vy--Fokker--Planck equations. We propose the L\'{e}vy score
function for such stochastic equations, which features a nonlocal
double-integral term, and we develop its training algorithm by minimizing the
proposed loss function from samples. Based on the equivalence of the
probability flow with deterministic dynamics, we develop a self-consistent
score-based transport particle algorithm to sample the interactive L\'{e}vy
stochastic process at discrete time grid points. We provide error bound for the
Kullback--Leibler divergence between the numerical and true probability density
functions by overcoming the nonlocal challenges in the L\'{e}vy score. The full
error analysis with the Monte Carlo error and the time discretization error is
furthermore established. To show the usefulness and efficiency of our approach,
numerical examples from applications in biology and finance are tested.",2024-12-27T08:23:04Z,http://arxiv.org/abs/2412.19520v1,"Yuanfei Huang, Chengyu Liu, Xiang Zhou"
"Dust to Tower: Coarse-to-Fine Photo-Realistic Scene Reconstruction from
  Sparse Uncalibrated Images","Photo-realistic scene reconstruction from sparse-view, uncalibrated images is
highly required in practice. Although some successes have been made, existing
methods are either Sparse-View but require accurate camera parameters (i.e.,
intrinsic and extrinsic), or SfM-free but need densely captured images. To
combine the advantages of both methods while addressing their respective
weaknesses, we propose Dust to Tower (D2T), an accurate and efficient
coarse-to-fine framework to optimize 3DGS and image poses simultaneously from
sparse and uncalibrated images. Our key idea is to first construct a coarse
model efficiently and subsequently refine it using warped and inpainted images
at novel viewpoints. To do this, we first introduce a Coarse Construction
Module (CCM) which exploits a fast Multi-View Stereo model to initialize a 3D
Gaussian Splatting (3DGS) and recover initial camera poses. To refine the 3D
model at novel viewpoints, we propose a Confidence Aware Depth Alignment (CADA)
module to refine the coarse depth maps by aligning their confident parts with
estimated depths by a Mono-depth model. Then, a Warped Image-Guided Inpainting
(WIGI) module is proposed to warp the training images to novel viewpoints by
the refined depth maps, and inpainting is applied to fulfill the ``holes"" in
the warped images caused by view-direction changes, providing high-quality
supervision to further optimize the 3D model and the camera poses. Extensive
experiments and ablation studies demonstrate the validity of D2T and its design
choices, achieving state-of-the-art performance in both tasks of novel view
synthesis and pose estimation while keeping high efficiency. Codes will be
publicly available.",2024-12-27T08:19:34Z,http://arxiv.org/abs/2412.19518v1,"Xudong Cai, Yongcai Wang, Zhaoxin Fan, Deng Haoran, Shuo Wang, Wanting Li, Deying Li, Lun Luo, Minhang Wang, Jintao Xu"
"Estimation of System Parameters Including Repeated Cross-Sectional Data
  through Emulator-Informed Deep Generative Model","Differential equations (DEs) are crucial for modeling the evolution of
natural or engineered systems. Traditionally, the parameters in DEs are
adjusted to fit data from system observations. However, in fields such as
politics, economics, and biology, available data are often independently
collected at distinct time points from different subjects (i.e., repeated
cross-sectional (RCS) data). Conventional optimization techniques struggle to
accurately estimate DE parameters when RCS data exhibit various
heterogeneities, leading to a significant loss of information. To address this
issue, we propose a new estimation method called the emulator-informed
deep-generative model (EIDGM), designed to handle RCS data. Specifically, EIDGM
integrates a physics-informed neural network-based emulator that immediately
generates DE solutions and a Wasserstein generative adversarial network-based
parameter generator that can effectively mimic the RCS data. We evaluated EIDGM
on exponential growth, logistic population models, and the Lorenz system,
demonstrating its superior ability to accurately capture parameter
distributions. Additionally, we applied EIDGM to an experimental dataset of
Amyloid beta 40 and beta 42, successfully capturing diverse parameter
distribution shapes. This shows that EIDGM can be applied to model a wide range
of systems and extended to uncover the operating principles of systems based on
limited data.",2024-12-27T08:19:23Z,http://arxiv.org/abs/2412.19517v1,"Hyunwoo Cho, Sung Woong Cho, Hyeontae Jo, Hyung Ju Hwang"
"Real-time classification of EEG signals using Machine Learning
  deployment","The prevailing educational methods predominantly rely on traditional
classroom instruction or online delivery, often limiting the teachers' ability
to engage effectively with all the students simultaneously. A more intrinsic
method of evaluating student attentiveness during lectures can enable the
educators to tailor the course materials and their teaching styles in order to
better meet the students' needs. The aim of this paper is to enhance teaching
quality in real time, thereby fostering a higher student engagement in the
classroom activities. By monitoring the students' electroencephalography (EEG)
signals and employing machine learning algorithms, this study proposes a
comprehensive solution for addressing this challenge. Machine learning has
emerged as a powerful tool for simplifying the analysis of complex variables,
enabling the effective assessment of the students' concentration levels based
on specific parameters. However, the real-time impact of machine learning
models necessitates a careful consideration as their deployment is concerned.
This study proposes a machine learning-based approach for predicting the level
of students' comprehension with regard to a certain topic. A browser interface
was introduced that accesses the values of the system's parameters to determine
a student's level of concentration on a chosen topic. The deployment of the
proposed system made it necessary to address the real-time challenges faced by
the students, consider the system's cost, and establish trust in its efficacy.
This paper presents the efforts made for approaching this pertinent issue
through the implementation of innovative technologies and provides a framework
for addressing key considerations for future research directions.",2024-12-27T08:14:28Z,http://arxiv.org/abs/2412.19515v1,"Swati Chowdhuri, Satadip Saha, Samadrita Karmakar, Ankur Chanda"
"Confidence v.s. Critique: A Decomposition of Self-Correction Capability
  for LLMs","Large Language Models (LLMs) can correct their self-generated responses, but
a decline in accuracy after self-correction is also witnessed. To have a deeper
understanding of self-correction, we endeavor to decompose, evaluate, and
analyze the self-correction behaviors of LLMs. By enumerating and analyzing
answer correctness before and after self-correction, we decompose the
self-correction capability into confidence (being confident to correct answers)
and critique (turning wrong answers to correct) capabilities, and propose two
metrics from a probabilistic perspective to measure these 2 capabilities, along
with another metric for overall self-correction capability evaluation. Based on
our decomposition and evaluation metrics, we conduct extensive experiments and
draw some empirical conclusions. For example, we find different models can
exhibit distinct behaviors: some models are confident while others are more
critical. We also find the trade-off between the two capabilities (i.e.
improving one can lead to a decline in the other) when manipulating model
self-correction behavior by prompts or in-context learning. Further, we find a
simple yet efficient strategy to improve self-correction capability by
transforming Supervision Fine-Tuning (SFT) data format, and our strategy
outperforms vanilla SFT in both capabilities and achieves much higher accuracy
after self-correction. Our code will be publicly available on GitHub.",2024-12-27T08:09:11Z,http://arxiv.org/abs/2412.19513v1,"Zhe Yang, Yichang Zhang, Yudong Wang, Ziyao Xu, Junyang Lin, Zhifang Sui"
"Uncertainty quantification for improving radiomic-based models in
  radiation pneumonitis prediction","Background and Objective: Radiation pneumonitis (RP) is a side effect of
thoracic radiation therapy. Recently, Machine learning (ML) models enhanced
with radiomic and dosiomic features provide better predictions by incorporating
spatial information beyond DVHs. However, to improve the clinical decision
process, we propose to use uncertainty quantification (UQ) to improve the
confidence in model prediction. This study evaluates the impact of post hoc UQ
methods on the discriminative performance and calibration of ML models for RP
prediction. Methods: This study evaluated four ML models: logistic regression
(LR), support vector machines (SVM), extreme gradient boosting (XGB), and
random forest (RF), using radiomic, dosiomic, and dosimetric features to
predict RP. We applied UQ methods, including Patt scaling, isotonic regression,
Venn-ABERS predictor, and Conformal Prediction, to quantify uncertainty. Model
performance was assessed through Area Under the Receiver Operating
Characteristic curve (AUROC), Area Under the Precision-Recall Curve (AUPRC),
and Adaptive Calibration Error (ACE) using Leave-One-Out Cross-Validation
(LOO-CV). Results: UQ methods enhanced predictive performance, particularly for
high-certainty predictions, while also improving calibration. Radiomic and
dosiomic features increased model accuracy but introduced calibration
challenges, especially for non-linear models like XGB and RF. Performance gains
from UQ methods were most noticeable at higher certainty thresholds.
Conclusion: Integrating UQ into ML models with radiomic and dosiomic features
improves both predictive accuracy and calibration, supporting more reliable
clinical decision-making. The findings emphasize the value of UQ methods in
enhancing applicability of predictive models for RP in healthcare settings.",2024-12-27T08:01:42Z,http://arxiv.org/abs/2412.19511v1,"Chanon Puttanawarut, Romen Samuel Wabina, Nat Sirirutbunkajorn"
"Parameter Efficient Fine-Tuning for Deep Learning-Based Full-Waveform
  Inversion","Seismic full waveform inversion (FWI) has seen promising advancements through
deep learning. Existing approaches typically focus on task-specific models
trained and evaluated in isolation that lead to limited generalization across
different geological scenarios. In this work we introduce a task-agnostic
foundational model for FWI that captures general features across tasks. We
first demonstrate that full fine-tuning of this foundational model outperforms
task-specific models built from scratch by delivering superior performance
across multiple benchmarks. Building upon this we employ parameter-efficient
fine-tuning (PEFT) to further reduce computational overhead. By fine-tuning
only a small fraction of the model parameters PEFT achieves comparable results
to full fine-tuning while significantly lowering memory and computational
requirements. Additionally, PEFT excels in out-of-distribution tasks where it
outperforms both full fine-tuning and task-specific models. These findings
establish the value of foundational modeling for FWI and highlight PEFT as an
effective strategy for efficient and scalable adaptation across diverse tasks.",2024-12-27T08:00:12Z,http://arxiv.org/abs/2412.19510v1,"Koustav Ghosal, Abhranta Panigrahi, Arnav Chavan, ArunSingh, Deepak Gupta"
Hybrid Local Causal Discovery,"Local causal discovery aims to learn and distinguish the direct causes and
effects of a target variable from observed data. Existing constraint-based
local causal discovery methods use AND or OR rules in constructing the local
causal skeleton, but using either rule alone is prone to produce cascading
errors in the learned local causal skeleton, and thus impacting the inference
of local causal relationships. On the other hand, directly applying score-based
global causal discovery methods to local causal discovery may randomly return
incorrect results due to the existence of local equivalence classes. To address
the above issues, we propose a Hybrid Local Causal Discovery algorithm, called
HLCD. Specifically, HLCD initially utilizes a constraint-based approach
combined with the OR rule to obtain a candidate skeleton and then employs a
score-based method to eliminate redundant portions in the candidate skeleton.
Furthermore, during the local causal orientation phase, HLCD distinguishes
between V-structures and equivalence classes by comparing the local structure
scores between the two, thereby avoiding orientation interference caused by
local equivalence classes. We conducted extensive experiments with seven
state-of-the-art competitors on 14 benchmark Bayesian network datasets, and the
experimental results demonstrate that HLCD significantly outperforms existing
local causal discovery algorithms.",2024-12-27T07:53:59Z,http://arxiv.org/abs/2412.19507v1,"Zhaolong Ling, Honghui Peng, Yiwen Zhang, Peng Zhou, Xingyu Wu, Kui Yu, Xindong Wu"
"DrivingWorld: ConstructingWorld Model for Autonomous Driving via Video
  GPT","Recent successes in autoregressive (AR) generation models, such as the GPT
series in natural language processing, have motivated efforts to replicate this
success in visual tasks. Some works attempt to extend this approach to
autonomous driving by building video-based world models capable of generating
realistic future video sequences and predicting ego states. However, prior
works tend to produce unsatisfactory results, as the classic GPT framework is
designed to handle 1D contextual information, such as text, and lacks the
inherent ability to model the spatial and temporal dynamics essential for video
generation. In this paper, we present DrivingWorld, a GPT-style world model for
autonomous driving, featuring several spatial-temporal fusion mechanisms. This
design enables effective modeling of both spatial and temporal dynamics,
facilitating high-fidelity, long-duration video generation. Specifically, we
propose a next-state prediction strategy to model temporal coherence between
consecutive frames and apply a next-token prediction strategy to capture
spatial information within each frame. To further enhance generalization
ability, we propose a novel masking strategy and reweighting strategy for token
prediction to mitigate long-term drifting issues and enable precise control.
Our work demonstrates the ability to produce high-fidelity and consistent video
clips of over 40 seconds in duration, which is over 2 times longer than
state-of-the-art driving world models. Experiments show that, in contrast to
prior works, our method achieves superior visual quality and significantly more
accurate controllable future video generation. Our code is available at
https://github.com/YvanYin/DrivingWorld.",2024-12-27T07:44:07Z,http://arxiv.org/abs/2412.19505v1,"Xiaotao Hu, Wei Yin, Mingkai Jia, Junyuan Deng, Xiaoyang Guo, Qian Zhang, Xiaoxiao Long, Ping Tan"
Hear the Scene: Audio-Enhanced Text Spotting,"Recent advancements in scene text spotting have focused on end-to-end
methodologies that heavily rely on precise location annotations, which are
often costly and labor-intensive to procure. In this study, we introduce an
innovative approach that leverages only transcription annotations for training
text spotting models, substantially reducing the dependency on elaborate
annotation processes. Our methodology employs a query-based paradigm that
facilitates the learning of implicit location features through the interaction
between text queries and image embeddings. These features are later refined
during the text recognition phase using an attention activation map. Addressing
the challenges associated with training a weakly-supervised model from scratch,
we implement a circular curriculum learning strategy to enhance model
convergence. Additionally, we introduce a coarse-to-fine cross-attention
localization mechanism for more accurate text instance localization. Notably,
our framework supports audio-based annotation, which significantly diminishes
annotation time and provides an inclusive alternative for individuals with
disabilities. Our approach achieves competitive performance against existing
benchmarks, demonstrating that high accuracy in text spotting can be attained
without extensive location annotations.",2024-12-27T07:44:05Z,http://arxiv.org/abs/2412.19504v1,"Jing Li, Bo Wang"
"RobotDiffuse: Motion Planning for Redundant Manipulator based on
  Diffusion Model","Redundant manipulators, with their higher Degrees of Freedom (DOFs), offer
enhanced kinematic performance and versatility, making them suitable for
applications like manufacturing, surgical robotics, and human-robot
collaboration. However, motion planning for these manipulators is challenging
due to increased DOFs and complex, dynamic environments. While traditional
motion planning algorithms struggle with high-dimensional spaces, deep
learning-based methods often face instability and inefficiency in complex
tasks. This paper introduces RobotDiffuse, a diffusion model-based approach for
motion planning in redundant manipulators. By integrating physical constraints
with a point cloud encoder and replacing the U-Net structure with an
encoder-only transformer, RobotDiffuse improves the model's ability to capture
temporal dependencies and generate smoother, more coherent motion plans. We
validate the approach using a complex simulator, and release a new dataset with
35M robot poses and 0.14M obstacle avoidance scenarios. Experimental results
demonstrate the effectiveness of RobotDiffuse and the promise of diffusion
models for motion planning tasks. The code can be accessed at
https://github.com/ACRoboT-buaa/RobotDiffuse.",2024-12-27T07:34:54Z,http://arxiv.org/abs/2412.19500v1,"Xiaohan Zhang, Xudong Mou, Rui Wang, Tianyu Wo, Ningbo Gu, Tiejun Wang, Cangbai Xu, Xudong Liu"
Casevo: A Cognitive Agents and Social Evolution Simulator,"In this paper, we introduce a multi-agent simulation framework Casevo
(Cognitive Agents and Social Evolution Simulator), that integrates large
language models (LLMs) to simulate complex social phenomena and decision-making
processes. Casevo is designed as a discrete-event simulator driven by agents
with features such as Chain of Thoughts (CoT), Retrieval-Augmented Generation
(RAG), and Customizable Memory Mechanism. Casevo enables dynamic social
modeling, which can support various scenarios such as social network analysis,
public opinion dynamics, and behavior prediction in complex social systems. To
demonstrate the effectiveness of Casevo, we utilize one of the U.S. 2020
midterm election TV debates as a simulation example. Our results show that
Casevo facilitates more realistic and flexible agent interactions, improving
the quality of dynamic social phenomena simulation. This work contributes to
the field by providing a robust system for studying large-scale, high-fidelity
social behaviors with advanced LLM-driven agents, expanding the capabilities of
traditional agent-based modeling (ABM). The open-source code repository address
of casevo is https://github.com/rgCASS/casevo.",2024-12-27T07:33:49Z,http://arxiv.org/abs/2412.19498v1,"Zexun Jiang, Yafang Shi, Maoxu Li, Hongjiang Xiao, Yunxiao Qin, Qinglan Wei, Ye Wang, Yuan Zhang"
"Disparate Model Performance and Stability in Machine Learning Clinical
  Support for Diabetes and Heart Diseases","Machine Learning (ML) algorithms are vital for supporting clinical
decision-making in biomedical informatics. However, their predictive
performance can vary across demographic groups, often due to the
underrepresentation of historically marginalized populations in training
datasets. The investigation reveals widespread sex- and age-related inequities
in chronic disease datasets and their derived ML models. Thus, a novel
analytical framework is introduced, combining systematic arbitrariness with
traditional metrics like accuracy and data complexity. The analysis of data
from over 25,000 individuals with chronic diseases revealed mild sex-related
disparities, favoring predictive accuracy for males, and significant
age-related differences, with better accuracy for younger patients. Notably,
older patients showed inconsistent predictive accuracy across seven datasets,
linked to higher data complexity and lower model performance. This highlights
that representativeness in training data alone does not guarantee equitable
outcomes, and model arbitrariness must be addressed before deploying models in
clinical settings.",2024-12-27T07:31:14Z,http://arxiv.org/abs/2412.19495v1,"Ioannis Bilionis, Ricardo C. Berrios, Luis Fernandez-Luque, Carlos Castillo"
Towards Open-Vocabulary Remote Sensing Image Semantic Segmentation,"Recently, deep learning based methods have revolutionized remote sensing
image segmentation. However, these methods usually rely on a pre-defined
semantic class set, thus needing additional image annotation and model training
when adapting to new classes. More importantly, they are unable to segment
arbitrary semantic classes. In this work, we introduce Open-Vocabulary Remote
Sensing Image Semantic Segmentation (OVRSISS), which aims to segment arbitrary
semantic classes in remote sensing images. To address the lack of OVRSISS
datasets, we develop LandDiscover50K, a comprehensive dataset of 51,846 images
covering 40 diverse semantic classes. In addition, we propose a novel framework
named GSNet that integrates domain priors from special remote sensing models
and versatile capabilities of general vision-language models. Technically,
GSNet consists of a Dual-Stream Image Encoder (DSIE), a Query-Guided Feature
Fusion (QGFF), and a Residual Information Preservation Decoder (RIPD). DSIE
first captures comprehensive features from both special models and general
models in dual streams. Then, with the guidance of variable vocabularies, QGFF
integrates specialist and generalist features, enabling them to complement each
other. Finally, RIPD is proposed to aggregate multi-source features for more
accurate mask predictions. Experiments show that our method outperforms other
methods by a large margin, and our proposed LandDiscover50K improves the
performance of OVRSISS methods. The proposed dataset and method will be made
publicly available at https://github.com/yecy749/GSNet.",2024-12-27T07:20:30Z,http://arxiv.org/abs/2412.19492v1,"Chengyang Ye, Yunzhi Zhuge, Pingping Zhang"
"Multi-label Classification using Deep Multi-order Context-aware Kernel
  Networks","Multi-label classification is a challenging task in pattern recognition. Many
deep learning methods have been proposed and largely enhanced classification
performance. However, most of the existing sophisticated methods ignore context
in the models' learning process. Since context may provide additional cues to
the learned models, it may significantly boost classification performances. In
this work, we make full use of context information (namely geometrical
structure of images) in order to learn better context-aware similarities
(a.k.a. kernels) between images. We reformulate context-aware kernel design as
a feed-forward network that outputs explicit kernel mapping features. Our
obtained context-aware kernel network further leverages multiple orders of
patch neighbors within different distances, resulting into a more
discriminating Deep Multi-order Context-aware Kernel Network (DMCKN) for
multi-label classification. We evaluate the proposed method on the challenging
Corel5K and NUS-WIDE benchmarks, and empirical results show that our method
obtains competitive performances against the related state-of-the-art, and both
quantitative and qualitative performances corroborate its effectiveness and
superiority for multi-label image classification.",2024-12-27T07:16:11Z,http://arxiv.org/abs/2412.19491v1,"Mingyuan Jiu, Hailong Zhu, Hichem Sahbi"
UniBrain: A Unified Model for Cross-Subject Brain Decoding,"Brain decoding aims to reconstruct original stimuli from fMRI signals,
providing insights into interpreting mental content. Current approaches rely
heavily on subject-specific models due to the complex brain processing
mechanisms and the variations in fMRI signals across individuals. Therefore,
these methods greatly limit the generalization of models and fail to capture
cross-subject commonalities. To address this, we present UniBrain, a unified
brain decoding model that requires no subject-specific parameters. Our approach
includes a group-based extractor to handle variable fMRI signal lengths, a
mutual assistance embedder to capture cross-subject commonalities, and a
bilevel feature alignment scheme for extracting subject-invariant features. We
validate our UniBrain on the brain decoding benchmark, achieving comparable
performance to current state-of-the-art subject-specific models with extremely
fewer parameters. We also propose a generalization benchmark to encourage the
community to emphasize cross-subject commonalities for more general brain
decoding. Our code is available at https://github.com/xiaoyao3302/UniBrain.",2024-12-27T07:03:47Z,http://arxiv.org/abs/2412.19487v1,"Zicheng Wang, Zhen Zhao, Luping Zhou, Parashkev Nachev"
Learning Radiance Fields from a Single Snapshot Compressive Image,"In this paper, we explore the potential of Snapshot Compressive Imaging (SCI)
technique for recovering the underlying 3D scene structure from a single
temporal compressed image. SCI is a cost-effective method that enables the
recording of high-dimensional data, such as hyperspectral or temporal
information, into a single image using low-cost 2D imaging sensors. To achieve
this, a series of specially designed 2D masks are usually employed, reducing
storage and transmission requirements and offering potential privacy
protection. Inspired by this, we take one step further to recover the encoded
3D scene information leveraging powerful 3D scene representation capabilities
of neural radiance fields (NeRF). Specifically, we propose SCINeRF, in which we
formulate the physical imaging process of SCI as part of the training of NeRF,
allowing us to exploit its impressive performance in capturing complex scene
structures. In addition, we further integrate the popular 3D Gaussian Splatting
(3DGS) framework and propose SCISplat to improve 3D scene reconstruction
quality and training/rendering speed by explicitly optimizing point clouds into
3D Gaussian representations. To assess the effectiveness of our method, we
conduct extensive evaluations using both synthetic data and real data captured
by our SCI system. Experimental results demonstrate that our proposed approach
surpasses the state-of-the-art methods in terms of image reconstruction and
novel view synthesis. Moreover, our method also exhibits the ability to render
high frame-rate multi-view consistent images in real time by leveraging SCI and
the rendering capabilities of 3DGS. Codes will be available at:
https://github.com/WU- CVGL/SCISplat.",2024-12-27T06:40:44Z,http://arxiv.org/abs/2412.19483v1,"Yunhao Li, Xiang Liu, Xiaodong Wang, Xin Yuan, Peidong Liu"
"Pre-training, Fine-tuning and Re-ranking: A Three-Stage Framework for
  Legal Question Answering","Legal question answering (QA) has attracted increasing attention from people
seeking legal advice, which aims to retrieve the most applicable answers from a
large-scale database of question-answer pairs. Previous methods mainly use a
dual-encoder architecture to learn dense representations of both questions and
answers. However, these methods could suffer from lacking domain knowledge and
sufficient labeled training data. In this paper, we propose a three-stage
(\underline{p}re-training, \underline{f}ine-tuning and \underline{r}e-ranking)
framework for \underline{l}egal \underline{QA} (called PFR-LQA), which promotes
the fine-grained text representation learning and boosts the performance of
dense retrieval with the dual-encoder architecture. Concretely, we first
conduct domain-specific pre-training on legal questions and answers through a
self-supervised training objective, allowing the pre-trained model to be
adapted to the legal domain. Then, we perform task-specific fine-tuning of the
dual-encoder on legal question-answer pairs by using the supervised learning
objective, leading to a high-quality dual-encoder for the specific downstream
QA task. Finally, we employ a contextual re-ranking objective to further refine
the output representations of questions produced by the document encoder, which
uses contextual similarity to increase the discrepancy between the anchor and
hard negative samples for better question re-ranking. We conduct extensive
experiments on a manually annotated legal QA dataset. Experimental results show
that our PFR-LQA method achieves better performance than the strong competitors
for legal question answering.",2024-12-27T06:33:42Z,http://arxiv.org/abs/2412.19482v1,"Shiwen Ni, Hao Cheng, Min Yang"
Generative Adversarial Network on Motion-Blur Image Restoration,"In everyday life, photographs taken with a camera often suffer from motion
blur due to hand vibrations or sudden movements. This phenomenon can
significantly detract from the quality of the images captured, making it an
interesting challenge to develop a deep learning model that utilizes the
principles of adversarial networks to restore clarity to these blurred pixels.
In this project, we will focus on leveraging Generative Adversarial Networks
(GANs) to effectively deblur images affected by motion blur. A GAN-based
Tensorflow model is defined, training and evaluating by GoPro dataset which
comprises paired street view images featuring both clear and blurred versions.
This adversarial training process between Discriminator and Generator helps to
produce increasingly realistic images over time. Peak Signal-to-Noise Ratio
(PSNR) and Structural Similarity Index Measure (SSIM) are the two evaluation
metrics used to provide quantitative measures of image quality, allowing us to
evaluate the effectiveness of the deblurring process. Mean PSNR in 29.1644 and
mean SSIM in 0.7459 with average 4.6921 seconds deblurring time are achieved in
this project. The blurry pixels are sharper in the output of GAN model shows a
good image restoration effect in real world applications.",2024-12-27T06:12:50Z,http://arxiv.org/abs/2412.19479v1,Zhengdong Li
"An Overview of Machine Learning-Driven Resource Allocation in IoT
  Networks","In the wake of disruptive IoT technologies generating massive amounts of
diverse data, Machine Learning (ML) will play a crucial role in bringing
intelligence to Internet of Things (IoT) networks. This paper provides a
comprehensive analysis of the current state of resource allocation within IoT
networks, focusing specifically on two key categories: Low-Power IoT Networks
and Mobile IoT Networks. We delve into the resource allocation strategies that
are crucial for optimizing network performance and energy efficiency in these
environments. Furthermore, the paper explores the transformative role of
Machine Learning (ML), Deep Learning (DL), and Reinforcement Learning (RL) in
enhancing IoT functionalities. We highlight a range of applications and use
cases where these advanced technologies can significantly improve
decision-making and optimization processes. In addition to the opportunities
presented by ML, DL, and RL, we also address the potential challenges that
organizations may face when implementing these technologies in IoT settings.
These challenges include crucial accuracy, low flexibility and adaptability,
and high computational cost, etc. Finally, the paper identifies promising
avenues for future research, emphasizing the need for innovative solutions to
overcome existing hurdles and improve the integration of ML, DL, and RL into
IoT networks. By providing this holistic perspective, we aim to contribute to
the ongoing discourse on resource allocation strategies and the application of
intelligent technologies in the IoT landscape.",2024-12-27T06:11:28Z,http://arxiv.org/abs/2412.19478v1,Zhengdong Li
"Meta-Learning-Based Delayless Subband Adaptive Filter using Complex
  Self-Attention for Active Noise Control","Active noise control typically employs adaptive filtering to generate
secondary noise, where the least mean square algorithm is the most widely used.
However, traditional updating rules are linear and exhibit limited
effectiveness in addressing nonlinear environments and nonstationary noise. To
tackle this challenge, we reformulate the active noise control problem as a
meta-learning problem and propose a meta-learning-based delayless subband
adaptive filter with deep neural networks. The core idea is to utilize a neural
network as an adaptive algorithm that can adapt to different environments and
types of noise. The neural network will train under noisy observations,
implying that it recognizes the optimized updating rule without true labels. A
single-headed attention recurrent neural network is devised with learnable
feature embedding to update the adaptive filter weight efficiently, enabling
accurate computation of the secondary source to attenuate the unwanted primary
noise. In order to relax the time constraint on updating the adaptive filter
weights, the delayless subband architecture is employed, which will allow the
system to be updated less frequently as the downsampling factor increases. In
addition, the delayless subband architecture does not introduce additional time
delays in active noise control systems. A skip updating strategy is introduced
to decrease the updating frequency further so that machines with limited
resources have more possibility to board our meta-learning-based model.
Extensive multi-condition training ensures generalization and robustness
against various types of noise and environments. Simulation results demonstrate
that our meta-learning-based model achieves superior noise reduction
performance compared to traditional methods.",2024-12-27T05:51:40Z,http://arxiv.org/abs/2412.19471v1,"Pengxing Feng, Hing Cheung So"
"Optimizing Helmet Detection with Hybrid YOLO Pipelines: A Detailed
  Analysis","Helmet detection is crucial for advancing protection levels in public road
traffic dynamics. This problem statement translates to an object detection
task. Therefore, this paper compares recent You Only Look Once (YOLO) models in
the context of helmet detection in terms of reliability and computational load.
Specifically, YOLOv8, YOLOv9, and the newly released YOLOv11 have been used.
Besides, a modified architectural pipeline that remarkably improves the overall
performance has been proposed in this manuscript. This hybridized YOLO model
(h-YOLO) has been pitted against the independent models for analysis that
proves h-YOLO is preferable for helmet detection over plain YOLO models. The
models were tested using a range of standard object detection benchmarks such
as recall, precision, and mAP (Mean Average Precision). In addition, training
and testing times were recorded to provide the overall scope of the models in a
real-time detection scenario.",2024-12-27T05:26:12Z,http://arxiv.org/abs/2412.19467v1,"Vaikunth M, Dejey D, Vishaal C, Balamurali S"
"MNet-SAt: A Multiscale Network with Spatial-enhanced Attention for
  Segmentation of Polyps in Colonoscopy","Objective: To develop a novel deep learning framework for the automated
segmentation of colonic polyps in colonoscopy images, overcoming the
limitations of current approaches in preserving precise polyp boundaries,
incorporating multi-scale features, and modeling spatial dependencies that
accurately reflect the intricate and diverse morphology of polyps. Methods: To
address these limitations, we propose a novel Multiscale Network with
Spatial-enhanced Attention (MNet-SAt) for polyp segmentation in colonoscopy
images. This framework incorporates four key modules: Edge-Guided Feature
Enrichment (EGFE) preserves edge information for improved boundary quality;
Multi-Scale Feature Aggregator (MSFA) extracts and aggregates multi-scale
features across channel spatial dimensions, focusing on salient regions;
Spatial-Enhanced Attention (SEAt) captures spatial-aware global dependencies
within the multi-scale aggregated features, emphasizing the region of interest;
and Channel-Enhanced Atrous Spatial Pyramid Pooling (CE-ASPP) resamples and
recalibrates attentive features across scales. Results: We evaluated MNet-SAt
on the Kvasir-SEG and CVC-ClinicDB datasets, achieving Dice Similarity
Coefficients of 96.61% and 98.60%, respectively. Conclusion: Both quantitative
(DSC) and qualitative assessments highlight MNet-SAt's superior performance and
generalization capabilities compared to existing methods. Significance:
MNet-SAt's high accuracy in polyp segmentation holds promise for improving
clinical workflows in early polyp detection and more effective treatment,
contributing to reduced colorectal cancer mortality rates.",2024-12-27T05:17:29Z,http://arxiv.org/abs/2412.19464v1,"Chandravardhan Singh Raghaw, Aryan Yadav, Jasmer Singh Sanjotra, Shalini Dangi, Nagendra Kumar"
Pulse-induced memory-like effect in cyclotron motion?,"We study how a charged particle moving in a uniform magnetic field along its
standard circular path (cyclotron motion) reacts to a short-duration,
homogeneous, uniform electric field pulse injected in the plane perpendicular
to the magnetic field. A `permanent' change in the radius of the initial circle
and a shift of its centre is noted at later times, after the pulse is switched
off. The magnitude of the velocity undergoes a change too, akin to a `velocity
kick'. In summary, our results suggest a pulse-induced `electromagnetic
memory-like effect', which is not quite a `wave memory', but, nevertheless, has
similar features within a simple, non-relativistic context.",2024-12-27T05:13:53Z,http://arxiv.org/abs/2412.19460v1,Sayan Kar
A Prototype Unit for Image De-raining using Time-Lapse Data,"We address the challenge of single-image de-raining, a task that involves
recovering rain-free background information from a single rain image. While
recent advancements have utilized real-world time-lapse data for training,
enabling the estimation of consistent backgrounds and realistic rain streaks,
these methods often suffer from computational and memory consumption, limiting
their applicability in real-world scenarios. In this paper, we introduce a
novel solution: the Rain Streak Prototype Unit (RsPU). The RsPU efficiently
encodes rain streak-relevant features as real-time prototypes derived from
time-lapse data, eliminating the need for excessive memory resources. Our
de-raining network combines encoder-decoder networks with the RsPU, allowing us
to learn and encapsulate diverse rain streak-relevant features as concise
prototypes, employing an attention-based approach. To ensure the effectiveness
of our approach, we propose a feature prototype loss encompassing cohesion and
divergence components. This loss function captures both the compactness and
diversity aspects of the prototypical rain streak features within the RsPU. Our
method evaluates various de-raining benchmarks, accompanied by comprehensive
ablation studies. We show that it can achieve competitive results in various
rain images compared to state-of-the-art methods.",2024-12-27T05:04:56Z,http://arxiv.org/abs/2412.19459v1,"Jaehoon Cho, Minjung Yoo, Jini Yang, Sunok Kim"
Focusing Image Generation to Mitigate Spurious Correlations,"Instance features in images exhibit spurious correlations with background
features, affecting the training process of deep neural classifiers. This leads
to insufficient attention to instance features by the classifier, resulting in
erroneous classification outcomes. In this paper, we propose a data
augmentation method called Spurious Correlations Guided Synthesis (SCGS) that
mitigates spurious correlations through image generation model. This approach
does not require expensive spurious attribute (group) labels for the training
data and can be widely applied to other debiasing methods. Specifically, SCGS
first identifies the incorrect attention regions of a pre-trained classifier on
the training images, and then uses an image generation model to generate new
training data based on these incorrect attended regions. SCGS increases the
diversity and scale of the dataset to reduce the impact of spurious
correlations on classifiers. Changes in the classifier's attention regions and
experimental results on three different domain datasets demonstrate that this
method is effective in reducing the classifier's reliance on spurious
correlations.",2024-12-27T04:48:56Z,http://arxiv.org/abs/2412.19457v1,"Xuewei Li, Zhenzhen Nie, Mei Yu, Zijian Zhang, Jie Gao, Tianyi Xu, Zhiqiang Liu"
"NijiGAN: Transform What You See into Anime with Contrastive
  Semi-Supervised Learning and Neural Ordinary Differential Equations","Generative AI has transformed the animation industry. Several models have
been developed for image-to-image translation, particularly focusing on
converting real-world images into anime through unpaired translation.
Scenimefy, a notable approach utilizing contrastive learning, achieves high
fidelity anime scene translation by addressing limited paired data through
semi-supervised training. However, it faces limitations due to its reliance on
paired data from a fine-tuned StyleGAN in the anime domain, often producing
low-quality datasets. Additionally, Scenimefy's high parameter architecture
presents opportunities for computational optimization. This research introduces
NijiGAN, a novel model incorporating Neural Ordinary Differential Equations
(NeuralODEs), which offer unique advantages in continuous transformation
modeling compared to traditional residual networks. NijiGAN successfully
transforms real-world scenes into high fidelity anime visuals using half of
Scenimefy's parameters. It employs pseudo-paired data generated through
Scenimefy for supervised training, eliminating dependence on low-quality paired
data and improving the training process. Our comprehensive evaluation includes
ablation studies, qualitative, and quantitative analysis comparing NijiGAN to
similar models. The testing results demonstrate that NijiGAN produces
higher-quality images compared to AnimeGAN, as evidenced by a Mean Opinion
Score (MOS) of 2.192, it surpasses AnimeGAN's MOS of 2.160. Furthermore, our
model achieved a Frechet Inception Distance (FID) score of 58.71, outperforming
Scenimefy's FID score of 60.32. These results demonstrate that NijiGAN achieves
competitive performance against existing state-of-the-arts, especially
Scenimefy as the baseline model.",2024-12-27T04:46:44Z,http://arxiv.org/abs/2412.19455v1,"Kevin Putra Santoso, Anny Yuniarti, Dwiyasa Nakula, Dimas Prihady Setyawan, Adam Haidar Azizi, Jeany Aurellia P. Dewati, Farah Dhia Fadhila, Maria T. Elvara Bumbungan"
"Feature Alignment-Based Knowledge Distillation for Efficient Compression
  of Large Language Models","This study proposes a knowledge distillation algorithm based on large
language models and feature alignment, aiming to effectively transfer the
knowledge of large pre-trained models into lightweight student models, thereby
reducing computational costs while maintaining high model performance.
Different from the traditional soft label distillation method, this method
introduces a multi-layer feature alignment strategy to deeply align the
intermediate features and attention mechanisms of the teacher model and the
student model, maximally retaining the semantic expression ability and context
modeling ability of the teacher model. In terms of method design, a multi-task
loss function is constructed, including feature matching loss, attention
alignment loss, and output distribution matching loss, to ensure multi-level
information transfer through joint optimization. The experiments were
comprehensively evaluated on the GLUE data set and various natural language
processing tasks. The results show that the proposed model performs very close
to the state-of-the-art GPT-4 model in terms of evaluation indicators such as
perplexity, BLEU, ROUGE, and CER. At the same time, it far exceeds baseline
models such as DeBERTa, XLNet, and GPT-3, showing significant performance
improvements and computing efficiency advantages. Research results show that
the feature alignment distillation strategy is an effective model compression
method that can significantly reduce computational overhead and storage
requirements while maintaining model capabilities. Future research can be
further expanded in the directions of self-supervised learning, cross-modal
feature alignment, and multi-task transfer learning to provide more flexible
and efficient solutions for the deployment and optimization of deep learning
models.",2024-12-27T04:37:06Z,http://arxiv.org/abs/2412.19449v1,"Shuo Wang, Chihang Wang, Jia Gao, Zhen Qi, Hongye Zheng, Xiaoxuan Liao"
Towards Simple and Provable Parameter-Free Adaptive Gradient Methods,"Optimization algorithms such as AdaGrad and Adam have significantly advanced
the training of deep models by dynamically adjusting the learning rate during
the optimization process. However, adhoc tuning of learning rates poses a
challenge, leading to inefficiencies in practice. To address this issue, recent
research has focused on developing ""learning-rate-free"" or ""parameter-free""
algorithms that operate effectively without the need for learning rate tuning.
Despite these efforts, existing parameter-free variants of AdaGrad and Adam
tend to be overly complex and/or lack formal convergence guarantees. In this
paper, we present AdaGrad++ and Adam++, novel and simple parameter-free
variants of AdaGrad and Adam with convergence guarantees. We prove that
AdaGrad++ achieves comparable convergence rates to AdaGrad in convex
optimization without predefined learning rate assumptions. Similarly, Adam++
matches the convergence rate of Adam without relying on any conditions on the
learning rates. Experimental results across various deep learning tasks
validate the competitive performance of AdaGrad++ and Adam++.",2024-12-27T04:22:02Z,http://arxiv.org/abs/2412.19444v1,"Yuanzhe Tao, Huizhuo Yuan, Xun Zhou, Yuan Cao, Quanquan Gu"
"Comparative Performance Analysis of Quantum Machine Learning
  Architectures for Credit Card Fraud Detection","As financial fraud becomes increasingly complex, effective detection methods
are essential. Quantum Machine Learning (QML) introduces certain capabilities
that may enhance both accuracy and efficiency in this area. This study examines
how different quantum feature map and ansatz configurations affect the
performance of three QML-based classifiers-the Variational Quantum Classifier
(VQC), the Sampler Quantum Neural Network (SQNN), and the Estimator Quantum
Neural Network (EQNN)-when applied to two non-standardized financial fraud
datasets. Different quantum feature map and ansatz configurations are
evaluated, revealing distinct performance patterns. The VQC consistently
demonstrates strong classification results, achieving an F1 score of 0.88,
while the SQNN also delivers promising outcomes. In contrast, the EQNN
struggles to produce robust results, emphasizing the challenges presented by
non-standardized data. These findings highlight the importance of careful model
configuration in QML-based financial fraud detection. By showing how specific
feature maps and ansatz choices influence predictive success, this work guides
researchers and practitioners in refining QML approaches for complex financial
applications.",2024-12-27T04:17:34Z,http://arxiv.org/abs/2412.19441v1,"Mansour El Alami, Nouhaila Innan, Muhammad Shafique, Mohamed Bennai"
DeepSeek-V3 Technical Report,"We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with
671B total parameters with 37B activated for each token. To achieve efficient
inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent
Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated
in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free
strategy for load balancing and sets a multi-token prediction training
objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion
diverse and high-quality tokens, followed by Supervised Fine-Tuning and
Reinforcement Learning stages to fully harness its capabilities. Comprehensive
evaluations reveal that DeepSeek-V3 outperforms other open-source models and
achieves performance comparable to leading closed-source models. Despite its
excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its
full training. In addition, its training process is remarkably stable.
Throughout the entire training process, we did not experience any irrecoverable
loss spikes or perform any rollbacks. The model checkpoints are available at
https://github.com/deepseek-ai/DeepSeek-V3.",2024-12-27T04:03:16Z,http://arxiv.org/abs/2412.19437v1,"DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, Zizheng Pan"
"Low-Rank Contextual Reinforcement Learning from Heterogeneous Human
  Feedback","Reinforcement learning from human feedback (RLHF) has become a cornerstone
for aligning large language models with human preferences. However, the
heterogeneity of human feedback, driven by diverse individual contexts and
preferences, poses significant challenges for reward learning. To address this,
we propose a Low-rank Contextual RLHF (LoCo-RLHF) framework that integrates
contextual information to better model heterogeneous feedback while maintaining
computational efficiency. Our approach builds on a contextual preference model,
leveraging the intrinsic low-rank structure of the interaction between user
contexts and query-answer pairs to mitigate the high dimensionality of feature
representations. Furthermore, we address the challenge of distributional shifts
in feedback through our Pessimism in Reduced Subspace (PRS) policy, inspired by
pessimistic offline reinforcement learning techniques. We theoretically
demonstrate that our policy achieves a tighter sub-optimality gap compared to
existing methods. Extensive experiments validate the effectiveness of
LoCo-RLHF, showcasing its superior performance in personalized RLHF settings
and its robustness to distribution shifts.",2024-12-27T04:02:46Z,http://arxiv.org/abs/2412.19436v1,"Seong Jin Lee, Will Wei Sun, Yufeng Liu"
"Residual Feature-Reutilization Inception Network for Image
  Classification","Capturing feature information effectively is of great importance in the field
of computer vision. With the development of convolutional neural networks
(CNNs), concepts like residual connection and multiple scales promote continual
performance gains in diverse deep learning vision tasks. In this paper, we
propose a novel CNN architecture that it consists of residual
feature-reutilization inceptions (ResFRI) or split-residual
feature-reutilization inceptions (Split-ResFRI). And it is composed of four
convolutional combinations of different structures connected by specially
designed information interaction passages, which are utilized to extract
multi-scale feature information and effectively increase the receptive field of
the model. Moreover, according to the network structure designed above,
Split-ResFRI can adjust the segmentation ratio of the input information,
thereby reducing the number of parameters and guaranteeing the model
performance. Specifically, in experiments based on popular vision datasets,
such as CIFAR10 ($97.94$\%), CIFAR100 ($85.91$\%) and Tiny Imagenet
($70.54$\%), we obtain state-of-the-art results compared with other modern
models under the premise that the model size is approximate and no additional
data is used.",2024-12-27T03:55:25Z,http://arxiv.org/abs/2412.19433v1,"Yuanpeng He, Wenjie Song, Lijian Li, Tianxiang Zhan, Wenpin Jiao"
"Seed-Driven Stepwise Crystallization (SDSC) for Growing Rutile GeO2
  Films via MOCVD","Germanium dioxide (r-GeO2) is an emerging new ultrawide bandgap (UWBG)
semiconductor with significant potential for power electronics, thanks to its
large-size substrate compatibility and ambipolar doping capability. However,
phase segregation during metal-organic chemical vapor deposition (MOCVD) on
substrates like r-TiO2 has posed a significant barrier to achieving
high-quality films. Conventional optimization of growth parameters has been
found so far not very insufficient in film coverage and film quality. To
address this, a seed-driven stepwise crystallization (SDSC) growth approach was
employed in this study, featuring multiple sequential deposition steps on a
pre-templated substrate enriched with r-GeO2 seeds. The process began with an
initial 180-minute deposition to establish r-GeO2 nucleation seeds, followed by
a sequence of shorter deposition steps (90, 60, 60, 60, 60, and 60 minutes).
This stepwise growth strategy progressively increased the crystalline coverage
to 57.4%, 77.49%, 79.73%, 93.27%, 99.17%, and ultimately 100%. Concurrently,
the crystalline quality improved substantially, evidenced by a ~30% reduction
in the Full Width at Half Maximum (FWHM) of X-ray diffraction rocking curves.
These findings demonstrate the potential of the SDSC approach for overcoming
phase segregation and achieving high-quality, large-area r-GeO2 films.",2024-12-27T03:47:35Z,http://arxiv.org/abs/2412.19429v1,"Imteaz Rahaman, Botong Li, Bobby Duersch, Hunter D. Ellis, Kai Fu"
"Temporal Context Consistency Above All: Enhancing Long-Term Anticipation
  by Learning and Enforcing Temporal Constraints","This paper proposes a method for long-term action anticipation (LTA), the
task of predicting action labels and their duration in a video given the
observation of an initial untrimmed video interval. We build on an
encoder-decoder architecture with parallel decoding and make two key
contributions. First, we introduce a bi-directional action context regularizer
module on the top of the decoder that ensures temporal context coherence in
temporally adjacent segments. Second, we learn from classified segments a
transition matrix that models the probability of transitioning from one action
to another and the sequence is optimized globally over the full prediction
interval. In addition, we use a specialized encoder for the task of action
segmentation to increase the quality of the predictions in the observation
interval at inference time, leading to a better understanding of the past. We
validate our methods on four benchmark datasets for LTA, the EpicKitchen-55,
EGTEA+, 50Salads and Breakfast demonstrating superior or comparable performance
to state-of-the-art methods, including probabilistic models and also those
based on Large Language Models, that assume trimmed video as input. The code
will be released upon acceptance.",2024-12-27T03:29:10Z,http://arxiv.org/abs/2412.19424v1,"Alberto Maté, Mariella Dimiccoli"
Revisiting PCA for time series reduction in temporal dimension,"Revisiting PCA for Time Series Reduction in Temporal Dimension; Jiaxin Gao,
Wenbo Hu, Yuntian Chen; Deep learning has significantly advanced time series
analysis (TSA), enabling the extraction of complex patterns for tasks like
classification, forecasting, and regression. Although dimensionality reduction
has traditionally focused on the variable space-achieving notable success in
minimizing data redundancy and computational complexity-less attention has been
paid to reducing the temporal dimension. In this study, we revisit Principal
Component Analysis (PCA), a classical dimensionality reduction technique, to
explore its utility in temporal dimension reduction for time series data. It is
generally thought that applying PCA to the temporal dimension would disrupt
temporal dependencies, leading to limited exploration in this area. However,
our theoretical analysis and extensive experiments demonstrate that applying
PCA to sliding series windows not only maintains model performance, but also
enhances computational efficiency. In auto-regressive forecasting, the temporal
structure is partially preserved through windowing, and PCA is applied within
these windows to denoise the time series while retaining their statistical
information. By preprocessing time-series data with PCA, we reduce the temporal
dimensionality before feeding it into TSA models such as Linear, Transformer,
CNN, and RNN architectures. This approach accelerates training and inference
and reduces resource consumption. Notably, PCA improves Informer training and
inference speed by up to 40% and decreases GPU memory usage of TimesNet by 30%,
without sacrificing model accuracy. Comparative analysis against other
reduction methods further highlights the effectiveness of PCA in improving the
efficiency of TSA models.",2024-12-27T03:17:26Z,http://arxiv.org/abs/2412.19423v1,"Jiaxin Gao, Wenbo Hu, Yuntian Chen"
"Gx2Mol: De Novo Generation of Hit-like Molecules from Gene Expression
  Profiles via Deep Learning","De novo generation of hit-like molecules is a challenging task in the drug
discovery process. Most methods in previous studies learn the semantics and
syntax of molecular structures by analyzing molecular graphs or simplified
molecular input line entry system (SMILES) strings; however, they do not take
into account the drug responses of the biological systems consisting of genes
and proteins. In this study we propose a deep generative model, Gx2Mol, which
utilizes gene expression profiles to generate molecular structures with
desirable phenotypes for arbitrary target proteins. In the algorithm, a
variational autoencoder is employed as a feature extractor to learn the latent
feature distribution of the gene expression profiles. Then, a long short-term
memory is leveraged as the chemical generator to produce syntactically valid
SMILES strings that satisfy the feature conditions of the gene expression
profile extracted by the feature extractor. Experimental results and case
studies demonstrate that the proposed Gx2Mol model can produce new molecules
with potential bioactivities and drug-like properties.",2024-12-27T03:16:56Z,http://arxiv.org/abs/2412.19422v1,"Chen Li, Yuki Matsukiyo, Yoshihiro Yamanishi"
"A Matrix Logic Approach to Efficient Frequent Itemset Discovery in Large
  Data Sets","This paper proposes a frequent itemset mining algorithm based on the Boolean
matrix method, aiming to solve the storage and computational bottlenecks of
traditional frequent pattern mining algorithms in high-dimensional and
large-scale transaction databases. By representing the itemsets in the
transaction database as Boolean matrices, the algorithm uses Boolean logic
operations such as AND and OR to efficiently calculate the support of the
itemsets, avoiding the generation and storage of a large number of candidates
itemsets in traditional algorithms. The algorithm recursively mines frequent
itemsets through matrix operations and can flexibly adapt to different data
scales and support thresholds. In the experiment, the public Groceries dataset
was selected, and the running efficiency test and frequent itemset mining
effect test were designed to evaluate the algorithm's performance indicators
such as running time, memory usage, and number of frequent itemsets under
different transaction numbers and support thresholds. The experimental results
show that the algorithm can efficiently mine a large number of frequent
itemsets when the support threshold is low, and focus on strong association
rules with high support when the threshold is high. In addition, the changing
trends of running time and memory usage show that the Boolean matrix method can
still maintain good running efficiency when the number of transactions
increases significantly and has high scalability and robustness. Future
research can improve memory optimization and matrix block operations, and
combine distributed computing and deep learning models to further enhance the
algorithm's applicability and real-time processing capabilities in
ultra-large-scale data environments. The algorithm has broad application
potential and development prospects in the fields of market analysis,
recommendation systems, and network security.",2024-12-27T03:13:13Z,http://arxiv.org/abs/2412.19420v1,"Xuan Li, Tingyi Ruan, Yankaiqi Li, Quanchao Lu, Xiaoxuan Sun"
"Introduction to Graph Neural Networks: A Starting Point for Machine
  Learning Engineers","Graph neural networks are deep neural networks designed for graphs with
attributes attached to nodes or edges. The number of research papers in the
literature concerning these models is growing rapidly due to their impressive
performance on a broad range of tasks. This survey introduces graph neural
networks through the encoder-decoder framework and provides examples of
decoders for a range of graph analytic tasks. It uses theory and numerous
experiments on homogeneous graphs to illustrate the behavior of graph neural
networks for different training sizes and degrees of graph complexity.",2024-12-27T03:13:02Z,http://arxiv.org/abs/2412.19419v1,"James H. Tanis, Chris Giannella, Adrian V. Mariano"
"Generalized Uncertainty-Based Evidential Fusion with Hybrid Multi-Head
  Attention for Weak-Supervised Temporal Action Localization","Weakly supervised temporal action localization (WS-TAL) is a task of
targeting at localizing complete action instances and categorizing them with
video-level labels. Action-background ambiguity, primarily caused by background
noise resulting from aggregation and intra-action variation, is a significant
challenge for existing WS-TAL methods. In this paper, we introduce a hybrid
multi-head attention (HMHA) module and generalized uncertainty-based evidential
fusion (GUEF) module to address the problem. The proposed HMHA effectively
enhances RGB and optical flow features by filtering redundant information and
adjusting their feature distribution to better align with the WS-TAL task.
Additionally, the proposed GUEF adaptively eliminates the interference of
background noise by fusing snippet-level evidences to refine uncertainty
measurement and select superior foreground feature information, which enables
the model to concentrate on integral action instances to achieve better action
localization and classification performance. Experimental results conducted on
the THUMOS14 dataset demonstrate that our method outperforms state-of-the-art
methods. Our code is available in
\url{https://github.com/heyuanpengpku/GUEF/tree/main}.",2024-12-27T03:04:57Z,http://arxiv.org/abs/2412.19418v1,"Yuanpeng He, Lijian Li, Tianxiang Zhan, Wenpin Jiao, Chi-Man Pun"
Multi-scale Latent Point Consistency Models for 3D Shape Generation,"Consistency Models (CMs) have significantly accelerated the sampling process
in diffusion models, yielding impressive results in synthesizing
high-resolution images. To explore and extend these advancements to
point-cloud-based 3D shape generation, we propose a novel Multi-scale Latent
Point Consistency Model (MLPCM). Our MLPCM follows a latent diffusion framework
and introduces hierarchical levels of latent representations, ranging from
point-level to super-point levels, each corresponding to a different spatial
resolution. We design a multi-scale latent integration module along with 3D
spatial attention to effectively denoise the point-level latent representations
conditioned on those from multiple super-point levels. Additionally, we propose
a latent consistency model, learned through consistency distillation, that
compresses the prior into a one-step generator. This significantly improves
sampling efficiency while preserving the performance of the original teacher
model. Extensive experiments on standard benchmarks ShapeNet and ShapeNet-Vol
demonstrate that MLPCM achieves a 100x speedup in the generation process, while
surpassing state-of-the-art diffusion models in terms of both shape quality and
diversity.",2024-12-27T02:41:33Z,http://arxiv.org/abs/2412.19413v1,"Bi'an Du, Wei Hu, Renjie Liao"
MINIMA: Modality Invariant Image Matching,"Image matching for both cross-view and cross-modality plays a critical role
in multimodal perception. In practice, the modality gap caused by different
imaging systems/styles poses great challenges to the matching task. Existing
works try to extract invariant features for specific modalities and train on
limited datasets, showing poor generalization. In this paper, we present
MINIMA, a unified image matching framework for multiple cross-modal cases.
Without pursuing fancy modules, our MINIMA aims to enhance universal
performance from the perspective of data scaling up. For such purpose, we
propose a simple yet effective data engine that can freely produce a large
dataset containing multiple modalities, rich scenarios, and accurate matching
labels. Specifically, we scale up the modalities from cheap but rich RGB-only
matching data, by means of generative models. Under this setting, the matching
labels and rich diversity of the RGB dataset are well inherited by the
generated multimodal data. Benefiting from this, we construct MD-syn, a new
comprehensive dataset that fills the data gap for general multimodal image
matching. With MD-syn, we can directly train any advanced matching pipeline on
randomly selected modality pairs to obtain cross-modal ability. Extensive
experiments on in-domain and zero-shot matching tasks, including $19$
cross-modal cases, demonstrate that our MINIMA can significantly outperform the
baselines and even surpass modality-specific methods. The dataset and code are
available at https://github.com/LSXI7/MINIMA .",2024-12-27T02:39:50Z,http://arxiv.org/abs/2412.19412v1,"Xingyu Jiang, Jiangwei Ren, Zizhuo Li, Xin Zhou, Dingkang Liang, Xiang Bai"
Efficient Feature Mapping Using a Collaborative Team of AUVs,"We present the results of experiments performed using a team of small
autonomous underwater vehicles (AUVs) to determine the location of an isobath.
The primary contributions of this work are (1) the development of a novel
objective function for level set estimation that utilizes a rigorous assessment
of uncertainty, and (2) a description of the practical challenges and
corresponding solutions needed to implement our approach in the field using a
team of AUVs. We combine path planning techniques and an approach to
decentralization from prior work that yields theoretical performance
guarantees. Experimentation with a team of AUVs provides empirical evidence
that the desirable performance guarantees can be preserved in practice even in
the presence of limitations that commonly arise in underwater robotics,
including slow and intermittent acoustic communications and limited
computational resources.",2024-12-27T02:22:52Z,http://arxiv.org/abs/2412.19409v1,"Benjamin Biggs, Daniel J. Stilwell, Harun Yetkin, James McMahon"
"MLLM-SUL: Multimodal Large Language Model for Semantic Scene
  Understanding and Localization in Traffic Scenarios","Multimodal large language models (MLLMs) have shown satisfactory effects in
many autonomous driving tasks. In this paper, MLLMs are utilized to solve joint
semantic scene understanding and risk localization tasks, while only relying on
front-view images. In the proposed MLLM-SUL framework, a dual-branch visual
encoder is first designed to extract features from two resolutions, and rich
visual information is conducive to the language model describing risk objects
of different sizes accurately. Then for the language generation, LLaMA model is
fine-tuned to predict scene descriptions, containing the type of driving
scenario, actions of risk objects, and driving intentions and suggestions of
ego-vehicle. Ultimately, a transformer-based network incorporating a regression
token is trained to locate the risk objects. Extensive experiments on the
existing DRAMA-ROLISP dataset and the extended DRAMA-SRIS dataset demonstrate
that our method is efficient, surpassing many state-of-the-art image-based and
video-based methods. Specifically, our method achieves 80.1% BLEU-1 score and
298.5% CIDEr score in the scene understanding task, and 59.6% accuracy in the
localization task. Codes and datasets are available at
https://github.com/fjq-tongji/MLLM-SUL.",2024-12-27T02:05:38Z,http://arxiv.org/abs/2412.19406v1,"Jiaqi Fan, Jianhua Wu, Jincheng Gao, Jianhao Yu, Yafei Wang, Hongqing Chu, Bingzhao Gao"
Spectral-Temporal Fusion Representation for Person-in-Bed Detection,"This study is based on the ICASSP 2025 Signal Processing Grand Challenge's
Accelerometer-Based Person-in-Bed Detection Challenge, which aims to determine
bed occupancy using accelerometer signals. The task is divided into two tracks:
""in bed"" and ""not in bed"" segmented detection, and streaming detection, facing
challenges such as individual differences, posture variations, and external
disturbances. We propose a spectral-temporal fusion-based feature
representation method with mixup data augmentation, and adopt Intersection over
Union (IoU) loss to optimize detection accuracy. In the two tracks, our method
achieved outstanding results of 100.00% and 95.55% in detection scores,
securing first place and third place, respectively.",2024-12-27T02:05:09Z,http://arxiv.org/abs/2412.19404v1,"Xuefeng Yang, Shiheng Zhang, Jian Guan, Feiyang Xiao, Wei Lu, Qiaoxi Zhu"
"Fully Data-driven but Interpretable Human Behavioural Modelling with
  Differentiable Discrete Choice Model","Discrete choice models are essential for modelling various decision-making
processes in human behaviour. However, the specification of these models has
depended heavily on domain knowledge from experts, and the fully automated but
interpretable modelling of complex human behaviours has been a long-standing
challenge. In this paper, we introduce the differentiable discrete choice model
(Diff-DCM), a fully data-driven method for the interpretable modelling,
learning, prediction, and control of complex human behaviours, which is
realised by differentiable programming. Solely from input features and choice
outcomes without any prior knowledge, Diff-DCM can estimate interpretable
closed-form utility functions that reproduce observed behaviours. Comprehensive
experiments with both synthetic and real-world data demonstrate that Diff-DCM
can be applied to various types of data and requires only a small amount of
computational resources for the estimations, which can be completed within tens
of seconds on a laptop without any accelerators. In these experiments, we also
demonstrate that, using its differentiability, Diff-DCM can provide useful
insights into human behaviours, such as an optimal intervention path for
effective behavioural changes. This study provides a strong basis for the fully
automated and reliable modelling, prediction, and control of human behaviours.",2024-12-27T01:53:18Z,http://arxiv.org/abs/2412.19403v1,"Fumiyasu Makinoshima, Tatsuya Mitomi, Fumiya Makihara, Eigo Segawa"
"A Generalized Einstein Relation for Markovian Friction Coefficients from
  Molecular Trajectories","We present a generalized Einstein relation for the friction coefficients
associated with an underlying memory kernel in terms of observable time
correlation functions. There is considerable freedom in the correlations
involved, and this allows the expression to be tailored to the particular
system to achieve numerical stability. We demonstrate this by recovering the
site-specific friction coefficients from trajectories of a freely diffusing
model trimer, and we show that the accuracy is greatly improved over
established Volterra inversion methods for kernel extraction.",2024-12-27T01:20:37Z,http://arxiv.org/abs/2412.19398v1,"J. M. Hall, M. G. Guenza"
"Comparing Few to Rank Many: Active Human Preference Learning using
  Randomized Frank-Wolfe","We study learning of human preferences from a limited comparison feedback.
This task is ubiquitous in machine learning. Its applications such as
reinforcement learning from human feedback, have been transformational. We
formulate this problem as learning a Plackett-Luce model over a universe of $N$
choices from $K$-way comparison feedback, where typically $K \ll N$. Our
solution is the D-optimal design for the Plackett-Luce objective. The design
defines a data logging policy that elicits comparison feedback for a small
collection of optimally chosen points from all ${N \choose K}$ feasible
subsets. The main algorithmic challenge in this work is that even fast methods
for solving D-optimal designs would have $O({N \choose K})$ time complexity. To
address this issue, we propose a randomized Frank-Wolfe (FW) algorithm that
solves the linear maximization sub-problems in the FW method on randomly chosen
variables. We analyze the algorithm, and evaluate it empirically on synthetic
and open-source NLP datasets.",2024-12-27T01:10:17Z,http://arxiv.org/abs/2412.19396v1,"Kiran Koshy Thekumparampil, Gaurush Hiranandani, Kousha Kalantari, Shoham Sabach, Branislav Kveton"
"Two-echelon Electric Vehicle Routing Problem in Parcel Delivery: A
  Literature Review","Multi-echelon parcel delivery systems using electric vehicles (EVs) are
crucial for managing urban logistics complexity and promoting sustainability.
In multi-echelon systems, particularly within two-stage systems, larger
vehicles transport parcels from a central depot to satellite hubs, where
smaller EVs pick up the parcels and carry out last-mile deliveries. This system
could increase efficiency, reduce emissions, and improve service reliability.
The two-echelon electric vehicle routing problem (2E-EVRP), an extension of the
traditional two-echelon vehicle routing problem (2E-VRP), addresses EV-specific
challenges such as battery constraints and recharging stations to tackle
environmental impacts, urban congestion, and e-commerce demands. While
effectively reducing costs, energy use, and emissions, the 2E-EVRP faces
modeling challenges due to multi-echelon structures, EV limitations, and
recharging station selection. This paper systematically reviews 2E-EVRP
literature, analyzing key studies. It proposes a classification scheme to
categorize the papers based on the problem variants, objectives, constraints,
and solution methods. It identifies gaps such as delivery tardiness,
environmental trade-offs, multi-objective optimization, multiple depots, split
deliveries, and time-dependent travel conditions. Future research directions
include aligning models with urban policies, integrating parcel lockers,
enabling same-day delivery, and incorporating advanced technologies like
autonomous vehicles. Methodological advancements suggest using machine
learning, reinforcement learning, and simulation-based approaches to enhance
dynamic routing and real-time decision-making. These directions aim to expand
the 2E-EVRP applicability, addressing theoretical and practical challenges in
sustainable urban logistics for future works.",2024-12-27T01:05:59Z,http://arxiv.org/abs/2412.19395v1,"Nima Moradi, Niloufar Mirzavand Boroujeni, Navid Aftabi, Amin Aslani"
"Asymptotically Optimal Search for a Change Point Anomaly under a
  Composite Hypothesis Model","We address the problem of searching for a change point in an anomalous
process among a finite set of M processes. Specifically, we address a composite
hypothesis model in which each process generates measurements following a
common distribution with an unknown parameter (vector). This parameter belongs
to either a normal or abnormal space depending on the current state of the
process. Before the change point, all processes, including the anomalous one,
are in a normal state; after the change point, the anomalous process
transitions to an abnormal state. Our goal is to design a sequential search
strategy that minimizes the Bayes risk by balancing sample complexity and
detection accuracy. We propose a deterministic search algorithm with the
following notable properties. First, we analytically demonstrate that when the
distributions of both normal and abnormal processes are unknown, the algorithm
is asymptotically optimal in minimizing the Bayes risk as the error probability
approaches zero. In the second setting, where the parameter under the null
hypothesis is known, the algorithm achieves asymptotic optimality with improved
detection time based on the true normal state. Simulation results are presented
to validate the theoretical findings.",2024-12-27T00:44:34Z,http://arxiv.org/abs/2412.19392v1,"Liad Lea Didi, Tomer Gafni, Kobi Cohen"
"An In-Depth Analysis of Adversarial Discriminative Domain Adaptation for
  Digit Classification","Domain adaptation is an active area of research driven by the growing demand
for robust machine learning models that perform well on real-world data.
Adversarial learning for deep neural networks (DNNs) has emerged as a promising
approach to improving generalization ability, particularly for image
classification. In this paper, we implement a specific adversarial learning
technique known as Adversarial Discriminative Domain Adaptation (ADDA) and
replicate digit classification experiments from the original ADDA paper. We
extend their findings by examining a broader range of domain shifts and provide
a detailed analysis of in-domain classification accuracy post-ADDA. Our results
demonstrate that ADDA significantly improves accuracy across certain domain
shifts with minimal impact on in-domain performance. Furthermore, we provide
qualitative analysis and propose potential explanations for ADDA's limitations
in less successful domain shifts. Code is at
https://github.com/eugenechoi2004/COS429_FINAL .",2024-12-27T00:36:40Z,http://arxiv.org/abs/2412.19391v1,"Eugene Choi, Julian Rodriguez, Edmund Young"
The lack of influence of the scalar hair on the DC conductivity,"Recently obtained black hole solutions within the framework of
beyond-Horndeski theories, which have the advantage of featuring primary hair,
are generalized in the presence of two axionic fields. In order to induce a
momentum dissipation, the axionic field solutions are homogeneously distributed
along the horizon coordinates of the planar base manifold. We show that,
despite the explicit dependence of the scalar field and the metric on the
primary hair, this latter does not directly affect the calculation of transport
properties. Its influence is indirect, modifying the horizon location, but the
transport properties themselves do not explicitly depend on the hair parameter.
We take a step further and show that even within a more general class of
beyond-Horndeski theories, where the scalar field depends linearly on the hair
parameter, the scalar hair still has no direct impact on the DC conductivity.
This result underscores the robustness of our earlier findings, and seem to
confirm that the transport properties remain unaffected by the explicit
presence of the hair parameter.",2024-12-27T00:28:58Z,http://arxiv.org/abs/2412.19388v1,Ulises Hernandez-Vera
"Preventive Energy Management for Distribution Systems Under Uncertain
  Events: A Deep Reinforcement Learning Approach","As power systems become more complex with the continuous integration of
intelligent distributed energy resources (DERs), new risks and uncertainties
arise. Consequently, to enhance system resiliency, it is essential to account
for various uncertain events when implementing the optimization problem for the
energy management system (EMS). This paper presents a preventive EMS
considering the probability of failure (PoF) of each system component across
different scenarios. A conditional-value-at-risk (CVaR)-based framework is
proposed to integrate the uncertainties of the distribution network. Loads are
classified into critical, semi-critical, and non-critical categories to
prioritize essential loads during generation resource shortages. A proximal
policy optimization (PPO)-based reinforcement learning (RL) agent is used to
solve the formulated problem and generate the control decisions. The proposed
framework is evaluated on a notional MVDC ship system and a modified IEEE
30-bus system, where the results demonstrate that the PPO agent can
successfully optimize the objective function while maintaining the network and
operational constraints. For validation, the RL-based method is benchmarked
against a traditional optimization approach, further highlighting its
effectiveness and robustness. This comparison shows that RL agents can offer
more resiliency against future uncertain events compared to the traditional
solution methods due to their adaptability and learning capacity.",2024-12-26T23:50:28Z,http://arxiv.org/abs/2412.19382v1,"Md Isfakul Anam, Tuyen Vu, Jianhua Zhang"
"Minimal Batch Adaptive Learning Policy Engine for Real-Time Mid-Price
  Forecasting in High-Frequency Trading","High-frequency trading (HFT) has transformed modern financial markets, making
reliable short-term price forecasting models essential. In this study, we
present a novel approach to mid-price forecasting using Level 1 limit order
book (LOB) data from NASDAQ, focusing on 100 U.S. stocks from the S&amp;P 500 index
during the period from September to November 2022. Expanding on our previous
work with Radial Basis Function Neural Networks (RBFNN), which leveraged
automated feature importance techniques based on mean decrease impurity (MDI)
and gradient descent (GD), we introduce the Adaptive Learning Policy Engine
(ALPE) - a reinforcement learning (RL)-based agent designed for batch-free,
immediate mid-price forecasting. ALPE incorporates adaptive epsilon decay to
dynamically balance exploration and exploitation, outperforming a diverse range
of highly effective machine learning (ML) and deep learning (DL) models in
forecasting performance.",2024-12-26T22:49:53Z,http://arxiv.org/abs/2412.19372v1,"Adamantios Ntakaris, Gbenga Ibikunle"
"BeSplat -- Gaussian Splatting from a Single Blurry Image and Event
  Stream","Novel view synthesis has been greatly enhanced by the development of radiance
field methods. The introduction of 3D Gaussian Splatting (3DGS) has effectively
addressed key challenges, such as long training times and slow rendering
speeds, typically associated with Neural Radiance Fields (NeRF), while
maintaining high-quality reconstructions. In this work (BeSplat), we
demonstrate the recovery of sharp radiance field (Gaussian splats) from a
single motion-blurred image and its corresponding event stream. Our method
jointly learns the scene representation via Gaussian Splatting and recovers the
camera motion through Bezier SE(3) formulation effectively, minimizing
discrepancies between synthesized and real-world measurements of both blurry
image and corresponding event stream. We evaluate our approach on both
synthetic and real datasets, showcasing its ability to render view-consistent,
sharp images from the learned radiance field and the estimated camera
trajectory. To the best of our knowledge, ours is the first work to address
this highly challenging ill-posed problem in a Gaussian Splatting framework
with the effective incorporation of temporal information captured using the
event stream.",2024-12-26T22:35:29Z,http://arxiv.org/abs/2412.19370v1,"Gopi Raju Matta, Reddypalli Trisha, Kaushik Mitra"
"Central limit theorems for vector-valued composite functionals with
  smoothing and applications","This paper focuses on vector-valued composite functionals, which may be
nonlinear in probability. Our primary goal is to establish central limit
theorems for these functionals when mixed estimators are employed. Our study is
relevant to the evaluation and comparison of risk in decision-making contexts
and extends to functionals that arise in machine learning methods. A
generalized family of composite risk functionals is presented, which
encompasses most of the known coherent risk measures including systemic
measures of risk. The paper makes two main contributions. First, we analyze
vector-valued functionals, providing a framework for evaluating
high-dimensional risks. This framework facilitates the comparison of multiple
risk measures, as well as the estimation and asymptotic analysis of systemic
risk and its optimal value in decision-making problems. Second, we derive novel
central limit theorems for optimized composite functionals when mixed types of
estimators: empirical and smoothed estimators are used. We provide verifiable
sufficient conditions for the central limit formulae and show their
applicability to several popular measures of risk.",2024-12-26T22:23:11Z,http://arxiv.org/abs/2412.19367v1,"Huhui Chen, Darinka Dentcheva, Yang Lin, Gregory J. Stock"
Constructive approximate transport maps with normalizing flows,"We study an approximate controllability problem for the continuity equation
and its application to constructing transport maps with normalizing flows.
Specifically, we construct time-dependent controls $\theta=(w, a, b)$ in the
vector field $w(a^\top x + b)_+$ to approximately transport a known base
density $\rho_{\mathrm{B}}$ to a target density $\rho_*$. The approximation
error is measured in relative entropy, and $\theta$ are constructed piecewise
constant, with bounds on the number of switches being provided. Our main result
relies on an assumption on the relative tail decay of $\rho_*$ and
$\rho_{\mathrm{B}}$, and provides hints on characterizing the reachable space
of the continuity equation in relative entropy.",2024-12-26T22:09:33Z,http://arxiv.org/abs/2412.19366v1,"Antonio Álvarez-López, Borjan Geshkovski, Domènec Ruiz-Balet"
Large Language Models for Market Research: A Data-augmentation Approach,"Large Language Models (LLMs) have transformed artificial intelligence by
excelling in complex natural language processing tasks. Their ability to
generate human-like text has opened new possibilities for market research,
particularly in conjoint analysis, where understanding consumer preferences is
essential but often resource-intensive. Traditional survey-based methods face
limitations in scalability and cost, making LLM-generated data a promising
alternative. However, while LLMs have the potential to simulate real consumer
behavior, recent studies highlight a significant gap between LLM-generated and
human data, with biases introduced when substituting between the two. In this
paper, we address this gap by proposing a novel statistical data augmentation
approach that efficiently integrates LLM-generated data with real data in
conjoint analysis. Our method leverages transfer learning principles to debias
the LLM-generated data using a small amount of human data. This results in
statistically robust estimators with consistent and asymptotically normal
properties, in contrast to naive approaches that simply substitute human data
with LLM-generated data, which can exacerbate bias. We validate our framework
through an empirical study on COVID-19 vaccine preferences, demonstrating its
superior ability to reduce estimation error and save data and costs by 24.9\%
to 79.8\%. In contrast, naive approaches fail to save data due to the inherent
biases in LLM-generated data compared to human data. Another empirical study on
sports car choices validates the robustness of our results. Our findings
suggest that while LLM-generated data is not a direct substitute for human
responses, it can serve as a valuable complement when used within a robust
statistical framework.",2024-12-26T22:06:29Z,http://arxiv.org/abs/2412.19363v1,"Mengxin Wang, Dennis J. Zhang, Heng Zhang"
"Evaluating Convolutional Neural Networks for COVID-19 classification in
  chest X-ray images","Coronavirus Disease 2019 (COVID-19) pandemic rapidly spread globally,
impacting the lives of billions of people. The effective screening of infected
patients is a critical step to struggle with COVID-19, and treating the
patients avoiding this quickly disease spread. The need for automated and
scalable methods has increased due to the unavailability of accurate automated
toolkits. Recent researches using chest X-ray images suggest they include
relevant information about the COVID-19 virus. Hence, applying machine learning
techniques combined with radiological imaging promises to identify this disease
accurately. It is straightforward to collect these images once it is spreadly
shared and analyzed in the world. This paper presents a method for automatic
COVID-19 detection using chest Xray images through four convolutional neural
networks, namely: AlexNet, VGG-11, SqueezeNet, and DenseNet-121. This method
had been providing accurate diagnostics for positive or negative COVID-19
classification. We validate our experiments using a ten-fold cross-validation
procedure over the training and test sets. Our findings include the shallow
fine-tuning and data augmentation strategies that can assist in dealing with
the low number of positive COVID-19 images publicly available. The accuracy for
all CNNs is higher than 97.00%, and the SqueezeNet model achieved the best
result with 99.20%.",2024-12-26T22:05:30Z,http://arxiv.org/abs/2412.19362v1,"Leonardo Gabriel Ferreira Rodrigues, Danilo Ferreira da Silva, Larissa Ferreira Rodrigues, João Fernando Mari"
Dynamic Skill Adaptation for Large Language Models,"We present Dynamic Skill Adaptation (DSA), an adaptive and dynamic framework
to adapt novel and complex skills to Large Language Models (LLMs). Compared
with previous work which learns from human-curated and static data in random
orders, we propose to first automatically generate and organize the training
data by mimicking the learning pathways of human and then dynamically tailor
the training data based on the training dynamics. Specifically, inspired by the
learning structures and teaching strategies in the human education system, we
first construct a skill graph by decomposing complex skills into sub-skills and
arranging them based on their dependencies in human syllables. For every skill,
we utilize LLMs to generate both textbook-like data which contains detailed
descriptions of skills for pre-training and exercise-like data which targets at
explicitly utilizing the skills to solve problems for instruction-tuning.
Furthermore, during the instruction-tuning, we dynamically update the training
data which down-weight easy-to-learn examples, generate more complex examples,
and filter out data with errors. Experiments on large language models such as
LLAMA and Mistral demonstrate the effectiveness of our proposed methods in
adapting math reasoning skills and social study skills.",2024-12-26T22:04:23Z,http://arxiv.org/abs/2412.19361v1,"Jiaao Chen, Diyi Yang"
Microscopic imprints of learned solutions in adaptive resistor networks,"In physical networks trained using supervised learning, physical parameters
are adjusted to produce desired responses to inputs. An example is electrical
contrastive local learning networks of nodes connected by edges that are
resistors that adjust their conductances during training. When an edge
conductance changes, it upsets the current balance of every node. In response,
physics adjusts the node voltages to minimize the dissipated power. Learning in
these systems is therefore a coupled double-optimization process, in which the
network descends both a cost landscape in the high-dimensional space of edge
conductances, and a physical landscape -- the power -- in the high-dimensional
space of node voltages. Because of this coupling, the physical landscape of a
trained network contains information about the learned task. Here we
demonstrate that all the physical information relevant to the trained
input-output relation can be captured by a susceptibility, an experimentally
measurable quantity. We supplement our theoretical results with simulations to
show that the susceptibility is positively correlated with functional
importance and that we can extract physical insight into how the system
performs the task from the conductances of highly susceptible edges.",2024-12-26T21:36:23Z,http://arxiv.org/abs/2412.19356v1,"Marcel Guzman, Felipe Martins, Menachem Stern, Andrea J. Liu"
"Quantum-Inspired Weight-Constrained Neural Network: Reducing Variable
  Numbers by 100x Compared to Standard Neural Networks","Although quantum machine learning has shown great promise, the practical
application of quantum computers remains constrained in the noisy
intermediate-scale quantum era. To take advantage of quantum machine learning,
we investigate the underlying mathematical principles of these quantum models
and adapt them to classical machine learning frameworks. Specifically, we
develop a classical weight-constrained neural network that generates weights
based on quantum-inspired insights. We find that this approach can reduce the
number of variables in a classical neural network by a factor of 135 while
preserving its learnability. In addition, we develop a dropout method to
enhance the robustness of quantum machine learning models, which are highly
susceptible to adversarial attacks. This technique can also be applied to
improve the adversarial resilience of the classical weight-constrained neural
network, which is essential for industry applications, such as self-driving
vehicles. Our work offers a novel approach to reduce the complexity of large
classical neural networks, addressing a critical challenge in machine learning.",2024-12-26T21:35:12Z,http://arxiv.org/abs/2412.19355v1,"Shaozhi Li, M Sabbir Salek, Binayyak Roy, Yao Wang, Mashrur Chowdhury"
"SuperSalt: Equivariant Neural Network Force Fields for Multicomponent
  Molten Salts System","Molten salts are crucial for clean energy applications, yet exploring their
thermophysical properties across diverse chemical space remains challenging. We
present the development of a machine learning interatomic potential (MLIP)
called SuperSalt, which targets 11-cation chloride melts and captures the
essential physics of molten salts with near-DFT accuracy. Using an efficient
workflow that integrates systems of one, two, and 11 components, the SuperSalt
potential can accurately predict thermophysical properties such as density,
bulk modulus, thermal expansion, and heat capacity. Our model is validated
across a broad chemical space, demonstrating excellent transferability. We
further illustrate how Bayesian optimization combined with SuperSalt can
accelerate the discovery of optimal salt compositions with desired properties.
This work provides a foundation for future studies that allows easy extensions
to more complex systems, such as those containing additional elements.
SuperSalt represents a shift towards a more universal, efficient, and accurate
modeling of molten salts for advanced energy applications.",2024-12-26T21:32:08Z,http://arxiv.org/abs/2412.19353v1,"Chen Shen, Siamak Attarian, Yixuan Zhang, Hongbin Zhang, Mark Asta, Izabela Szlufarska, Dane Morgan"
"Federated Hybrid Training and Self-Adversarial Distillation: Towards
  Robust Edge Networks","Federated learning (FL) is a distributed training technology that enhances
data privacy in mobile edge networks by allowing data owners to collaborate
without transmitting raw data to the edge server. However, data heterogeneity
and adversarial attacks pose challenges to develop an unbiased and robust
global model for edge deployment. To address this, we propose Federated hyBrid
Adversarial training and self-adversarial disTillation (FedBAT), a new
framework designed to improve both robustness and generalization of the global
model. FedBAT seamlessly integrates hybrid adversarial training and
self-adversarial distillation into the conventional FL framework from data
augmentation and feature distillation perspectives. From a data augmentation
perspective, we propose hybrid adversarial training to defend against
adversarial attacks by balancing accuracy and robustness through a weighted
combination of standard and adversarial training. From a feature distillation
perspective, we introduce a novel augmentation-invariant adversarial
distillation method that aligns local adversarial features of augmented images
with their corresponding unbiased global clean features. This alignment can
effectively mitigate bias from data heterogeneity while enhancing both the
robustness and generalization of the global model. Extensive experimental
results across multiple datasets demonstrate that FedBAT yields comparable or
superior performance gains in improving robustness while maintaining accuracy
compared to several baselines.",2024-12-26T21:32:08Z,http://arxiv.org/abs/2412.19354v1,"Yu Qiao, Apurba Adhikary, Kitae Kim, Eui-Nam Huh, Zhu Han, Choong Seon Hong"
ETTA: Elucidating the Design Space of Text-to-Audio Models,"Recent years have seen significant progress in Text-To-Audio (TTA) synthesis,
enabling users to enrich their creative workflows with synthetic audio
generated from natural language prompts. Despite this progress, the effects of
data, model architecture, training objective functions, and sampling strategies
on target benchmarks are not well understood. With the purpose of providing a
holistic understanding of the design space of TTA models, we set up a
large-scale empirical experiment focused on diffusion and flow matching models.
Our contributions include: 1) AF-Synthetic, a large dataset of high quality
synthetic captions obtained from an audio understanding model; 2) a systematic
comparison of different architectural, training, and inference design choices
for TTA models; 3) an analysis of sampling methods and their Pareto curves with
respect to generation quality and inference speed. We leverage the knowledge
obtained from this extensive analysis to propose our best model dubbed
Elucidated Text-To-Audio (ETTA). When evaluated on AudioCaps and MusicCaps,
ETTA provides improvements over the baselines trained on publicly available
data, while being competitive with models trained on proprietary data. Finally,
we show ETTA's improved ability to generate creative audio following complex
and imaginative captions -- a task that is more challenging than current
benchmarks.",2024-12-26T21:13:12Z,http://arxiv.org/abs/2412.19351v1,"Sang-gil Lee, Zhifeng Kong, Arushi Goel, Sungwon Kim, Rafael Valle, Bryan Catanzaro"
"On the Expressiveness and Length Generalization of Selective State-Space
  Models on Regular Languages","Selective state-space models (SSMs) are an emerging alternative to the
Transformer, offering the unique advantage of parallel training and sequential
inference. Although these models have shown promising performance on a variety
of tasks, their formal expressiveness and length generalization properties
remain underexplored. In this work, we provide insight into the workings of
selective SSMs by analyzing their expressiveness and length generalization
performance on regular language tasks, i.e., finite-state automaton (FSA)
emulation. We address certain limitations of modern SSM-based architectures by
introducing the Selective Dense State-Space Model (SD-SSM), the first selective
SSM that exhibits perfect length generalization on a set of various regular
language tasks using a single layer. It utilizes a dictionary of dense
transition matrices, a softmax selection mechanism that creates a convex
combination of dictionary matrices at each time step, and a readout consisting
of layer normalization followed by a linear map. We then proceed to evaluate
variants of diagonal selective SSMs by considering their empirical performance
on commutative and non-commutative automata. We explain the experimental
results with theoretical considerations. Our code is available at
https://github.com/IBM/selective-dense-state-space-model.",2024-12-26T20:53:04Z,http://arxiv.org/abs/2412.19350v1,"Aleksandar Terzić, Michael Hersche, Giacomo Camposampiero, Thomas Hofmann, Abu Sebastian, Abbas Rahimi"
"Semi-Supervised Learning from Small Annotated Data and Large Unlabeled
  Data for Fine-grained PICO Entity Recognition","Objective: Extracting PICO elements -- Participants, Intervention,
Comparison, and Outcomes -- from clinical trial literature is essential for
clinical evidence retrieval, appraisal, and synthesis. Existing approaches do
not distinguish the attributes of PICO entities. This study aims to develop a
named entity recognition (NER) model to extract PICO entities with fine
granularities.
  Materials and Methods: Using a corpus of 2,511 abstracts with PICO mentions
from 4 public datasets, we developed a semi-supervised method to facilitate the
training of a NER model, FinePICO, by combining limited annotated data of PICO
entities and abundant unlabeled data. For evaluation, we divided the entire
dataset into two subsets: a smaller group with annotations and a larger group
without annotations. We then established the theoretical lower and upper
performance bounds based on the performance of supervised learning models
trained solely on the small, annotated subset and on the entire set with
complete annotations, respectively. Finally, we evaluated FinePICO on both the
smaller annotated subset and the larger, initially unannotated subset. We
measured the performance of FinePICO using precision, recall, and F1.
  Results: Our method achieved precision/recall/F1 of 0.567/0.636/0.60,
respectively, using a small set of annotated samples, outperforming the
baseline model (F1: 0.437) by more than 16\%. The model demonstrates
generalizability to a different PICO framework and to another corpus, which
consistently outperforms the benchmark in diverse experimental settings
(p-value \textless0.001).
  Conclusion: This study contributes a generalizable and effective
semi-supervised approach to named entity recognition leveraging large unlabeled
data together with small, annotated data. It also initially supports
fine-grained PICO extraction.",2024-12-26T20:24:35Z,http://arxiv.org/abs/2412.19346v1,"Fangyi Chen, Gongbo Zhang, Yilu Fang, Yifan Peng, Chunhua Weng"
"A Reinforcement Learning-Based Task Mapping Method to Improve the
  Reliability of Clustered Manycores","The increasing scale of manycore systems poses significant challenges in
managing reliability while meeting performance demands. Simultaneously, these
systems become more susceptible to different aging mechanisms such as
negative-bias temperature instability (NBTI), hot carrier injection (HCI), and
thermal cycling (TC), as well as the electromigration (EM) phenomenon. In this
paper, we propose a reinforcement learning (RL)-based task mapping method to
improve the reliability of manycore systems considering the aforementioned
aging mechanisms, which consists of three steps including bin packing,
task-to-bin mapping, and task-to-core mapping. In the initial step, a
density-based spatial application with noise (DBSCAN) clustering method is
employed to compose some clusters (bins) based on the cores temperature. Then,
the Q-learning algorithm is used for the two latter steps, to map the arrived
task on a core such that the minimum thermal variation is occurred among all
the bins. Compared to the state-of-the-art works, the proposed method is
performed during runtime without requiring any parameter to be calculated
offline. The effectiveness of the proposed technique is evaluated on 16, 32,
and 64 cores systems using SPLASH2 and PARSEC benchmark suite applications. The
results demonstrate up to 27% increase in the mean time to failure (MTTF)
compared to the state-of-the-art task mapping techniques.",2024-12-26T20:08:10Z,http://arxiv.org/abs/2412.19340v1,"Fatemeh Hossein-Khani, Omid Akbari"
Modular quantum extreme reservoir computing,"The connectivity between qubits plays a crucial role in the performance of
quantum extreme reservoir computing (QERC), particularly regarding long-range
and inter-modular connections. We demonstrate that sufficiently long-range
connections within a single module can achieve performance comparable to fully
connected networks in supervised learning tasks. Further analysis of
inter-modular connection schemes -- such as boundary, parallel, and arbitrary
links -- shows that even a small number of well-placed connections can
significantly enhance QERC performance. These findings suggest that modular
QERC architectures, which could be more easily implemented on two-dimensional
quantum chips or through the integration of small quantum systems, provide an
effective approach for machine learning tasks.",2024-12-26T19:15:09Z,http://arxiv.org/abs/2412.19336v1,"Hon Wai Lau, Aoi Hayashi, Akitada Sakurai, William John Munro, Kae Nemoto"
"DPmoire: A tool for constructing accurate machine learning force fields
  in moiré systems","In moir\'e systems, the impact of lattice relaxation on electronic band
structures is significant, yet the computational demands of first-principles
relaxation are prohibitively high due to the large number of atoms involved. To
address this challenge, We introduce a robust methodology for the construction
of machine learning potentials specifically tailored for moir\'e structures and
present an open-source software package DPmoire designed to facilitate this
process. Utilizing this package, we have developed machine learning force
fields (MLFFs) for MX$_2$ (M = Mo, W; X = S, Se, Te) materials. Our approach
not only streamlines the computational process but also ensures accurate
replication of the detailed electronic and structural properties typically
observed in density functional theory (DFT) relaxations. The MLFFs were
rigorously validated against standard DFT results, confirming their efficacy in
capturing the complex interplay of atomic interactions within these layered
materials. This development not only enhances our ability to explore the
physical properties of moir\'e systems with reduced computational overhead but
also opens new avenues for the study of relaxation effects and their impact on
material properties in two-dimensional layered structures.",2024-12-26T19:00:40Z,http://arxiv.org/abs/2412.19333v1,"Jiaxuan Liu, Zhong Fang, Hongming Weng, Quansheng Wu"
"CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language
  Models","Recent advances in Large Vision-Language Models (LVLMs) have sparked
significant progress in general-purpose vision tasks through visual instruction
tuning. While some works have demonstrated the capability of LVLMs to generate
segmentation masks that align phrases with natural language descriptions in a
single image, they struggle with segmentation-grounded comparisons across
multiple images, particularly at finer granularities such as object parts. In
this paper, we introduce the new task of part-focused semantic co-segmentation,
which seeks to identify and segment common and unique objects and parts across
images. To address this task, we present CALICO, the first LVLM that can
segment and reason over multiple masks across images, enabling object
comparison based on their constituent parts. CALICO features two proposed
components, a novel Correspondence Extraction Module, which captures
semantic-rich information to identify part-level correspondences between
objects, and a Correspondence Adaptation Module, which embeds this information
into the LVLM to facilitate multi-image understanding in a parameter-efficient
manner. To support training and evaluation, we curate MixedParts, a
comprehensive multi-image segmentation dataset containing $\sim$2.4M samples
across $\sim$44K images with diverse object and part categories. Experimental
results show CALICO, finetuned on only 0.3% of its architecture, achieves
robust performance in part-focused semantic co-segmentation.",2024-12-26T18:59:37Z,http://arxiv.org/abs/2412.19331v1,"Kiet A. Nguyen, Adheesh Juvekar, Tianjiao Yu, Muntasir Wahed, Ismini Lourentzou"
Identifying Split Vacancies with Foundation Models and Electrostatics,"Point defects are ubiquitous in solid-state compounds, dictating many
functional properties such as conductivity, catalytic activity and carrier
recombination. Over the past decade, the prevalence of metastable defect
geometries and their importance to relevant properties has been increasingly
recognised. A particularly striking example of this is split vacancies, where
an isolated atomic vacancy transforms to a stoichiometry-conserving complex of
two vacancies and an interstitial ($V_X \rightarrow [V_X + X_i + V_X]$), which
can be accompanied by a dramatic lowering of the defect energy and change in
behaviour. Such species are particularly challenging to identify from
computation, due to the `non-local' nature of this reconstruction. Here, I
present an approach for efficiently identifying such species in solid-state
compounds, through tiered screening which combines geometric analysis,
electrostatic energies and foundation machine learning (ML) models. This
approach allows the screening of all compounds in the Materials Project
database (including all entries in the ICSD, along with several thousand
predicted metastable materials), identifying thousands of split vacancy
configurations, hitherto unknown. This study highlights both the potential
utility of machine-learning potentials for defect investigations, with
important caveats, and the importance of global optimisation approaches for
correctly identifying stable defect geometries.",2024-12-26T18:58:52Z,http://arxiv.org/abs/2412.19330v1,Seán R. Kavanagh
"Deep learning and whole-brain networks for biomarker discovery: modeling
  the dynamics of brain fluctuations in resting-state and cognitive tasks","Background: Brain network models offer insights into brain dynamics, but the
utility of model-derived bifurcation parameters as biomarkers remains
underexplored. Objective: This study evaluates bifurcation parameters from a
whole-brain network model as biomarkers for distinguishing brain states
associated with resting-state and task-based cognitive conditions. Methods:
Synthetic BOLD signals were generated using a supercritical Hopf brain network
model to train deep learning models for bifurcation parameter prediction.
Inference was performed on Human Connectome Project data, including both
resting-state and task-based conditions. Statistical analyses assessed the
separability of brain states based on bifurcation parameter distributions.
Results: Bifurcation parameter distributions differed significantly across task
and resting-state conditions ($p &lt; 0.0001$ for all but one comparison).
Task-based brain states exhibited higher bifurcation values compared to rest.
Conclusion: Bifurcation parameters effectively differentiate cognitive and
resting states, warranting further investigation as biomarkers for brain state
characterization and neurological disorder assessment.",2024-12-26T18:58:38Z,http://arxiv.org/abs/2412.19329v1,"Facundo Roffet, Gustavo Deco, Claudio Delrieux, Gustavo Patow"
"Resolving the Ambiguity of Complete-to-Partial Point Cloud Registration
  for Image-Guided Liver Surgery with Patches-to-Partial Matching","In image-guided liver surgery, the initial rigid alignment between
preoperative and intraoperative data, often represented as point clouds, is
crucial for providing sub-surface information from preoperative CT/MRI images
to the surgeon during the procedure. Currently, this alignment is typically
performed using semi-automatic methods, which, while effective to some extent,
are prone to errors that demand manual correction. Point cloud
correspondence-based registration methods are promising to serve as a fully
automatic solution. However, they may struggle in scenarios with limited
intraoperative surface visibility, a common challenge in liver surgery,
particularly in laparoscopic procedures, which we refer to as
complete-to-partial ambiguity. We first illustrate this ambiguity by evaluating
the performance of state-of-the-art learning-based point cloud registration
methods on our carefully constructed in silico and in vitro datasets. Then, we
propose a patches-to-partial matching strategy as a plug-and-play module to
resolve the ambiguity, which can be seamlessly integrated into learning-based
registration methods without disrupting their end-to-end structure. It has
proven effective and efficient in improving registration performance for cases
with limited intraoperative visibility. The constructed benchmark and the
proposed module establish a solid foundation for advancing applications of
point cloud correspondence-based registration methods in image-guided liver
surgery.",2024-12-26T18:58:29Z,http://arxiv.org/abs/2412.19328v1,"Zixin Yang, Jon S. Heiselman, Cheng Han, Kelly Merrell, Richard Simon, Cristian. A. Linte"
"Task Preference Optimization: Improving Multimodal Large Language Models
  with Vision Task Alignment","Current multimodal large language models (MLLMs) struggle with fine-grained
or precise understanding of visuals though they give comprehensive perception
and reasoning in a spectrum of vision applications. Recent studies either
develop tool-using or unify specific visual tasks into the autoregressive
framework, often at the expense of overall multimodal performance. To address
this issue and enhance MLLMs with visual tasks in a scalable fashion, we
propose Task Preference Optimization (TPO), a novel method that utilizes
differentiable task preferences derived from typical fine-grained visual tasks.
TPO introduces learnable task tokens that establish connections between
multiple task-specific heads and the MLLM. By leveraging rich visual labels
during training, TPO significantly enhances the MLLM's multimodal capabilities
and task-specific performance. Through multi-task co-training within TPO, we
observe synergistic benefits that elevate individual task performance beyond
what is achievable through single-task training methodologies. Our
instantiation of this approach with VideoChat and LLaVA demonstrates an overall
14.6% improvement in multimodal performance compared to baseline models.
Additionally, MLLM-TPO demonstrates robust zero-shot capabilities across
various tasks, performing comparably to state-of-the-art supervised models. The
code will be released at https://github.com/OpenGVLab/TPO",2024-12-26T18:56:05Z,http://arxiv.org/abs/2412.19326v1,"Ziang Yan, Zhilin Li, Yinan He, Chenting Wang, Kunchang Li, Xinhao Li, Xiangyu Zeng, Zilei Wang, Yali Wang, Yu Qiao, Limin Wang, Yi Wang"
"Performance Control in Early Exiting to Deploy Large Models at the Same
  Cost of Smaller Ones","Early Exiting (EE) is a promising technique for speeding up inference by
adaptively allocating compute resources to data points based on their
difficulty. The approach enables predictions to exit at earlier layers for
simpler samples while reserving more computation for challenging ones. In this
study, we first present a novel perspective on the EE approach, showing that
larger models deployed with EE can achieve higher performance than smaller
models while maintaining similar computational costs. As existing EE approaches
rely on confidence estimation at each exit point, we further study the impact
of overconfidence on the controllability of the compute-performance trade-off.
We introduce Performance Control Early Exiting (PCEE), a method that enables
accuracy thresholding by basing decisions not on a data point's confidence but
on the average accuracy of samples with similar confidence levels from a
held-out validation set. In our experiments, we show that PCEE offers a simple
yet computationally efficient approach that provides better control over
performance than standard confidence-based approaches, and allows us to scale
up model sizes to yield performance gain while reducing the computational cost.",2024-12-26T18:54:32Z,http://arxiv.org/abs/2412.19325v1,"Mehrnaz Mofakhami, Reza Bayat, Ioannis Mitliagkas, Joao Monteiro, Valentina Zantedeschi"
"A novel framework for MCDM based on Z numbers and soft likelihood
  function","The optimization on the structure of process of information management under
uncertain environment has attracted lots of attention from researchers around
the world. Nevertheless, how to obtain accurate and rational evaluation from
assessments produced by experts is still an open problem. Specially,
intuitionistic fuzzy set provides an effective solution in handling
indeterminate information. And Yager proposes a novel method for fusion of
probabilistic evidence to handle uncertain and conflicting information lately
which is called soft likelihood function. This paper devises a novel framework
of soft likelihood function based on information volume of fuzzy membership and
credibility measure for extracting truly useful and valuable information from
uncertainty. An application is provided to verify the validity and correctness
of the proposed framework. Besides, the comparisons with other existing methods
further demonstrate the superiority of the novel framework of soft likelihood
function.",2024-12-26T18:47:19Z,http://arxiv.org/abs/2412.19321v1,Yuanpeng He
Adaptive Conformal Inference by Betting,"Conformal prediction is a valuable tool for quantifying predictive
uncertainty of machine learning models. However, its applicability relies on
the assumption of data exchangeability, a condition which is often not met in
real-world scenarios. In this paper, we consider the problem of adaptive
conformal inference without any assumptions about the data generating process.
Existing approaches for adaptive conformal inference are based on optimizing
the pinball loss using variants of online gradient descent. A notable
shortcoming of such approaches is in their explicit dependence on and
sensitivity to the choice of the learning rates. In this paper, we propose a
different approach for adaptive conformal inference that leverages
parameter-free online convex optimization techniques. We prove that our method
controls long-term miscoverage frequency at a nominal level and demonstrate its
convincing empirical performance without any need of performing cumbersome
parameter tuning.",2024-12-26T18:42:08Z,http://arxiv.org/abs/2412.19318v1,"Aleksandr Podkopaev, Darren Xu, Kuang-Chih Lee"
"xSRL: Safety-Aware Explainable Reinforcement Learning -- Safety as a
  Product of Explainability","Reinforcement learning (RL) has shown great promise in simulated
environments, such as games, where failures have minimal consequences. However,
the deployment of RL agents in real-world systems such as autonomous vehicles,
robotics, UAVs, and medical devices demands a higher level of safety and
transparency, particularly when facing adversarial threats. Safe RL algorithms
have been developed to address these concerns by optimizing both task
performance and safety constraints. However, errors are inevitable, and when
they occur, it is essential that the RL agents can also explain their actions
to human operators. This makes trust in the safety mechanisms of RL systems
crucial for effective deployment. Explainability plays a key role in building
this trust by providing clear, actionable insights into the agent's
decision-making process, ensuring that safety-critical decisions are well
understood. While machine learning (ML) has seen significant advances in
interpretability and visualization, explainability methods for RL remain
limited. Current tools fail to address the dynamic, sequential nature of RL and
its needs to balance task performance with safety constraints over time. The
re-purposing of traditional ML methods, such as saliency maps, is inadequate
for safety-critical RL applications where mistakes can result in severe
consequences. To bridge this gap, we propose xSRL, a framework that integrates
both local and global explanations to provide a comprehensive understanding of
RL agents' behavior. xSRL also enables developers to identify policy
vulnerabilities through adversarial attacks, offering tools to debug and patch
agents without retraining. Our experiments and user studies demonstrate xSRL's
effectiveness in increasing safety in RL systems, making them more reliable and
trustworthy for real-world deployment. Code is available at
https://github.com/risal-shefin/xSRL.",2024-12-26T18:19:04Z,http://arxiv.org/abs/2412.19311v1,"Risal Shahriar Shefin, Md Asifur Rahman, Thai Le, Sarra Alqahtani"
"Perceive, Query &amp; Reason: Enhancing Video QA with Question-Guided
  Temporal Queries","Video Question Answering (Video QA) is a challenging video understanding task
that requires models to comprehend entire videos, identify the most relevant
information based on contextual cues from a given question, and reason
accurately to provide answers. Recent advancements in Multimodal Large Language
Models (MLLMs) have transformed video QA by leveraging their exceptional
commonsense reasoning capabilities. This progress is largely driven by the
effective alignment between visual data and the language space of MLLMs.
However, for video QA, an additional space-time alignment poses a considerable
challenge for extracting question-relevant information across frames. In this
work, we investigate diverse temporal modeling techniques to integrate with
MLLMs, aiming to achieve question-guided temporal modeling that leverages
pre-trained visual and textual alignment in MLLMs. We propose T-Former, a novel
temporal modeling method that creates a question-guided temporal bridge between
frame-wise visual perception and the reasoning capabilities of LLMs. Our
evaluation across multiple video QA benchmarks demonstrates that T-Former
competes favorably with existing temporal modeling approaches and aligns with
recent advancements in video QA.",2024-12-26T17:53:14Z,http://arxiv.org/abs/2412.19304v1,"Roberto Amoroso, Gengyuan Zhang, Rajat Koner, Lorenzo Baraldi, Rita Cucchiara, Volker Tresp"
RecLM: Recommendation Instruction Tuning,"Modern recommender systems aim to deeply understand users' complex
preferences through their past interactions. While deep collaborative filtering
approaches using Graph Neural Networks (GNNs) excel at capturing user-item
relationships, their effectiveness is limited when handling sparse data or
zero-shot scenarios, primarily due to constraints in ID-based embedding
functions. To address these challenges, we propose a model-agnostic
recommendation instruction-tuning paradigm that seamlessly integrates large
language models with collaborative filtering. Our proposed Recommendation
Language Model (RecLM) enhances the capture of user preference diversity
through a carefully designed reinforcement learning reward function that
facilitates self-augmentation of language models. Comprehensive evaluations
demonstrate significant advantages of our approach across various settings, and
its plug-and-play compatibility with state-of-the-art recommender systems
results in notable performance enhancements.",2024-12-26T17:51:54Z,http://arxiv.org/abs/2412.19302v1,"Yangqin Jiang, Yuhao Yang, Lianghao Xia, Da Luo, Kangyi Lin, Chao Huang"
RAG with Differential Privacy,"Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to
provide *Large Language Models* (LLM) with fresh and relevant context,
mitigating the risk of hallucinations and improving the overall quality of
responses in environments with large and fast moving knowledge bases. However,
the integration of external documents into the generation process raises
significant privacy concerns. Indeed, when added to a prompt, it is not
possible to guarantee a response will not inadvertently expose confidential
data, leading to potential breaches of privacy and ethical dilemmas. This paper
explores a practical solution to this problem suitable to general knowledge
extraction from personal data. It shows *differentially private token
generation* is a viable approach to private RAG.",2024-12-26T17:34:26Z,http://arxiv.org/abs/2412.19291v1,Nicolas Grislain
"ViPCap: Retrieval Text-Based Visual Prompts for Lightweight Image
  Captioning","Recent lightweight image captioning models using retrieved data mainly focus
on text prompts. However, previous works only utilize the retrieved text as
text prompts, and the visual information relies only on the CLIP visual
embedding. Because of this issue, there is a limitation that the image
descriptions inherent in the prompt are not sufficiently reflected in the
visual embedding space. To tackle this issue, we propose ViPCap, a novel
retrieval text-based visual prompt for lightweight image captioning. ViPCap
leverages the retrieved text with image information as visual prompts to
enhance the ability of the model to capture relevant visual information. By
mapping text prompts into the CLIP space and generating multiple randomized
Gaussian distributions, our method leverages sampling to explore randomly
augmented distributions and effectively retrieves the semantic features that
contain image information. These retrieved features are integrated into the
image and designated as the visual prompt, leading to performance improvements
on the datasets such as COCO, Flickr30k, and NoCaps. Experimental results
demonstrate that ViPCap significantly outperforms prior lightweight captioning
models in efficiency and effectiveness, demonstrating the potential for a
plug-and-play solution.",2024-12-26T17:29:38Z,http://arxiv.org/abs/2412.19289v1,"Taewhan Kim, Soeun Lee, Si-Woo Kim, Dong-Jin Kim"
"Time Series Foundational Models: Their Role in Anomaly Detection and
  Prediction","Time series foundational models (TSFM) have gained prominence in time series
forecasting, promising state-of-the-art performance across various
applications. However, their application in anomaly detection and prediction
remains underexplored, with growing concerns regarding their black-box nature,
lack of interpretability and applicability. This paper critically evaluates the
efficacy of TSFM in anomaly detection and prediction tasks. We systematically
analyze TSFM across multiple datasets, including those characterized by the
absence of discernible patterns, trends and seasonality. Our analysis shows
that while TSFMs can be extended for anomaly detection and prediction,
traditional statistical and deep learning models often match or outperform TSFM
in these tasks. Additionally, TSFMs require high computational resources but
fail to capture sequential dependencies effectively or improve performance in
few-shot or zero-shot scenarios. \noindent The preprocessed datasets, codes to
reproduce the results and supplementary materials are available at
https://github.com/smtmnfg/TSFM.",2024-12-26T17:15:30Z,http://arxiv.org/abs/2412.19286v1,"Chathurangi Shyalika, Harleen Kaur Bagga, Ahan Bhatt, Renjith Prasad, Alaa Al Ghazo, Amit Sheth"
"Thermal amplification and melting of phases in spin-orbit-coupled spin-1
  Bose-Einstein condensates","We implement Hartree-Fock-Bogoliubov theory with Popov approximation for a
homogeneous Raman-induced spin-orbit-coupled spin-1 Bose-Einstein condensate
and investigate the effects of finite temperature ($T$) on the ground-state
phase diagram. We calculate the roton gap as a function of Raman coupling
($\Omega$) or quadratic Zeeman field strength ($\epsilon$) to extract the
critical points separating the supersolid stripe phase from the plane wave or
zero-momentum phase at finite temperatures. We present a few representative
finite-temperature phase diagrams for the system in the $T-\Omega$ and
$T-\epsilon$ planes. Our observations indicate that the supersolid stripe phase
melts at finite temperatures. We also discuss the contrasting roles of quantum
and thermal fluctuations in shifting the phase boundary separating the
supersolid stripe from the plane-wave phase.",2024-12-26T17:03:05Z,http://arxiv.org/abs/2412.19285v1,"Ritu, Rajat, Arko Roy, Sandeep Gautam"
"PearSAN: A Machine Learning Method for Inverse Design using Pearson
  Correlated Surrogate Annealing","PearSAN is a machine learning-assisted optimization algorithm applicable to
inverse design problems with large design spaces, where traditional optimizers
struggle. The algorithm leverages the latent space of a generative model for
rapid sampling and employs a Pearson correlated surrogate model to predict the
figure of merit of the true design metric. As a showcase example, PearSAN is
applied to thermophotovoltaic (TPV) metasurface design by matching the working
bands between a thermal radiator and a photovoltaic cell. PearSAN can work with
any pretrained generative model with a discretized latent space, making it easy
to integrate with VQ-VAEs and binary autoencoders. Its novel Pearson
correlational loss can be used as both a latent regularization method, similar
to batch and layer normalization, and as a surrogate training loss. We compare
both to previous energy matching losses, which are shown to enforce poor
regularization and performance, even with upgraded affine parameters. PearSAN
achieves a state-of-the-art maximum design efficiency of 97%, and is at least
an order of magnitude faster than previous methods, with an improved maximum
figure-of-merit gain.",2024-12-26T17:02:19Z,http://arxiv.org/abs/2412.19284v1,"Michael Bezick, Blake A. Wilson, Vaishnavi Iyer, Yuheng Chen, Vladimir M. Shalaev, Sabre Kais, Alexander V. Kildishev, Alexandra Boltasseva, Brad Lackey"
Probing medium response via strangeness enhancement around quenched jets,"Jet-induced medium excitation is a crucial part of jet interactions with the
quark-gluon plasma (QGP) in relativistic heavy-ion collisions, and has recently
been confirmed by experiment for the first time. Based on the AMPT model
simulation, we propose the strangeness enhancement around quenched jets as a
novel signature of jet-induced medium excitation. By applying the jet-particle
correlation techniques, we calculate jet-induced particle yields around the
jets and find a significant enhancement of the strange-to-non-strange-hadron
ratio and the double-to-single-strange-hadron ratio correlated with jets in
relativistic nucleus-nucleus collisions relative to proton-proton collisions.
This enhancement increases with both the strength of jet-QGP interactions and
the radial distance from jet axis. These observations align with the features
of jet-induced medium excitation and parton coalescence in hadron formation,
and await experimental validation in the future measurements.",2024-12-26T17:00:56Z,http://arxiv.org/abs/2412.19283v1,"Ao Luo, Shanshan Cao, Guang-You Qin"
Improving Generalization for AI-Synthesized Voice Detection,"AI-synthesized voice technology has the potential to create realistic human
voices for beneficial applications, but it can also be misused for malicious
purposes. While existing AI-synthesized voice detection models excel in
intra-domain evaluation, they face challenges in generalizing across different
domains, potentially becoming obsolete as new voice generators emerge. Current
solutions use diverse data and advanced machine learning techniques (e.g.,
domain-invariant representation, self-supervised learning), but are limited by
predefined vocoders and sensitivity to factors like background noise and
speaker identity. In this work, we introduce an innovative disentanglement
framework aimed at extracting domain-agnostic artifact features related to
vocoders. Utilizing these features, we enhance model learning in a flat loss
landscape, enabling escape from suboptimal solutions and improving
generalization. Extensive experiments on benchmarks show our approach
outperforms state-of-the-art methods, achieving up to 5.12% improvement in the
equal error rate metric in intra-domain and 7.59% in cross-domain evaluations.",2024-12-26T16:45:20Z,http://arxiv.org/abs/2412.19279v1,"Hainan Ren, Lin Li, Chun-Hao Liu, Xin Wang, Shu Hu"
"Flat panel laser displays enabled by large-scale visible photonic
  integrated circuits","Laser-based displays are highly sought after for their superior brightness
and color performance, especially in advanced applications like augmented
reality (AR). However, their broader adoption has been hindered by bulky
projector designs and complex optical module assemblies. Here, we introduce a
new laser display architecture enabled by large-scale visible photonic
integrated circuits (PICs) to address these challenges. Unlike previous
projector-style laser displays, this architecture features an ultra-thin,
flat-panel form factor, replacing bulky free-space illumination modules with a
single, high-performance photonic chip. Centimeter-scale PIC devices, which
integrate thousands of distinct optical components on-chip, are carefully
tailored to achieve high display uniformity, contrast, and efficiency. We
demonstrate a 2 mm-thick flat-panel laser display combining the PIC with a
liquid-crystal-on-silicon (LCoS) panel, achieving 211% of the color gamut and
more than 80% volume reduction compared to traditional LCoS displays. We
further showcase its application in a see-through AR system. Our work
represents a major advancement in the integration of nanophotonics with display
technology, enabling a range of new display concepts, from high-performance
immersive displays to slim-panel 3D holography.",2024-12-26T16:31:00Z,http://arxiv.org/abs/2412.19274v1,"Zhujun Shi, Risheng Cheng, Guohua Wei, Steven A. Hickman, Min Chul Shin, Peter Topalian, Lei Wang, Dusan Coso, Brian Le, Lizzy Lee, Sean Braxton, Alexander Koshelev, Maxwell F. Parsons, Rahul Agarwal, Barry Silverstein, Yun Wang, Giuseppe Calafiore"
Optimizing Multi-Stage Language Models for Effective Text Retrieval,"Efficient text retrieval is critical for applications such as legal document
analysis, particularly in specialized contexts like Japanese legal systems.
Existing retrieval methods often underperform in such domain-specific
scenarios, necessitating tailored approaches. In this paper, we introduce a
novel two-phase text retrieval pipeline optimized for Japanese legal datasets.
Our method leverages advanced language models to achieve state-of-the-art
performance, significantly improving retrieval efficiency and accuracy. To
further enhance robustness and adaptability, we incorporate an ensemble model
that integrates multiple retrieval strategies, resulting in superior outcomes
across diverse tasks. Extensive experiments validate the effectiveness of our
approach, demonstrating strong performance on both Japanese legal datasets and
widely recognized benchmarks like MS-MARCO. Our work establishes new standards
for text retrieval in domain-specific and general contexts, providing a
comprehensive solution for addressing complex queries in legal and multilingual
environments.",2024-12-26T16:05:19Z,http://arxiv.org/abs/2412.19265v1,"Quang Hoang Trung, Le Trung Hoang, Nguyen Van Hoang Phuc"
"MEDEC: A Benchmark for Medical Error Detection and Correction in
  Clinical Notes","Several studies showed that Large Language Models (LLMs) can answer medical
questions correctly, even outperforming the average human score in some medical
exams. However, to our knowledge, no study has been conducted to assess the
ability of language models to validate existing or generated medical text for
correctness and consistency. In this paper, we introduce MEDEC
(https://github.com/abachaa/MEDEC), the first publicly available benchmark for
medical error detection and correction in clinical notes, covering five types
of errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal
Organism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes
from three US hospital systems that were not previously seen by any LLM. The
dataset has been used for the MEDIQA-CORR shared task to evaluate seventeen
participating systems [Ben Abacha et al., 2024]. In this paper, we describe the
data creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,
Claude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and
correcting medical errors requiring both medical knowledge and reasoning
capabilities. We also conducted a comparative study where two medical doctors
performed the same task on the MEDEC test set. The results showed that MEDEC is
a sufficiently challenging benchmark to assess the ability of models to
validate existing or generated notes and to correct medical errors. We also
found that although recent LLMs have a good performance in error detection and
correction, they are still outperformed by medical doctors in these tasks. We
discuss the potential factors behind this gap, the insights from our
experiments, the limitations of current evaluation metrics, and share potential
pointers for future research.",2024-12-26T15:54:10Z,http://arxiv.org/abs/2412.19260v1,"Asma Ben Abacha, Wen-wai Yim, Yujuan Fu, Zhaoyi Sun, Meliha Yetisgen, Fei Xia, Thomas Lin"
"Prospects for detecting the dark matter particles and primordial black
  holes with the Hongmeng mission using the 21 cm global spectrum at cosmic
  dawn","Dark matter is believed to account for a significant portion of the mass in
the universe, exerting a critical influence on the formation and evolution of
cosmic structures. This research delves into the processes of annihilation and
decay of dark matter particles, which generate observable signals that deepen
our comprehension of their characteristics and behaviors. Furthermore, the
study explores the potential role of primordial black holes, with a focus on
the emissions of Hawking radiation that could offer valuable insights into
their distribution and size range. A key aspect of this investigation revolves
around the 21 cm signal, a vital tool for scrutinizing the effects of dark
matter particles and primordial black hole phenomena on the intergalactic
medium. The upcoming Hongmeng mission, featuring a lunar orbital interferometer
array, is poised to revolutionize our ability to observe the 21 cm signal. By
conducting measurements devoid of atmospheric disturbances, the mission will
significantly boost sensitivity to subtle signals associated with dark matter
particle annihilation, decay, and primordial black hole emissions. This study
assesses the expected performance of the Hongmeng mission in detecting these
telltale signs and aims to unveil fresh insights into the nature and
interactions of dark matter particles and primordial black hole emissions
through a meticulous analysis of the global 21 cm spectrum. The mission holds
immense promise for reshaping our understanding of the universe's concealed
components.",2024-12-26T15:47:51Z,http://arxiv.org/abs/2412.19257v1,"Meng-Lin Zhao, Sai Wang, Xin Zhang"
Multi-matrix Factorization Attention,"We propose novel attention architectures, Multi-matrix Factorization
Attention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard
Multi-Head Attention (MHA), including SOTA methods like MLA, fail to maintain
as strong performance under stringent Key-Value cache (KV cache) constraints.
MFA enhances model capacity by efficiently scaling up both the number and
dimension of attention heads through low-rank matrix factorization in the
Query-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory
requirements by repurposing the key cache as value through value projection
re-parameterization. MFA's design enables strong model capacity when working
under tight KV cache budget, while MFA-KR is suitable for even harsher KV cache
limits with minor performance trade-off. Notably, in our extensive and
large-scale experiments, the proposed architecture outperforms MLA and performs
comparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,
respectively.",2024-12-26T15:45:45Z,http://arxiv.org/abs/2412.19255v1,"Jingcheng Hu, Houyi Li, Yinmin Zhang, Zili Wang, Shuigeng Zhou, Xiangyu Zhang, Heung-Yeung Shum"
"Leveraging Self-Training and Variational Autoencoder for Agitation
  Detection in People with Dementia Using Wearable Sensors","Dementia is a neurodegenerative disorder that has been growing among elder
people over the past decades. This growth profoundly impacts the quality of
life for patients and caregivers due to the symptoms arising from it. Agitation
and aggression (AA) are some of the symptoms of people with severe dementia
(PwD) in long-term care or hospitals. AA not only causes discomfort but also
puts the patients or others at potential risk. Existing monitoring solutions
utilizing different wearable sensors integrated with Artificial Intelligence
(AI) offer a way to detect AA early enough for timely and adequate medical
intervention. However, most studies are limited by the availability of
accurately labeled datasets, which significantly affects the efficacy of such
solutions in real-world scenarios. This study presents a novel comprehensive
approach to detect AA in PwD using physiological data from the Empatica E4
wristbands. The research creates a diverse dataset, consisting of three
distinct datasets gathered from 14 participants across multiple hospitals in
Canada. These datasets have not been extensively explored due to their limited
labeling. We propose a novel approach employing self-training and a variational
autoencoder (VAE) to detect AA in PwD effectively. The proposed approach aims
to learn the representation of the features extracted using the VAE and then
uses a semi-supervised block to generate labels, classify events, and detect
AA. We demonstrate that combining Self-Training and Variational Autoencoder
mechanism significantly improves model performance in classifying AA in PwD.
Among the tested techniques, the XGBoost classifier achieved the highest
accuracy of 90.16\%. By effectively addressing the challenge of limited labeled
data, the proposed system not only learns new labels but also proves its
superiority in detecting AA.",2024-12-26T15:34:25Z,http://arxiv.org/abs/2412.19254v1,"Abeer Badawi, Somayya Elmoghazy, Samira Choudhury, Khalid Elgazzar, Amer Burhan"
"Localized exploration in contextual dynamic pricing achieves
  dimension-free regret","We study the problem of contextual dynamic pricing with a linear demand
model. We propose a novel localized exploration-then-commit (LetC) algorithm
which starts with a pure exploration stage, followed by a refinement stage that
explores near the learned optimal pricing policy, and finally enters a pure
exploitation stage. The algorithm is shown to achieve a minimax optimal,
dimension-free regret bound when the time horizon exceeds a polynomial of the
covariate dimension. Furthermore, we provide a general theoretical framework
that encompasses the entire time spectrum, demonstrating how to balance
exploration and exploitation when the horizon is limited. The analysis is
powered by a novel critical inequality that depicts the
exploration-exploitation trade-off in dynamic pricing, mirroring its existing
counterpart for the bias-variance trade-off in regularized regression. Our
theoretical results are validated by extensive experiments on synthetic and
real-world data.",2024-12-26T15:29:58Z,http://arxiv.org/abs/2412.19252v1,"Jinhang Chai, Yaqi Duan, Jianqing Fan, Kaizheng Wang"
"Causal Speech Enhancement with Predicting Semantics based on Quantized
  Self-supervised Learning Features","Real-time speech enhancement (SE) is essential to online speech
communication. Causal SE models use only the previous context while predicting
future information, such as phoneme continuation, may help performing causal
SE. The phonetic information is often represented by quantizing latent features
of self-supervised learning (SSL) models. This work is the first to incorporate
SSL features with causality into an SE model. The causal SSL features are
encoded and combined with spectrogram features using feature-wise linear
modulation to estimate a mask for enhancing the noisy input speech.
Simultaneously, we quantize the causal SSL features using vector quantization
to represent phonetic characteristics as semantic tokens. The model not only
encodes SSL features but also predicts the future semantic tokens in multi-task
learning (MTL). The experimental results using VoiceBank + DEMAND dataset show
that our proposed method achieves 2.88 in PESQ, especially with semantic
prediction MTL, in which we confirm that the semantic prediction played an
important role in causal SE.",2024-12-26T15:08:36Z,http://arxiv.org/abs/2412.19248v1,"Emiru Tsunoo, Yuki Saito, Wataru Nakata, Hiroshi Saruwatari"
Sentiment trading with large language models,"We investigate the efficacy of large language models (LLMs) in sentiment
analysis of U.S. financial news and their potential in predicting stock market
returns. We analyze a dataset comprising 965,375 news articles that span from
January 1, 2010, to June 30, 2023; we focus on the performance of various LLMs,
including BERT, OPT, FINBERT, and the traditional Loughran-McDonald dictionary
model, which has been a dominant methodology in the finance literature. The
study documents a significant association between LLM scores and subsequent
daily stock returns. Specifically, OPT, which is a GPT-3 based LLM, shows the
highest accuracy in sentiment prediction with an accuracy of 74.4%, slightly
ahead of BERT (72.5%) and FINBERT (72.2%). In contrast, the Loughran-McDonald
dictionary model demonstrates considerably lower effectiveness with only 50.1%
accuracy. Regression analyses highlight a robust positive impact of OPT model
scores on next-day stock returns, with coefficients of 0.274 and 0.254 in
different model specifications. BERT and FINBERT also exhibit predictive
relevance, though to a lesser extent. Notably, we do not observe a significant
relationship between the Loughran-McDonald dictionary model scores and stock
returns, challenging the efficacy of this traditional method in the current
financial context. In portfolio performance, the long-short OPT strategy excels
with a Sharpe ratio of 3.05, compared to 2.11 for BERT and 2.07 for FINBERT
long-short strategies. Strategies based on the Loughran-McDonald dictionary
yield the lowest Sharpe ratio of 1.23. Our findings emphasize the superior
performance of advanced LLMs, especially OPT, in financial market prediction
and portfolio management, marking a significant shift in the landscape of
financial analysis tools with implications to financial regulation and policy
analysis.",2024-12-26T15:01:24Z,http://arxiv.org/abs/2412.19245v1,"Kemal Kirtac, Guido Germano"
"6Diffusion: IPv6 Target Generation Using a Diffusion Model with
  Global-Local Attention Mechanisms for Internet-wide IPv6 Scanning","Due to the vast address space of IPv6, the brute-force scanning methods
originally applicable to IPv4 are no longer suitable for proactive scanning of
IPv6. The recently proposed target generation algorithms have a low hit rate
for existing IPv6 target generation algorithms, primarily because they do not
accurately fit the distribution patterns of active IPv6 addresses. This paper
introduces a diffusion model-based IPv6 target generation algorithm called
6Diffusion. 6Diffusion first maps addresses to vector space for language
modeling, adds noise to active IPv6 addresses in the forward process, diffusing
them throughout the entire IPv6 address space, and then performs a reverse
process to gradually denoise and recover to active IPv6 addresses. We use the
DDIM sampler to increase the speed of generating candidate sets. At the same
time, we introduce the GLF-MSA (Global-Local Fusion Multi-Head Self-Attention)
mechanism to adapt to the top-down global allocation pattern of IPv6 addresses
and the local characteristics of IPv6 address segments, thus better learning
the deep-level features of active IPv6 addresses. Experimental results show
that compared to existing methods, 6Diffusion can generate higher quality
candidate sets and outperforms state-of-the-art target generation algorithms
across multiple metrics.",2024-12-26T14:57:58Z,http://arxiv.org/abs/2412.19243v1,"Nabo He, DanDan Li, Xiaohong Huang"
Functional structural equation modeling with latent variables,"Handling latent variables in Structural Equation Models (SEMs) in a case
where both the latent variables and their corresponding indicators in the
measurement error part of the model are random curves presents significant
challenges, especially with sparse data. In this paper, we develop a novel
family of Functional Structural Equation Models (FSEMs) that incorporate latent
variables modeled as Gaussian Processes (GPs). The introduced FSEMs are built
upon functional regression models having response variables modeled as
underlying GPs. The model flexibly adapts to cases when the random curves'
realizations are observed only over a sparse subset of the domain, and the
inferential framework is based on a restricted maximum likelihood approach. The
advantage of this framework lies in its ability and flexibility in handling
various data scenarios, including regularly and irregularly spaced points and
thus missing data. To extract smooth estimates for the functional parameters,
we employ a penalized likelihood approach that selects the smoothing parameters
using a cross-validation method. We evaluate the performance of the proposed
model using simulation studies and a real data example, which suggests that our
model performs well in practice. The uncertainty associated with the estimates
of the functional coefficients is also assessed by constructing confidence
regions for each estimate. The goodness of fit indices that are commonly used
to evaluate the fit of SEMs are developed for the FSEMs introduced in this
paper. Overall, the proposed method is a promising approach for modeling
functional data in SEMs with functional latent variables.",2024-12-26T14:57:14Z,http://arxiv.org/abs/2412.19242v1,"Fatemeh Asgari, Valeria Vitelli, Uta Sailer"
"Latenrgy: Model Agnostic Latency and Energy Consumption Prediction for
  Binary Classifiers","Machine learning systems increasingly drive innovation across scientific
fields and industry, yet challenges in compute overhead, specifically during
inference, limit their scalability and sustainability. Responsible AI
guardrails, essential for ensuring fairness, transparency, and privacy, further
exacerbate these computational demands. This study addresses critical gaps in
the literature, chiefly the lack of generalized predictive techniques for
latency and energy consumption, limited cross-comparisons of classifiers, and
unquantified impacts of RAI guardrails on inference performance. Using Theory
Construction Methodology, this work constructed a model-agnostic theoretical
framework for predicting latency and energy consumption in binary
classification models during inference. The framework synthesizes classifier
characteristics, dataset properties, and RAI guardrails into a unified
analytical instrument. Two predictive equations are derived that capture the
interplay between these factors while offering generalizability across diverse
classifiers. The proposed framework provides foundational insights for
designing efficient, responsible ML systems. It enables researchers to
benchmark and optimize inference performance and assists practitioners in
deploying scalable solutions. Finally, this work establishes a theoretical
foundation for balancing computational efficiency with ethical AI principles,
paving the way for future empirical validation and broader applications.",2024-12-26T14:51:24Z,http://arxiv.org/abs/2412.19241v1,Jason M. Pittman
FineVQ: Fine-Grained User Generated Content Video Quality Assessment,"The rapid growth of user-generated content (UGC) videos has produced an
urgent need for effective video quality assessment (VQA) algorithms to monitor
video quality and guide optimization and recommendation procedures. However,
current VQA models generally only give an overall rating for a UGC video, which
lacks fine-grained labels for serving video processing and recommendation
applications. To address the challenges and promote the development of UGC
videos, we establish the first large-scale Fine-grained Video quality
assessment Database, termed FineVD, which comprises 6104 UGC videos with
fine-grained quality scores and descriptions across multiple dimensions. Based
on this database, we propose a Fine-grained Video Quality assessment (FineVQ)
model to learn the fine-grained quality of UGC videos, with the capabilities of
quality rating, quality scoring, and quality attribution. Extensive
experimental results demonstrate that our proposed FineVQ can produce
fine-grained video-quality results and achieve state-of-the-art performance on
FineVD and other commonly used UGC-VQA datasets. Both Both FineVD and FineVQ
will be made publicly available.",2024-12-26T14:44:47Z,http://arxiv.org/abs/2412.19238v1,"Huiyu Duan, Qiang Hu, Jiarui Wang, Liu Yang, Zitong Xu, Lu Liu, Xiongkuo Min, Chunlei Cai, Tianxiao Ye, Xiaoyun Zhang, Guangtao Zhai"
SeaMo: A Multi-Seasonal and Multimodal Remote Sensing Foundation Model,"Remote Sensing (RS) data contains a wealth of multi-dimensional information
crucial for Earth observation. Owing to its vast volume, diverse sources, and
temporal properties, RS data is highly suitable for the development of large
Visual Foundation Models (VFMs). VFMs act as robust feature extractors,
learning from extensive RS data, and are subsequently fine-tuned for deployment
in various geoscientific tasks. However, current VFMs in the RS domain are
predominantly pretrained and tailored exclusively for specific characteristics
of RS imagery, neglecting the potential of utilizing the multi-dimensional
properties of RS data. Therefore, in this work, we propose SeaMo, a pioneering
visual foundation model that integrates multi-seasonal and multimodal
information in the RS field. SeaMo is designed to harness multiple properties
of RS data. Within the masked image modeling framework, we employ non-aligned
cropping techniques to extract spatial properties, use multi-source inputs for
multimodal integration, and incorporate temporal-multimodal fusion blocks for
effective assimilation of multi-seasonal data. SeaMo explicitly models the
multi-dimensional properties of RS data, making the model more comprehensive,
robust, and versatile. We applied SeaMo to several downstream geoscience tasks,
which demonstrated exceptional performance. Extensive ablation studies were
conducted to validate the model's superiority.",2024-12-26T14:40:38Z,http://arxiv.org/abs/2412.19237v1,"Xuyang Li, Danfeng Hong, Chenyu Li, Jocelyn Chanussot"
"Are Two Hidden Layers Still Enough for the Physics-Informed Neural
  Networks?","The article discusses the development of various methods and techniques for
initializing and training neural networks with a single hidden layer, as well
as training a separable physics-informed neural network consisting of neural
networks with a single hidden layer to solve physical problems described by
ordinary differential equations (ODEs) and partial differential equations
(PDEs). A method for strictly deterministic initialization of a neural network
with one hidden layer for solving physical problems described by an ODE is
proposed. Modifications to existing methods for weighting the loss function are
given, as well as new methods developed for training strictly
deterministic-initialized neural networks to solve ODEs (detaching, additional
weighting based on the second derivative, predicted solution-based weighting,
relative residuals). An algorithm for physics-informed data-driven
initialization of a neural network with one hidden layer is proposed. A neural
network with pronounced generalizing properties is presented, whose
generalizing abilities of which can be precisely controlled by adjusting
network parameters. A metric for measuring the generalization of such neural
network has been introduced. A gradient-free neuron-by-neuron fitting method
has been developed for adjusting the parameters of a single-hidden-layer neural
network, which does not require the use of an optimizer or solver for its
implementation. The proposed methods have been extended to 2D problems using
the separable physics-informed neural networks approach. Numerous experiments
have been carried out to develop the above methods and approaches. Experiments
on physical problems, such as solving various ODEs and PDEs, have demonstrated
that these methods for initializing and training neural networks with one or
two hidden layers (SPINN) achieve competitive accuracy and, in some cases,
state-of-the-art results.",2024-12-26T14:30:54Z,http://arxiv.org/abs/2412.19235v1,"Vasiliy A. Es'kin, Alexey O. Malkhanov, Mikhail E. Smorkalov"
"Virtual Nodes Can Help: Tackling Distribution Shifts in Federated Graph
  Learning","Federated Graph Learning (FGL) enables multiple clients to jointly train
powerful graph learning models, e.g., Graph Neural Networks (GNNs), without
sharing their local graph data for graph-related downstream tasks, such as
graph property prediction. In the real world, however, the graph data can
suffer from significant distribution shifts across clients as the clients may
collect their graph data for different purposes. In particular, graph
properties are usually associated with invariant label-relevant substructures
(i.e., subgraphs) across clients, while label-irrelevant substructures can
appear in a client-specific manner. The issue of distribution shifts of graph
data hinders the efficiency of GNN training and leads to serious performance
degradation in FGL. To tackle the aforementioned issue, we propose a novel FGL
framework entitled FedVN that eliminates distribution shifts through
client-specific graph augmentation strategies with multiple learnable Virtual
Nodes (VNs). Specifically, FedVN lets the clients jointly learn a set of shared
VNs while training a global GNN model. To eliminate distribution shifts, each
client trains a personalized edge generator that determines how the VNs connect
local graphs in a client-specific manner. Furthermore, we provide theoretical
analyses indicating that FedVN can eliminate distribution shifts of graph data
across clients. Comprehensive experiments on four datasets under five settings
demonstrate the superiority of our proposed FedVN over nine baselines.",2024-12-26T14:16:15Z,http://arxiv.org/abs/2412.19229v1,"Xingbo Fu, Zihan Chen, Yinhan He, Song Wang, Binchi Zhang, Chen Chen, Jundong Li"
"Learning Cross-Domain Representations for Transferable Drug
  Perturbations on Single-Cell Transcriptional Responses","Phenotypic drug discovery has attracted widespread attention because of its
potential to identify bioactive molecules. Transcriptomic profiling provides a
comprehensive reflection of phenotypic changes in cellular responses to
external perturbations. In this paper, we propose XTransferCDR, a novel
generative framework designed for feature decoupling and transferable
representation learning across domains. Given a pair of perturbed expression
profiles, our approach decouples the perturbation representations from basal
states through domain separation encoders and then cross-transfers them in the
latent space. The transferred representations are then used to reconstruct the
corresponding perturbed expression profiles via a shared decoder. This
cross-transfer constraint effectively promotes the learning of transferable
drug perturbation representations. We conducted extensive evaluations of our
model on multiple datasets, including single-cell transcriptional responses to
drugs and single- and combinatorial genetic perturbations. The experimental
results show that XTransferCDR achieved better performance than current
state-of-the-art methods, showcasing its potential to advance phenotypic drug
discovery.",2024-12-26T14:09:16Z,http://arxiv.org/abs/2412.19228v1,"Hui Liu, Shikai Jin"
Multi-view Fake News Detection Model Based on Dynamic Hypergraph,"With the rapid development of online social networks and the inadequacies in
content moderation mechanisms, the detection of fake news has emerged as a
pressing concern for the public. Various methods have been proposed for fake
news detection, including text-based approaches as well as a series of
graph-based approaches. However, the deceptive nature of fake news renders
text-based approaches less effective. Propagation tree-based methods focus on
the propagation process of individual news, capturing pairwise relationships
but lacking the capability to capture high-order complex relationships. Large
heterogeneous graph-based approaches necessitate the incorporation of
substantial additional information beyond news text and user data, while
hypergraph-based approaches rely on predefined hypergraph structures. To tackle
these issues, we propose a novel dynamic hypergraph-based multi-view fake news
detection model (DHy-MFND) that learns news embeddings across three distinct
views: text-level, propagation tree-level, and hypergraph-level. By employing
hypergraph structures to model complex high-order relationships among multiple
news pieces and introducing dynamic hypergraph structure learning, we optimize
predefined hypergraph structures while learning news embeddings. Additionally,
we introduce contrastive learning to capture authenticity-relevant embeddings
across different views. Extensive experiments on two benchmark datasets
demonstrate the effectiveness of our proposed DHy-MFND compared with a broad
range of competing baselines.",2024-12-26T14:05:51Z,http://arxiv.org/abs/2412.19227v1,"Rongping Ye, Xiaobing Pei"
"VINEVI: A Virtualized Network Vision Architecture for Smart Monitoring
  of Heterogeneous Applications and Infrastructures","Monitoring heterogeneous infrastructures and applications is essential to
cope with user requirements properly, but it still lacks enhancements. The
well-known state-of-the-art methods and tools do not support seamless
monitoring of bare-metal, low-cost infrastructures, neither hosted nor
virtualized services with fine-grained details. This work proposes VIrtualized
NEtwork VIsion architecture (VINEVI), an intelligent method for seamless
monitoring heterogeneous infrastructures and applications. The VINEVI
architecture advances state of the art with a node-embedded traffic
classification agent placing physical and virtualized infrastructures enabling
real-time traffic classification. VINEVI combines this real-time traffic
classification with well-known tools such as Prometheus and Victoria Metrics to
monitor the entire stack from the hardware to the virtualized applications.
Experimental results showcased that VINEVI architecture allowed seamless
heterogeneous infrastructure monitoring with a higher level of detail beyond
literature. Also, our node-embedded real-time Internet traffic classifier
evolved with flexibility the methods with monitoring heterogeneous
infrastructures seamlessly.",2024-12-26T14:05:14Z,http://arxiv.org/abs/2412.19226v1,"Rodrigo Moreira, Hugo G. V. O. da Cunha, Larissa F. Rodrigues Moreira, Flávio de Oliveira Silva"
"Completion as Enhancement: A Degradation-Aware Selective Image Guided
  Network for Depth Completion","In this paper, we introduce the Selective Image Guided Network (SigNet), a
novel degradation-aware framework that transforms depth completion into depth
enhancement for the first time. Moving beyond direct completion using
convolutional neural networks (CNNs), SigNet initially densifies sparse depth
data through non-CNN densification tools to obtain coarse yet dense depth. This
approach eliminates the mismatch and ambiguity caused by direct convolution
over irregularly sampled sparse data. Subsequently, SigNet redefines completion
as enhancement, establishing a self-supervised degradation bridge between the
coarse depth and the targeted dense depth for effective RGB-D fusion. To
achieve this, SigNet leverages the implicit degradation to adaptively select
high-frequency components (e.g., edges) of RGB data to compensate for the
coarse depth. This degradation is further integrated into a multi-modal
conditional Mamba, dynamically generating the state parameters to enable
efficient global high-frequency information interaction. We conduct extensive
experiments on the NYUv2, DIML, SUN RGBD, and TOFDC datasets, demonstrating the
state-of-the-art (SOTA) performance of SigNet.",2024-12-26T14:05:01Z,http://arxiv.org/abs/2412.19225v1,"Zhiqiang Yan, Zhengxue Wang, Kun Wang, Jun Li, Jian Yang"
"Transformer-Based Wireless Capsule Endoscopy Bleeding Tissue Detection
  and Classification","Informed by the success of the transformer model in various computer vision
tasks, we design an end-to-end trainable model for the automatic detection and
classification of bleeding and non-bleeding frames extracted from Wireless
Capsule Endoscopy (WCE) videos. Based on the DETR model, our model uses the
Resnet50 for feature extraction, the transformer encoder-decoder for bleeding
and non-bleeding region detection, and a feedforward neural network for
classification. Trained in an end-to-end approach on the Auto-WCEBleedGen
Version 1 challenge training set, our model performs both detection and
classification tasks as a single unit. Our model achieves an accuracy, recall,
and F1-score classification percentage score of 98.28, 96.79, and 98.37
respectively, on the Auto-WCEBleedGen version 1 validation set. Further, we
record an average precision (AP @ 0.5), mean-average precision (mAP) of 0.7447
and 0.7328 detection results. This earned us a 3rd place position in the
challenge. Our code is publicly available via
https://github.com/BasitAlawode/WCEBleedGen.",2024-12-26T13:49:39Z,http://arxiv.org/abs/2412.19218v1,"Basit Alawode, Shibani Hamza, Adarsh Ghimire, Divya Velayudhan"
"Applying the maximum entropy principle to multi-species neural networks
  improves species distribution models","The rapid expansion of citizen science initiatives has led to a significant
growth of biodiversity databases, and particularly presence-only (PO)
observations. PO data are invaluable for understanding species distributions
and their dynamics, but their use in Species Distribution Models (SDM) is
curtailed by sampling biases and the lack of information on absences. Poisson
point processes are widely used for SDMs, with Maxent being one of the most
popular methods. Maxent maximises the entropy of a probability distribution
across sites as a function of predefined transformations of environmental
variables, called features. In contrast, neural networks and deep learning have
emerged as a promising technique for automatic feature extraction from complex
input variables. In this paper, we propose DeepMaxent, which harnesses neural
networks to automatically learn shared features among species, using the
maximum entropy principle. To do so, it employs a normalised Poisson loss where
for each species, presence probabilities across sites are modelled by a neural
network. We evaluate DeepMaxent on a benchmark dataset known for its spatial
sampling biases, using PO data for calibration and presence-absence (PA) data
for validation across six regions with different biological groups and
environmental covariates. Our results indicate that DeepMaxent improves model
performance over Maxent and other state-of-the-art SDMs across regions and
taxonomic groups. The method performs particularly well in regions of uneven
sampling, demonstrating substantial potential to improve species distribution
modelling. The method opens the possibility to learn more robust environmental
features predicting jointly many species and scales to arbitrary large numbers
of sites without an increased memory demand.",2024-12-26T13:47:04Z,http://arxiv.org/abs/2412.19217v1,"Maxime Ryckewaert, Diego Marcos, Christophe Botella, Maximilien Servajean, Pierre Bonnet, Alexis Joly"
"On the convergence of continuous-time fictitious play in $3\times3$
  games via the geometrical approach","As the earliest and one of the most fundamental learning dynamics for
computing NE, fictitious play (FP) has being receiving incessant research
attention and finding games where FP would converge (games with FPP) is one
central question in related fields. In this paper, we identify a new class of
games with FPP, i.e., $3\times3$ games without IIP, based on the geometrical
approach by leveraging the location of NE and the partition of best response
region. During the process, we devise a new projection mapping to reduce a
high-dimensional dynamical system to a planar system. And to overcome the
non-smoothness of the systems, we redefine the concepts of saddle and sink NE,
which are proven to exist and help prove the convergence of CFP by separating
the projected space into two parts. Furthermore, we show that our projection
mapping can be extended to higher-dimensional and degenerate games.",2024-12-26T13:42:58Z,http://arxiv.org/abs/2412.19216v1,"Zhouming Wu, Yifen Mu, Xiaoguang Yang"
"Optimizing Fantasy Sports Team Selection with Deep Reinforcement
  Learning","Fantasy sports, particularly fantasy cricket, have garnered immense
popularity in India in recent years, offering enthusiasts the opportunity to
engage in strategic team-building and compete based on the real-world
performance of professional athletes. In this paper, we address the challenge
of optimizing fantasy cricket team selection using reinforcement learning (RL)
techniques. By framing the team creation process as a sequential
decision-making problem, we aim to develop a model that can adaptively select
players to maximize the team's potential performance. Our approach leverages
historical player data to train RL algorithms, which then predict future
performance and optimize team composition. This not only represents a huge
business opportunity by enabling more accurate predictions of high-performing
teams but also enhances the overall user experience. Through empirical
evaluation and comparison with traditional fantasy team drafting methods, we
demonstrate the effectiveness of RL in constructing competitive fantasy teams.
Our results show that RL-based strategies provide valuable insights into player
selection in fantasy sports.",2024-12-26T13:36:18Z,http://arxiv.org/abs/2412.19215v1,"Shamik Bhattacharjee, Kamlesh Marathe, Hitesh Kapoor, Nilesh Patil"
"Towards Better Spherical Sliced-Wasserstein Distance Learning with
  Data-Adaptive Discriminative Projection Direction","Spherical Sliced-Wasserstein (SSW) has recently been proposed to measure the
discrepancy between spherical data distributions in various fields, such as
geology, medical domains, computer vision, and deep representation learning.
However, in the original SSW, all projection directions are treated equally,
which is too idealistic and cannot accurately reflect the importance of
different projection directions for various data distributions. To address this
issue, we propose a novel data-adaptive Discriminative Spherical
Sliced-Wasserstein (DSSW) distance, which utilizes a projected energy function
to determine the discriminative projection direction for SSW. In our new DSSW,
we introduce two types of projected energy functions to generate the weights
for projection directions with complete theoretical guarantees. The first type
employs a non-parametric deterministic function that transforms the projected
Wasserstein distance into its corresponding weight in each projection
direction. This improves the performance of the original SSW distance with
negligible additional computational overhead. The second type utilizes a neural
network-induced function that learns the projection direction weight through a
parameterized neural network based on data projections. This further enhances
the performance of the original SSW distance with less extra computational
overhead. Finally, we evaluate the performance of our proposed DSSW by
comparing it with several state-of-the-art methods across a variety of machine
learning tasks, including gradient flows, density estimation on real earth
data, and self-supervised learning.",2024-12-26T13:23:37Z,http://arxiv.org/abs/2412.19212v1,"Hongliang Zhang, Shuo Chen, Lei Luo, Jian Yang"
"Large Language Models Meet Graph Neural Networks: A Perspective of Graph
  Mining","Graph mining is an important area in data mining and machine learning that
involves extracting valuable information from graph-structured data. In recent
years, significant progress has been made in this field through the development
of graph neural networks (GNNs). However, GNNs are still deficient in
generalizing to diverse graph data. Aiming to this issue, Large Language Models
(LLMs) could provide new solutions for graph mining tasks with their superior
semantic understanding. In this review, we systematically review the
combination and application techniques of LLMs and GNNs and present a novel
taxonomy for research in this interdisciplinary field, which involves three
main categories: GNN-driving-LLM, LLM-driving-GNN, and GNN-LLM-co-driving.
Within this framework, we reveal the capabilities of LLMs in enhancing graph
feature extraction as well as improving the effectiveness of downstream tasks
such as node classification, link prediction, and community detection. Although
LLMs have demonstrated their great potential in handling graph-structured data,
their high computational requirements and complexity remain challenges. Future
research needs to continue to explore how to efficiently fuse LLMs and GNNs to
achieve more powerful graph learning and reasoning capabilities and provide new
impetus for the development of graph mining techniques.",2024-12-26T13:21:09Z,http://arxiv.org/abs/2412.19211v1,"Yuxin You, Zhen Liu, Xiangchao Wen, Yongtao Zhang, Wei Ai"
Context-Aware Deep Learning for Multi Modal Depression Detection,"In this study, we focus on automated approaches to detect depression from
clinical interviews using multi-modal machine learning (ML). Our approach
differentiates from other successful ML methods such as context-aware analysis
through feature engineering and end-to-end deep neural networks for depression
detection utilizing the Distress Analysis Interview Corpus. We propose a novel
method that incorporates: (1) pre-trained Transformer combined with data
augmentation based on topic modelling for textual data; and (2) deep 1D
convolutional neural network (CNN) for acoustic feature modeling. The
simulation results demonstrate the effectiveness of the proposed method for
training multi-modal deep learning models. Our deep 1D CNN and Transformer
models achieved state-of-the-art performance for audio and text modalities
respectively. Combining them in a multi-modal framework also outperforms
state-of-the-art for the combined setting. Code available at
https://github.com/genandlam/multi-modal-depression-detection",2024-12-26T13:19:26Z,http://arxiv.org/abs/2412.19209v1,"Genevieve Lam, Huang Dongyan, Weisi Lin"
"Developing Explainable Machine Learning Model using Augmented Concept
  Activation Vector","Machine learning models use high dimensional feature spaces to map their
inputs to the corresponding class labels. However, these features often do not
have a one-to-one correspondence with physical concepts understandable by
humans, which hinders the ability to provide a meaningful explanation for the
decisions made by these models. We propose a method for measuring the
correlation between high-level concepts and the decisions made by a machine
learning model. Our method can isolate the impact of a given high-level concept
and accurately measure it quantitatively. Additionally, this study aims to
determine the prevalence of frequent patterns in machine learning models, which
often occur in imbalanced datasets. We have successfully applied the proposed
method to fundus images and managed to quantitatively measure the impact of
radiomic patterns on the model decisions.",2024-12-26T13:18:16Z,http://arxiv.org/abs/2412.19208v1,"Reza Hassanpour, Kasim Oztoprak, Niels Netten, Tony Busker, Mortaza S. Bargh, Sunil Choenni, Beyza Kizildag, Leyla Sena Kilinc"
NADER: Neural Architecture Design via Multi-Agent Collaboration,"Designing effective neural architectures poses a significant challenge in
deep learning. While Neural Architecture Search (NAS) automates the search for
optimal architectures, existing methods are often constrained by predetermined
search spaces and may miss critical neural architectures. In this paper, we
introduce NADER (Neural Architecture Design via multi-agEnt collaboRation), a
novel framework that formulates neural architecture design (NAD) as a LLM-based
multi-agent collaboration problem. NADER employs a team of specialized agents
to enhance a base architecture through iterative modification. Current
LLM-based NAD methods typically operate independently, lacking the ability to
learn from past experiences, which results in repeated mistakes and inefficient
exploration. To address this issue, we propose the Reflector, which effectively
learns from immediate feedback and long-term experiences. Additionally, unlike
previous LLM-based methods that use code to represent neural architectures, we
utilize a graph-based representation. This approach allows agents to focus on
design aspects without being distracted by coding. We demonstrate the
effectiveness of NADER in discovering high-performing architectures beyond
predetermined search spaces through extensive experiments on benchmark tasks,
showcasing its advantages over state-of-the-art methods. The codes will be
released soon.",2024-12-26T13:07:03Z,http://arxiv.org/abs/2412.19206v1,"Zekang Yang, Wang Zeng, Sheng Jin, Chen Qian, Ping Luo, Wentao Liu"
"GAIS: A Novel Approach to Instance Selection with Graph Attention
  Networks","Instance selection (IS) is a crucial technique in machine learning that aims
to reduce dataset size while maintaining model performance. This paper
introduces a novel method called Graph Attention-based Instance Selection
(GAIS), which leverages Graph Attention Networks (GATs) to identify the most
informative instances in a dataset. GAIS represents the data as a graph and
uses GATs to learn node representations, enabling it to capture complex
relationships between instances. The method processes data in chunks, applies
random masking and similarity thresholding during graph construction, and
selects instances based on confidence scores from the trained GAT model.
Experiments on 13 diverse datasets demonstrate that GAIS consistently
outperforms traditional IS methods in terms of effectiveness, achieving high
reduction rates (average 96\%) while maintaining or improving model
performance. Although GAIS exhibits slightly higher computational costs, its
superior performance in maintaining accuracy with significantly reduced
training data makes it a promising approach for graph-based data selection.",2024-12-26T12:51:14Z,http://arxiv.org/abs/2412.19201v1,"Zahiriddin Rustamov, Ayham Zaitouny, Rafat Damseh, Nazar Zaki"
"Personalized Dynamic Music Emotion Recognition with Dual-Scale
  Attention-Based Meta-Learning","Dynamic Music Emotion Recognition (DMER) aims to predict the emotion of
different moments in music, playing a crucial role in music information
retrieval. The existing DMER methods struggle to capture long-term dependencies
when dealing with sequence data, which limits their performance. Furthermore,
these methods often overlook the influence of individual differences on emotion
perception, even though everyone has their own personalized emotional
perception in the real world. Motivated by these issues, we explore more
effective sequence processing methods and introduce the Personalized DMER
(PDMER) problem, which requires models to predict emotions that align with
personalized perception. Specifically, we propose a Dual-Scale Attention-Based
Meta-Learning (DSAML) method. This method fuses features from a dual-scale
feature extractor and captures both short and long-term dependencies using a
dual-scale attention transformer, improving the performance in traditional
DMER. To achieve PDMER, we design a novel task construction strategy that
divides tasks by annotators. Samples in a task are annotated by the same
annotator, ensuring consistent perception. Leveraging this strategy alongside
meta-learning, DSAML can predict personalized perception of emotions with just
one personalized annotation sample. Our objective and subjective experiments
demonstrate that our method can achieve state-of-the-art performance in both
traditional DMER and PDMER.",2024-12-26T12:47:35Z,http://arxiv.org/abs/2412.19200v1,"Dengming Zhang, Weitao You, Ziheng Liu, Lingyun Sun, Pei Chen"
Multi-Attribute Constraint Satisfaction via Language Model Rewriting,"Obeying precise constraints on top of multiple external attributes is a
common computational problem underlying seemingly different domains, from
controlled text generation to protein engineering. Existing language model (LM)
controllability methods for multi-attribute constraint satisfaction often rely
on specialized architectures or gradient-based classifiers, limiting their
flexibility to work with arbitrary black-box evaluators and pretrained models.
Current general-purpose large language models, while capable, cannot achieve
fine-grained multi-attribute control over external attributes. Thus, we create
Multi-Attribute Constraint Satisfaction (MACS), a generalized method capable of
finetuning language models on any sequential domain to satisfy user-specified
constraints on multiple external real-value attributes. Our method trains LMs
as editors by sampling diverse multi-attribute edit pairs from an initial set
of paraphrased outputs. During inference, LM iteratively improves upon its
previous solution to satisfy constraints for all attributes by leveraging our
designed constraint satisfaction reward. We additionally experiment with
reward-weighted behavior cloning to further improve the constraint satisfaction
rate of LMs. To evaluate our approach, we present a new Fine-grained Constraint
Satisfaction (FineCS) benchmark, featuring two challenging tasks: (1) Text
Style Transfer, where the goal is to simultaneously modify the sentiment and
complexity of reviews, and (2) Protein Design, focusing on modulating
fluorescence and stability of Green Fluorescent Proteins (GFP). Our empirical
results show that MACS achieves the highest threshold satisfaction in both
FineCS tasks, outperforming strong domain-specific baselines. Our work opens
new avenues for generalized and real-value multi-attribute control, with
implications for diverse applications spanning NLP and bioinformatics.",2024-12-26T12:36:39Z,http://arxiv.org/abs/2412.19198v1,"Ashutosh Baheti, Debanjana Chakraborty, Faeze Brahman, Ronan Le Bras, Ximing Lu, Nouha Dziri, Yejin Choi, Mark Riedl, Maarten Sap"
Provably Efficient Exploration in Reward Machines with Low Regret,"We study reinforcement learning (RL) for decision processes with
non-Markovian reward, in which high-level knowledge of the task in the form of
reward machines is available to the learner. We consider probabilistic reward
machines with initially unknown dynamics, and investigate RL under the
average-reward criterion, where the learning performance is assessed through
the notion of regret. Our main algorithmic contribution is a model-based RL
algorithm for decision processes involving probabilistic reward machines that
is capable of exploiting the structure induced by such machines. We further
derive high-probability and non-asymptotic bounds on its regret and demonstrate
the gain in terms of regret over existing algorithms that could be applied, but
obliviously to the structure. We also present a regret lower bound for the
studied setting. To the best of our knowledge, the proposed algorithm
constitutes the first attempt to tailor and analyze regret specifically for RL
with probabilistic reward machines.",2024-12-26T12:25:04Z,http://arxiv.org/abs/2412.19194v1,"Hippolyte Bourel, Anders Jonsson, Odalric-Ambrym Maillard, Chenxiao Ma, Mohammad Sadegh Talebi"
"Biology Instructions: A Dataset and Benchmark for Multi-Omics Sequence
  Understanding Capability of Large Language Models","Large language models have already demonstrated their formidable capabilities
in general domains, ushering in a revolutionary transformation. However,
exploring and exploiting the extensive knowledge of these models to comprehend
multi-omics biology remains underexplored. To fill this research gap, we first
introduce Biology-Instructions, the first large-scale multi-omics biological
sequences-related instruction-tuning dataset including DNA, RNA, proteins, and
multi-molecules, designed to bridge the gap between large language models
(LLMs) and complex biological sequences-related tasks. This dataset can enhance
the versatility of LLMs by integrating diverse biological sequenced-based
prediction tasks with advanced reasoning capabilities, while maintaining
conversational fluency. Additionally, we reveal significant performance
limitations in even state-of-the-art LLMs on biological sequence-related
multi-omics tasks without specialized pre-training and instruction-tuning. We
further develop a strong baseline called ChatMultiOmics with a novel
three-stage training pipeline, demonstrating the powerful ability to understand
biology by using Biology-Instructions. Biology-Instructions and ChatMultiOmics
are publicly available and crucial resources for enabling more effective
integration of LLMs with multi-omics sequence analysis.",2024-12-26T12:12:23Z,http://arxiv.org/abs/2412.19191v1,"Haonan He, Yuchen Ren, Yining Tang, Ziyang Xu, Junxian Li, Minghao Yang, Di Zhang, Dong Yuan, Tao Chen, Shufei Zhang, Yuqiang Li, Nanqing Dong, Wanli Ouyang, Dongzhan Zhou, Peng Ye"
An End-to-End Depth-Based Pipeline for Selfie Image Rectification,"Portraits or selfie images taken from a close distance typically suffer from
perspective distortion. In this paper, we propose an end-to-end deep
learning-based rectification pipeline to mitigate the effects of perspective
distortion. We learn to predict the facial depth by training a deep CNN. The
estimated depth is utilized to adjust the camera-to-subject distance by moving
the camera farther, increasing the camera focal length, and reprojecting the 3D
image features to the new perspective. The reprojected features are then fed to
an inpainting module to fill in the missing pixels. We leverage a
differentiable renderer to enable end-to-end training of our depth estimation
and feature extraction nets to improve the rectified outputs. To boost the
results of the inpainting module, we incorporate an auxiliary module to predict
the horizontal movement of the camera which decreases the area that requires
hallucination of challenging face parts such as ears. Unlike previous works, we
process the full-frame input image at once without cropping the subject's face
and processing it separately from the rest of the body, eliminating the need
for complex post-processing steps to attach the face back to the subject's
body. To train our network, we utilize the popular game engine Unreal Engine to
generate a large synthetic face dataset containing various subjects, head
poses, expressions, eyewear, clothes, and lighting. Quantitative and
qualitative results show that our rectification pipeline outperforms previous
methods, and produces comparable results with a time-consuming 3D GAN-based
method while being more than 260 times faster.",2024-12-26T11:57:54Z,http://arxiv.org/abs/2412.19189v1,"Ahmed Alhawwary, Phong Nguyen-Ha, Janne Mustaniemi, Janne Heikkilä"
New Physics in the 3-3-1 models,"Two main ingredients of current particle physics
  such as local gauge symmetry and mass generation via the Higgs mechanism
being basic ground of the Standard Model are widely confirmed by
  experimental data. However, some problems such as neutrino masses, dark
matter, baryon asymmetry of Universe have clearly indicated that the Standard
Model cannot be the ultimate theory of nature. To surpass the mentioned
puzzles,
  many extensions of the Standard Model (called beyond Standard Model) have
been proposed. Among beyond Standard Models, the 3-3-1 models have some
intriguing features and they get wide attention. The pioneer models develop in
some directions. In this paper, %some new main versions of the 3-3-1 models and
their consequences are presented.",2024-12-26T11:55:43Z,http://arxiv.org/abs/2412.19188v1,H. N. Long
"Multi-Head Attention Driven Dynamic Visual-Semantic Embedding for
  Enhanced Image-Text Matching","With the rapid development of multimodal learning, the image-text matching
task, as a bridge connecting vision and language, has become increasingly
important. Based on existing research, this study proposes an innovative visual
semantic embedding model, Multi-Headed Consensus-Aware Visual-Semantic
Embedding (MH-CVSE). This model introduces a multi-head self-attention
mechanism based on the consensus-aware visual semantic embedding model (CVSE)
to capture information in multiple subspaces in parallel, significantly
enhancing the model's ability to understand and represent the complex
relationship between images and texts. In addition, we adopt a parameterized
feature fusion strategy to flexibly integrate feature information at different
levels, further improving the model's expressive power. In terms of loss
function design, the MH-CVSE model adopts a dynamic weight adjustment strategy
to dynamically adjust the weight according to the loss value itself, so that
the model can better balance the contribution of different loss terms during
training. At the same time, we introduce a cosine annealing learning rate
strategy to help the model converge more stably in the later stages of
training. Extensive experimental verification on the Flickr30k dataset shows
that the MH-CVSE model achieves better performance than previous methods in
both bidirectional image and text retrieval tasks, fully demonstrating its
effectiveness and superiority.",2024-12-26T11:46:22Z,http://arxiv.org/abs/2412.19184v1,Wenjing Chen
"Unraveling the magnetic and electronic complexity of intermetallic
  ErPd$_2$Si$_2$: Anisotropic thermal expansion, phase transitions, and twofold
  magnetotransport behavior","We present a comprehensive investigation into the physical properties of
intermetallic ErPd$_2$Si$_2$, a compound renowned for its intriguing magnetic
and electronic characteristics. We confirm the tetragonal crystal structure of
ErPd$_2$Si$_2$ within the $I4/mmm$ space group. Notably, we observed
anisotropic thermal expansion, with the lattice constant $a$ expanding and $c$
contracting between 15 K and 300 K. This behavior is attributed to lattice
vibrations and electronic contributions. Heat capacity measurements revealed
three distinct temperature regimes: $T_1 \sim 3.0$ K, $T_\textrm{N} \sim 4.20$
K, and $T_2 \sim 15.31$ K. These correspond to the disappearance of
spin-density waves, the onset of an incommensurate antiferromagnetic (AFM)
structure, and the crystal-field splitting and/or the presence of short-range
spin fluctuations, respectively. Remarkably, the AFM phase transition anomaly
was observed exclusively in low-field magnetization data (120 Oe) at
$T_\textrm{N}$. A high magnetic field ($B =$ 3 T) effectively suppressed this
anomaly, likely due to spin-flop and spin-flip transitions. Furthermore, the
extracted effective PM moments closely matched the expected theoretical value,
suggesting a dominant magnetic contribution from localized 4$f$ spins of Er.
Additionally, significant differences in resistance ($R$) values at low
temperatures under applied $B$ indicated a magnetoresistance (MR) effect with a
minimum value of -4.36\%. Notably, the measured MR effect exhibited anisotropic
behavior, where changes in the strength or direction of the applied $B$ induced
variations in the MR effect. A twofold symmetry of $R$ was discerned at 3 T and
9 T, originating from the orientation of spin moments relative to the applied
$B$. Intriguingly, above $T_\textrm{N}$, short-range spin fluctuations also
displayed a preferred orientation along the $c$-axis due to single-ion
anisotropy.",2024-12-26T11:39:24Z,http://arxiv.org/abs/2412.19181v1,"Kaitong Sun, Si Wu, Guanping Xu, Lingwei Li, Hongyu Chen, Qian Zhao, Muqing Su, Wolfgang Schmidt, Chongde Cao, Hai-Feng Li"
"Mask Approximation Net: Merging Feature Extraction and Distribution
  Learning for Remote Sensing Change Captioning","Remote sensing image change description, as a novel multimodal task in the
field of remote sensing processing, not only enables the detection of changes
in surface conditions but also provides detailed descriptions of these changes,
thereby enhancing human interpretability and interactivity. However, previous
methods mainly employed Convolutional Neural Network (CNN) architectures to
extract bitemporal image features. This approach often leads to an overemphasis
on designing specific network architectures and limits the captured feature
distributions to the current dataset, resulting in poor generalizability and
robustness when applied to other datasets or real-world scenarios. To address
these limitations, this paper proposes a novel approach for remote sensing
image change detection and description that integrates diffusion models, aiming
to shift the focus from conventional feature learning paradigms to data
distribution learning. The proposed method primarily includes a simple
multi-scale change detection module, whose output features are subsequently
refined using a diffusion model. Additionally, we introduce a frequency-guided
complex filter module to handle high-frequency noise during the diffusion
process, which helps to maintain model performance. Finally, we validate the
effectiveness of our proposed method on several remote sensing change detection
description datasets, demonstrating its superior performance. The code
available at MaskApproxNet.",2024-12-26T11:35:57Z,http://arxiv.org/abs/2412.19179v1,"Dongwei Sun, Xiangyong Cao"
"Reversed in Time: A Novel Temporal-Emphasized Benchmark for Cross-Modal
  Video-Text Retrieval","Cross-modal (e.g. image-text, video-text) retrieval is an important task in
information retrieval and multimodal vision-language understanding field.
Temporal understanding makes video-text retrieval more challenging than
image-text retrieval. However, we find that the widely used video-text
benchmarks have shortcomings in comprehensively assessing abilities of models,
especially in temporal understanding, causing large-scale image-text
pre-trained models can already achieve comparable zero-shot performance with
video-text pre-trained models. In this paper, we introduce RTime, a novel
temporal-emphasized video-text retrieval dataset. We first obtain videos of
actions or events with significant temporality, and then reverse these videos
to create harder negative samples. We then recruit annotators to judge the
significance and reversibility of candidate videos, and write captions for
qualified videos. We further adopt GPT-4 to extend more captions based on
human-written captions. Our RTime dataset currently consists of 21k videos with
10 captions per video, totalling about 122 hours. Based on RTime, we propose
three retrieval benchmark tasks: RTime-Origin, RTime-Hard, and RTime-Binary. We
further enhance the use of harder-negatives in model training, and benchmark a
variety of video-text models on RTime. Extensive experiment analysis proves
that RTime indeed poses new and higher challenges to video-text retrieval. We
release our RTime
dataset\footnote{\url{https://github.com/qyr0403/Reversed-in-Time}} to further
advance video-text retrieval and multimodal understanding research.",2024-12-26T11:32:00Z,http://arxiv.org/abs/2412.19178v1,"Yang Du, Yuqi Liu, Qin Jin"
Physical nature of quasi-stable structures existing in antimony melt,"Equilibrium antimony melt near the melting temperature is characterised by
structural features that are not present in simple single-component liquids.
The cause of these features may be long-lived structural formations that are
not yet fully understood. The present work provides the detailed
characterization of the structures formed in liquid antimony near the melting
temperature based on the results of quantum chemical calculations and the
available neutron and X-ray diffraction data. The quasi-stable structures in
antimony melt are detected with lifetimes exceeding the structural relaxation
time of this melt. These structures are characterised by a low degree of order
and spatial localisation. It is shown for the first time that the elementary
units of these quasi-stable structures are triplets of atoms with
characteristic lengths of $3.07$\,\AA~and $4.7$\,\AA~and characteristic angles
of $45$ and $90$ degrees. It was found that these triplets can form chains and
percolating clusters up to $\sim15$\,\AA~in length. The characteristic lengths
of these triplets are fully consistent with the correlation lengths associated
with short-range order in the antimony melt as determined by diffraction
experiments.",2024-12-26T11:30:38Z,http://arxiv.org/abs/2412.19177v1,"Artem A. Tsygankov, Bulat N. Galimzyanov, Anatolii V. Mokshin"
"Towards Popularity-Aware Recommendation: A Multi-Behavior Enhanced
  Framework with Orthogonality Constraint","Top-$K$ recommendation involves inferring latent user preferences and
generating personalized recommendations accordingly, which is now ubiquitous in
various decision systems. Nonetheless, recommender systems usually suffer from
severe \textit{popularity bias}, leading to the over-recommendation of popular
items. Such a bias deviates from the central aim of reflecting user preference
faithfully, compromising both customer satisfaction and retailer profits.
Despite the prevalence, existing methods tackling popularity bias still have
limitations due to the considerable accuracy-debias tradeoff and the
sensitivity to extensive parameter selection, further exacerbated by the
extreme sparsity in positive user-item interactions.
  In this paper, we present a \textbf{Pop}ularity-aware top-$K$ recommendation
algorithm integrating multi-behavior \textbf{S}ide \textbf{I}nformation
(PopSI), aiming to enhance recommendation accuracy and debias performance
simultaneously. Specifically, by leveraging multiple user feedback that mirrors
similar user preferences and formulating it as a three-dimensional tensor,
PopSI can utilize all slices to capture the desiring user preferences
effectively. Subsequently, we introduced a novel orthogonality constraint to
refine the estimated item feature space, enforcing it to be invariant to item
popularity features thereby addressing our model's sensitivity to popularity
bias. Comprehensive experiments on real-world e-commerce datasets demonstrate
the general improvements of PopSI over state-of-the-art debias methods with a
marginal accuracy-debias tradeoff and scalability to practical applications.
The source code for our algorithm and experiments is available at
\url{https://github.com/Eason-sys/PopSI}.",2024-12-26T11:06:49Z,http://arxiv.org/abs/2412.19172v1,"Yishan Han, Biao Xu, Yao Wang, Shanxing Gao"
"High-Precision Schottky Diagnostics for Low-SNR Betatron Tune
  Measurement in Ramping Synchrotrons","This paper presents a novel Schottky diagnostics-based method for real-time
betatron tune measurement in ramping synchrotrons, exemplified by the Shanghai
Advanced Proton Therapy (SAPT) facility. The proposed approach achieves high
precision under challenging conditions, including low frequency resolution and
signal-to-noise ratios (SNR) as low as -15 dB within the bandwidth of a
narrowband detector. By employing Short-Time Fourier Transform (STFT) analysis
with automatically optimized time windows, the method effectively addresses the
rapid increase in revolution frequency from 4 MHz to 7.5 MHz over 0.35 seconds,
assuming constant beam properties within each window. Monte Carlo
macro-particle simulations are employed to generate Schottky signals, which are
subsequently combined with real noise collected from an analog-to-digital
converter to emulate practical conditions. The betatron tune measurement
procedure integrates longitudinal signal exclusion, spectrum smoothing, and
spectral multiplication to reliably extract transverse Schottky spectra buried
in noise, to enable precise betatron tune determination. Experimental results
demonstrate that the proposed method surpasses existing approaches in
precision, accuracy, and robustness, while meeting stringent design
requirements. This innovative approach addresses key limitations of Schottky
diagnostics for betatron tune measurement in ramping synchrotrons, providing a
foundation for applications such as proton therapy.",2024-12-26T11:05:21Z,http://arxiv.org/abs/2412.19171v1,"Peihan Sun, Manzhou Zhang, Renxian Yuan, Deming Li, Jian Dong, Ying Shi"
"Accelerating Stochastic Gravitational Wave Backgrounds Parameter
  Estimation in Pulsar Timing Arrays with Flow Matching","Pulsar timing arrays (PTAs) are essential tools for detecting the stochastic
gravitational wave background (SGWB), but their analysis faces significant
computational challenges. Traditional methods like Markov-chain Monte Carlo
(MCMC) struggle with high-dimensional parameter spaces where noise parameters
often dominate, while existing deep learning approaches fail to model the
Hellings-Downs (HD) correlation or are validated only on synthetic datasets. We
propose a flow-matching-based continuous normalizing flow (CNF) for efficient
and accurate PTA parameter estimation. By focusing on the 10 most contributive
pulsars from the NANOGrav 15-year dataset, our method achieves posteriors
consistent with MCMC, with a Jensen-Shannon divergence below \(10^{-2}\) nat,
while reducing sampling time from 50 hours to 4 minutes. Powered by a versatile
embedding network and a reweighting loss function, our approach prioritizes the
SGWB parameters and scales effectively for future datasets. It enables precise
reconstruction of SGWB and opens new avenues for exploring vast observational
data and uncovering potential new physics, offering a transformative tool for
advancing gravitational wave astronomy.",2024-12-26T11:02:11Z,http://arxiv.org/abs/2412.19169v1,"Bo Liang, Chang Liu, Tianyu Zhao, Minghui Du, Manjia Liang, Ruijun Shi, Hong Guo, Yuxiang Xu, Li-e Qiang, Peng Xu, Wei-Liang Qian, Ziren Luo"
GFG -- Gender-Fair Generation: A CALAMITA Challenge,"Gender-fair language aims at promoting gender equality by using terms and
expressions that include all identities and avoid reinforcing gender
stereotypes. Implementing gender-fair strategies is particularly challenging in
heavily gender-marked languages, such as Italian. To address this, the
Gender-Fair Generation challenge intends to help shift toward gender-fair
language in written communication. The challenge, designed to assess and
monitor the recognition and generation of gender-fair language in both mono-
and cross-lingual scenarios, includes three tasks: (1) the detection of
gendered expressions in Italian sentences, (2) the reformulation of gendered
expressions into gender-fair alternatives, and (3) the generation of
gender-fair language in automatic translation from English to Italian. The
challenge relies on three different annotated datasets: the GFL-it corpus,
which contains Italian texts extracted from administrative documents provided
by the University of Brescia; GeNTE, a bilingual test set for gender-neutral
rewriting and translation built upon a subset of the Europarl dataset; and
Neo-GATE, a bilingual test set designed to assess the use of non-binary
neomorphemes in Italian for both fair formulation and translation tasks.
Finally, each task is evaluated with specific metrics: average of F1-score
obtained by means of BERTScore computed on each entry of the datasets for task
1, an accuracy measured with a gender-neutral classifier, and a
coverage-weighted accuracy for tasks 2 and 3.",2024-12-26T10:58:40Z,http://arxiv.org/abs/2412.19168v1,"Simona Frenda, Andrea Piergentili, Beatrice Savoldi, Marco Madeddu, Martina Rosola, Silvia Casola, Chiara Ferrando, Viviana Patti, Matteo Negri, Luisa Bentivogli"
"Accelerating global search of adsorbate molecule position using
  machine-learning interatomic potentials with active learning","We present an algorithm for accelerating the search of molecule's adsorption
site based on global optimization of surface adsorbate geometries. Our approach
uses a machine-learning interatomic potential (moment tensor potential) to
approximate the potential energy surface and an active learning algorithm for
the automatic construction of an optimal training dataset. To validate our
methodology, we compare the results across various well-known catalytic systems
with surfaces of different crystallographic orientations and adsorbate
geometries, including CO/Pd(111), NO/Pd(100), NH$_3$/Cu(100),
C$_6$H$_6$/Ag(111), and CH$_2$CO/Rh(211). In the all cases, we observed an
agreement of our results with the literature.",2024-12-26T10:44:55Z,http://arxiv.org/abs/2412.19162v1,"Olga Klimanova, Nikita Rybin, Alexander Shapeev"
"Dual Channel Multi-Attention in ViT for Biometric Authentication using
  Forehead Subcutaneous Vein Pattern and Periocular Pattern","Traditional biometric systems, like face and fingerprint recognition, have
encountered significant setbacks due to wearing face masks and hygiene
concerns. To meet the challenges of the partially covered face due to face
masks and hygiene concerns of fingerprint recognition, this paper proposes a
novel dual-channel multi-attention Vision Transformer (ViT) framework for
biometric authentication using forehead subcutaneous vein patterns and
periocular patterns, offering a promising alternative to traditional methods,
capable of performing well even with face masks and without any physical touch.
The proposed framework leverages a dual-channel ViT architecture, designed to
handle two distinct biometric traits. It can capture long-range dependencies of
independent features from the vein and periocular patterns. A custom classifier
is then designed to integrate the independently extracted features, producing a
final class prediction. The performance of the proposed algorithm was
rigorously evaluated using the Forehead Subcutaneous Vein Pattern and
Periocular Biometric Pattern (FSVP-PBP) database. The results demonstrated the
superiority of the algorithm over state-of-the-art methods, achieving
remarkable classification accuracy of $99.3 \pm 0.02\%$ with the combined vein
and periocular patterns.",2024-12-26T10:40:15Z,http://arxiv.org/abs/2412.19160v1,"Arun K. Sharma, Shubhobrata Bhattacharya, Motahar Reza"
"Mobile Robots through Task-Based Human Instructions using Incremental
  Curriculum Learning","This paper explores the integration of incremental curriculum learning (ICL)
with deep reinforcement learning (DRL) techniques to facilitate mobile robot
navigation through task-based human instruction. By adopting a curriculum that
mirrors the progressive complexity encountered in human learning, our approach
systematically enhances robots' ability to interpret and execute complex
instructions over time. We explore the principles of DRL and its synergy with
ICL, demonstrating how this combination not only improves training efficiency
but also equips mobile robots with the generalization capability required for
navigating through dynamic indoor environments. Empirical results indicate that
robots trained with our ICL-enhanced DRL framework outperform those trained
without curriculum learning, highlighting the benefits of structured learning
progressions in robotic training.",2024-12-26T10:38:40Z,http://arxiv.org/abs/2412.19159v1,"Muhammad A. Muttaqien, Ayanori Yorozu, Akihisa Ohya"
"Referencing Where to Focus: Improving VisualGrounding with Referential
  Query","Visual Grounding aims to localize the referring object in an image given a
natural language expression. Recent advancements in DETR-based visual grounding
methods have attracted considerable attention, as they directly predict the
coordinates of the target object without relying on additional efforts, such as
pre-generated proposal candidates or pre-defined anchor boxes. However,
existing research primarily focuses on designing stronger multi-modal decoder,
which typically generates learnable queries by random initialization or by
using linguistic embeddings. This vanilla query generation approach inevitably
increases the learning difficulty for the model, as it does not involve any
target-related information at the beginning of decoding. Furthermore, they only
use the deepest image feature during the query learning process, overlooking
the importance of features from other levels. To address these issues, we
propose a novel approach, called RefFormer. It consists of the query adaption
module that can be seamlessly integrated into CLIP and generate the referential
query to provide the prior context for decoder, along with a task-specific
decoder. By incorporating the referential query into the decoder, we can
effectively mitigate the learning difficulty of the decoder, and accurately
concentrate on the target object. Additionally, our proposed query adaption
module can also act as an adapter, preserving the rich knowledge within CLIP
without the need to tune the parameters of the backbone network. Extensive
experiments demonstrate the effectiveness and efficiency of our proposed
method, outperforming state-of-the-art approaches on five visual grounding
benchmarks.",2024-12-26T10:19:20Z,http://arxiv.org/abs/2412.19155v1,"Yabing Wang, Zhuotao Tian, Qingpei Guo, Zheng Qin, Sanping Zhou, Ming Yang, Le Wang"
"To Predict or Not To Predict? Proportionally Masked Autoencoders for
  Tabular Data Imputation","Masked autoencoders (MAEs) have recently demonstrated effectiveness in
tabular data imputation. However, due to the inherent heterogeneity of tabular
data, the uniform random masking strategy commonly used in MAEs can disrupt the
distribution of missingness, leading to suboptimal performance. To address
this, we propose a proportional masking strategy for MAEs. Specifically, we
first compute the statistics of missingness based on the observed proportions
in the dataset, and then generate masks that align with these statistics,
ensuring that the distribution of missingness is preserved after masking.
Furthermore, we argue that simple MLP-based token mixing offers competitive or
often superior performance compared to attention mechanisms while being more
computationally efficient, especially in the tabular domain with the inherent
heterogeneity. Experimental results validate the effectiveness of the proposed
proportional masking strategy across various missing data patterns in tabular
datasets. Code is available at: \url{https://github.com/normal-kim/PMAE}.",2024-12-26T10:12:08Z,http://arxiv.org/abs/2412.19152v1,"Jungkyu Kim, Kibok Lee, Taeyoung Park"
Generating Editable Head Avatars with 3D Gaussian GANs,"Generating animatable and editable 3D head avatars is essential for various
applications in computer vision and graphics. Traditional 3D-aware generative
adversarial networks (GANs), often using implicit fields like Neural Radiance
Fields (NeRF), achieve photorealistic and view-consistent 3D head synthesis.
However, these methods face limitations in deformation flexibility and
editability, hindering the creation of lifelike and easily modifiable 3D heads.
We propose a novel approach that enhances the editability and animation control
of 3D head avatars by incorporating 3D Gaussian Splatting (3DGS) as an explicit
3D representation. This method enables easier illumination control and improved
editability. Central to our approach is the Editable Gaussian Head (EG-Head)
model, which combines a 3D Morphable Model (3DMM) with texture maps, allowing
precise expression control and flexible texture editing for accurate animation
while preserving identity. To capture complex non-facial geometries like hair,
we use an auxiliary set of 3DGS and tri-plane features. Extensive experiments
demonstrate that our approach delivers high-quality 3D-aware synthesis with
state-of-the-art controllability. Our code and models are available at
https://github.com/liguohao96/EGG3D.",2024-12-26T10:10:03Z,http://arxiv.org/abs/2412.19149v1,"Guohao Li, Hongyu Yang, Yifang Men, Di Huang, Weixin Li, Ruijie Yang, Yunhong Wang"
AskChart: Universal Chart Understanding through Textual Enhancement,"Chart understanding tasks such as ChartQA and Chart-to-Text involve
automatically extracting and interpreting key information from charts, enabling
users to query or convert visual data into structured formats. State-of-the-art
approaches primarily focus on visual cues from chart images, failing to
explicitly incorporate rich textual information (e.g., data labels and axis
labels) embedded within the charts. This textual information is vital for
intuitive human comprehension and interpretation of charts. Moreover, existing
models are often large and computationally intensive, limiting their practical
applicability. In this paper, we introduce AskChart, a universal model that
explicitly integrates both textual and visual cues from charts using a Mixture
of Experts (MoE) architecture. AskChart facilitates the learning of enhanced
visual-textual representations of charts for effectively handling multiple
chart understanding tasks, while maintaining a smaller model size. To capture
the synergy between visual and textual modalities, we curate a large-scale
dataset named ChartBank with about 7.5M data samples, which helps align textual
and visual information and facilitates the extraction of visual entities and
text. To effectively train AskChart, we design a three-stage training strategy
to align visual and textual modalities for learning robust visual-textual
representations and optimizing the learning of the MoE layer. Extensive
experiments across five datasets demonstrate the significant performance gains
of AskChart in four chart understanding tasks. Remarkably, AskChart with 4.6B
parameters outperforms state-of-the-art models with 13B parameters by 68.3% in
Open-ended ChartQA and 49.2% in Chart-to-Text tasks, while achieving comparable
performance in ChartQA and Chart-to-Table tasks.",2024-12-26T09:59:43Z,http://arxiv.org/abs/2412.19146v1,"Xudong Yang, Yifan Wu, Yizhang Zhu, Nan Tang, Yuyu Luo"
"Impact of color and mixing proportion of synthetic point clouds on
  semantic segmentation","Semantic segmentation of point clouds is essential for understanding the
built environment, and a large amount of high-quality data is required for
training deep learning models. Despite synthetic point clouds (SPC) having the
potential to compensate for the shortage of real data, how to exploit the
benefits of SPC is still open. Therefore, this study systematically
investigates how color and mixing proportion of SPC impact semantic
segmentation for the first time. First, a new method to mimic the scanning
process and generate SPC based on BIM is proposed, to create a synthetic
dataset with consistent colors of BIM (UniSPC) and a synthetic dataset with
real colors (RealSPC) respectively. Subsequently, by integrating with the S3DIS
dataset, further experiments on PointNet, PointNet++, and DGCNN are conducted.
Meanwhile, benchmark experiments and new evaluation metrics are introduced to
better evaluate the performance of different models. Experiments show that
synthetic color significantly impacts model performance, the performance for
common components of the models trained with pure RealSPC is comparable to
models with real data, and RealSPC contributes average improvements of 14.1% on
overall accuracy and 7.3% on mIoU than UniSPC. Furthermore, the proportion of
SPC also has a significant impact on the performance. In mixing training
experiments, adding more than 70% SPC achieves an average of 3.9% on overall
accuracy and 3.4% on mIoU better than benchmark on three models. It is also
revealed that for large flat elements such as floors, ceilings, and walls, the
SPC can even replace real point clouds without compromising model performance.",2024-12-26T09:58:04Z,http://arxiv.org/abs/2412.19145v1,"Shaojie Zhou, Jia-Rui Lin, Peng Pan, Yuandong Pan, Ioannis Brilakis"
LibAFL-DiFuzz: Advanced Architecture Enabling Directed Fuzzing,"Directed fuzzing performs best for targeted program testing via estimating
the impact of each input in reaching predefined program points. But due to
insufficient analysis of the program structure and lack of flexibility and
configurability it can lose efficiency. In this paper, we enhance directed
fuzzing with context weights for graph nodes and resolve indirect edges during
call graph construction. We construct flexible tool for directed fuzzing with
components able to be easily combined with other techniques. We implement
proposed method in three separate modules: DiFuzzLLVM library for graph
construction and indirect calls resolving, DiFuzz static analysis tool for
processing program graphs and computing proximity metrics, and LibAFL-DiFuzz
directed fuzzer based on LibAFL fuzzing library. We create additional LibAFL
modules for enabling custom power scheduling and static instrumentation. We
evaluate indirect calls resolving and get increase in directed fuzzing
efficiency for reaching deeper target points. We evaluate context weights
contribution and get benefits in TTE and scheduling iterations number. We
evaluate our fuzzer in comparison with AFLGo and BEACON, and reveal speedup in
time to exposure on several benchmarks. Furthermore, our tool implements some
important usability features that are not available in mentioned tools: target
points detection, multiple target points support, etc.",2024-12-26T09:54:57Z,http://arxiv.org/abs/2412.19143v1,"Darya Parygina, Timofey Mezhuev, Daniil Kuts"
"CLIP-GS: Unifying Vision-Language Representation with 3D Gaussian
  Splatting","Recent works in 3D multimodal learning have made remarkable progress.
However, typically 3D multimodal models are only capable of handling point
clouds. Compared to the emerging 3D representation technique, 3D Gaussian
Splatting (3DGS), the spatially sparse point cloud cannot depict the texture
information of 3D objects, resulting in inferior reconstruction capabilities.
This limitation constrains the potential of point cloud-based 3D multimodal
representation learning. In this paper, we present CLIP-GS, a novel multimodal
representation learning framework grounded in 3DGS. We introduce the GS
Tokenizer to generate serialized gaussian tokens, which are then processed
through transformer layers pre-initialized with weights from point cloud
models, resulting in the 3DGS embeddings. CLIP-GS leverages contrastive loss
between 3DGS and the visual-text embeddings of CLIP, and we introduce an image
voting loss to guide the directionality and convergence of gradient
optimization. Furthermore, we develop an efficient way to generate triplets of
3DGS, images, and text, facilitating CLIP-GS in learning unified multimodal
representations. Leveraging the well-aligned multimodal representations,
CLIP-GS demonstrates versatility and outperforms point cloud-based models on
various 3D tasks, including multimodal retrieval, zero-shot, and few-shot
classification.",2024-12-26T09:54:25Z,http://arxiv.org/abs/2412.19142v1,"Siyu Jiao, Haoye Dong, Yuyang Yin, Zequn Jie, Yinlong Qian, Yao Zhao, Humphrey Shi, Yunchao Wei"
"How Panel Layouts Define Manga: Insights from Visual Ablation
  Experiments","Today, manga has gained worldwide popularity. However, the question of how
various elements of manga, such as characters, text, and panel layouts, reflect
the uniqueness of a particular work, or even define it, remains an unexplored
area. In this paper, we aim to quantitatively and qualitatively analyze the
visual characteristics of manga works, with a particular focus on panel layout
features. As a research method, we used facing page images of manga as input to
train a deep learning model for predicting manga titles, examining
classification accuracy to quantitatively analyze these features. Specifically,
we conducted ablation studies by limiting page image information to panel
frames to analyze the characteristics of panel layouts. Through a series of
quantitative experiments using all 104 works, 12 genres, and 10,122 facing page
images from the Manga109 dataset, as well as qualitative analysis using
Grad-CAM, our study demonstrates that the uniqueness of manga works is strongly
reflected in their panel layouts.",2024-12-26T09:53:37Z,http://arxiv.org/abs/2412.19141v1,"Siyuan Feng, Teruya Yoshinaga, Katsuhiko Hayashi, Koki Washio, Hidetaka Kamigaito"
"SILC-EFSA: Self-aware In-context Learning Correction for Entity-level
  Financial Sentiment Analysis","In recent years, fine-grained sentiment analysis in finance has gained
significant attention, but the scarcity of entity-level datasets remains a key
challenge. To address this, we have constructed the largest English and Chinese
financial entity-level sentiment analysis datasets to date. Building on this
foundation, we propose a novel two-stage sentiment analysis approach called
Self-aware In-context Learning Correction (SILC). The first stage involves
fine-tuning a base large language model to generate pseudo-labeled data
specific to our task. In the second stage, we train a correction model using a
GNN-based example retriever, which is informed by the pseudo-labeled data. This
two-stage strategy has allowed us to achieve state-of-the-art performance on
the newly constructed datasets, advancing the field of financial sentiment
analysis. In a case study, we demonstrate the enhanced practical utility of our
data and methods in monitoring the cryptocurrency market. Our datasets and code
are available at https://github.com/NLP-Bin/SILC-EFSA.",2024-12-26T09:53:01Z,http://arxiv.org/abs/2412.19140v1,"Senbin Zhu, Chenyuan He, Hongde Liu, Pengcheng Dong, Hanjie Zhao, Yuchen Yan, Yuxiang Jia, Hongying Zan, Min Peng"
PlanLLM: Video Procedure Planning with Refinable Large Language Models,"Video procedure planning, i.e., planning a sequence of action steps given the
video frames of start and goal states, is an essential ability for embodied AI.
Recent works utilize Large Language Models (LLMs) to generate enriched action
step description texts to guide action step decoding. Although LLMs are
introduced, these methods decode the action steps into a closed-set of one-hot
vectors, limiting the model's capability of generalizing to new steps or tasks.
Additionally, fixed action step descriptions based on world-level commonsense
may contain noise in specific instances of visual states. In this paper, we
propose PlanLLM, a cross-modal joint learning framework with LLMs for video
procedure planning. We propose an LLM-Enhanced Planning module which fully uses
the generalization ability of LLMs to produce free-form planning output and to
enhance action step decoding. We also propose Mutual Information Maximization
module to connect world-level commonsense of step descriptions and
sample-specific information of visual states, enabling LLMs to employ the
reasoning ability to generate step sequences. With the assistance of LLMs, our
method can both closed-set and open vocabulary procedure planning tasks. Our
PlanLLM achieves superior performance on three benchmarks, demonstrating the
effectiveness of our designs.",2024-12-26T09:51:05Z,http://arxiv.org/abs/2412.19139v1,"Dejie Yang, Zijing Zhao, YangLiu"
SUTrack: Towards Simple and Unified Single Object Tracking,"In this paper, we propose a simple yet unified single object tracking (SOT)
framework, dubbed SUTrack. It consolidates five SOT tasks (RGB-based,
RGB-Depth, RGB-Thermal, RGB-Event, RGB-Language Tracking) into a unified model
trained in a single session. Due to the distinct nature of the data, current
methods typically design individual architectures and train separate models for
each task. This fragmentation results in redundant training processes,
repetitive technological innovations, and limited cross-modal knowledge
sharing. In contrast, SUTrack demonstrates that a single model with a unified
input representation can effectively handle various common SOT tasks,
eliminating the need for task-specific designs and separate training sessions.
Additionally, we introduce a task-recognition auxiliary training strategy and a
soft token type embedding to further enhance SUTrack's performance with minimal
overhead. Experiments show that SUTrack outperforms previous task-specific
counterparts across 11 datasets spanning five SOT tasks. Moreover, we provide a
range of models catering edge devices as well as high-performance GPUs,
striking a good trade-off between speed and accuracy. We hope SUTrack could
serve as a strong foundation for further compelling research into unified
tracking models. Code and models are available at
github.com/chenxin-dlut/SUTrack.",2024-12-26T09:41:36Z,http://arxiv.org/abs/2412.19138v1,"Xin Chen, Ben Kang, Wanting Geng, Jiawen Zhu, Yi Liu, Dong Wang, Huchuan Lu"
"Extended Cross-Modality United Learning for Unsupervised
  Visible-Infrared Person Re-identification","Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)
aims to learn modality-invariant features from unlabeled cross-modality
datasets and reduce the inter-modality gap. However, the existing methods lack
cross-modality clustering or excessively pursue cluster-level association,
which makes it difficult to perform reliable modality-invariant features
learning. To deal with this issue, we propose a Extended Cross-Modality United
Learning (ECUL) framework, incorporating Extended Modality-Camera Clustering
(EMCC) and Two-Step Memory Updating Strategy (TSMem) modules. Specifically, we
design ECUL to naturally integrates intra-modality clustering, inter-modality
clustering and inter-modality instance selection, establishing compact and
accurate cross-modality associations while reducing the introduction of noisy
labels. Moreover, EMCC captures and filters the neighborhood relationships by
extending the encoding vector, which further promotes the learning of
modality-invariant and camera-invariant knowledge in terms of clustering
algorithm. Finally, TSMem provides accurate and generalized proxy points for
contrastive learning by updating the memory in stages. Extensive experiments
results on SYSU-MM01 and RegDB datasets demonstrate that the proposed ECUL
shows promising performance and even outperforms certain supervised methods.",2024-12-26T09:30:26Z,http://arxiv.org/abs/2412.19134v1,"Ruixing Wu, Yiming Yang, Jiakai He, Haifeng Hu"
"Unveiling the Chiral States in Multi-Weyl Semimetals through
  Magneto-Optical Spectroscopy","This study investigates the transport parameters in multi-Weyl semimetals,
focusing on their magneto-optical properties and the role of chiral states. The
tilting parameter is identified as a key factor in higher-order Weyl nodes,
significantly influencing the magneto-optical response. We obtain a generic
Landau-level expression for multi-Weyl semimetals, establishing a robust
framework for analyzing their quantum transport properties. A comprehensive
expression for the conductivity tensor components is presented, uncovering
distinctive low-frequency peaks and other features shaped by the tilting
parameter. Our findings reveal that the signatures of chiral states in the
conductivity tensors become increasingly pronounced with the Weyl node order.
Particularly, the tilting parameter is shown to impact Faraday rotation, at
energies near the tilted Dirac cone energies. These results provide critical
insights into the magneto-optical behavior of multi-Weyl semimetals and their
potential for exploring topological phenomena.",2024-12-26T09:29:59Z,http://arxiv.org/abs/2412.19132v1,"Sushmita Saha, Deepannita Das, Alestin Mawrie"
"A Rhetorical Relations-Based Framework for Tailored Multimedia Document
  Summarization","In the rapidly evolving landscape of digital content, the task of summarizing
multimedia documents, which encompass textual, visual, and auditory elements,
presents intricate challenges. These challenges include extracting pertinent
information from diverse formats, maintaining the structural integrity and
semantic coherence of the original content, and generating concise yet
informative summaries. This paper introduces a novel framework for multimedia
document summarization that capitalizes on the inherent structure of the
document to craft coherent and succinct summaries. Central to this framework is
the incorporation of a rhetorical structure for structural analysis, augmented
by a graph-based representation to facilitate the extraction of pivotal
information. Weighting algorithms are employed to assign significance values to
document units, thereby enabling effective ranking and selection of relevant
content. Furthermore, the framework is designed to accommodate user preferences
and time constraints, ensuring the production of personalized and contextually
relevant summaries. The summarization process is elaborately delineated,
encompassing document specification, graph construction, unit weighting, and
summary extraction, supported by illustrative examples and algorithmic
elucidation. This proposed framework represents a significant advancement in
automatic summarization, with broad potential applications across multimedia
document processing, promising transformative impacts in the field.",2024-12-26T09:29:59Z,http://arxiv.org/abs/2412.19133v1,"Azze-Eddine Maredj, Madjid Sadallah"
Semantic Residual for Multimodal Unified Discrete Representation,"Recent research in the domain of multimodal unified representations
predominantly employs codebook as representation forms, utilizing Vector
Quantization(VQ) for quantization, yet there has been insufficient exploration
of other quantization representation forms. Our work explores more precise
quantization methods and introduces a new framework, Semantic Residual
Cross-modal Information Disentanglement (SRCID), inspired by the numerical
residual concept inherent to Residual Vector Quantization (RVQ). SRCID employs
semantic residual-based information disentanglement for multimodal data to
better handle the inherent discrepancies between different modalities. Our
method enhances the capabilities of unified multimodal representations and
demonstrates exceptional performance in cross-modal generalization and
cross-modal zero-shot retrieval. Its average results significantly surpass
existing state-of-the-art models, as well as previous attempts with RVQ and
Finite Scalar Quantization (FSQ) based on these modals.",2024-12-26T09:08:52Z,http://arxiv.org/abs/2412.19128v1,"Hai Huang, Shulei Wang, Yan Xia"
"Advanced Knowledge Transfer: Refined Feature Distillation for Zero-Shot
  Quantization in Edge Computing","We introduce AKT (Advanced Knowledge Transfer), a novel method to enhance the
training ability of low-bit quantized (Q) models in the field of zero-shot
quantization (ZSQ). Existing research in ZSQ has focused on generating
high-quality data from full-precision (FP) models. However, these approaches
struggle with reduced learning ability in low-bit quantization due to its
limited information capacity. To overcome this limitation, we propose effective
training strategy compared to data generation. Particularly, we analyzed that
refining feature maps in the feature distillation process is an effective way
to transfer knowledge to the Q model. Based on this analysis, AKT efficiently
transfer core information from the FP model to the Q model. AKT is the first
approach to utilize both spatial and channel attention information in feature
distillation in ZSQ. Our method addresses the fundamental gradient exploding
problem in low-bit Q models. Experiments on CIFAR-10 and CIFAR-100 datasets
demonstrated the effectiveness of the AKT. Our method led to significant
performance enhancement in existing generative models. Notably, AKT achieved
significant accuracy improvements in low-bit Q models, achieving
state-of-the-art in the 3,5bit scenarios on CIFAR-10. The code is available at
https://github.com/Inpyo-Hong/AKT-Advanced-knowledge-Transfer.",2024-12-26T08:52:27Z,http://arxiv.org/abs/2412.19125v1,"Inpyo Hong, Youngwan Jo, Hyojeong Lee, Sunghyun Ahn, Sanghyun Park"
"Evaluating Self-Supervised Learning in Medical Imaging: A Benchmark for
  Robustness, Generalizability, and Multi-Domain Impact","Self-supervised learning (SSL) has emerged as a promising paradigm in medical
imaging, addressing the chronic challenge of limited labeled data in healthcare
settings. While SSL has shown impressive results, existing studies in the
medical domain are often limited in scope, focusing on specific datasets or
modalities, or evaluating only isolated aspects of model performance. This
fragmented evaluation approach poses a significant challenge, as models
deployed in critical medical settings must not only achieve high accuracy but
also demonstrate robust performance and generalizability across diverse
datasets and varying conditions. To address this gap, we present a
comprehensive evaluation of SSL methods within the medical domain, with a
particular focus on robustness and generalizability. Using the MedMNIST dataset
collection as a standardized benchmark, we evaluate 8 major SSL methods across
11 different medical datasets. Our study provides an in-depth analysis of model
performance in both in-domain scenarios and the detection of
out-of-distribution (OOD) samples, while exploring the effect of various
initialization strategies, model architectures, and multi-domain pre-training.
We further assess the generalizability of SSL methods through cross-dataset
evaluations and the in-domain performance with varying label proportions (1%,
10%, and 100%) to simulate real-world scenarios with limited supervision. We
hope this comprehensive benchmark helps practitioners and researchers make more
informed decisions when applying SSL methods to medical applications.",2024-12-26T08:51:56Z,http://arxiv.org/abs/2412.19124v1,"Valay Bundele, Oğuz Ata Çal, Bora Kargi, Karahan Sarıtaş, Kıvanç Tezören, Zohreh Ghaderi, Hendrik Lensch"
"CoheDancers: Enhancing Interactive Group Dance Generation through
  Music-Driven Coherence Decomposition","Dance generation is crucial and challenging, particularly in domains like
dance performance and virtual gaming. In the current body of literature, most
methodologies focus on Solo Music2Dance. While there are efforts directed
towards Group Music2Dance, these often suffer from a lack of coherence,
resulting in aesthetically poor dance performances. Thus, we introduce
CoheDancers, a novel framework for Music-Driven Interactive Group Dance
Generation. CoheDancers aims to enhance group dance generation coherence by
decomposing it into three key aspects: synchronization, naturalness, and
fluidity. Correspondingly, we develop a Cycle Consistency based Dance
Synchronization strategy to foster music-dance correspondences, an
Auto-Regressive-based Exposure Bias Correction strategy to enhance the fluidity
of the generated dances, and an Adversarial Training Strategy to augment the
naturalness of the group dance output. Collectively, these strategies enable
CohdeDancers to produce highly coherent group dances with superior quality.
Furthermore, to establish better benchmarks for Group Music2Dance, we construct
the most diverse and comprehensive open-source dataset to date, I-Dancers,
featuring rich dancer interactions, and create comprehensive evaluation
metrics. Experimental evaluations on I-Dancers and other extant datasets
substantiate that CoheDancers achieves unprecedented state-of-the-art
performance. Code will be released.",2024-12-26T08:47:13Z,http://arxiv.org/abs/2412.19123v1,"Kaixing Yang, Xulong Tang, Haoyu Wu, Qinliang Xue, Biao Qin, Hongyan Liu, Zhaoxin Fan"
Discrete vs. Continuous Trade-offs for Generative Models,"This work explores the theoretical and practical foundations of denoising
diffusion probabilistic models (DDPMs) and score-based generative models, which
leverage stochastic processes and Brownian motion to model complex data
distributions. These models employ forward and reverse diffusion processes
defined through stochastic differential equations (SDEs) to iteratively add and
remove noise, enabling high-quality data generation. By analyzing the
performance bounds of these models, we demonstrate how score estimation errors
propagate through the reverse process and bound the total variation distance
using discrete Girsanov transformations, Pinsker's inequality, and the data
processing inequality (DPI) for an information theoretic lens.",2024-12-26T08:14:27Z,http://arxiv.org/abs/2412.19114v1,"Jathin Korrapati, Tanish Baranwal, Rahul Shah"
"SketchFill: Sketch-Guided Code Generation for Imputing Derived Missing
  Values","Missing value is a critical issue in data science, significantly impacting
the reliability of analyses and predictions. Missing value imputation (MVI) is
a longstanding problem because it highly relies on domain knowledge. Large
language models (LLMs) have emerged as a promising tool for data cleaning,
including MVI for tabular data, offering advanced capabilities for
understanding and generating content. However, despite their promise, existing
LLM techniques such as in-context learning and Chain-of-Thought (CoT) often
fall short in guiding LLMs to perform complex reasoning for MVI, particularly
when imputing derived missing values, which require mathematical formulas and
data relationships across rows and columns. This gap underscores the need for
further advancements in LLM methodologies to enhance their reasoning
capabilities for more reliable imputation outcomes. To fill this gap, we
propose SketchFill, a novel sketch-based method to guide LLMs in generating
accurate formulas to impute missing numerical values. Our experimental results
demonstrate that SketchFill significantly outperforms state-of-the-art methods,
achieving 56.2% higher accuracy than CoT-based methods and 78.8% higher
accuracy than MetaGPT. This sets a new standard for automated data cleaning and
advances the field of MVI for numerical values.",2024-12-26T08:13:34Z,http://arxiv.org/abs/2412.19113v1,"Yunfan Zhang, Changlun Li, Yuyu Luo, Nan Tang"
"Spectral Enhancement and Pseudo-Anchor Guidance for Infrared-Visible
  Person Re-Identification","The development of deep learning has facilitated the application of person
re-identification (ReID) technology in intelligent security. Visible-infrared
person re-identification (VI-ReID) aims to match pedestrians across infrared
and visible modality images enabling 24-hour surveillance. Current studies
relying on unsupervised modality transformations as well as inefficient
embedding constraints to bridge the spectral differences between infrared and
visible images, however, limit their potential performance. To tackle the
limitations of the above approaches, this paper introduces a simple yet
effective Spectral Enhancement and Pseudo-anchor Guidance Network, named
SEPG-Net. Specifically, we propose a more homogeneous spectral enhancement
scheme based on frequency domain information and greyscale space, which avoids
the information loss typically caused by inefficient modality transformations.
Further, a Pseudo Anchor-guided Bidirectional Aggregation (PABA) loss is
introduced to bridge local modality discrepancies while better preserving
discriminative identity embeddings. Experimental results on two public
benchmark datasets demonstrate the superior performance of SEPG-Net against
other state-of-the-art methods. The code is available at
https://github.com/1024AILab/ReID-SEPG.",2024-12-26T08:03:53Z,http://arxiv.org/abs/2412.19111v1,"Yiyuan Ge, Zhihao Chen, Ziyang Wang, Jiaju Kang, Mingya Zhang"
Stochastic normalizing flows for Effective String Theory,"Effective String Theory (EST) is a powerful tool used to study confinement in
pure gauge theories by modeling the confining flux tube connecting a static
quark-anti-quark pair as a thin vibrating string. Recently, flow-based samplers
have been applied as an efficient numerical method to study EST regularized on
the lattice, opening the route to study observables previously inaccessible to
standard analytical methods. Flow-based samplers are a class of algorithms
based on Normalizing Flows (NFs), deep generative models recently proposed as a
promising alternative to traditional Markov Chain Monte Carlo methods in
lattice field theory calculations. By combining NF layers with
out-of-equilibrium stochastic updates, we obtain Stochastic Normalizing Flows
(SNFs), a scalable class of machine learning algorithms that can be explained
in terms of stochastic thermodynamics. In this contribution, we outline EST and
SNFs, and report some numerical results for the shape of the flux tube.",2024-12-26T07:58:09Z,http://arxiv.org/abs/2412.19109v1,"Michele Caselle, Elia Cellini, Alessandro Nada"
"Graph Mixture of Experts and Memory-augmented Routers for Multivariate
  Time Series Anomaly Detection","Multivariate time series (MTS) anomaly detection is a critical task that
involves identifying abnormal patterns or events in data that consist of
multiple interrelated time series. In order to better model the complex
interdependence between entities and the various inherent characteristics of
each entity, the GNN based methods are widely adopted by existing methods. In
each layer of GNN, node features aggregate information from their neighboring
nodes to update their information. In doing so, from shallow layer to deep
layer in GNN, original individual node features continue to be weakened and
more structural information,i.e., from short-distance neighborhood to
long-distance neighborhood, continues to be enhanced. However, research to date
has largely ignored the understanding of how hierarchical graph information is
represented and their characteristics that can benefit anomaly detection.
Existing methods simply leverage the output from the last layer of GNN for
anomaly estimation while neglecting the essential information contained in the
intermediate GNN layers. To address such limitations, in this paper, we propose
a Graph Mixture of Experts (Graph-MoE) network for multivariate time series
anomaly detection, which incorporates the mixture of experts (MoE) module to
adaptively represent and integrate hierarchical multi-layer graph information
into entity representations. It is worth noting that our Graph-MoE can be
integrated into any GNN-based MTS anomaly detection method in a plug-and-play
manner. In addition, the memory-augmented routers are proposed in this paper to
capture the correlation temporal information in terms of the global historical
features of MTS to adaptively weigh the obtained entity representations to
achieve successful anomaly estimation. Extensive experiments on five
challenging datasets prove the superiority of our approach and each proposed
module.",2024-12-26T07:49:51Z,http://arxiv.org/abs/2412.19108v1,"Xiaoyu Huang, Weidong Chen, Bo Hu, Zhendong Mao"
"ERGNN: Spectral Graph Neural Network with Explicitly-optimized Rational
  Graph Filters","Approximation-based spectral graph neural networks, which construct graph
filters with function approximation, have shown substantial performance in
graph learning tasks. Despite their great success, existing works primarily
employ polynomial approximation to construct the filters, whereas another
superior option, namely ration approximation, remains underexplored. Although a
handful of prior works have attempted to deploy the rational approximation,
their implementations often involve intensive computational demands or still
resort to polynomial approximations, hindering full potential of the rational
graph filters. To address the issues, this paper introduces ERGNN, a novel
spectral GNN with explicitly-optimized rational filter. ERGNN adopts a unique
two-step framework that sequentially applies the numerator filter and the
denominator filter to the input signals, thus streamlining the model paradigm
while enabling explicit optimization of both numerator and denominator of the
rational filter. Extensive experiments validate the superiority of ERGNN over
state-of-the-art methods, establishing it as a practical solution for deploying
rational-based GNNs.",2024-12-26T07:48:47Z,http://arxiv.org/abs/2412.19106v1,"Guoming Li, Jian Yang, Shangsong Liang"
"Improving Generative Pre-Training: An In-depth Study of Masked Image
  Modeling and Denoising Models","In this work, we dive deep into the impact of additive noise in pre-training
deep networks. While various methods have attempted to use additive noise
inspired by the success of latent denoising diffusion models, when used in
combination with masked image modeling, their gains have been marginal when it
comes to recognition tasks. We thus investigate why this would be the case, in
an attempt to find effective ways to combine the two ideas. Specifically, we
find three critical conditions: corruption and restoration must be applied
within the encoder, noise must be introduced in the feature space, and an
explicit disentanglement between noised and masked tokens is necessary. By
implementing these findings, we demonstrate improved pre-training performance
for a wide range of recognition tasks, including those that require
fine-grained, high-frequency information to solve.",2024-12-26T07:47:20Z,http://arxiv.org/abs/2412.19104v1,"Hyesong Choi, Daeun Kim, Sungmin Cha, Kwang Moo Yi, Dongbo Min"
"Reconstruction Target Matters in Masked Image Modeling for Cross-Domain
  Few-Shot Learning","Cross-Domain Few-Shot Learning (CDFSL) requires the model to transfer
knowledge from the data-abundant source domain to data-scarce target domains
for fast adaptation, where the large domain gap makes CDFSL a challenging
problem. Masked Autoencoder (MAE) excels in effectively using unlabeled data
and learning image's global structures, enhancing model generalization and
robustness. However, in the CDFSL task with significant domain shifts, we find
MAE even shows lower performance than the baseline supervised models. In this
paper, we first delve into this phenomenon for an interpretation. We find that
MAE tends to focus on low-level domain information during reconstructing pixels
while changing the reconstruction target to token features could mitigate this
problem. However, not all features are beneficial, as we then find
reconstructing high-level features can hardly improve the model's
transferability, indicating a trade-off between filtering domain information
and preserving the image's global structure. In all, the reconstruction target
matters for the CDFSL task. Based on the above findings and interpretations, we
further propose Domain-Agnostic Masked Image Modeling (DAMIM) for the CDFSL
task. DAMIM includes an Aggregated Feature Reconstruction module to
automatically aggregate features for reconstruction, with balanced learning of
domain-agnostic information and images' global structure, and a Lightweight
Decoder module to further benefit the encoder's generalizability. Experiments
on four CDFSL datasets demonstrate that our method achieves state-of-the-art
performance.",2024-12-26T07:43:01Z,http://arxiv.org/abs/2412.19101v1,"Ran Ma, Yixiong Zou, Yuhua Li, Ruixuan Li"
"BSDB-Net: Band-Split Dual-Branch Network with Selective State Spaces
  Mechanism for Monaural Speech Enhancement","Although the complex spectrum-based speech enhancement(SE) methods have
achieved significant performance, coupling amplitude and phase can lead to a
compensation effect, where amplitude information is sacrificed to compensate
for the phase that is harmful to SE. In addition, to further improve the
performance of SE, many modules are stacked onto SE, resulting in increased
model complexity that limits the application of SE. To address these problems,
we proposed a dual-path network based on compressed frequency using Mamba.
First, we extract amplitude and phase information through parallel dual
branches. This approach leverages structured complex spectra to implicitly
capture phase information and solves the compensation effect by decoupling
amplitude and phase, and the network incorporates an interaction module to
suppress unnecessary parts and recover missing components from the other
branch. Second, to reduce network complexity, the network introduces a
band-split strategy to compress the frequency dimension. To further reduce
complexity while maintaining good performance, we designed a Mamba-based module
that models the time and frequency dimensions under linear complexity. Finally,
compared to baselines, our model achieves an average 8.3 times reduction in
computational complexity while maintaining superior performance. Furthermore,
it achieves a 25 times reduction in complexity compared to transformer-based
models.",2024-12-26T07:42:07Z,http://arxiv.org/abs/2412.19099v1,"Cunhang Fan, Enrui Liu, Andong Li, Jianhua Tao, Jian Zhou, Jiahao Li, Chengshi Zheng, Zhao Lv"
Tint Your Models Task-wise for Improved Multi-task Model Merging,"Traditional model merging methods for multi-task learning (MTL) address task
conflicts with straightforward strategies such as weight averaging, sign
consensus, or minimal test-time adjustments. This presumably counts on the
assumption that a merged encoder still retains abundant task knowledge from
individual encoders, implying that its shared representation is sufficiently
general across tasks. However, our insight is that adding just a single
trainable task-specific layer further can bring striking performance gains, as
demonstrated by our pilot study. Motivated by this finding, we propose Model
Tinting, a new test-time approach that introduces a single task-specific layer
for each task as trainable adjustments. Our method jointly trains merging
coefficients and task-specific layers, which effectively reduces task conflicts
with minimal additional costs. Additionally, we propose a sampling method that
utilizes the difference in confidence levels of both merged and individual
encoders. Extensive experiments demonstrate our method's effectiveness, which
achieves state-of-the-art performance across both computer vision and natural
language processing tasks and significantly surpasses prior works. Our code is
available at https://github.com/AIM-SKKU/ModelTinting.",2024-12-26T07:42:06Z,http://arxiv.org/abs/2412.19098v1,"Aecheon Jung, Seunghwan Lee, Dongyoon Han, Sungeun Hong"
"Anisotropic and tunable vortex topology in multiband iron-based
  superconductors","Building on the multiband nature of iron-based superconductors (FeSCs), we
have uncovered pronounced anisotropy in Majorana vortex topology arising from
the interaction between vortex orientation and multiple electronic topologies.
This anisotropy manifests in two distinct vortex configurations: the z-vortex
and x-vortex, oriented perpendicular and parallel to the Dirac axis (z-axis for
FeSCs), respectively. The x-vortex exhibits a unique duality, displaying two
distinct topological phase diagrams. One is strikingly simple, comprising only
trivial and topological superconducting phases, and remains resilient to
multiband entanglement. The other mirrors the z-vortex's complex diagram,
featuring alternating trivial, topological crystalline and topological
superconducting phases. Crucially, the former is exclusive to the x-vortex and
supports unpaired Majorana vortices across a wide parameter range, even with
Dirac nodes in electronic bands. Notably, uniaxial strain can modulate these
x-vortex phases, enabling the x-vortex to support both stable Majorana vortices
and rich exotic physics in a controllable manner. Moreover, we propose that the
x-vortex offers promising advantages for developing iron-based superconducting
quantum devices. Our findings introduce a novel paradigm in vortex topology
within multiband superconducting systems, highlighting the x-vortex as a
promising platform for exploring Majorana physics and advancing iron-based
superconducting quantum technology.",2024-12-26T07:31:11Z,http://arxiv.org/abs/2412.19096v1,"Si-Qi Yu, Wei Cheng, Chuang Li, Xiao-Hong Pan, Gang Xu, Fu-Chun Zhang, Xin Liu"
"TrajGEOS: Trajectory Graph Enhanced Orientation-based Sequential Network
  for Mobility Prediction","Human mobility studies how people move to access their needed resources and
plays a significant role in urban planning and location-based services. As a
paramount task of human mobility modeling, next location prediction is
challenging because of the diversity of users' historical trajectories that
gives rise to complex mobility patterns and various contexts. Deep sequential
models have been widely used to predict the next location by leveraging the
inherent sequentiality of trajectory data. However, they do not fully leverage
the relationship between locations and fail to capture users' multi-level
preferences. This work constructs a trajectory graph from users' historical
traces and proposes a \textbf{Traj}ectory \textbf{G}raph \textbf{E}nhanced
\textbf{O}rientation-based \textbf{S}equential network (TrajGEOS) for
next-location prediction tasks. TrajGEOS introduces hierarchical graph
convolution to capture location and user embeddings. Such embeddings consider
not only the contextual feature of locations but also the relation between
them, and serve as additional features in downstream modules. In addition, we
design an orientation-based module to learn users' mid-term preferences from
sequential modeling modules and their recent trajectories. Extensive
experiments on three real-world LBSN datasets corroborate the value of graph
and orientation-based modules and demonstrate that TrajGEOS outperforms the
state-of-the-art methods on the next location prediction task.",2024-12-26T07:18:38Z,http://arxiv.org/abs/2412.19092v1,"Zhaoping Hu, Zongyuan Huang, Jinming Yang, Tao Yang, Yaohui Jin, Yanyan Xu"
From Coin to Data: The Impact of Object Detection on Digital Numismatics,"In this work we investigate the application of advanced object detection
techniques to digital numismatics, focussing on the analysis of historical
coins. Leveraging models such as Contrastive Language-Image Pre-training
(CLIP), we develop a flexible framework for identifying and classifying
specific coin features using both image and textual descriptions. By examining
two distinct datasets, modern Russian coins featuring intricate ""Saint George
and the Dragon"" designs and degraded 1st millennium AD Southeast Asian coins
bearing Hindu-Buddhist symbols, we evaluate the efficacy of different detection
algorithms in search and classification tasks. Our results demonstrate the
superior performance of larger CLIP models in detecting complex imagery, while
traditional methods excel in identifying simple geometric patterns.
Additionally, we propose a statistical calibration mechanism to enhance the
reliability of similarity scores in low-quality datasets. This work highlights
the transformative potential of integrating state-of-the-art object detection
into digital numismatics, enabling more scalable, precise, and efficient
analysis of historical artifacts. These advancements pave the way for new
methodologies in cultural heritage research, artefact provenance studies, and
the detection of forgeries.",2024-12-26T07:05:53Z,http://arxiv.org/abs/2412.19091v1,"Rafael Cabral, Maria De Iorio, Andrew Harris"
"Humans as a Calibration Pattern: Dynamic 3D Scene Reconstruction from
  Unsynchronized and Uncalibrated Videos","Recent works on dynamic neural field reconstruction assume input from
synchronized multi-view videos with known poses. These input constraints are
often unmet in real-world setups, making the approach impractical. We
demonstrate that unsynchronized videos with unknown poses can generate dynamic
neural fields if the videos capture human motion. Humans are one of the most
common dynamic subjects whose poses can be estimated using state-of-the-art
methods. While noisy, the estimated human shape and pose parameters provide a
decent initialization for the highly non-convex and under-constrained problem
of training a consistent dynamic neural representation. Given the sequences of
pose and shape of humans, we estimate the time offsets between videos, followed
by camera pose estimations by analyzing 3D joint locations. Then, we train
dynamic NeRF employing multiresolution rids while simultaneously refining both
time offsets and camera poses. The setup still involves optimizing many
parameters, therefore, we introduce a robust progressive learning strategy to
stabilize the process. Experiments show that our approach achieves accurate
spatiotemporal calibration and high-quality scene reconstruction in challenging
conditions.",2024-12-26T07:04:20Z,http://arxiv.org/abs/2412.19089v1,"Changwoon Choi, Jeongjun Kim, Geonho Cha, Minkwan Kim, Dongyoon Wee, Young Min Kim"
MoPD: Mixture-of-Prompts Distillation for Vision-Language Models,"Soft prompt learning methods are effective for adapting vision-language
models (VLMs) to downstream tasks. Nevertheless, empirical evidence reveals a
tendency of existing methods that they overfit seen classes and exhibit
degraded performance on unseen classes. This limitation is due to the inherent
bias in the training data towards the seen classes. To address this issue, we
propose a novel soft prompt learning method, named Mixture-of-Prompts
Distillation (MoPD), which can effectively transfer useful knowledge from hard
prompts manually hand-crafted (a.k.a. teacher prompts) to the learnable soft
prompt (a.k.a. student prompt), thereby enhancing the generalization ability of
soft prompts on unseen classes. Moreover, the proposed MoPD method utilizes a
gating network that learns to select hard prompts used for prompt distillation.
Extensive experiments demonstrate that the proposed MoPD method outperforms
state-of-the-art baselines especially on on unseen classes.",2024-12-26T06:57:04Z,http://arxiv.org/abs/2412.19087v1,"Yang Chen, Shuai Fu, Yu Zhang"
"Assessing Pre-trained Models for Transfer Learning through Distribution
  of Spectral Components","Pre-trained model assessment for transfer learning aims to identify the
optimal candidate for the downstream tasks from a model hub, without the need
of time-consuming fine-tuning. Existing advanced works mainly focus on
analyzing the intrinsic characteristics of the entire features extracted by
each pre-trained model or how well such features fit the target labels. This
paper proposes a novel perspective for pre-trained model assessment through the
Distribution of Spectral Components (DISCO). Through singular value
decomposition of features extracted from pre-trained models, we investigate
different spectral components and observe that they possess distinct
transferability, contributing diversely to the fine-tuning performance.
Inspired by this, we propose an assessment method based on the distribution of
spectral components which measures the proportions of their corresponding
singular values. Pre-trained models with features concentrating on more
transferable components are regarded as better choices for transfer learning.
We further leverage the labels of downstream data to better estimate the
transferability of each spectral component and derive the final assessment
criterion. Our proposed method is flexible and can be applied to both
classification and regression tasks. We conducted comprehensive experiments
across three benchmarks and two tasks including image classification and object
detection, demonstrating that our method achieves state-of-the-art performance
in choosing proper pre-trained models from the model hub for transfer learning.",2024-12-26T06:54:22Z,http://arxiv.org/abs/2412.19085v1,"Tengxue Zhang, Yang Shu, Xinyang Chen, Yifei Long, Chenjuan Guo, Bin Yang"
A Microservice Graph Generator with Production Characteristics,"A production microservice application may provide multiple services, queries
of a service may have different call graphs, and a microservice may be shared
across call graphs. It is challenging to improve the resource efficiency of
such complex applications without proper benchmarks, while production traces
are too large to be used in experiments. To this end, we propose a Service
Dependency Graph Generator (DGG) that comprises a Data Handler and a Graph
Generator, for generating the service dependency graphs of benchmarks that
incorporate production-level characteristics from traces. The data handler
first constructs fine-grained call graphs with dynamic interface and repeated
calling features from the trace and merges them into dependency graphs, and
then clusters them into different categories based on the topological and
invocation types. Taking the organized data and the selected category, the
graph generator simulates the process of real microservices invoking downstream
microservices using a random graph model, generates multiple call graphs, and
merges the call graphs to form the small-scale service dependency graph with
production-level characteristics. Case studies show that DGG's generated graphs
are similar to real traces in terms of topologies. Moreover, the resource
scaling based on DGG's fine-grained call graph constructing increases the
resource efficiency by up to 44.8% while ensuring the required QoS.",2024-12-26T06:51:35Z,http://arxiv.org/abs/2412.19083v1,"Fanrong Du, Jiuchen Shi, Quan Chen, Li Li, Minyi Guo"
"Social Optima in Linear Quadratic Graphon Field Control: Analysis via
  Infinite Dimensional Approach","This paper is concerned with linear quadratic graphon field social control
problem where the noises of individual agents are correlated. Compared with the
well-studied mean field system, the graphon field system consists of a large
number of agents coupled weakly via a weighted undirected graph where each node
represents an individual agent. Another notable feature of this paper is that
the dynamics of states of agents are driven by Brownian motions with a
correlation matrix. The infinite dimensional approach is adopted to design the
centralized and decentralized controls for our large population system. By
graphon theory, we prove that the linear quadratic (LQ) social optimum control
problem under the centralized information pattern is equivalent to an LQ
optimal control problem concerned with a stochastic evolution equation, and the
feedback-type optimal centralized control is obtained. Then, by designing an
auxiliary infinite dimensional optimal control problem through agent number
$N\rightarrow\infty$, a set of decentralized strategies are constructed, which
are further shown to be asymptotically social optimal.",2024-12-26T06:46:19Z,http://arxiv.org/abs/2412.19082v1,"De-xuan Xu, Zhun Gou, Nan-jing Huang"
"Graph-Enhanced Dual-Stream Feature Fusion with Pre-Trained Model for
  Acoustic Traffic Monitoring","Microphone array techniques are widely used in sound source localization and
smart city acoustic-based traffic monitoring, but these applications face
significant challenges due to the scarcity of labeled real-world traffic audio
data and the complexity and diversity of application scenarios. The DCASE
Challenge's Task 10 focuses on using multi-channel audio signals to count
vehicles (cars or commercial vehicles) and identify their directions
(left-to-right or vice versa). In this paper, we propose a graph-enhanced
dual-stream feature fusion network (GEDF-Net) for acoustic traffic monitoring,
which simultaneously considers vehicle type and direction to improve detection.
We propose a graph-enhanced dual-stream feature fusion strategy which consists
of a vehicle type feature extraction (VTFE) branch, a vehicle direction feature
extraction (VDFE) branch, and a frame-level feature fusion module to combine
the type and direction feature for enhanced performance. A pre-trained model
(PANNs) is used in the VTFE branch to mitigate data scarcity and enhance the
type features, followed by a graph attention mechanism to exploit temporal
relationships and highlight important audio events within these features. The
frame-level fusion of direction and type features enables fine-grained feature
representation, resulting in better detection performance. Experiments
demonstrate the effectiveness of our proposed method. GEDF-Net is our
submission that achieved 1st place in the DCASE 2024 Challenge Task 10.",2024-12-26T06:28:42Z,http://arxiv.org/abs/2412.19078v1,"Shitong Fan, Feiyang Xiao, Wenbo Wang, Shuhan Qi, Qiaoxi Zhu, Wenwu Wang, Jian Guan"
"Robust Speech and Natural Language Processing Models for Depression
  Screening","Depression is a global health concern with a critical need for increased
patient screening. Speech technology offers advantages for remote screening but
must perform robustly across patients. We have described two deep learning
models developed for this purpose. One model is based on acoustics; the other
is based on natural language processing. Both models employ transfer learning.
Data from a depression-labeled corpus in which 11,000 unique users interacted
with a human-machine application using conversational speech is used. Results
on binary depression classification have shown that both models perform at or
above AUC=0.80 on unseen data with no speaker overlap. Performance is further
analyzed as a function of test subset characteristics, finding that the models
are generally robust over speaker and session variables. We conclude that
models based on these approaches offer promise for generalized automated
depression screening.",2024-12-26T06:05:52Z,http://arxiv.org/abs/2412.19072v1,"Y. Lu, A. Harati, T. Rutowski, R. Oliveira, P. Chlebek, E. Shriberg"
Cross-Demographic Portability of Deep NLP-Based Depression Models,"Deep learning models are rapidly gaining interest for real-world applications
in behavioral health. An important gap in current literature is how well such
models generalize over different populations. We study Natural Language
Processing (NLP) based models to explore portability over two different corpora
highly mismatched in age. The first and larger corpus contains younger
speakers. It is used to train an NLP model to predict depression. When testing
on unseen speakers from the same age distribution, this model performs at
AUC=0.82. We then test this model on the second corpus, which comprises seniors
from a retirement community. Despite the large demographic differences in the
two corpora, we saw only modest degradation in performance for the
senior-corpus data, achieving AUC=0.76. Interestingly, in the senior
population, we find AUC=0.81 for the subset of patients whose health state is
consistent over time. Implications for demographic portability of speech-based
applications are discussed.",2024-12-26T05:54:24Z,http://arxiv.org/abs/2412.19070v1,"Tomek Rutowski, Elizabeth Shriberg, Amir Harati, Yang Lu, Ricardo Oliveira, Piotr Chlebek"
Effective and secure federated online learning to rank,"Online Learning to Rank (OLTR) optimises ranking models using implicit user
feedback, such as clicks. Unlike traditional Learning to Rank (LTR) methods
that rely on a static set of training data with relevance judgements to learn a
ranking model, OLTR methods update the model continually as new data arrives.
Thus, it addresses several drawbacks such as the high cost of human
annotations, potential misalignment between user preferences and human
judgments, and the rapid changes in user query intents. However, OLTR methods
typically require the collection of searchable data, user queries, and clicks,
which poses privacy concerns for users.
  Federated Online Learning to Rank (FOLTR) integrates OLTR within a Federated
Learning (FL) framework to enhance privacy by not sharing raw data. While
promising, FOLTR methods currently lag behind traditional centralised OLTR due
to challenges in ranking effectiveness, robustness with respect to data
distribution across clients, susceptibility to attacks, and the ability to
unlearn client interactions and data. This thesis presents a comprehensive
study on Federated Online Learning to Rank, addressing its effectiveness,
robustness, security, and unlearning capabilities, thereby expanding the
landscape of FOLTR.",2024-12-26T05:53:10Z,http://arxiv.org/abs/2412.19069v1,Shuyi Wang
"Attacking Voice Anonymization Systems with Augmented Feature and Speaker
  Identity Difference","This study focuses on the First VoicePrivacy Attacker Challenge within the
ICASSP 2025 Signal Processing Grand Challenge, which aims to develop speaker
verification systems capable of determining whether two anonymized speech
signals are from the same speaker. However, differences between feature
distributions of original and anonymized speech complicate this task. To
address this challenge, we propose an attacker system that combines Data
Augmentation enhanced feature representation and Speaker Identity Difference
enhanced classifier to improve verification performance, termed DA-SID.
Specifically, data augmentation strategies (i.e., data fusion and SpecAugment)
are utilized to mitigate feature distribution gaps, while probabilistic linear
discriminant analysis (PLDA) is employed to further enhance speaker identity
difference. Our system significantly outperforms the baseline, demonstrating
exceptional effectiveness and robustness against various voice anonymization
systems, ultimately securing a top-5 ranking in the challenge.",2024-12-26T05:52:44Z,http://arxiv.org/abs/2412.19068v1,"Yanzhe Zhang, Zhonghao Bi, Feiyang Xiao, Xuefeng Yang, Qiaoxi Zhu, Jian Guan"
Learning Monocular Depth from Events via Egomotion Compensation,"Event cameras are neuromorphically inspired sensors that sparsely and
asynchronously report brightness changes. Their unique characteristics of high
temporal resolution, high dynamic range, and low power consumption make them
well-suited for addressing challenges in monocular depth estimation (e.g.,
high-speed or low-lighting conditions). However, current existing methods
primarily treat event streams as black-box learning systems without
incorporating prior physical principles, thus becoming over-parameterized and
failing to fully exploit the rich temporal information inherent in event camera
data. To address this limitation, we incorporate physical motion principles to
propose an interpretable monocular depth estimation framework, where the
likelihood of various depth hypotheses is explicitly determined by the effect
of motion compensation. To achieve this, we propose a Focus Cost Discrimination
(FCD) module that measures the clarity of edges as an essential indicator of
focus level and integrates spatial surroundings to facilitate cost estimation.
Furthermore, we analyze the noise patterns within our framework and improve it
with the newly introduced Inter-Hypotheses Cost Aggregation (IHCA) module,
where the cost volume is refined through cost trend prediction and multi-scale
cost consistency constraints. Extensive experiments on real-world and synthetic
datasets demonstrate that our proposed framework outperforms cutting-edge
methods by up to 10\% in terms of the absolute relative error metric, revealing
superior performance in predicting accuracy.",2024-12-26T05:41:18Z,http://arxiv.org/abs/2412.19067v1,"Haitao Meng, Chonghao Zhong, Sheng Tang, Lian JunJia, Wenwei Lin, Zhenshan Bing, Yi Chang, Gang Chen, Alois Knoll"
"FFCG: Effective and Fast Family Column Generation for Solving
  Large-Scale Linear Program","Column Generation (CG) is an effective and iterative algorithm to solve
large-scale linear programs (LP). During each CG iteration, new columns are
added to improve the solution of the LP. Typically, CG greedily selects one
column with the most negative reduced cost, which can be improved by adding
more columns at once. However, selecting all columns with negative reduced
costs would lead to the addition of redundant columns that do not improve the
objective value. Therefore, selecting the appropriate columns to add is still
an open problem and previous machine-learning-based approaches for CG only add
a constant quantity of columns per iteration due to the state-space explosion
problem. To address this, we propose Fast Family Column Generation (FFCG) -- a
novel reinforcement-learning-based CG that selects a variable number of columns
as needed in an iteration. Specifically, we formulate the column selection
problem in CG as an MDP and design a reward metric that balances both the
convergence speed and the number of redundant columns. In our experiments, FFCG
converges faster on the common benchmarks and reduces the number of CG
iterations by 77.1% for Cutting Stock Problem (CSP) and 84.8% for Vehicle
Routing Problem with Time Windows (VRPTW), and a 71.4% reduction in computing
time for CSP and 84.0% for VRPTW on average compared to several
state-of-the-art baselines.",2024-12-26T05:35:48Z,http://arxiv.org/abs/2412.19066v1,"Yi-Xiang Hu, Feng Wu, Shaoang Li, Yifang Zhao, Xiang-Yang Li"
"Predicting Accurate X-ray Absorption Spectra for CN$^+$, CN$^\bullet$,
  and CN$^-$: Insights from First-Principles Simulations","High-resolution X-ray spectroscopy is an essential tool in X-ray astronomy,
enabling detailed studies of celestial objects and their physical and chemical
properties. However, comprehensive mapping of high-resolution X-ray spectra for
even simple interstellar and circumstellar molecules is still lacking. In this
study, we conducted systematic quantum chemical simulations to predict the C1s
X-ray absorption spectra of CN$^+$, CN, and CN$^-$. Our findings provide
valuable references for both X-ray astronomy and laboratory studies. We
assigned the first electronic peak of CN$^+$ and CN to C1s $\rightarrow
\sigma^*$ transitions, while the peak for CN$^-$ corresponds to a C1s
$\rightarrow \pi^*$ transition. We further calculated the vibronic fine
structures for these transitions using the quantum wavepacket method based on
multiconfigurational-level, anharmonic potential energy curves, revealing
distinct energy positions for the 0-0 absorptions at 280.7 eV, 279.6 eV, and
285.8 eV. Each vibronic profile features a prominent 0-0 peak, showing overall
similarity but differing intensity ratios of the 0-0 and 0-1 peaks. Notably,
introducing a C1s core hole leads to shortened C-N bond lengths and increased
vibrational frequencies across all species. These findings enhance our
understanding of the electronic structures and X-ray spectra of carbon-nitrogen
species, emphasizing the influence of charge state on X-ray absorptions.",2024-12-26T05:27:06Z,http://arxiv.org/abs/2412.19065v1,"Jinyu Li, Sheng-Yu Wang, Lu Zhang, Guoyan Ge, Minrui Wei, Junxiang Zuo, Weijie Hua"
"Hierarchical Multi-agent Meta-Reinforcement Learning for Cross-channel
  Bidding","Real-time bidding (RTB) plays a pivotal role in online advertising
ecosystems. Advertisers employ strategic bidding to optimize their advertising
impact while adhering to various financial constraints, such as the
return-on-investment (ROI) and cost-per-click (CPC). Primarily focusing on
bidding with fixed budget constraints, traditional approaches cannot
effectively manage the dynamic budget allocation problem where the goal is to
achieve global optimization of bidding performance across multiple channels
with a shared budget. In this paper, we propose a hierarchical multi-agent
reinforcement learning framework for multi-channel bidding optimization. In
this framework, the top-level strategy applies a CPC constrained diffusion
model to dynamically allocate budgets among the channels according to their
distinct features and complex interdependencies, while the bottom-level
strategy adopts a state-action decoupled actor-critic method to address the
problem of extrapolation errors in offline learning caused by
out-of-distribution actions and a context-based meta-channel knowledge learning
method to improve the state representation capability of the policy based on
the shared knowledge among different channels. Comprehensive experiments
conducted on a large scale real-world industrial dataset from the Meituan ad
bidding platform demonstrate that our method achieves a state-of-the-art
performance.",2024-12-26T05:26:30Z,http://arxiv.org/abs/2412.19064v1,"Shenghong He, Chao Yu"
DAPoinTr: Domain Adaptive Point Transformer for Point Cloud Completion,"Point Transformers (PoinTr) have shown great potential in point cloud
completion recently. Nevertheless, effective domain adaptation that improves
transferability toward target domains remains unexplored. In this paper, we
delve into this topic and empirically discover that direct feature alignment on
point Transformer's CNN backbone only brings limited improvements since it
cannot guarantee sequence-wise domain-invariant features in the Transformer. To
this end, we propose a pioneering Domain Adaptive Point Transformer (DAPoinTr)
framework for point cloud completion. DAPoinTr consists of three key
components: Domain Query-based Feature Alignment (DQFA), Point Token-wise
Feature alignment (PTFA), and Voted Prediction Consistency (VPC). In
particular, DQFA is presented to narrow the global domain gaps from the
sequence via the presented domain proxy and domain query at the Transformer
encoder and decoder, respectively. PTFA is proposed to close the local domain
shifts by aligning the tokens, \emph{i.e.,} point proxy and dynamic query, at
the Transformer encoder and decoder, respectively. VPC is designed to consider
different Transformer decoders as multiple of experts (MoE) for ensembled
prediction voting and pseudo-label generation. Extensive experiments with
visualization on several domain adaptation benchmarks demonstrate the
effectiveness and superiority of our DAPoinTr compared with state-of-the-art
methods. Code will be publicly available at:
https://github.com/Yinghui-Li-New/DAPoinTr",2024-12-26T05:16:54Z,http://arxiv.org/abs/2412.19062v1,"Yinghui Li, Qianyu Zhou, Jingyu Gong, Ye Zhu, Richard Dazeley, Xinkui Zhao, Xuequan Lu"
"An active hydroelastic liquid crystal phase of a fluttering
  ferroelectric nematic","Polarization flutter, produced by an applied AC electric field drives an
equilibrium ferroelectric nematic ($\mathrm{N_F}$) liquid crystal (LC) through
a transition into a dissipative active ferroelectric nematic state exhibiting
strong elasto-hydrodynamic intermolecular interaction. In such a fluttering
ferroelectric, the typical equilibrium $\mathrm{N_F}$ textural features adopted
to reduce electrostatic energy, such as preferences for director bend, and
alignment of polarization parallel to LC/air interfaces, are overcome, giving
way to nonequilibrium conjugate structures in which director splay, and
alignment of polarization normal to $\mathrm{N_F}$/air interfaces are
preferred. Viewing the latter textures as those of an active nematic phase
reveals that self-organization to reduce effective viscosity and resulting
dissipation generates a flow-driven apparent nematic elasticity and interface
structuring that dominates equilibrium LC elastic and surface forces.",2024-12-26T05:13:55Z,http://arxiv.org/abs/2412.19061v1,"Xi Chen, Cory Pecinovsky, Eva Korblova, Matthew A. Glaser, Leo Radzihovsky, Joseph E. Maclennan, David M. Walba, Noel A. Clark"
Coarse-grained binning in Drell-Yan transverse momentum spectra,"We report a study of the determination of the intrinsic transverse momentum
of partons, the intrinsic $k_T$, from the dilepton transverse momentum $p_T$ in
Drell-Yan (DY) production at hadron colliders. The result shows that a good
sensitivity to the intrinsic $k_T$ distribution is achieved by measuring
relative ratios between the cross sections of suitably defined low-$p_T$ and
high-$p_T$ regions. The study is performed through both a pseudo-data test and
an extraction from measurements of the DY process by the CMS collaboration.
Since the methodology does not rely on any dedicated partition of bins, this
$p_T$-ratio observable requires less special treatment in very low $p_T$
regions, and propagates lower systematic uncertainties induced from unfolding
or momentum migration, in contrast with previous proposals of using a
fine-binning measurement of the differential cross section.",2024-12-26T05:13:39Z,http://arxiv.org/abs/2412.19060v1,"Wenxiao Zhan, Siqi Yang, Minghui Liu, Francesco Hautmann, Liang Han"
"The System of BSDEs with Singular Terminal Values Arising in Optimal
  Liquidation with Regime Switching","We study a stochastic control problem with regime switching arising in an
optimal liquidation problem with dark pools and multiple regimes. The new
feature of this model is that it introduces a system of BSDEs with jumps and
with singular terminal values, which appears in literature for the first time.
The existence result for this system is obtained. As a result, we solve the
stochastic control problem with regime switching. More importantly, the
uniqueness result of this system is also obtained, in contrast to merely
minimal solutions established in most related literature.",2024-12-26T05:03:16Z,http://arxiv.org/abs/2412.19058v1,"Guanxing Fu, Xiaomin Shi, Zuoquan Xu"
"SpectralKD: Understanding and Optimizing Vision Transformer Distillation
  through Spectral Analysis","Knowledge distillation effectively reduces model complexity while improving
performance, yet the underlying knowledge transfer mechanisms remain poorly
understood. We propose novel spectral analysis methods and guidelines to
optimize distillation, making the knowledge transfer process more
interpretable. Our analysis reveals that CaiT models concentrate information in
their first and last few layers, informing optimal layer selection for feature
map distillation. Surprisingly, we discover that Swin Transformer and CaiT
exhibit similar spectral encoding patterns despite their architectural
differences, enhancing our understanding of transformer architectures and
leading to improved feature map alignment strategies. Based on these insights,
we introduce a simple yet effective spectral alignment method named SpectralKD.
Experimental results demonstrate that following our guidelines enables
SpectralKD to achieve state-of-the-art performance (DeiT-Tiny: $+5.2\%$,
Swin-Tiny: $+1.4\%$ in ImageNet-1k Top-1 accuracy). Furthermore, through
spectral analysis of student models trained with and without distillation, we
show that distilled models mirror spectral patterns of their teachers,
providing a new lens for interpreting knowledge distillation dynamics. Our
code, pre-trained models, and experimental logs will be made publicly
available.",2024-12-26T04:45:05Z,http://arxiv.org/abs/2412.19055v1,"Huiyuan Tian, Bonan Xu, Shijian Li, Gang Pan"
"Performance Characterization and Optimizations of Traditional ML
  Applications","Even in the era of Deep Learning based methods, traditional machine learning
methods with large data sets continue to attract significant attention.
However, we find an apparent lack of a detailed performance characterization of
these methods in the context of large training datasets. In this work, we study
the system's behavior of a number of traditional ML methods as implemented in
popular free software libraries/modules to identify critical performance
bottlenecks experienced by these applications. The performance characterization
study reveals several interesting insights on the performance of these
applications. Then we evaluate the performance benefits of applying some
well-known optimizations at the levels of caches and the main memory. More
specifically, we test the usefulness of optimizations such as (i) software
prefetching to improve cache performance and (ii) data layout and computation
reordering optimizations to improve locality in DRAM accesses. These
optimizations are implemented as modifications to the well-known scikit-learn
library, and hence can be easily leveraged by application programmers. We
evaluate the impact of the proposed optimizations using a combination of
simulation and execution on a real system. The software prefetching
optimization results in performance benefits varying from 5.2%-27.1% on
different ML applications while the data layout and computation reordering
approaches yield 6.16%-28.0% performance improvement.",2024-12-26T04:13:52Z,http://arxiv.org/abs/2412.19051v1,"Harsh Kumar, R. Govindarajan"
Jasper and Stella: distillation of SOTA embedding models,"A crucial component of many deep learning applications (such as FAQ and RAG)
is dense retrieval, in which embedding models are used to convert raw text to
numerical vectors and then get the most similar text by MIPS (Maximum Inner
Product Search). Some text embedding benchmarks (e.g. MTEB, BEIR, and
AIR-Bench) have been established to evaluate embedding models accurately.
Thanks to these benchmarks, we can use SOTA models; however, the deployment and
application of these models in industry were hampered by their large vector
dimensions and numerous parameters. To alleviate this problem, 1) we present a
distillation technique that can enable a smaller student model to achieve good
performance. 2) Inspired by MRL we present a training approach of reducing the
vector dimensions based on its own vectors or its teacher vectors. 3) We do
simple yet effective alignment training between images and text to make our
model a multimodal encoder. We trained Stella and Jasper models using the
technologies above and achieved high scores on the MTEB leaderboard. We release
the model and data at Hugging Face Hub
(https://huggingface.co/infgrad/jasper_en_vision_language_v1) and the training
logs are at https://api.wandb.ai/links/dunnzhang0/z8jqoqpb.",2024-12-26T04:05:28Z,http://arxiv.org/abs/2412.19048v1,"Dun Zhang, FulongWang"
Revealing the Self: Brainwave-Based Human Trait Identification,"People exhibit unique emotional responses. In the same scenario, the
emotional reactions of two individuals can be either similar or vastly
different. For instance, consider one person's reaction to an invitation to
smoke versus another person's response to a query about their sleep quality.
The identification of these individual traits through the observation of common
physical parameters opens the door to a wide range of applications, including
psychological analysis, criminology, disease prediction, addiction control, and
more. While there has been previous research in the fields of psychometrics,
inertial sensors, computer vision, and audio analysis, this paper introduces a
novel technique for identifying human traits in real time using brainwave data.
To achieve this, we begin with an extensive study of brainwave data collected
from 80 participants using a portable EEG headset. We also conduct a
statistical analysis of the collected data utilizing box plots. Our analysis
uncovers several new insights, leading us to a groundbreaking unified approach
for identifying diverse human traits by leveraging machine learning techniques
on EEG data. Our analysis demonstrates that this proposed solution achieves
high accuracy. Moreover, we explore two deep-learning models to compare the
performance of our solution. Consequently, we have developed an integrated,
real-time trait identification solution using EEG data, based on the insights
from our analysis. To validate our approach, we conducted a rigorous user
evaluation with an additional 20 participants. The outcomes of this evaluation
illustrate both high accuracy and favorable user ratings, emphasizing the
robust potential of our proposed method to serve as a versatile solution for
human trait identification.",2024-12-26T03:27:34Z,http://arxiv.org/abs/2412.19041v1,"Md Mirajul Islam, Md Nahiyan Uddin, Maoyejatun Hasana, Debojit Pandit, Nafis Mahmud Rahman, Sriram Chellappan, Sami Azam, A. B. M. Alim Al Islam"
Neural Networks Perform Sufficient Dimension Reduction,"This paper investigates the connection between neural networks and sufficient
dimension reduction (SDR), demonstrating that neural networks inherently
perform SDR in regression tasks under appropriate rank regularizations.
Specifically, the weights in the first layer span the central mean subspace. We
establish the statistical consistency of the neural network-based estimator for
the central mean subspace, underscoring the suitability of neural networks in
addressing SDR-related challenges. Numerical experiments further validate our
theoretical findings, and highlight the underlying capability of neural
networks to facilitate SDR compared to the existing methods. Additionally, we
discuss an extension to unravel the central subspace, broadening the scope of
our investigation.",2024-12-26T03:05:43Z,http://arxiv.org/abs/2412.19033v1,"Shuntuo Xu, Zhou Yu"
"Modality-Projection Universal Model for Comprehensive Full-Body Medical
  Imaging Segmentation","The integration of deep learning in medical imaging has shown great promise
for enhancing diagnostic, therapeutic, and research outcomes. However, applying
universal models across multiple modalities remains challenging due to the
inherent variability in data characteristics. This study aims to introduce and
evaluate a Modality Projection Universal Model (MPUM). MPUM employs a novel
modality-projection strategy, which allows the model to dynamically adjust its
parameters to optimize performance across different imaging modalities. The
MPUM demonstrated superior accuracy in identifying anatomical structures,
enabling precise quantification for improved clinical decision-making. It also
identifies metabolic associations within the brain-body axis, advancing
research on brain-body physiological correlations. Furthermore, MPUM's unique
controller-based convolution layer enables visualization of saliency maps
across all network layers, significantly enhancing the model's
interpretability.",2024-12-26T02:23:27Z,http://arxiv.org/abs/2412.19026v1,"Yixin Chen, Lin Gao, Yajuan Gao, Rui Wang, Jingge Lian, Xiangxi Meng, Yanhua Duan, Leiying Chai, Hongbin Han, Zhaoping Cheng, Zhaoheng Xie"
"Channel-Aware Optimal Transport: A Theoretical Framework for Generative
  Communication","Optimal transport has numerous applications, particularly in machine learning
tasks involving generative models. In practice, the transportation process
often encounters an information bottleneck, typically arising from the
conversion of a communication channel into a rate-limited bit pipeline using
error correction codes. While this conversion enables a channel-oblivious
approach to optimal transport, it fails to fully exploit the available degrees
of freedom. Motivated by the emerging paradigm of generative communication,
this paper examines the problem of channel-aware optimal transport, where a
block of i.i.d. random variables is transmitted through a memoryless channel to
generate another block of i.i.d. random variables with a prescribed marginal
distribution such that the end-to-end distortion is minimized. With unlimited
common randomness available to the encoder and decoder, the source-channel
separation architecture is shown to be asymptotically optimal as the
blocklength approaches infinity. On the other hand, in the absence of common
randomness, the source-channel separation architecture is generally suboptimal.
For this scenario, a hybrid coding scheme is proposed, which partially retains
the generative capabilities of the given channel while enabling reliable
transmission of digital information. It is demonstrated that the proposed
hybrid coding scheme can outperform both separation-based and uncoded schemes.",2024-12-26T02:23:08Z,http://arxiv.org/abs/2412.19025v1,"Xiqiang Qu, Ruibin Li, Jun Chen, Lei Yu, Xinbing Wang"
Adaptivity can help exponentially for shadow tomography,"In recent years there has been significant interest in understanding the
statistical complexity of learning from quantum data under the constraint that
one can only make unentangled measurements. While a key challenge in
establishing tight lower bounds in this setting is to deal with the fact that
the measurements can be chosen in an adaptive fashion, a recurring theme has
been that adaptivity offers little advantage over more straightforward,
nonadaptive protocols.
  In this note, we offer a counterpoint to this. We show that for the basic
task of shadow tomography, protocols that use adaptively chosen two-copy
measurements can be exponentially more sample-efficient than any protocol that
uses nonadaptive two-copy measurements.",2024-12-26T02:13:04Z,http://arxiv.org/abs/2412.19022v1,"Sitan Chen, Weiyuan Gong, Zhihan Zhang"
"Let the Rule Speak: Enhancing In-context Learning Debiasing with
  Interpretability","In-context learning, which allows large language models to perform diverse
tasks with a few demonstrations, is found to have imbalanced per-class
prediction accuracy on multi-class text classification. Although notable output
correction methods have been developed to tackle the issue and simultaneously
improve downstream prediction accuracy, they may fail to answer the core
interpretability challenges: why and which certain classes need corrections,
and more importantly, a tailored correction for per-sample, per-class's
probability. To address such interpretability gaps, we first find that the
imbalance arises from certain classes consistently receiving high ICL output
probabilities, whereas others receiving lower or mixed ranges, so the former is
more frequently chosen, resulting in higher accuracy; more crucially, we find
that these ranges have significantly varying degrees of influence on the
accuracy bias, highlighting the need for precise, interpretable probability
corrections by range. Motivated by this, we propose FuRud, a Fuzzy Rule
Optimization based Debiasing method, that (1) detects which classes need
corrections, and (2) for each correction-needed class, detects its probability
ranges and applies asymmetric amplifications or reductions to correct them
interpretably. Notably, across seven benchmark datasets, FuRud reduces the
pairwise class accuracy bias (COBias) by more than half (56%), while achieving
a relative increase of 21% in accuracy, outperforming state-of-the-art
debiasing methods. Moreover, FuRud can optimize downstream tasks with as few as
10 optimization examples. Furthermore, FuRud can work for prompt formats that
lead to highly skewed predictions. For example, FuRud greatly improves ICL
outputs which use letter options, with 44% relative accuracy increase and 54%
relative COBias reduction.",2024-12-26T01:56:42Z,http://arxiv.org/abs/2412.19018v1,"Ruixi Lin, Yang You"
"Brain Ageing Prediction using Isolation Forest Technique and Residual
  Neural Network (ResNet)","Brain aging is a complex and dynamic process, leading to functional and
structural changes in the brain. These changes could lead to the increased risk
of neurodegenerative diseases and cognitive decline. Accurate brain-age
estimation utilizing neuroimaging data has become necessary for detecting
initial signs of neurodegeneration. Here, we propose a novel deep learning
approach using the Residual Neural Network 101 Version 2 (ResNet101V2) model to
predict brain age from MRI scans. To train, validate and test our proposed
model, we used a large dataset of 2102 images which were selected randomly from
the International Consortium for Brain Mapping (ICBM). Next, we applied data
preprocessing techniques, including normalizing the images and using outlier
detection via Isolation Forest method. Then, we evaluated various pre-trained
approaches (namely: MobileNetV2, ResNet50V2, ResNet101V2, Xception). The
results demonstrated that the ResNet101V2 model has higher performance compared
with the other models, attaining MAEs of 0.9136 and 0.8242 years for before and
after using Isolation Forest process. Our method achieved a high accuracy in
brain age estimation in ICBM dataset and it provides a reliable brain age
prediction.",2024-12-26T01:49:21Z,http://arxiv.org/abs/2412.19017v1,"Saadat Behzadi, Danial Sharifrazi, Roohallah Alizadehsani, Mojtaba Lotfaliany, Mohammadreza Mohebbi"
"Imperceptible Adversarial Attacks on Point Clouds Guided by
  Point-to-Surface Field","Adversarial attacks on point clouds are crucial for assessing and improving
the adversarial robustness of 3D deep learning models. Traditional solutions
strictly limit point displacement during attacks, making it challenging to
balance imperceptibility with adversarial effectiveness. In this paper, we
attribute the inadequate imperceptibility of adversarial attacks on point
clouds to deviations from the underlying surface. To address this, we introduce
a novel point-to-surface (P2S) field that adjusts adversarial perturbation
directions by dragging points back to their original underlying surface.
Specifically, we use a denoising network to learn the gradient field of the
logarithmic density function encoding the shape's surface, and apply a
distance-aware adjustment to perturbation directions during attacks, thereby
enhancing imperceptibility. Extensive experiments show that adversarial attacks
guided by our P2S field are more imperceptible, outperforming state-of-the-art
methods.",2024-12-26T01:36:35Z,http://arxiv.org/abs/2412.19015v1,"Keke Tang, Weiyao Ke, Weilong Peng, Xiaofei Wang, Ziyong Du, Zhize Wu, Peican Zhu, Zhihong Tian"
Dynamic networks clustering via mirror distance,"The classification of different patterns of network evolution, for example in
brain connectomes or social networks, is a key problem in network inference and
modern data science. Building on the notion of a network's Euclidean mirror,
which captures its evolution as a curve in Euclidean space, we develop the
Dynamic Network Clustering through Mirror Distance (DNCMD), an algorithm for
clustering dynamic networks based on a distance measure between their
associated mirrors. We provide theoretical guarantees for DNCMD to achieve
exact recovery of distinct evolutionary patterns for latent position random
networks both when underlying vertex features change deterministically and when
they follow a stochastic process. We validate our theoretical results through
numerical simulations and demonstrate the application of DNCMD to understand
edge functions in Drosophila larval connectome data, as well as to analyze
temporal patterns in dynamic trade networks.",2024-12-26T01:14:21Z,http://arxiv.org/abs/2412.19012v1,"Runbing Zheng, Avanti Athreya, Marta Zlatic, Michael Clayton, Carey E. Priebe"
"FACEMUG: A Multimodal Generative and Fusion Framework for Local Facial
  Editing","Existing facial editing methods have achieved remarkable results, yet they
often fall short in supporting multimodal conditional local facial editing. One
of the significant evidences is that their output image quality degrades
dramatically after several iterations of incremental editing, as they do not
support local editing. In this paper, we present a novel multimodal generative
and fusion framework for globally-consistent local facial editing (FACEMUG)
that can handle a wide range of input modalities and enable fine-grained and
semantic manipulation while remaining unedited parts unchanged. Different
modalities, including sketches, semantic maps, color maps, exemplar images,
text, and attribute labels, are adept at conveying diverse conditioning
details, and their combined synergy can provide more explicit guidance for the
editing process. We thus integrate all modalities into a unified generative
latent space to enable multimodal local facial edits. Specifically, a novel
multimodal feature fusion mechanism is proposed by utilizing multimodal
aggregation and style fusion blocks to fuse facial priors and multimodalities
in both latent and feature spaces. We further introduce a novel self-supervised
latent warping algorithm to rectify misaligned facial features, efficiently
transferring the pose of the edited image to the given latent codes. We
evaluate our FACEMUG through extensive experiments and comparisons to
state-of-the-art (SOTA) methods. The results demonstrate the superiority of
FACEMUG in terms of editing quality, flexibility, and semantic control, making
it a promising solution for a wide range of local facial editing tasks.",2024-12-26T00:53:54Z,http://arxiv.org/abs/2412.19009v1,"Wanglong Lu, Jikai Wang, Xiaogang Jin, Xianta Jiang, Hanli Zhao"
"Enhancing Audiovisual Speech Recognition through Bifocal Preference
  Optimization","Audiovisual Automatic Speech Recognition (AV-ASR) aims to improve speech
recognition accuracy by leveraging visual signals. It is particularly
challenging in unconstrained real-world scenarios across various domains due to
noisy acoustic environments, spontaneous speech, and the uncertain use of
visual information. Most previous works fine-tune audio-only ASR models on
audiovisual datasets, optimizing them for conventional ASR objectives. However,
they often neglect visual features and common errors in unconstrained video
scenarios. In this paper, we propose using a preference optimization strategy
to improve speech recognition accuracy for real-world videos. First, we create
preference data via simulating common errors that occurred in AV-ASR from two
focals: manipulating the audio or vision input and rewriting the output
transcript. Second, we propose BPO-AVASR, a Bifocal Preference Optimization
method to improve AV-ASR models by leveraging both input-side and output-side
preference. Extensive experiments demonstrate that our approach significantly
improves speech recognition accuracy across various domains, outperforming
previous state-of-the-art models on real-world video speech recognition.",2024-12-26T00:26:45Z,http://arxiv.org/abs/2412.19005v1,"Yihan Wu, Yichen Lu, Yifan Peng, Xihua Wang, Ruihua Song, Shinji Watanabe"
"New Theorem on Chaos Transitions in Second-Order Dynamical Systems with
  Tikhonov Regularization","This study examines second-order dynamical systems incorporating Tikhonov
regularization. It focuses on how nonlinearities induce bifurcations and
chaotic dynamics. By using Lyapunov functions, bifurcation theory, and
numerical simulations, we identify critical transitions that lead to complex
behaviors like strange attractors and chaos. The findings provide a theoretical
framework for applications in optimization, machine learning, and biological
modeling. Key contributions include stability conditions, characterization of
chaotic regimes, and methods for managing nonlinear instabilities in
interdisciplinary systems.",2024-12-25T23:54:16Z,http://arxiv.org/abs/2412.19003v1,Illych Alvarez
"Tempus Core: Area-Power Efficient Temporal-Unary Convolution Core for
  Low-Precision Edge DLAs","The increasing complexity of deep neural networks (DNNs) poses significant
challenges for edge inference deployment due to resource and power constraints
of edge devices. Recent works on unary-based matrix multiplication hardware aim
to leverage data sparsity and low-precision values to enhance hardware
efficiency. However, the adoption and integration of such unary hardware into
commercial deep learning accelerators (DLA) remain limited due to processing
element (PE) array dataflow differences. This work presents Tempus Core, a
convolution core with highly scalable unary-based PE array comprising of tub
(temporal-unary-binary) multipliers that seamlessly integrates with the NVDLA
(NVIDIA's open-source DLA for accelerating CNNs) while maintaining dataflow
compliance and boosting hardware efficiency. Analysis across various datapath
granularities shows that for INT8 precision in 45nm CMOS, Tempus Core's PE cell
unit (PCU) yields 59.3% and 15.3% reductions in area and power consumption,
respectively, over NVDLA's CMAC unit. Considering a 16x16 PE array in Tempus
Core, area and power improves by 75% and 62%, respectively, while delivering 5x
and 4x iso-area throughput improvements for INT8 and INT4 precisions.
Post-place and route analysis of Tempus Core's PCU shows that the 16x4 PE array
for INT4 precision in 45nm CMOS requires only 0.017 mm^2 die area and consumes
only 6.2mW of total power. We demonstrate that area-power efficient unary-based
hardware can be seamlessly integrated into conventional DLAs, paving the path
for efficient unary hardware for edge AI inference.",2024-12-25T23:20:02Z,http://arxiv.org/abs/2412.19002v1,"Prabhu Vellaisamy, Harideep Nair, Thomas Kang, Yichen Ni, Haoyang Fan, Bin Qi, Jeff Chen, Shawn Blanton, John Paul Shen"
"Impact of resummation on the production and experimental bounds of
  scalar high-electric-charge objects","A one-loop Dyson-Schwinger-like resummation scheme is applied to scalar
High-Electric-Charge compact Objects (HECOs), extending previous work on
spin-1/2 case. The electromagnetic interactions of HECOs are considered within
the framework of strongly coupled scalar Quantun Electrodynamics. The
resummation amounts to determining non-trivial ultraviolet (UV) fixed points,
at which the effective Lagrangian, which will lead to the pertinent predictions
on the cross sections, is computed. In contrast to the fermionic HECO case, in
which the fixed point structure was determined solely by the interactions of
the HECOs with the photon field, in the scalar case the existence of
non-trivial UV fixed points requires the presence of additional strong self
interactions among the HECOs. Our resummation scheme, which is notably
different from a lattice strong-coupling approach, makes the computation of the
pertinent scalar-HECO-production cross sections reliable, thus allowing
revisiting the mass bounds obtained from searches for such objects in current
or future colliders. Our MadGraph implementation of the results leads to
enhanced (up to ~30%) lower bounds on the mass of scalar HECOs, as compared to
those extracted from the tree-level processes typically used in LHC collider
searches by ATLAS and MoEDAL experiments.",2024-12-25T23:13:36Z,http://arxiv.org/abs/2412.19001v1,"Jean Alexandre, Nick E. Mavromatos, Vasiliki A. Mitsou, Emanuela Musumeci"
"MGAN-CRCM: A Novel Multiple Generative Adversarial Network and
  Coarse-Refinement Based Cognizant Method for Image Inpainting","Image inpainting is a widely used technique in computer vision for
reconstructing missing or damaged pixels in images. Recent advancements with
Generative Adversarial Networks (GANs) have demonstrated superior performance
over traditional methods due to their deep learning capabilities and
adaptability across diverse image domains. Residual Networks (ResNet) have also
gained prominence for their ability to enhance feature representation and
compatibility with other architectures. This paper introduces a novel
architecture combining GAN and ResNet models to improve image inpainting
outcomes. Our framework integrates three components: Transpose
Convolution-based GAN for guided and blind inpainting, Fast
ResNet-Convolutional Neural Network (FR-CNN) for object removal, and
Co-Modulation GAN (Co-Mod GAN) for refinement. The model's performance was
evaluated on benchmark datasets, achieving accuracies of 96.59% on Image-Net,
96.70% on Places2, and 96.16% on CelebA. Comparative analyses demonstrate that
the proposed architecture outperforms existing methods, highlighting its
effectiveness in both qualitative and quantitative evaluations.",2024-12-25T22:54:28Z,http://arxiv.org/abs/2412.19000v1,"Nafiz Al Asad, Md. Appel Mahmud Pranto, Shbiruzzaman Shiam, Musaddeq Mahmud Akand, Mohammad Abu Yousuf, Khondokar Fida Hasan, Mohammad Ali Moni"
"GeoMatch++: Morphology Conditioned Geometry Matching for
  Multi-Embodiment Grasping","Despite recent progress on multi-finger dexterous grasping, current methods
focus on single grippers and unseen objects, and even the ones that explore
cross-embodiment, often fail to generalize well to unseen end-effectors. This
work addresses the problem of dexterous grasping generalization to unseen
end-effectors via a unified policy that learns correlation between gripper
morphology and object geometry. Robot morphology contains rich information
representing how joints and links connect and move with respect to each other
and thus, we leverage it through attention to learn better end-effector
geometry features. Our experiments show an average of 9.64% increase in grasp
success rate across 3 out-of-domain end-effectors compared to previous methods.",2024-12-25T22:36:57Z,http://arxiv.org/abs/2412.18998v1,"Yunze Wei, Maria Attarian, Igor Gilitschenski"
"WaveDiffUR: A diffusion SDE-based solver for ultra magnification
  super-resolution in remote sensing images","Deep neural networks have recently achieved significant advancements in
remote sensing superresolu-tion (SR). However, most existing methods are
limited to low magnification rates (e.g., 2 or 4) due to the escalating
ill-posedness at higher magnification scales. To tackle this challenge, we
redefine high-magnification SR as the ultra-resolution (UR) problem, reframing
it as solving a conditional diffusion stochastic differential equation (SDE).
In this context, we propose WaveDiffUR, a novel wavelet-domain diffusion UR
solver that decomposes the UR process into sequential sub-processes addressing
conditional wavelet components. WaveDiffUR iteratively reconstructs
low-frequency wavelet details (ensuring global consistency) and high-frequency
components (enhancing local fidelity) by incorporating pre-trained SR models as
plug-and-play modules. This modularity mitigates the ill-posedness of the SDE
and ensures scalability across diverse applications. To address limitations in
fixed boundary conditions at extreme magnifications, we introduce the
cross-scale pyramid (CSP) constraint, a dynamic and adaptive framework that
guides WaveDiffUR in generating fine-grained wavelet details, ensuring
consistent and high-fidelity outputs even at extreme magnification rates.",2024-12-25T22:26:39Z,http://arxiv.org/abs/2412.18996v1,"Yue Shi, Liangxiu Han, Darren Dancy, Lianghao Han"
"MiTREE: Multi-input Transformer Ecoregion Encoder for Species
  Distribution Modelling","Climate change poses an extreme threat to biodiversity, making it imperative
to efficiently model the geographical range of different species. The
availability of large-scale remote sensing images and environmental data has
facilitated the use of machine learning in Species Distribution Models (SDMs),
which aim to predict the presence of a species at any given location.
Traditional SDMs, reliant on expert observation, are labor-intensive, but
advancements in remote sensing and citizen science data have facilitated
machine learning approaches to SDM development. However, these models often
struggle with leveraging spatial relationships between different inputs -- for
instance, learning how climate data should inform the data present in satellite
imagery -- without upsampling or distorting the original inputs. Additionally,
location information and ecological characteristics at a location play a
crucial role in predicting species distribution models, but these aspects have
not yet been incorporated into state-of-the-art approaches. In this work, we
introduce MiTREE: a multi-input Vision-Transformer-based model with an
ecoregion encoder. MiTREE computes spatial cross-modal relationships without
upsampling as well as integrates location and ecological context. We evaluate
our model on the SatBird Summer and Winter datasets, the goal of which is to
predict bird species encounter rates, and we find that our approach improves
upon state-of-the-art baselines.",2024-12-25T22:20:47Z,http://arxiv.org/abs/2412.18995v1,"Theresa Chen, Yao-Yi Chiang"
"Geospatial Data Fusion: Combining Lidar, SAR, and Optical Imagery with
  AI for Enhanced Urban Mapping","This study explores the integration of Lidar, Synthetic Aperture Radar (SAR),
and optical imagery through advanced artificial intelligence techniques for
enhanced urban mapping. By fusing these diverse geospatial datasets, we aim to
overcome the limitations associated with single-sensor data, achieving a more
comprehensive representation of urban environments. The research employs Fully
Convolutional Networks (FCNs) as the primary deep learning model for urban
feature extraction, enabling precise pixel-wise classification of essential
urban elements, including buildings, roads, and vegetation. To optimize the
performance of the FCN model, we utilize Particle Swarm Optimization (PSO) for
hyperparameter tuning, significantly enhancing model accuracy. Key findings
indicate that the FCN-PSO model achieved a pixel accuracy of 92.3% and a mean
Intersection over Union (IoU) of 87.6%, surpassing traditional single-sensor
approaches. These results underscore the potential of fused geospatial data and
AI-driven methodologies in urban mapping, providing valuable insights for urban
planning and management. The implications of this research pave the way for
future developments in real-time mapping and adaptive urban infrastructure
planning.",2024-12-25T22:17:31Z,http://arxiv.org/abs/2412.18994v1,"Sajjad Afroosheh, Mohammadreza Askari"
"On the architecture of the Symplectic $(A_\infty,2)$-Category","This note relates to the author's construction of the Symplectic
$(A_\infty,2)$-Category, $\mathsf{Symp}$. Here we explain two ways of encoding
the information in $\mathsf{Symp}$, one topological, one algebraic. The
topological encoding is as an $(A_\infty,2)$-flow category, which we define
here. The algebraic encoding is as a linear $(A_\infty,2)$-category, which we
extract from the topological encoding. In upcoming work, the author and
Wehrheim plan to use the adiabatic Fredholm theory recently developed by
Bottman-Wehrheim to construct $\mathsf{Symp}$ as an $(A_\infty,2)$-flow
category.
  The definition of linear $(A_\infty,2)$-category that we give in this note is
different than the one proposed by Bottman-Carmeli. The recursive structure of
the 2-associahedra identifies faces with fiber products of 2-associahedra over
associahedra, and these fiber products led Bottman-Carmeli to associate
operations to singular chains on 2-associahedra. The innovation in our new
definition of linear $(A_\infty,2)$-category is to extend the family of
2-associahedra to include all fiber products of 2-associahedra over
associahedra. This allows us to associate operations to cellular chains, which
in particular enables us to produce a definition that involves only one
operation in each arity, governed by a collection of $(A_\infty,2)$-equations.",2024-12-25T22:11:18Z,http://arxiv.org/abs/2412.18993v1,Nathaniel Bottman
"Optimal Federated Learning for Functional Mean Estimation under
  Heterogeneous Privacy Constraints","Federated learning (FL) is a distributed machine learning technique designed
to preserve data privacy and security, and it has gained significant importance
due to its broad range of applications. This paper addresses the problem of
optimal functional mean estimation from discretely sampled data in a federated
setting.
  We consider a heterogeneous framework where the number of individuals,
measurements per individual, and privacy parameters vary across one or more
servers, under both common and independent design settings. In the common
design setting, the same design points are measured for each individual,
whereas in the independent design, each individual has their own random
collection of design points. Within this framework, we establish minimax upper
and lower bounds for the estimation error of the underlying mean function,
highlighting the nuanced differences between common and independent designs
under distributed privacy constraints.
  We propose algorithms that achieve the optimal trade-off between privacy and
accuracy and provide optimality results that quantify the fundamental limits of
private functional mean estimation across diverse distributed settings. These
results characterize the cost of privacy and offer practical insights into the
potential for privacy-preserving statistical analysis in federated
environments.",2024-12-25T22:06:12Z,http://arxiv.org/abs/2412.18992v1,"Tony Cai, Abhinav Chakraborty, Lasse Vuursteen"
"Detection and classification of DDoS flooding attacks by machine
  learning method","This study focuses on a method for detecting and classifying distributed
denial of service (DDoS) attacks, such as SYN Flooding, ACK Flooding, HTTP
Flooding, and UDP Flooding, using neural networks. Machine learning,
particularly neural networks, is highly effective in detecting malicious
traffic. A dataset containing normal traffic and various DDoS attacks was used
to train a neural network model with a 24-106-5 architecture. The model
achieved high Accuracy (99.35%), Precision (99.32%), Recall (99.54%), and
F-score (0.99) in the classification task. All major attack types were
correctly identified. The model was also further tested in the lab using
virtual infrastructures to generate normal and DDoS traffic. The results showed
that the model can accurately classify attacks under near-real-world
conditions, demonstrating 95.05% accuracy and balanced F-score scores for all
attack types. This confirms that neural networks are an effective tool for
detecting DDoS attacks in modern information security systems.",2024-12-25T21:58:52Z,http://arxiv.org/abs/2412.18990v1,"Dmytro Tymoshchuk, Oleh Yasniy, Mykola Mytnyk, Nataliya Zagorodna, Vitaliy Tymoshchuk"
"MTCAE-DFER: Multi-Task Cascaded Autoencoder for Dynamic Facial
  Expression Recognition","This paper expands the cascaded network branch of the autoencoder-based
multi-task learning (MTL) framework for dynamic facial expression recognition,
namely Multi-Task Cascaded Autoencoder for Dynamic Facial Expression
Recognition (MTCAE-DFER). MTCAE-DFER builds a plug-and-play cascaded decoder
module, which is based on the Vision Transformer (ViT) architecture and employs
the decoder concept of Transformer to reconstruct the multi-head attention
module. The decoder output from the previous task serves as the query (Q),
representing local dynamic features, while the Video Masked Autoencoder
(VideoMAE) shared encoder output acts as both the key (K) and value (V),
representing global dynamic features. This setup facilitates interaction
between global and local dynamic features across related tasks. Additionally,
this proposal aims to alleviate overfitting of complex large model. We utilize
autoencoder-based multi-task cascaded learning approach to explore the impact
of dynamic face detection and dynamic face landmark on dynamic facial
expression recognition, which enhances the model's generalization ability.
After we conduct extensive ablation experiments and comparison with
state-of-the-art (SOTA) methods on various public datasets for dynamic facial
expression recognition, the robustness of the MTCAE-DFER model and the
effectiveness of global-local dynamic feature interaction among related tasks
have been proven.",2024-12-25T21:52:31Z,http://arxiv.org/abs/2412.18988v1,"Peihao Xiang, Kaida Wu, Chaohao Lin, Ou Bai"
"Deep Learning-Based Traffic-Aware Base Station Sleep Mode and Cell
  Zooming Strategy in RIS-Aided Multi-Cell Networks","Advances in wireless technology have significantly increased the number of
wireless connections, leading to higher energy consumption in networks. Among
these, base stations (BSs) in radio access networks (RANs) account for over
half of the total energy usage. To address this, we propose a multi-cell sleep
strategy combined with adaptive cell zooming, user association, and
reconfigurable intelligent surface (RIS) to minimize BS energy consumption.
This approach allows BSs to enter sleep during low traffic, while adaptive cell
zooming and user association dynamically adjust coverage to balance traffic
load and enhance data rates through RIS, minimizing the number of active BSs.
However, it is important to note that the proposed method may achieve
energy-savings at the cost of increased delay, requiring a trade-off between
these two factors. Moreover, minimizing BS energy consumption under the delay
constraint is a complicated non-convex problem. To address this issue, we model
the RIS-aided multi-cell network as a Markov decision process (MDP) and use the
proximal policy optimization (PPO) algorithm to optimize sleep mode (SM), cell
zooming, and user association. Besides, we utilize a double cascade correlation
network (DCCN) algorithm to optimize the RIS reflection coefficients.
Simulation results demonstrate that PPO balances energy-savings and delay,
while DCCN-optimized RIS enhances BS energy-savings. Compared to systems
optimised by the benchmark DQN algorithm, energy consumption is reduced by
49.61%",2024-12-25T21:06:40Z,http://arxiv.org/abs/2412.18983v1,"Shuo Sun, Chong Huang, Gaojie Chen, Pei Xiao, Rahim Tafazolli"
"HAND: Hierarchical Attention Network for Multi-Scale Handwritten
  Document Recognition and Layout Analysis","Handwritten document recognition (HDR) is one of the most challenging tasks
in the field of computer vision, due to the various writing styles and complex
layouts inherent in handwritten texts. Traditionally, this problem has been
approached as two separate tasks, handwritten text recognition and layout
analysis, and struggled to integrate the two processes effectively. This paper
introduces HAND (Hierarchical Attention Network for Multi-Scale Document), a
novel end-to-end and segmentation-free architecture for simultaneous text
recognition and layout analysis tasks. Our model's key components include an
advanced convolutional encoder integrating Gated Depth-wise Separable and
Octave Convolutions for robust feature extraction, a Multi-Scale Adaptive
Processing (MSAP) framework that dynamically adjusts to document complexity and
a hierarchical attention decoder with memory-augmented and sparse attention
mechanisms. These components enable our model to scale effectively from
single-line to triple-column pages while maintaining computational efficiency.
Additionally, HAND adopts curriculum learning across five complexity levels. To
improve the recognition accuracy of complex ancient manuscripts, we fine-tune
and integrate a Domain-Adaptive Pre-trained mT5 model for post-processing
refinement. Extensive evaluations on the READ 2016 dataset demonstrate the
superior performance of HAND, achieving up to 59.8% reduction in CER for
line-level recognition and 31.2% for page-level recognition compared to
state-of-the-art methods. The model also maintains a compact size of 5.60M
parameters while establishing new benchmarks in both text recognition and
layout analysis. Source code and pre-trained models are available at :
https://github.com/MHHamdan/HAND.",2024-12-25T20:36:29Z,http://arxiv.org/abs/2412.18981v1,"Mohammed Hamdan, Abderrahmane Rahiche, Mohamed Cheriet"
"Evaluating deep learning models for fault diagnosis of a rotating
  machinery with epistemic and aleatoric uncertainty","Uncertainty-aware deep learning (DL) models recently gained attention in
fault diagnosis as a way to promote the reliable detection of faults when
out-of-distribution (OOD) data arise from unseen faults (epistemic uncertainty)
or the presence of noise (aleatoric uncertainty). In this paper, we present the
first comprehensive comparative study of state-of-the-art uncertainty-aware DL
architectures for fault diagnosis in rotating machinery, where different
scenarios affected by epistemic uncertainty and different types of aleatoric
uncertainty are investigated. The selected architectures include sampling by
dropout, Bayesian neural networks, and deep ensembles. Moreover, to distinguish
between in-distribution and OOD data in the different scenarios two uncertainty
thresholds, one of which is introduced in this paper, are alternatively
applied. Our empirical findings offer guidance to practitioners and researchers
who have to deploy real-world uncertainty-aware fault diagnosis systems. In
particular, they reveal that, in the presence of epistemic uncertainty, all DL
models are capable of effectively detecting, on average, a substantial portion
of OOD data across all the scenarios. However, deep ensemble models show
superior performance, independently of the uncertainty threshold used for
discrimination. In the presence of aleatoric uncertainty, the noise level plays
an important role. Specifically, low noise levels hinder the models' ability to
effectively detect OOD data. Even in this case, however, deep ensemble models
exhibit a milder degradation in performance, dominating the others. These
achievements, combined with their shorter inference time, make deep ensemble
architectures the preferred choice.",2024-12-25T20:22:59Z,http://arxiv.org/abs/2412.18980v1,"Reza Jalayer, Masoud Jalayer, Andrea Mor, Carlotta Orsenigo, Carlo Vercellis"
Quantum memristors for neuromorphic quantum machine learning,"Quantum machine learning may permit to realize more efficient machine
learning calculations with near-term quantum devices. Among the diverse quantum
machine learning paradigms which are currently being considered, quantum
memristors are promising as a way of combining, in the same quantum hardware, a
unitary evolution with the nonlinearity provided by the measurement and
feedforward. Thus, an efficient way of deploying neuromorphic quantum computing
for quantum machine learning may be enabled.",2024-12-25T20:21:24Z,http://arxiv.org/abs/2412.18979v1,Lucas Lamata
CGCOD: Class-Guided Camouflaged Object Detection,"Camouflaged Object Detection (COD) is designed to identify objects that blend
seamlessly with their surroundings. Due to the complexity of camouflaged
objects (such as shape, color, and texture), their semantic cues are often
blurred or completely lost, posing a significant challenge for COD. Existing
COD methods often rely on visual features, which are not stable enough in
changeable camouflage environments. This instability leads to false positives
and false negatives, resulting in incomplete or inaccurate segmentation
results. In this paper, to solve this problem, we propose a new task,
Class-Guided Camouflaged Object Detection (CG-COD), which extends the
traditional COD task by introducing object class knowledge, significantly
improving the robustness and segmentation accuracy of the model in complex
environments. Toward this end, we construct a dataset, CamoClass, containing
the camouflaged objects in the real scenes and their corresponding class
annotation. Based on this, we propose a multi-stage framework CGNet which
consists of a plug-and-play class prompt generator and a class-guided detector.
Under the guidance of textual information, CGNet enables efficient
segmentation. It is worth emphasizing that for the first time, we extend the
object class annotations on existing COD benchmark datasets, and introduce a
flexible framework to improve the performance of the existing COD model under
text guidance.",2024-12-25T19:38:32Z,http://arxiv.org/abs/2412.18977v1,"Chenxi Zhang, Qing Zhang, Jiayun Wu, Youwei Pang"
Injecting Bias into Text Classification Models using Backdoor Attacks,"The rapid growth of natural language processing (NLP) and pre-trained
language models have enabled accurate text classification in a variety of
settings. However, text classification models are susceptible to backdoor
attacks, where an attacker embeds a trigger into the victim model to make the
model predict attacker-desired labels in targeted scenarios. In this paper, we
propose to utilize backdoor attacks for a new purpose: bias injection. We
develop a backdoor attack in which a subset of the training dataset is poisoned
to associate strong male actors with negative sentiment. We execute our attack
on two popular text classification datasets (IMDb and SST) and seven different
models ranging from traditional Doc2Vec-based models to LSTM networks and
modern transformer-based BERT and RoBERTa models. Our results show that the
reduction in backdoored models' benign classification accuracy is limited,
implying that our attacks remain stealthy, whereas the models successfully
learn to associate strong male actors with negative sentiment (100% attack
success rate with &gt;= 3% poison rate). Attacks on BERT and RoBERTa are
particularly more stealthy and effective, demonstrating an increased risk of
using modern and larger models. We also measure the generalizability of our
bias injection by proposing two metrics: (i) U-BBSR which uses previously
unseen words when measuring attack success, and (ii) P-BBSR which measures
attack success using paraphrased test samples. U-BBSR and P-BBSR results show
that the bias injected by our attack can go beyond memorizing a trigger phrase.",2024-12-25T19:32:02Z,http://arxiv.org/abs/2412.18975v1,"A. Dilara Yavuz, M. Emre Gursoy"
"Derandomized shallow shadows: Efficient Pauli learning with
  bounded-depth circuits","Efficiently estimating large numbers of non-commuting observables is an
important subroutine of many quantum science tasks. We present the derandomized
shallow shadows (DSS) algorithm for efficiently learning a large set of
non-commuting observables, using shallow circuits to rotate into measurement
bases. Exploiting tensor network techniques to ensure polynomial scaling of
classical resources, our algorithm outputs a set of shallow measurement
circuits that approximately minimizes the sample complexity of estimating a
given set of Pauli strings. We numerically demonstrate systematic improvement,
in comparison with state-of-the-art techniques, for energy estimation of
quantum chemistry benchmarks and verification of quantum many-body systems, and
we observe DSS's performance consistently improves as one allows deeper
measurement circuits. These results indicate that in addition to being an
efficient, low-depth, stand-alone algorithm, DSS can also benefit many larger
quantum algorithms requiring estimation of multiple non-commuting observables.",2024-12-25T19:23:29Z,http://arxiv.org/abs/2412.18973v1,"Katherine Van Kirk, Christian Kokail, Jonathan Kunjummen, Hong-Ye Hu, Yanting Teng, Madelyn Cain, Jacob Taylor, Susanne F. Yelin, Hannes Pichler, Mikhail Lukin"
Recommending Pre-Trained Models for IoT Devices,"The availability of pre-trained models (PTMs) has enabled faster deployment
of machine learning across applications by reducing the need for extensive
training. Techniques like quantization and distillation have further expanded
PTM applicability to resource-constrained IoT hardware. Given the many PTM
options for any given task, engineers often find it too costly to evaluate each
model's suitability. Approaches such as LogME, LEEP, and ModelSpider help
streamline model selection by estimating task relevance without exhaustive
tuning. However, these methods largely leave hardware constraints as future
work-a significant limitation in IoT settings. In this paper, we identify the
limitations of current model recommendation approaches regarding hardware
constraints and introduce a novel, hardware-aware method for PTM selection. We
also propose a research agenda to guide the development of effective,
hardware-conscious model recommendation systems for IoT applications.",2024-12-25T19:19:55Z,http://arxiv.org/abs/2412.18972v1,"Parth V. Patil, Wenxin Jiang, Huiyun Peng, Daniel Lugo, Kelechi G. Kalu, Josh LeBlanc, Lawrence Smith, Hyeonwoo Heo, Nathanael Aou, James C. Davis"
"Adopting Trustworthy AI for Sleep Disorder Prediction: Deep Time Series
  Analysis with Temporal Attention Mechanism and Counterfactual Explanations","Sleep disorders have a major impact on both lifestyle and health. Effective
sleep disorder prediction from lifestyle and physiological data can provide
essential details for early intervention. This research utilizes three deep
time series models and facilitates them with explainability approaches for
sleep disorder prediction. Specifically, our approach adopts Temporal
Convolutional Networks (TCN), Long Short-Term Memory (LSTM) for time series
data analysis, and Temporal Fusion Transformer model (TFT). Meanwhile, the
temporal attention mechanism and counterfactual explanation with SHapley
Additive exPlanations (SHAP) approach are employed to ensure dependable,
accurate, and interpretable predictions. Finally, using a large dataset of
sleep health measures, our evaluation demonstrates the effect of our method in
predicting sleep disorders.",2024-12-25T19:19:45Z,http://arxiv.org/abs/2412.18971v1,"Pegah Ahadian, Wei Xu, Sherry Wang, Qiang Guan"
"ModelGrow: Continual Text-to-Video Pre-training with Model Expansion and
  Language Understanding Enhancement","Text-to-video (T2V) generation has gained significant attention recently.
However, the costs of training a T2V model from scratch remain persistently
high, and there is considerable room for improving the generation performance,
especially under limited computation resources. This work explores the
continual general pre-training of text-to-video models, enabling the model to
""grow"" its abilities based on a pre-trained foundation, analogous to how humans
acquire new knowledge based on past experiences. There is a lack of extensive
study of the continual pre-training techniques in T2V generation. In this work,
we take the initial step toward exploring this task systematically and propose
ModelGrow. Specifically, we break this task into two key aspects: increasing
model capacity and improving semantic understanding. For model capacity, we
introduce several novel techniques to expand the model size, enabling it to
store new knowledge and improve generation performance. For semantic
understanding, we propose a method that leverages large language models as
advanced text encoders, integrating them into T2V models to enhance language
comprehension and guide generation results according to detailed prompts. This
approach enables the model to achieve better semantic alignment, particularly
in response to complex user prompts. Extensive experiments demonstrate the
effectiveness of our method across various metrics. The source code and the
model of ModelGrow will be publicly available.",2024-12-25T18:58:07Z,http://arxiv.org/abs/2412.18966v1,"Zhefan Rao, Liya Ji, Yazhou Xing, Runtao Liu, Zhaoyang Liu, Jiaxin Xie, Ziqiao Peng, Yingqing He, Qifeng Chen"
"Don't Lose Yourself: Boosting Multimodal Recommendation via Reducing
  Node-neighbor Discrepancy in Graph Convolutional Network","The rapid expansion of multimedia contents has led to the emergence of
multimodal recommendation systems. It has attracted increasing attention in
recommendation systems because its full utilization of data from different
modalities alleviates the persistent data sparsity problem. As such, multimodal
recommendation models can learn personalized information about nodes in terms
of visual and textual. To further alleviate the data sparsity problem, some
previous works have introduced graph convolutional networks (GCNs) for
multimodal recommendation systems, to enhance the semantic representation of
users and items by capturing the potential relationships between them. However,
adopting GCNs inevitably introduces the over-smoothing problem, which make
nodes to be too similar. Unfortunately, incorporating multimodal information
will exacerbate this challenge because nodes that are too similar will lose the
personalized information learned through multimodal information. To address
this problem, we propose a novel model that retains the personalized
information of ego nodes during feature aggregation by Reducing Node-neighbor
Discrepancy (RedN^nD). Extensive experiments on three public datasets show that
RedN^nD achieves state-of-the-art performance on accuracy and robustness, with
significant improvements over existing GCN-based multimodal frameworks.",2024-12-25T18:41:36Z,http://arxiv.org/abs/2412.18962v1,"Zheyu Chen, Jinfeng Xu, Haibo Hu"
"RIS-Assisted Aerial Non-Terrestrial Networks: An Intelligent Synergy
  with Deep Reinforcement Learning","Reconfigurable intelligent surface (RIS)-assisted aerial non-terrestrial
networks (NTNs) offer a promising paradigm for enhancing wireless
communications in the era of 6G and beyond. By integrating RIS with aerial
platforms such as unmanned aerial vehicles (UAVs) and high-altitude platforms
(HAPs), these networks can intelligently control signal propagation, extending
coverage, improving capacity, and enhancing link reliability. This article
explores the application of deep reinforcement learning (DRL) as a powerful
tool for optimizing RIS-assisted aerial NTNs. We focus on hybrid proximal
policy optimization (H-PPO), a robust DRL algorithm well-suited for handling
the complex, hybrid action spaces inherent in these networks. Through a case
study of an aerial RIS (ARIS)-aided coordinated multi-point non-orthogonal
multiple access (CoMP-NOMA) network, we demonstrate how H-PPO can effectively
optimize the system and maximize the sum rate while adhering to system
constraints. Finally, we discuss key challenges and promising research
directions for DRL-powered RIS-assisted aerial NTNs, highlighting their
potential to transform next-generation wireless networks.",2024-12-25T18:11:34Z,http://arxiv.org/abs/2412.18957v1,"Muhammad Umer, Muhammad Ahmed Mohsin, Aryan Kaushik, Qurrat-ul-Ain Nadeem, Ali Arshad Nasir, Syed Ali Hassan"
"Leave-One-EquiVariant: Alleviating invariance-related information loss
  in contrastive music representations","Contrastive learning has proven effective in self-supervised musical
representation learning, particularly for Music Information Retrieval (MIR)
tasks. However, reliance on augmentation chains for contrastive view generation
and the resulting learnt invariances pose challenges when different downstream
tasks require sensitivity to certain musical attributes. To address this, we
propose the Leave One EquiVariant (LOEV) framework, which introduces a
flexible, task-adaptive approach compared to previous work by selectively
preserving information about specific augmentations, allowing the model to
maintain task-relevant equivariances. We demonstrate that LOEV alleviates
information loss related to learned invariances, improving performance on
augmentation related tasks and retrieval without sacrificing general
representation quality. Furthermore, we introduce a variant of LOEV, LOEV++,
which builds a disentangled latent space by design in a self-supervised manner,
and enables targeted retrieval based on augmentation related attributes.",2024-12-25T18:06:44Z,http://arxiv.org/abs/2412.18955v1,"Julien Guinot, Elio Quinton, György Fazekas"
"Engineering whispering gallery modes in MoSe$_2$/WS$_2$ double
  heterostructure nanocavities: Towards developing all-TMDC light sources","Transition metal dichalcogenides (TMDCs) have emerged as highly promising
materials for nanophotonics and optoelectronics due to their exceptionally high
refractive indices, strong excitonic photoluminescence (PL) in monolayer
configurations, and the versatility to engineer van der Waals (vdW)
heterostructures. In this work, we exploit the intense excitonic PL of a
MoSe$_2$ monolayer combined with the high refractive index of bulk WS$_2$ to
fabricate microdisk cavities with tunable light emission characteristics. These
microdisks are created from a 50-nm-thick WS$_2$/MoSe$_2$/WS$_2$ double
heterostructure using frictional mechanical scanning probe lithography. The
resulting cavities achieve a 4-10-fold enhancement in excitonic PL from the
MoSe$_2$ monolayer at wavelengths near 800 nm. The excitonic PL peak is
modulated by sharp spectral features, which correspond to whispering gallery
modes (WGMs) supported by the cavity. A microdisk with a diameter of 2.35
$\mu$m demonstrates WGMs with a quality factor of up to 700, significantly
surpassing theoretical predictions and suggesting strong potential for lasing
applications. The spectral positions of the WGMs can be finely tuned by
adjusting the microdisk's diameter and thickness, as confirmed by theoretical
calculations. This approach offers a novel route for developing ultra-compact,
all-TMDC double heterostructure light sources with record-small size.",2024-12-25T17:36:35Z,http://arxiv.org/abs/2412.18953v1,"P. A. Alekseev, I. A. Milekhin, K. A. Gasnikova, I. A. Eliseyev, V. Yu. Davydov, A. A. Bogdanov, V. Kravtsov, A. O. Mikhin, B. R. Borodin, A. G. Milekhin"
"Bridging Interpretability and Robustness Using LIME-Guided Model
  Refinement","This paper explores the intricate relationship between interpretability and
robustness in deep learning models. Despite their remarkable performance across
various tasks, deep learning models often exhibit critical vulnerabilities,
including susceptibility to adversarial attacks, over-reliance on spurious
correlations, and a lack of transparency in their decision-making processes. To
address these limitations, we propose a novel framework that leverages Local
Interpretable Model-Agnostic Explanations (LIME) to systematically enhance
model robustness. By identifying and mitigating the influence of irrelevant or
misleading features, our approach iteratively refines the model, penalizing
reliance on these features during training. Empirical evaluations on multiple
benchmark datasets demonstrate that LIME-guided refinement not only improves
interpretability but also significantly enhances resistance to adversarial
perturbations and generalization to out-of-distribution data.",2024-12-25T17:32:45Z,http://arxiv.org/abs/2412.18952v1,"Navid Nayyem, Abdullah Rakin, Longwei Wang"
"TopoBDA: Towards Bezier Deformable Attention for Road Topology
  Understanding","Understanding road topology is crucial for autonomous driving. This paper
introduces TopoBDA (Topology with Bezier Deformable Attention), a novel
approach that enhances road topology understanding by leveraging Bezier
Deformable Attention (BDA). BDA utilizes Bezier control points to drive the
deformable attention mechanism, significantly improving the detection and
representation of elongated and thin polyline structures, such as lane
centerlines. TopoBDA processes multi-camera 360-degree imagery to generate
Bird's Eye View (BEV) features, which are refined through a transformer decoder
employing BDA. This method enhances computational efficiency while maintaining
high accuracy in centerline prediction. Additionally, TopoBDA incorporates an
instance mask formulation and an auxiliary one-to-many set prediction loss
strategy to further refine centerline detection and improve road topology
understanding. Experimental evaluations on the OpenLane-V2 dataset demonstrate
that TopoBDA outperforms existing methods, achieving state-of-the-art results
in centerline detection and topology reasoning. The integration of multi-modal
data, including lidar and radar, specifically for road topology understanding,
further enhances the model's performance, underscoring its importance in
autonomous driving applications.",2024-12-25T17:31:54Z,http://arxiv.org/abs/2412.18951v1,"Muhammet Esat Kalfaoglu, Halil Ibrahim Ozturk, Ozsel Kilinc, Alptekin Temizel"
"MedHallBench: A New Benchmark for Assessing Hallucination in Medical
  Large Language Models","Medical Large Language Models (MLLMs) have demonstrated potential in
healthcare applications, yet their propensity for hallucinations -- generating
medically implausible or inaccurate information -- presents substantial risks
to patient care. This paper introduces MedHallBench, a comprehensive benchmark
framework for evaluating and mitigating hallucinations in MLLMs. Our
methodology integrates expert-validated medical case scenarios with established
medical databases to create a robust evaluation dataset. The framework employs
a sophisticated measurement system that combines automated ACHMI (Automatic
Caption Hallucination Measurement in Medical Imaging) scoring with rigorous
clinical expert evaluations and utilizes reinforcement learning methods to
achieve automatic annotation. Through an optimized reinforcement learning from
human feedback (RLHF) training pipeline specifically designed for medical
applications, MedHallBench enables thorough evaluation of MLLMs across diverse
clinical contexts while maintaining stringent accuracy standards. We conducted
comparative experiments involving various models, utilizing the benchmark to
establish a baseline for widely adopted large language models (LLMs). Our
findings indicate that ACHMI provides a more nuanced understanding of the
effects of hallucinations compared to traditional metrics, thereby highlighting
its advantages in hallucination assessment. This research establishes a
foundational framework for enhancing MLLMs' reliability in healthcare settings
and presents actionable strategies for addressing the critical challenge of AI
hallucinations in medical applications.",2024-12-25T16:51:29Z,http://arxiv.org/abs/2412.18947v1,"Kaiwen Zuo, Yirui Jiang"
"Constraint-Adaptive Policy Switching for Offline Safe Reinforcement
  Learning","Offline safe reinforcement learning (OSRL) involves learning a
decision-making policy to maximize rewards from a fixed batch of training data
to satisfy pre-defined safety constraints. However, adapting to varying safety
constraints during deployment without retraining remains an under-explored
challenge. To address this challenge, we introduce constraint-adaptive policy
switching (CAPS), a wrapper framework around existing offline RL algorithms.
During training, CAPS uses offline data to learn multiple policies with a
shared representation that optimize different reward and cost trade-offs.
During testing, CAPS switches between those policies by selecting at each state
the policy that maximizes future rewards among those that satisfy the current
cost constraint. Our experiments on 38 tasks from the DSRL benchmark
demonstrate that CAPS consistently outperforms existing methods, establishing a
strong wrapper-based baseline for OSRL. The code is publicly available at
https://github.com/yassineCh/CAPS.",2024-12-25T16:42:27Z,http://arxiv.org/abs/2412.18946v1,"Yassine Chemingui, Aryan Deshwal, Honghao Wei, Alan Fern, Janardhan Rao Doppa"
Amuse: Human-AI Collaborative Songwriting with Multimodal Inspirations,"Songwriting is often driven by multimodal inspirations, such as imagery,
narratives, or existing music, yet songwriters remain unsupported by current
music AI systems in incorporating these multimodal inputs into their creative
processes. We introduce Amuse, a songwriting assistant that transforms
multimodal (image, text, or audio) inputs into chord progressions that can be
seamlessly incorporated into songwriters' creative processes. A key feature of
Amuse is its novel method for generating coherent chords that are relevant to
music keywords in the absence of datasets with paired examples of multimodal
inputs and chords. Specifically, we propose a method that leverages multimodal
large language models (LLMs) to convert multimodal inputs into noisy chord
suggestions and uses a unimodal chord model to filter the suggestions. A user
study with songwriters shows that Amuse effectively supports transforming
multimodal ideas into coherent musical suggestions, enhancing users' agency and
creativity throughout the songwriting process.",2024-12-25T16:23:32Z,http://arxiv.org/abs/2412.18940v1,"Yewon Kim, Sung-Ju Lee, Chris Donahue"
"Label-free SERS Discrimination of Proline from Hydroxylated Proline at
  Single-molecule Level Assisted by a Deep Learning Model","Discriminating the low-abundance hydroxylated proline from hydroxylated
proline is crucial for monitoring diseases and eval-uating therapeutic outcomes
that require single-molecule sensors. While the plasmonic nanopore sensor can
detect the hydrox-ylation with single-molecule sensitivity by surface enhanced
Raman spectroscopy (SERS), it suffers from intrinsic fluctuations of
single-molecule signals as well as strong interference from citrates. Here, we
used the occurrence frequency histogram of the single-molecule SERS peaks to
extract overall dataset spectral features, overcome the signal fluctuations and
investigate the citrate-replaced plasmonic nanopore sensors for clean and
distinguishable signals of proline and hydroxylated proline. By ligand exchange
of the citrates by analyte molecules, the representative peaks of citrates
decreased with incubation time, prov-ing occupation of the plasmonic hot spot
by the analytes. As a result, the discrimination of the single-molecule SERS
signals of proline and hydroxylated proline was possible with the convolutional
neural network model with 96.6% accuracy.",2024-12-25T15:46:52Z,http://arxiv.org/abs/2412.18935v1,"Yingqi Zhao, Kuo Zhan, Pei-Lin Xin, Zuyan Chen, Shuai Li, Francesco De Angelis, Jianan Huang"
Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference,"Due to the high resource demands of Large Language Models (LLMs), achieving
widespread deployment on consumer-grade devices presents significant
challenges. Typically, personal or consumer-grade devices, including servers
configured prior to the era of large-scale models, generally have relatively
weak GPUs and relatively strong CPUs. However, most current methods primarily
depend on GPUs for computation. Therefore, we propose Dovetail, an approach
that deploys the draft model on the GPU to generate draft tokens while allowing
the target model to perform parallel verification on the CPU, thereby improving
the utilization of all available hardware resources and occupying less
inter-device communication bandwidth. Accordingly, we have redesigned the draft
model to better align with heterogeneous hardware characteristics. To this end,
we implemented several optimizations: reducing the number of draft tokens to
mitigate latency in parallel verification, increasing the depth of the draft
model to enhance its predictive capacity, and introducing DGF (Dynamic Gating
Fusion) to improve the integration of features and token embeddings. In the
HumanEval benchmark, Dovetail achieved an inference speed of 5.86 tokens per
second for LLaMA2-Chat-7B using 3GB of VRAM, representing an approximately
2.77x improvement over CPU-only inference. Furthermore, the inference speed was
increased to 8 tokens per second when utilizing 7GB of VRAM.",2024-12-25T15:45:18Z,http://arxiv.org/abs/2412.18934v1,"Libo Zhang, Zhaoning Zhang, Baizhou Xu, Songzhu Mei, Dongsheng Li"
TINQ: Temporal Inconsistency Guided Blind Video Quality Assessment,"Blind video quality assessment (BVQA) has been actively researched for
user-generated content (UGC) videos. Recently, super-resolution (SR) techniques
have been widely applied in UGC. Therefore, an effective BVQA method for both
UGC and SR scenarios is essential. Temporal inconsistency, referring to
irregularities between consecutive frames, is relevant to video quality.
Current BVQA approaches typically model temporal relationships in UGC videos
using statistics of motion information, but inconsistencies remain unexplored.
Additionally, different from temporal inconsistency in UGC videos, such
inconsistency in SR videos is amplified due to upscaling algorithms. In this
paper, we introduce the Temporal Inconsistency Guided Blind Video Quality
Assessment (TINQ) metric, demonstrating that exploring temporal inconsistency
is crucial for effective BVQA. Since temporal inconsistencies vary between UGC
and SR videos, they are calculated in different ways. Based on this, a spatial
module highlights inconsistent areas across consecutive frames at coarse and
fine granularities. In addition, a temporal module aggregates features over
time in two stages. The first stage employs a visual memory capacity block to
adaptively segment the time dimension based on estimated complexity, while the
second stage focuses on selecting key features. The stages work together
through Consistency-aware Fusion Units to regress cross-time-scale video
quality. Extensive experiments on UGC and SR video quality datasets show that
our method outperforms existing state-of-the-art BVQA methods. Code is
available at https://github.com/Lighting-YXLI/TINQ.",2024-12-25T15:43:41Z,http://arxiv.org/abs/2412.18933v1,"Yixiao Li, Xiaoyuan Yang, Weide Liu, Xin Jin, Xu Jia, Yukun Lai, Haotao Liu, Paul L Rosin, Wei Zhou"
"Malware Classification using a Hybrid Hidden Markov Model-Convolutional
  Neural Network","The proliferation of malware variants poses a significant challenges to
traditional malware detection approaches, such as signature-based methods,
necessitating the development of advanced machine learning techniques. In this
research, we present a novel approach based on a hybrid architecture combining
features extracted using a Hidden Markov Model (HMM), with a Convolutional
Neural Network (CNN) then used for malware classification. Inspired by the
strong results in previous work using an HMM-Random Forest model, we propose
integrating HMMs, which serve to capture sequential patterns in opcode
sequences, with CNNs, which are adept at extracting hierarchical features. We
demonstrate the effectiveness of our approach on the popular Malicia dataset,
and we obtain superior performance, as compared to other machine learning
methods -- our results surpass the aforementioned HMM-Random Forest model. Our
findings underscore the potential of hybrid HMM-CNN architectures in bolstering
malware classification capabilities, offering several promising avenues for
further research in the field of cybersecurity.",2024-12-25T15:34:57Z,http://arxiv.org/abs/2412.18932v1,"Ritik Mehta, Olha Jureckova, Mark Stamp"
"Graph Cut-guided Maximal Coding Rate Reduction for Learning Image
  Embedding and Clustering","In the era of pre-trained models, image clustering task is usually addressed
by two relevant stages: a) to produce features from pre-trained vision models;
and b) to find clusters from the pre-trained features. However, these two
stages are often considered separately or learned by different paradigms,
leading to suboptimal clustering performance. In this paper, we propose a
unified framework, termed graph Cut-guided Maximal Coding Rate Reduction
(CgMCR$^2$), for jointly learning the structured embeddings and the clustering.
To be specific, we attempt to integrate an efficient clustering module into the
principled framework for learning structured representation, in which the
clustering module is used to provide partition information to guide the
cluster-wise compression and the learned embeddings is aligned to desired
geometric structures in turn to help for yielding more accurate partitions. We
conduct extensive experiments on both standard and out-of-domain image datasets
and experimental results validate the effectiveness of our approach.",2024-12-25T15:20:54Z,http://arxiv.org/abs/2412.18930v1,"W. He, Z. Huang, X. Meng, X. Qi, R. Xiao, C. -G. Li"
"UNIC-Adapter: Unified Image-instruction Adapter with Multi-modal
  Transformer for Image Generation","Recently, text-to-image generation models have achieved remarkable
advancements, particularly with diffusion models facilitating high-quality
image synthesis from textual descriptions. However, these models often struggle
with achieving precise control over pixel-level layouts, object appearances,
and global styles when using text prompts alone. To mitigate this issue,
previous works introduce conditional images as auxiliary inputs for image
generation, enhancing control but typically necessitating specialized models
tailored to different types of reference inputs. In this paper, we explore a
new approach to unify controllable generation within a single framework.
Specifically, we propose the unified image-instruction adapter (UNIC-Adapter)
built on the Multi-Modal-Diffusion Transformer architecture, to enable flexible
and controllable generation across diverse conditions without the need for
multiple specialized models. Our UNIC-Adapter effectively extracts multi-modal
instruction information by incorporating both conditional images and task
instructions, injecting this information into the image generation process
through a cross-attention mechanism enhanced by Rotary Position Embedding.
Experimental results across a variety of tasks, including pixel-level spatial
control, subject-driven image generation, and style-image-based image
synthesis, demonstrate the effectiveness of our UNIC-Adapter in unified
controllable image generation.",2024-12-25T15:19:02Z,http://arxiv.org/abs/2412.18928v1,"Lunhao Duan, Shanshan Zhao, Wenjun Yan, Yinglun Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Mingming Gong, Gui-Song Xia"
Exemplar-condensed Federated Class-incremental Learning,"We propose Exemplar-Condensed federated class-incremental learning (ECoral)
to distil the training characteristics of real images from streaming data into
informative rehearsal exemplars. The proposed method eliminates the limitations
of exemplar selection in replay-based approaches for mitigating catastrophic
forgetting in federated continual learning (FCL). The limitations particularly
related to the heterogeneity of information density of each summarized data.
Our approach maintains the consistency of training gradients and the
relationship to past tasks for the summarized exemplars to represent the
streaming data compared to the original images effectively. Additionally, our
approach reduces the information-level heterogeneity of the summarized data by
inter-client sharing of the disentanglement generative model. Extensive
experiments show that our ECoral outperforms several state-of-the-art methods
and can be seamlessly integrated with many existing approaches to enhance
performance.",2024-12-25T15:13:40Z,http://arxiv.org/abs/2412.18926v1,"Rui Sun, Yumin Zhang, Varun Ojha, Tejal Shah, Haoran Duan, Bo Wei, Rajiv Ranjan"
"HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs","The breakthrough of OpenAI o1 highlights the potential of enhancing reasoning
to improve LLM. Yet, most research in reasoning has focused on mathematical
tasks, leaving domains like medicine underexplored. The medical domain, though
distinct from mathematics, also demands robust reasoning to provide reliable
answers, given the high standards of healthcare. However, verifying medical
reasoning is challenging, unlike those in mathematics. To address this, we
propose verifiable medical problems with a medical verifier to check the
correctness of model outputs. This verifiable nature enables advancements in
medical reasoning through a two-stage approach: (1) using the verifier to guide
the search for a complex reasoning trajectory for fine-tuning LLMs, (2)
applying reinforcement learning (RL) with verifier-based rewards to enhance
complex reasoning further. Finally, we introduce HuatuoGPT-o1, a medical LLM
capable of complex reasoning, which outperforms general and medical-specific
baselines using only 40K verifiable problems. Experiments show complex
reasoning improves medical problem-solving and benefits more from RL. We hope
our approach inspires advancements in reasoning across medical and other
specialized domains.",2024-12-25T15:12:34Z,http://arxiv.org/abs/2412.18925v1,"Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, Benyou Wang"
"Generative Face Parsing Map Guided 3D Face Reconstruction Under Occluded
  Scenes","Over the past few years, single-view 3D face reconstruction methods can
produce beautiful 3D models. Nevertheless,the input of these works is
unobstructed faces.We describe a system designed to reconstruct convincing face
texture in the case of occlusion.Motivated by parsing facial features,we
propose a complete face parsing map generation method guided by landmarks.We
estimate the 2D face structure of the reasonable position of the occlusion
area,which is used for the construction of 3D texture.An excellent
anti-occlusion face reconstruction method should ensure the authenticity of the
output,including the topological structure between the eyes,nose, and mouth. We
extensively tested our method and its components, qualitatively demonstrating
the rationality of our estimated facial structure. We conduct extensive
experiments on general 3D face reconstruction tasks as concrete examples to
demonstrate the method's superior regulation ability over existing methods
often break down.We further provide numerous quantitative examples showing that
our method advances both the quality and the robustness of 3D face
reconstruction under occlusion scenes.",2024-12-25T14:49:41Z,http://arxiv.org/abs/2412.18920v1,"Dapeng Zhao, Yue Qi"
"An Attentive Dual-Encoder Framework Leveraging Multimodal Visual and
  Semantic Information for Automatic OSAHS Diagnosis","Obstructive sleep apnea-hypopnea syndrome (OSAHS) is a common sleep disorder
caused by upper airway blockage, leading to oxygen deprivation and disrupted
sleep. Traditional diagnosis using polysomnography (PSG) is expensive,
time-consuming, and uncomfortable. Existing deep learning methods using facial
image analysis lack accuracy due to poor facial feature capture and limited
sample sizes. To address this, we propose a multimodal dual encoder model that
integrates visual and language inputs for automated OSAHS diagnosis. The model
balances data using randomOverSampler, extracts key facial features with
attention grids, and converts physiological data into meaningful text.
Cross-attention combines image and text data for better feature extraction, and
ordered regression loss ensures stable learning. Our approach improves
diagnostic efficiency and accuracy, achieving 91.3% top-1 accuracy in a
four-class severity classification task, demonstrating state-of-the-art
performance. Code will be released upon acceptance.",2024-12-25T14:42:17Z,http://arxiv.org/abs/2412.18919v1,"Yingchen Wei, Xihe Qiu, Xiaoyu Tan, Jingjing Huang, Wei Chu, Yinghui Xu, Yuan Qi"
"BCR-Net: Boundary-Category Refinement Network for Weakly Semi-Supervised
  X-Ray Prohibited Item Detection with Points","Automatic prohibited item detection in X-ray images is crucial for public
safety. However, most existing detection methods either rely on expensive box
annotations to achieve high performance or use weak annotations but suffer from
limited accuracy. To balance annotation cost and detection performance, we
study Weakly Semi-Supervised X-ray Prohibited Item Detection with Points
(WSSPID-P) and propose a novel \textbf{B}oundary-\textbf{C}ategory
\textbf{R}efinement \textbf{Net}work (\textbf{BCR-Net}) that requires only a
few box annotations and a large number of point annotations. BCR-Net is built
based on Group R-CNN and introduces a new Boundary Refinement (BR) module and a
new Category Refinement (CR) module. The BR module develops a dual attention
mechanism to focus on both the boundaries and salient features of prohibited
items. Meanwhile, the CR module incorporates contrastive branches into the
heads of RPN and ROI by introducing a scale- and rotation-aware contrastive
loss, enhancing intra-class consistency and inter-class separability in the
feature space. Based on the above designs, BCR-Net effectively addresses the
closely related problems of imprecise localization and inaccurate
classification. Experimental results on public X-ray datasets show the
effectiveness of BCR-Net, achieving significant performance improvements to
state-of-the-art methods under limited annotations.",2024-12-25T14:37:05Z,http://arxiv.org/abs/2412.18918v1,Sanjoeng Wong
"Open-Vocabulary Panoptic Segmentation Using BERT Pre-Training of
  Vision-Language Multiway Transformer Model","Open-vocabulary panoptic segmentation remains a challenging problem. One of
the biggest difficulties lies in training models to generalize to an unlimited
number of classes using limited categorized training data. Recent popular
methods involve large-scale vision-language pre-trained foundation models, such
as CLIP. In this paper, we propose OMTSeg for open-vocabulary segmentation
using another large-scale vision-language pre-trained model called BEiT-3 and
leveraging the cross-modal attention between visual and linguistic features in
BEiT-3 to achieve better performance. Experiments result demonstrates that
OMTSeg performs favorably against state-of-the-art models.",2024-12-25T14:31:00Z,http://arxiv.org/abs/2412.18917v1,"Yi-Chia Chen, Wei-Hua Li, Chu-Song Chen"
"Optimization-based model order reduction of fluid-structure interaction
  problems","We introduce optimization-based full-order and reduced-order formulations of
fluid structure interaction problems. We study the flow of an incompressible
Newtonian fluid which interacts with an elastic body: we consider an arbitrary
Lagrangian Eulerian formulation of the fluid problem and a fully Lagrangian
formulation of the solid problem; we rely on a finite element discretization of
both fluid and solid equations. The distinctive feature of our approach is an
implicit coupling of fluid and structural problems that relies on the solution
to a constrained optimization problem with equality constraints. We discuss the
application of projection-based model reduction to both fluid and solid
subproblems: we rely on Galerkin projection for the solid equations and on
least-square Petrov-Galerkin projection for the fluid equations. Numerical
results for three model problems illustrate the many features of the
formulation.",2024-12-25T14:26:54Z,http://arxiv.org/abs/2412.18916v1,"Tommaso Taddei, Xuejun Xu, Lei Zhang"
Robust Target Speaker Direction of Arrival Estimation,"In multi-speaker environments the direction of arrival (DOA) of a target
speaker is key for improving speech clarity and extracting target speaker's
voice. However, traditional DOA estimation methods often struggle in the
presence of noise, reverberation, and particularly when competing speakers are
present. To address these challenges, we propose RTS-DOA, a robust real-time
DOA estimation system. This system innovatively uses the registered speech of
the target speaker as a reference and leverages full-band and sub-band spectral
information from a microphone array to estimate the DOA of the target speaker's
voice. Specifically, the system comprises a speech enhancement module for
initially improving speech quality, a spatial module for learning spatial
information, and a speaker module for extracting voiceprint features.
Experimental results on the LibriSpeech dataset demonstrate that our RTS-DOA
system effectively tackles multi-speaker scenarios and established new optimal
benchmarks.",2024-12-25T14:04:21Z,http://arxiv.org/abs/2412.18913v1,"Zixuan Li, Shulin He, Xueliang Zhang"
Accelerating Diffusion Transformers with Dual Feature Caching,"Diffusion Transformers (DiT) have become the dominant methods in image and
video generation yet still suffer substantial computational costs. As an
effective approach for DiT acceleration, feature caching methods are designed
to cache the features of DiT in previous timesteps and reuse them in the next
timesteps, allowing us to skip the computation in the next timesteps. However,
on the one hand, aggressively reusing all the features cached in previous
timesteps leads to a severe drop in generation quality. On the other hand,
conservatively caching only the features in the redundant layers or tokens but
still computing the important ones successfully preserves the generation
quality but results in reductions in acceleration ratios. Observing such a
tradeoff between generation quality and acceleration performance, this paper
begins by quantitatively studying the accumulated error from cached features.
Surprisingly, we find that aggressive caching does not introduce significantly
more caching errors in the caching step, and the conservative feature caching
can fix the error introduced by aggressive caching. Thereby, we propose a dual
caching strategy that adopts aggressive and conservative caching iteratively,
leading to significant acceleration and high generation quality at the same
time. Besides, we further introduce a V-caching strategy for token-wise
conservative caching, which is compatible with flash attention and requires no
training and calibration data.
  Our codes have been released in Github: \textbf{Code:
\href{https://github.com/Shenyi-Z/DuCa}{\texttt{\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",2024-12-25T14:00:14Z,http://arxiv.org/abs/2412.18911v1,"Chang Zou, Evelyn Zhang, Runlin Guo, Haohang Xu, Conghui He, Xuming Hu, Linfeng Zhang"
"Research Experiment on Multi-Model Comparison for Chinese Text
  Classification Tasks","With the explosive growth of Chinese text data and advancements in natural
language processing technologies, Chinese text classification has become one of
the key techniques in fields such as information retrieval and sentiment
analysis, attracting increasing attention. This paper conducts a comparative
study on three deep learning models:TextCNN, TextRNN, and FastText.specifically
for Chinese text classification tasks. By conducting experiments on the
THUCNews dataset, the performance of these models is evaluated, and their
applicability in different scenarios is discussed.",2024-12-25T13:54:40Z,http://arxiv.org/abs/2412.18908v1,JiaCheng Li
"EC-Diffuser: Multi-Object Manipulation via Entity-Centric Behavior
  Generation","Object manipulation is a common component of everyday tasks, but learning to
manipulate objects from high-dimensional observations presents significant
challenges. These challenges are heightened in multi-object environments due to
the combinatorial complexity of the state space as well as of the desired
behaviors. While recent approaches have utilized large-scale offline data to
train models from pixel observations, achieving performance gains through
scaling, these methods struggle with compositional generalization in unseen
object configurations with constrained network and dataset sizes. To address
these issues, we propose a novel behavioral cloning (BC) approach that
leverages object-centric representations and an entity-centric Transformer with
diffusion-based optimization, enabling efficient learning from offline image
data. Our method first decomposes observations into an object-centric
representation, which is then processed by our entity-centric Transformer that
computes attention at the object level, simultaneously predicting object
dynamics and the agent's actions. Combined with the ability of diffusion models
to capture multi-modal behavior distributions, this results in substantial
performance improvements in multi-object tasks and, more importantly, enables
compositional generalization. We present BC agents capable of zero-shot
generalization to tasks with novel compositions of objects and goals, including
larger numbers of objects than seen during training. We provide video rollouts
on our webpage: https://sites.google.com/view/ec-diffuser.",2024-12-25T13:50:15Z,http://arxiv.org/abs/2412.18907v1,"Carl Qi, Dan Haramati, Tal Daniel, Aviv Tamar, Amy Zhang"
"FedCFA: Alleviating Simpson's Paradox in Model Aggregation with
  Counterfactual Federated Learning","Federated learning (FL) is a promising technology for data privacy and
distributed optimization, but it suffers from data imbalance and heterogeneity
among clients. Existing FL methods try to solve the problems by aligning client
with server model or by correcting client model with control variables. These
methods excel on IID and general Non-IID data but perform mediocrely in
Simpson's Paradox scenarios. Simpson's Paradox refers to the phenomenon that
the trend observed on the global dataset disappears or reverses on a subset,
which may lead to the fact that global model obtained through aggregation in FL
does not accurately reflect the distribution of global data. Thus, we propose
FedCFA, a novel FL framework employing counterfactual learning to generate
counterfactual samples by replacing local data critical factors with global
average data, aligning local data distributions with the global and mitigating
Simpson's Paradox effects. In addition, to improve the quality of
counterfactual samples, we introduce factor decorrelation (FDC) loss to reduce
the correlation among features and thus improve the independence of extracted
factors. We conduct extensive experiments on six datasets and verify that our
method outperforms other FL methods in terms of efficiency and global model
accuracy under limited communication rounds.",2024-12-25T13:35:54Z,http://arxiv.org/abs/2412.18904v1,"Zhonghua Jiang, Jimin Xu, Shengyu Zhang, Tao Shen, Jiwei Li, Kun Kuang, Haibin Cai, Fei Wu"
"Stationary Processes, Wiener-Granger Causality, and Matrix Spectral
  Factorization","Granger causality has become an indispensable tool for analyzing causal
relationships between time series. In this paper, we provide a detailed
overview of its mathematical foundations, trace its historical development, and
explore how recent computational advancements can enhance its application in
various fields. We will not hesitate to present the proofs in full if they are
simple and transparent. For more complex theorems on which we rely, we will
provide supporting citations. We also discuss potential future directions for
the method, particularly in the context of largescale data analysis.",2024-12-25T13:29:39Z,http://arxiv.org/abs/2412.18901v1,Lasha Ephremidze
"Observation of on- and off-resonant interaction between a solid-state
  spin qubit and a superconducting resonator","Hybrid systems consisting of multiple materials with distinct physical
properties and tunable interactions provide a promising route for fulfilling
transformative quantum innovations. Solid-state spin qubits and superconducting
circuits stand out as leading candidates in this context due to their
complementary device performance and quantum mechanical properties. Here, we
report experimental integration of a single nitrogen-vacancy (NV) spin qubit
and an on-chip superconducting resonator for realizing multimodal quantum
applications. Specifically, we have observed superconductivity enhanced NV spin
relaxation, which shows a similar Hebel-Slichter peak feature around the phase
transition point. In the coherent interaction regime, we show that the
superconducting resonator mode is capable of exciting NV Rabi oscillations.
Taking advantage of scanning NV magnetometry, we further visualized microscopic
electromagnetic behaviors of the superconducting resonator, revealing the
formation and evolution of superconducting vortices at the nanoscale. Our
results highlight the potential of harnessing NV centers and superconducting
circuits for designing hybrid systems to advance the burgeoning quantum
revolution. The current study will also open a new pathway to test and evaluate
miniaturized superconducting electronics for their future design and
performance improvements.",2024-12-25T13:06:25Z,http://arxiv.org/abs/2412.18896v1,"Senlei Li, Shane P. Kelly, Jingcheng Zhou, Hanyi Lu, Yaroslav Tserkovnyak, Hailong Wang, Chunhui Rita Du"
"Effects of chiral symmetry restoration on dilepton production in heavy
  ion collisions","Because of their weak interactions with the strongly interacting matter
produced in relativistic heavy-ion collisions, dileptons provide an ideal probe
of the early dynamics of these collisions. Here, we study dilepton production
using a partonic transport model that is based on an extended
Nambu-Jona-Lasinio (NJL) model. In this model, the in-medium quark masses
decrease with increasing temperature as a result of the restoration of chiral
symmetry. We find that the extracted temperature from dileptons of intermediate
masses agrees well with the temperature of the partonic matter, suggesting that
dilepton production can be used as a thermometer for the produced partonic
matter. Our results also indicate that the extracted in-medium quark masses
decrease with increasing dilepton temperature, implying that dilepton
production can further serve as a probe of chiral symmetry restoration in high
energy heavy-ion collisions.",2024-12-25T12:57:25Z,http://arxiv.org/abs/2412.18895v1,"Wen-Hao Zhou, Che Ming Ko, Kai-Jia Sun"
"Comprehensive Study on Lumbar Disc Segmentation Techniques Using MRI
  Data","Lumbar disk segmentation is essential for diagnosing and curing spinal
disorders by enabling precise detection of disk boundaries in medical imaging.
The advent of deep learning has resulted in the development of many
segmentation methods, offering differing levels of accuracy and effectiveness.
This study assesses the effectiveness of several sophisticated deep learning
architectures, including ResUnext, Ef3 Net, UNet, and TransUNet, for lumbar
disk segmentation, highlighting key metrics like as Pixel Accuracy, Mean
Intersection over Union (Mean IoU), and Dice Coefficient. The findings indicate
that ResUnext achieved the highest segmentation accuracy, with a Pixel Accuracy
of 0.9492 and a Dice Coefficient of 0.8425, with TransUNet following closely
after. Filtering techniques somewhat enhanced the performance of most models,
particularly Dense UNet, improving stability and segmentation quality. The
findings underscore the efficacy of these models in lumbar disk segmentation
and highlight potential areas for improvement.",2024-12-25T12:54:52Z,http://arxiv.org/abs/2412.18894v1,"Serkan Salturk, Irem Sayin, Ibrahim Cem Balci, Taha Emre Pamukcu, Zafer Soydan, Huseyin Uvet"
"Emergent Intermediate Phase in the $J_1$-$J_2$ XY model from Tensor
  Network Approaches","We investigate the finite-temperature phase diagram of the classical
$J_1$-$J_2$ XY model on a square lattice using a tensor network approach
designed for frustrated spin systems. This model, characterized by competing
nearest-neighbor and next-to-nearest-neighbor interactions, exhibits a complex
interplay between $U(1)$ and $Z_2$ symmetries. Our study reveals an emergent
intermediate phase around $J_2/J_1 \sim 0.505$, which is characterized by a
$Z_2$ long-range stripe order without phase coherence in the XY spins. The
intermediate phase features two well-separated phase transitions: a
higher-temperature Ising transition and a lower-temperature
Berezinskii-Kosterlitz-Thouless transition. The relative separation between
these transitions is significantly larger than previously reported, enabling a
clearer investigation of their distinct thermodynamic properties. For
$0.5&lt;J_2/J_1 &lt; 0.501$, two transitions merge into a single first-order phase
transition, a phenomenon that cannot be explained solely by mapping to the
Ising-XY model. As $J_2/J_1 \to \infty$, the transition evolves continuously
into the BKT universality class. These findings advance the understanding of
the mechanisms driving phase transitions in frustrated spin systems and suggest
potential experimental realizations in platforms such as ultracold atoms,
Josephson junction arrays, and optical lattices.",2024-12-25T12:51:07Z,http://arxiv.org/abs/2412.18892v1,"Feng-Feng Song, Hanggai Nuomin, Naoki Kawashima"
"CoEvo: Continual Evolution of Symbolic Solutions Using Large Language
  Models","Large Language Models (LLMs) have emerged as transformative tools in
artificial intelligence, capable of processing and understanding extensive
human knowledge to enhance problem-solving across various domains. This paper
explores the potential of LLMs to drive the discovery of symbolic solutions
within scientific and engineering disciplines, where such solutions are crucial
for advancing theoretical and practical applications. We propose a novel
framework that utilizes LLMs in an evolutionary search methodology, augmented
by a dynamic knowledge library that integrates and refines insights in an
\textit{open-ended manner}. This approach aims to tackle the dual challenges of
efficiently navigating complex symbolic representation spaces and leveraging
both existing and newly generated knowledge to foster open-ended innovation. By
enabling LLMs to interact with and expand upon a knowledge library, we
facilitate the continuous generation of novel solutions in diverse forms such
as language, code, and mathematical expressions. Our experimental results
demonstrate that this method not only enhances the efficiency of searching for
symbolic solutions but also supports the ongoing discovery process, akin to
human scientific endeavors. This study represents a first effort in
conceptualizing the search for symbolic solutions as a lifelong, iterative
process, marking a significant step towards harnessing AI in the perpetual
pursuit of scientific and engineering breakthroughs. We have open-sourced our
code and data, please visit \url{https://github.com/pgg3/CoEvo} for more
information.",2024-12-25T12:27:27Z,http://arxiv.org/abs/2412.18890v1,"Ping Guo, Qingfu Zhang, Xi Lin"
"Adversarial Training for Graph Neural Networks via Graph Subspace Energy
  Optimization","Despite impressive capability in learning over graph-structured data, graph
neural networks (GNN) suffer from adversarial topology perturbation in both
training and inference phases. While adversarial training has demonstrated
remarkable effectiveness in image classification tasks, its suitability for GNN
models has been doubted until a recent advance that shifts the focus from
transductive to inductive learning. Still, GNN robustness in the inductive
setting is under-explored, and it calls for deeper understanding of GNN
adversarial training. To this end, we propose a new concept of graph subspace
energy (GSE) -- a generalization of graph energy that measures graph stability
-- of the adjacency matrix, as an indicator of GNN robustness against topology
perturbations. To further demonstrate the effectiveness of such concept, we
propose an adversarial training method with the perturbed graphs generated by
maximizing the GSE regularization term, referred to as AT-GSE. To deal with the
local and global topology perturbations raised respectively by LRBCD and PRBCD,
we employ randomized SVD (RndSVD) and Nystrom low-rank approximation to favor
the different aspects of the GSE terms. An extensive set of experiments shows
that AT-GSE outperforms consistently the state-of-the-art GNN adversarial
training methods over different homophily and heterophily datasets in terms of
adversarial accuracy, whilst more surprisingly achieving a superior clean
accuracy on non-perturbed graphs.",2024-12-25T12:04:18Z,http://arxiv.org/abs/2412.18886v1,"Ganlin Liu, Ziling Liang, Xiaowei Huang, Xinping Yi, Shi Jin"
"HV-BEV: Decoupling Horizontal and Vertical Feature Sampling for
  Multi-View 3D Object Detection","The application of vision-based multi-view environmental perception system
has been increasingly recognized in autonomous driving technology, especially
the BEV-based models. Current state-of-the-art solutions primarily encode image
features from each camera view into the BEV space through explicit or implicit
depth prediction. However, these methods often focus on improving the accuracy
of projecting 2D features into corresponding depth regions, while overlooking
the highly structured information of real-world objects and the varying height
distributions of objects across different scenes. In this work, we propose
HV-BEV, a novel approach that decouples feature sampling in the BEV grid
queries paradigm into horizontal feature aggregation and vertical adaptive
height-aware reference point sampling, aiming to improve both the aggregation
of objects' complete information and generalization to diverse road
environments. Specifically, we construct a learnable graph structure in the
horizontal plane aligned with the ground for 3D reference points, reinforcing
the association of the same instance across different BEV grids, especially
when the instance spans multiple image views around the vehicle. Additionally,
instead of relying on uniform sampling within a fixed height range, we
introduce a height-aware module that incorporates historical information,
enabling the reference points to adaptively focus on the varying heights at
which objects appear in different scenes. Extensive experiments validate the
effectiveness of our proposed method, demonstrating its superior performance
over the baseline across the nuScenes dataset. Moreover, our best-performing
model achieves a remarkable 50.5% mAP and 59.8% NDS on the nuScenes testing
set.",2024-12-25T11:49:14Z,http://arxiv.org/abs/2412.18884v1,"Di Wu, Feng Yang, Benlian Xu, Pan Liao, Wenhui Zhao, Dingwen Zhang"
MotionMap: Representing Multimodality in Human Pose Forecasting,"Human pose forecasting is inherently multimodal since multiple futures exist
for an observed pose sequence. However, evaluating multimodality is challenging
since the task is ill-posed. Therefore, we first propose an alternative
paradigm to make the task well-posed. Next, while state-of-the-art methods
predict multimodality, this requires oversampling a large volume of
predictions. This raises key questions: (1) Can we capture multimodality by
efficiently sampling a smaller number of predictions? (2) Subsequently, which
of the predicted futures is more likely for an observed pose sequence? We
address these questions with MotionMap, a simple yet effective heatmap based
representation for multimodality. We extend heatmaps to represent a spatial
distribution over the space of all possible motions, where different local
maxima correspond to different forecasts for a given observation. MotionMap can
capture a variable number of modes per observation and provide confidence
measures for different modes. Further, MotionMap allows us to introduce the
notion of uncertainty and controllability over the forecasted pose sequence.
Finally, MotionMap captures rare modes that are non-trivial to evaluate yet
critical for safety. We support our claims through multiple qualitative and
quantitative experiments using popular 3D human pose datasets: Human3.6M and
AMASS, highlighting the strengths and limitations of our proposed method.
Project Page: https://www.epfl.ch/labs/vita/research/prediction/motionmap/",2024-12-25T11:47:26Z,http://arxiv.org/abs/2412.18883v1,"Reyhaneh Hosseininejad, Megh Shukla, Saeed Saadatnejad, Mathieu Salzmann, Alexandre Alahi"
"Towards Compatible Semantic Communication: A Perspective on Digital
  Coding and Modulation","Semantic communication (SC) is emerging as a pivotal innovation within the 6G
framework, aimed at enabling more intelligent transmission. This development
has led to numerous studies focused on designing advanced systems through
powerful deep learning techniques. Nevertheless, many of these approaches
envision an analog transmission manner by formulating the transmitted signals
as continuous-valued semantic representation vectors, limiting their
compatibility with existing digital systems. To enhance compatibility, it is
essential to explore digitized SC systems. This article systematically
identifies two promising paradigms for designing digital SC: probabilistic and
deterministic approaches, according to the modulation strategies. For both, we
first provide a comprehensive analysis of the methodologies. Then, we put
forward the principles of designing digital SC systems with a specific focus on
informativeness and robustness of semantic representations to enhance
performance, along with constellation design. Additionally, we present a case
study to demonstrate the effectiveness of these methods. Moreover, this article
also explores the intrinsic advantages and opportunities provided by digital SC
systems, and then outlines several potential research directions for future
investigation.",2024-12-25T11:19:40Z,http://arxiv.org/abs/2412.18876v1,"Guangyi Zhang, Kequan Zhou, Yunlong Cai, Qiyu Hu, Guanding Yu"
IUST_PersonReId: A New Domain in Person Re-Identification Datasets,"Person re-identification (ReID) models often struggle to generalize across
diverse cultural contexts, particularly in Islamic regions like Iran, where
modest clothing styles are prevalent. Existing datasets predominantly feature
Western and East Asian fashion, limiting their applicability in these settings.
To address this gap, we introduce IUST_PersonReId, a dataset designed to
reflect the unique challenges of ReID in new cultural environments, emphasizing
modest attire and diverse scenarios from Iran, including markets, campuses, and
mosques. Experiments on IUST_PersonReId with state-of-the-art models, such as
Solider and CLIP-ReID, reveal significant performance drops compared to
benchmarks like Market1501 and MSMT17, highlighting the challenges posed by
occlusion and limited distinctive features. Sequence-based evaluations show
improvements by leveraging temporal context, emphasizing the dataset's
potential for advancing culturally sensitive and robust ReID systems.
IUST_PersonReId offers a critical resource for addressing fairness and bias in
ReID research globally. The dataset is publicly available at
https://computervisioniust.github.io/IUST_PersonReId/.",2024-12-25T11:17:43Z,http://arxiv.org/abs/2412.18874v1,"Alireza Sedighi Moghaddam, Fatemeh Anvari, Mohammadjavad Mirshekari Haghighi, Mohammadali Fakhari, Mohammad Reza Mohammadi"
Cross-PCR: A Robust Cross-Source Point Cloud Registration Framework,"Due to the density inconsistency and distribution difference between
cross-source point clouds, previous methods fail in cross-source point cloud
registration. We propose a density-robust feature extraction and matching
scheme to achieve robust and accurate cross-source registration. To address the
density inconsistency between cross-source data, we introduce a density-robust
encoder for extracting density-robust features. To tackle the issue of
challenging feature matching and few correct correspondences, we adopt a
loose-to-strict matching pipeline with a ``loose generation, strict selection''
idea. Under it, we employ a one-to-many strategy to loosely generate initial
correspondences. Subsequently, high-quality correspondences are strictly
selected to achieve robust registration through sparse matching and dense
matching. On the challenging Kinect-LiDAR scene in the cross-source 3DCSR
dataset, our method improves feature matching recall by 63.5 percentage points
(pp) and registration recall by 57.6 pp. It also achieves the best performance
on 3DMatch, while maintaining robustness under diverse downsampling densities.",2024-12-25T11:14:59Z,http://arxiv.org/abs/2412.18873v1,"Guiyu Zhao, Zhentao Guo, Zewen Du, Hongbin Ma"
Proton Flux Measurement from Neutron Monitor Data Using Neural Networks,"Accurate measurements of cosmic ray proton flux are crucial for studying the
modulation processes of cosmic rays during the solar activity cycle. We present
a proton flux measurement method based on ground-based neutron monitor (NM)
data and machine learning techniques. After preprocessing the NM data, we use a
convolutional neural network (CNN) model to simulate the relationship between
the NM observations and proton flux measured by the Alpha Magnetic Spectrometer
(AMS-02). We obtain daily proton flux data ranging from 1GV to 100GV for the
period from 2011 to 2024, showing that the measured values are in good
agreement with the observed ones. In particular, we provide daily proton flux
measurements for periods when AMS-02 data are unavailable due to operational
reasons. We also perform wavelet analyses on the continuous proton flux data to
investigate the relationship between proton flux and solar activity variations,
particularly during late 2014 when AMS-02 measurements were missing.",2024-12-25T11:14:13Z,http://arxiv.org/abs/2412.18872v1,"Pengwei Zhao, Jie Feng"
"TSceneJAL: Joint Active Learning of Traffic Scenes for 3D Object
  Detection","Most autonomous driving (AD) datasets incur substantial costs for collection
and labeling, inevitably yielding a plethora of low-quality and redundant data
instances, thereby compromising performance and efficiency. Many applications
in AD systems necessitate high-quality training datasets using both existing
datasets and newly collected data. In this paper, we propose a traffic scene
joint active learning (TSceneJAL) framework that can efficiently sample the
balanced, diverse, and complex traffic scenes from both labeled and unlabeled
data. The novelty of this framework is threefold: 1) a scene sampling scheme
based on a category entropy, to identify scenes containing multiple object
classes, thus mitigating class imbalance for the active learner; 2) a
similarity sampling scheme, estimated through the directed graph representation
and a marginalize kernel algorithm, to pick sparse and diverse scenes; 3) an
uncertainty sampling scheme, predicted by a mixture density network, to select
instances with the most unclear or complex regression outcomes for the learner.
Finally, the integration of these three schemes in a joint selection strategy
yields an optimal and valuable subdataset. Experiments on the KITTI, Lyft,
nuScenes and SUScape datasets demonstrate that our approach outperforms
existing state-of-the-art methods on 3D object detection tasks with up to 12%
improvements.",2024-12-25T11:07:04Z,http://arxiv.org/abs/2412.18870v1,"Chenyang Lei, Meiying Zhang, Weiyuan Peng, Qi Hao, Chengzhong Xu, Chunlin Ji, Guang Zhou"
"Autonomous Navigation of 4WIS4WID Agricultural Field Mobile Robot using
  Deep Reinforcement Learning","In the futuristic agricultural fields compatible with Agriculture 4.0, robots
are envisaged to navigate through crops to perform functions like pesticide
spraying and fruit harvesting, which are complex tasks due to factors such as
non-geometric internal obstacles, space constraints, and outdoor conditions. In
this paper, we attempt to employ Deep Reinforcement Learning (DRL) to solve the
problem of 4WIS4WID mobile robot navigation in a structured, automated
agricultural field. This paper consists of three sections: parameterization of
four-wheel steering configurations, crop row tracking using DRL, and autonomous
navigation of 4WIS4WID mobile robot using DRL through multiple crop rows. We
show how to parametrize various configurations of four-wheel steering to two
variables. This includes symmetric four-wheel steering, zero-turn, and an
additional steering configuration that allows the 4WIS4WID mobile robot to move
laterally. Using DRL, we also followed an irregularly shaped crop row with
symmetric four-wheel steering. In the multiple crop row simulation environment,
with the help of waypoints, we effectively performed point-to-point navigation.
Finally, a comparative analysis of various DRL algorithms that use continuous
actions was carried out.",2024-12-25T10:33:33Z,http://arxiv.org/abs/2412.18865v1,"Tom Baby, Mahendra Kumar Gohil, Bishakh Bhattacharya"
"WeatherGS: 3D Scene Reconstruction in Adverse Weather Conditions via
  Gaussian Splatting","3D Gaussian Splatting (3DGS) has gained significant attention for 3D scene
reconstruction, but still suffers from complex outdoor environments, especially
under adverse weather. This is because 3DGS treats the artifacts caused by
adverse weather as part of the scene and will directly reconstruct them,
largely reducing the clarity of the reconstructed scene. To address this
challenge, we propose WeatherGS, a 3DGS-based framework for reconstructing
clear scenes from multi-view images under different weather conditions.
Specifically, we explicitly categorize the multi-weather artifacts into the
dense particles and lens occlusions that have very different characters, in
which the former are caused by snowflakes and raindrops in the air, and the
latter are raised by the precipitation on the camera lens. In light of this, we
propose a dense-to-sparse preprocess strategy, which sequentially removes the
dense particles by an Atmospheric Effect Filter (AEF) and then extracts the
relatively sparse occlusion masks with a Lens Effect Detector (LED). Finally,
we train a set of 3D Gaussians by the processed images and generated masks for
excluding occluded areas, and accurately recover the underlying clear scene by
Gaussian splatting. We conduct a diverse and challenging benchmark to
facilitate the evaluation of 3D reconstruction under complex weather scenarios.
Extensive experiments on this benchmark demonstrate that our WeatherGS
consistently produces high-quality, clean scenes across various weather
scenarios, outperforming existing state-of-the-art methods. See project
page:https://jumponthemoon.github.io/weather-gs.",2024-12-25T10:16:57Z,http://arxiv.org/abs/2412.18862v1,"Chenghao Qian, Yuhu Guo, Wenjing Li, Gustav Markkula"
"Few-shot Metric Domain Adaptation: Practical Learning Strategies for an
  Automated Plant Disease Diagnosis","Numerous studies have explored image-based automated systems for plant
disease diagnosis, demonstrating impressive diagnostic capabilities. However,
recent large-scale analyses have revealed a critical limitation: that the
diagnostic capability suffers significantly when validated on images captured
in environments (domains) differing from those used during training. This
shortfall stems from the inherently limited dataset size and the diverse
manifestation of disease symptoms, combined with substantial variations in
cultivation environments and imaging conditions, such as equipment and
composition. These factors lead to insufficient variety in training data,
ultimately constraining the system's robustness and generalization. To address
these challenges, we propose Few-shot Metric Domain Adaptation (FMDA), a
flexible and effective approach for enhancing diagnostic accuracy in practical
systems, even when only limited target data is available. FMDA reduces domain
discrepancies by introducing a constraint to the diagnostic model that
minimizes the ""distance"" between feature spaces of source (training) data and
target data with limited samples. FMDA is computationally efficient, requiring
only basic feature distance calculations and backpropagation, and can be
seamlessly integrated into any machine learning (ML) pipeline. In large-scale
experiments, involving 223,015 leaf images across 20 fields and 3 crop species,
FMDA achieved F1 score improvements of 11.1 to 29.3 points compared to cases
without target data, using only 10 images per disease from the target domain.
Moreover, FMDA consistently outperformed fine-tuning methods utilizing the same
data, with an average improvement of 8.5 points.",2024-12-25T10:01:30Z,http://arxiv.org/abs/2412.18859v1,"Shoma Kudo, Satoshi Kagiwada, Hitoshi Iyatomi"
Computing Approximate Graph Edit Distance via Optimal Transport,"Given a graph pair $(G^1, G^2)$, graph edit distance (GED) is defined as the
minimum number of edit operations converting $G^1$ to $G^2$. GED is a
fundamental operation widely used in many applications, but its exact
computation is NP-hard, so the approximation of GED has gained a lot of
attention. Data-driven learning-based methods have been found to provide
superior results compared to classical approximate algorithms, but they
directly fit the coupling relationship between a pair of vertices from their
vertex features. We argue that while pairwise vertex features can capture the
coupling cost (discrepancy) of a pair of vertices, the vertex coupling matrix
should be derived from the vertex-pair cost matrix through a more
well-established method that is aware of the global context of the graph pair,
such as optimal transport. In this paper, we propose an ensemble approach that
integrates a supervised learning-based method and an unsupervised method, both
based on optimal transport. Our learning method, GEDIOT, is based on inverse
optimal transport that leverages a learnable Sinkhorn algorithm to generate the
coupling matrix. Our unsupervised method, GEDGW, models GED computation as a
linear combination of optimal transport and its variant, Gromov-Wasserstein
discrepancy, for node and edge operations, respectively, which can be solved
efficiently without needing the ground truth. Our ensemble method, GEDHOT,
combines GEDIOT and GEDGW to further boost the performance. Extensive
experiments demonstrate that our methods significantly outperform the existing
methods in terms of the performance of GED computation, edit path generation,
and model generalizability.",2024-12-25T09:55:14Z,http://arxiv.org/abs/2412.18857v1,"Qihao Cheng, Da Yan, Tianhao Wu, Zhongyi Huang, Qin Zhang"
"Digital Twin Enhanced Deep Reinforcement Learning for Intelligent
  Omni-Surface Configurations in MU-MIMO Systems","Intelligent omni-surface (IOS) is a promising technique to enhance the
capacity of wireless networks, by reflecting and refracting the incident signal
simultaneously. Traditional IOS configuration schemes, relying on all
sub-channels' channel state information and user equipments' mobility, are
difficult to implement in complex realistic systems. Existing works attempt to
address this issue employing deep reinforcement learning (DRL), but this method
requires a lot of trial-and-error interactions with the external environment
for efficient results and thus cannot satisfy the real-time decision-making. To
enable model-free and real-time IOS control, this paper puts forth a new
framework that integrates DRL and digital twins. DeepIOS, a DRL based IOS
configuration scheme with the goal of maximizing the sum data rate, is first
developed to jointly optimize the phase-shift and amplitude of IOS in
multi-user multiple-input-multiple-output systems. Thereafter, to further
reduce the computational complexity, DeepIOS introduces an action branch
architecture, which separately decides two optimization variables in parallel.
Finally, a digital twin module is constructed through supervised learning as a
pre-verification platform for DeepIOS, such that the decision-making's
real-time can be guaranteed. The formulated framework is a closed-loop system,
in which the physical space provides data to establish and calibrate the
digital space, while the digital space generates experience samples for DeepIOS
training and sends the trained parameters to the IOS controller for
configurations. Numerical results show that compared with random and MAB
schemes, the proposed framework attains a higher data rate and is more robust
to different settings. Furthermore, the action branch architecture reduces
DeepIOS's computational complexity, and the digital twin module improves the
convergence speed and run-time.",2024-12-25T09:53:07Z,http://arxiv.org/abs/2412.18856v1,"Xiaowen Ye, Xianghao Yu, Liqun Fu"
"Optimistic Critic Reconstruction and Constrained Fine-Tuning for General
  Offline-to-Online RL","Offline-to-online (O2O) reinforcement learning (RL) provides an effective
means of leveraging an offline pre-trained policy as initialization to improve
performance rapidly with limited online interactions. Recent studies often
design fine-tuning strategies for a specific offline RL method and cannot
perform general O2O learning from any offline method. To deal with this
problem, we disclose that there are evaluation and improvement mismatches
between the offline dataset and the online environment, which hinders the
direct application of pre-trained policies to online fine-tuning. In this
paper, we propose to handle these two mismatches simultaneously, which aims to
achieve general O2O learning from any offline method to any online method.
Before online fine-tuning, we re-evaluate the pessimistic critic trained on the
offline dataset in an optimistic way and then calibrate the misaligned critic
with the reliable offline actor to avoid erroneous update. After obtaining an
optimistic and and aligned critic, we perform constrained fine-tuning to combat
distribution shift during online learning. We show empirically that the
proposed method can achieve stable and efficient performance improvement on
multiple simulated tasks when compared to the state-of-the-art methods.",2024-12-25T09:52:22Z,http://arxiv.org/abs/2412.18855v1,"Qin-Wen Luo, Ming-Kun Xie, Ye-Wen Wang, Sheng-Jun Huang"
"Gravitational waves from equatorially eccentric extreme mass ratio
  inspirals around swirling Kerr black holes","We have studied the gravitational wave generated by extreme mass ratio
inspirals (EMRIs) along eccentric orbits on equatorial plane within the frame
of the swirling-Kerr black hole spacetime. The swirling-Kerr black hole has an
extra swirling parameter, which characterizes the rotation of universe
background. Our findings indicate that this swirling parameter leads to a delay
phase shift in the gravitational waveforms. The impact of the swirling
parameter on EMRI gravitational waves is suppressed by the black hole's spin
parameter. As a result, extracting information about the swirling parameter
from gravitational waves in a static black hole spacetime is much easier than
in the case of a rapidly rotating black hole. Our analysis also shows that a
high black hole spin leads to a greater overlap of gravitational waveforms for
different swirling parameters. We further investigate the potential issue of
waveform confusion caused by the orbital eccentricity and semi-latus rectum
parameters. As the swirling parameter increases, the relative variation in
eccentricity also increases, while the variation in the semi-latus rectum
decreases rapidly. The trends in these changes with the swirling parameter
resemble those observed with the MOG (Modified Gravity) parameter, though with
different rates of change. These results provide deeper insights into the
properties of EMRI gravitational waves and the swirling of the universe
background.",2024-12-25T09:51:58Z,http://arxiv.org/abs/2412.18854v1,"Yuhang Gu, Songbai Chen, Jiliang Jing"
Cross-View Image Set Geo-Localization,"Cross-view geo-localization (CVGL) has been widely applied in fields such as
robotic navigation and augmented reality. Existing approaches primarily use
single images or fixed-view image sequences as queries, which limits
perspective diversity. In contrast, when humans determine their location
visually, they typically move around to gather multiple perspectives. This
behavior suggests that integrating diverse visual cues can improve
geo-localization reliability. Therefore, we propose a novel task: Cross-View
Image Set Geo-Localization (Set-CVGL), which gathers multiple images with
diverse perspectives as a query set for localization. To support this task, we
introduce SetVL-480K, a benchmark comprising 480,000 ground images captured
worldwide and their corresponding satellite images, with each satellite image
corresponds to an average of 40 ground images from varied perspectives and
locations. Furthermore, we propose FlexGeo, a flexible method designed for
Set-CVGL that can also adapt to single-image and image-sequence inputs. FlexGeo
includes two key modules: the Similarity-guided Feature Fuser (SFF), which
adaptively fuses image features without prior content dependency, and the
Individual-level Attributes Learner (IAL), leveraging geo-attributes of each
image for comprehensive scene perception. FlexGeo consistently outperforms
existing methods on SetVL-480K and two public datasets, SeqGeo and KITTI-CVL,
achieving a localization accuracy improvement of over 22% on SetVL-480K.",2024-12-25T09:46:14Z,http://arxiv.org/abs/2412.18852v1,"Qiong Wu, Panwang Xia, Lei Yu, Yi Liu, Mingtao Xiong, Liheng Zhong, Jingdong Chen, Ming Yang, Yongjun Zhang, Yi Wan"
"Attention-Enhanced Short-Time Wiener Solution for Acoustic Echo
  Cancellation","Acoustic Echo Cancellation (AEC) is an essential speech signal processing
technology that removes echoes from microphone inputs to facilitate
natural-sounding full-duplex communication. Currently, deep learning-based AEC
methods primarily focus on refining model architectures, frequently neglecting
the incorporation of knowledge from traditional filter theory. This paper
presents an innovative approach to AEC by introducing an attention-enhanced
short-time Wiener solution. Our method strategically harnesses attention
mechanisms to mitigate the impact of double-talk interference, thereby
optimizing the efficiency of knowledge utilization. The derivation of the
short-term Wiener solution, which adapts classical Wiener solutions to finite
input causality, integrates established insights from filter theory into this
method. The experimental outcomes corroborate the effectiveness of our proposed
approach, surpassing other baseline models in performance and generalization.
The official code is available at https://github.com/ZhaoF-i/ASTWS-AEC",2024-12-25T09:42:41Z,http://arxiv.org/abs/2412.18851v1,"Fei Zhao, Xueliang Zhang"
"SWAG: Long-term Surgical Workflow Prediction with Generative-based
  Anticipation","While existing recognition approaches excel at identifying current surgical
phases, they provide limited foresight into future procedural steps,
restricting their intraoperative utility. Similarly, current anticipation
methods are constrained to predicting short-term events or singular future
occurrences, neglecting the dynamic and sequential nature of surgical
workflows. To address these limitations, we propose SWAG (Surgical Workflow
Anticipative Generation), a unified framework for phase recognition and
long-term anticipation of surgical workflows. SWAG employs two generative
decoding methods -- single-pass (SP) and auto-regressive (AR) -- to predict
sequences of future surgical phases. A novel prior knowledge embedding
mechanism enhances the accuracy of anticipatory predictions. The framework
addresses future phase classification and remaining time regression tasks.
Additionally, a regression-to-classification (R2C) method is introduced to map
continuous predictions to discrete temporal segments. SWAG's performance was
evaluated on the Cholec80 and AutoLaparo21 datasets. The single-pass
classification model with prior knowledge embeddings (SWAG-SP\*) achieved
53.5\% accuracy in 15-minute anticipation on AutoLaparo21, while the R2C model
reached 60.8\% accuracy on Cholec80. SWAG's single-pass regression approach
outperformed existing methods for remaining time prediction, achieving weighted
mean absolute errors of 0.32 and 0.48 minutes for 2- and 3-minute horizons,
respectively. SWAG demonstrates versatility across classification and
regression tasks, offering robust tools for real-time surgical workflow
anticipation. By unifying recognition and anticipatory capabilities, SWAG
provides actionable predictions to enhance intraoperative decision-making.",2024-12-25T09:29:57Z,http://arxiv.org/abs/2412.18849v1,"Maxence Boels, Yang Liu, Prokar Dasgupta, Alejandro Granados, Sebastien Ourselin"
Machine Learning-Based Detection of Pump-and-Dump Schemes in Real-Time,"Cryptocurrency markets often face manipulation through prevalent
pump-and-dump (P&amp;D) schemes, where self-organized Telegram groups, some
exceeding two million members, artificially inflate target cryptocurrency
prices. These groups sell premium access to inside information, worsening
information asymmetry and financial risks for subscribers and all investors.
This paper presents a real-time prediction pipeline to forecast target coins
and alert investors to possible P&amp;D schemes. In a Poloniex case study, the
model accurately identified the target coin among the top five from 50 random
coins in 24 out of 43 (55.81%) P&amp;D events. The pipeline uses advanced natural
language processing (NLP) to classify Telegram messages, identifying 2,079 past
pump events and detecting new ones in real-time. Our analysis also evaluates
the susceptibility of token standards - ERC-20, ERC-721, BRC-20, Inscriptions,
and Runes - to manipulation and identifies exchanges commonly involved in P&amp;D
schemes.",2024-12-25T09:23:36Z,http://arxiv.org/abs/2412.18848v1,"Manuel Bolz, Kevin Bründler, Liam Kane, Panagiotis Patsias, Liam Tessendorf, Krzysztof Gogol, Taehoon Kim, Claudio Tessone"
"TPCH: Tensor-interacted Projection and Cooperative Hashing for
  Multi-view Clustering","In recent years, anchor and hash-based multi-view clustering methods have
gained attention for their efficiency and simplicity in handling large-scale
data. However, existing methods often overlook the interactions among
multi-view data and higher-order cooperative relationships during projection,
negatively impacting the quality of hash representation in low-dimensional
spaces, clustering performance, and sensitivity to noise. To address this
issue, we propose a novel approach named Tensor-Interacted Projection and
Cooperative Hashing for Multi-View Clustering(TPCH). TPCH stacks multiple
projection matrices into a tensor, taking into account the synergies and
communications during the projection process. By capturing higher-order
multi-view information through dual projection and Hamming space, TPCH employs
an enhanced tensor nuclear norm to learn more compact and distinguishable hash
representations, promoting communication within and between views. Experimental
results demonstrate that this refined method significantly outperforms
state-of-the-art methods in clustering on five large-scale multi-view datasets.
Moreover, in terms of CPU time, TPCH achieves substantial acceleration compared
to the most advanced current methods. The code is available at
\textcolor{red}{\url{https://github.com/jankin-wang/TPCH}}.",2024-12-25T09:22:11Z,http://arxiv.org/abs/2412.18847v1,"Zhongwen Wang, Xingfeng Li, Yinghui Sun, Quansen Sun, Yuan Sun, Han Ling, Jian Dai, Zhenwen Ren"
"Enhancing Federated Graph Learning via Adaptive Fusion of Structural and
  Node Characteristics","Federated Graph Learning (FGL) has demonstrated the advantage of training a
global Graph Neural Network (GNN) model across distributed clients using their
local graph data. Unlike Euclidean data (\eg, images), graph data is composed
of nodes and edges, where the overall node-edge connections determine the
topological structure, and individual nodes along with their neighbors capture
local node features. However, existing studies tend to prioritize one aspect
over the other, leading to an incomplete understanding of the data and the
potential misidentification of key characteristics across varying graph
scenarios. Additionally, the non-independent and identically distributed
(non-IID) nature of graph data makes the extraction of these two data
characteristics even more challenging. To address the above issues, we propose
a novel FGL framework, named FedGCF, which aims to simultaneously extract and
fuse structural properties and node features to effectively handle diverse
graph scenarios. FedGCF first clusters clients by structural similarity,
performing model aggregation within each cluster to form the shared structural
model. Next, FedGCF selects the clients with common node features and
aggregates their models to generate a common node model. This model is then
propagated to all clients, allowing common node features to be shared. By
combining these two models with a proper ratio, FedGCF can achieve a
comprehensive understanding of the graph data and deliver better performance,
even under non-IID distributions. Experimental results show that FedGCF
improves accuracy by 4.94%-7.24% under different data distributions and reduces
communication cost by 64.18%-81.25% to reach the same accuracy compared to
baselines.",2024-12-25T09:20:06Z,http://arxiv.org/abs/2412.18845v1,"Xianjun Gao, Jianchun Liu, Hongli Xu, Shilong Wang, Liusheng Huang"
"Improving Integrated Gradient-based Transferable Adversarial Examples by
  Refining the Integration Path","Transferable adversarial examples are known to cause threats in practical,
black-box attack scenarios. A notable approach to improving transferability is
using integrated gradients (IG), originally developed for model
interpretability. In this paper, we find that existing IG-based attacks have
limited transferability due to their naive adoption of IG in model
interpretability. To address this limitation, we focus on the IG integration
path and refine it in three aspects: multiplicity, monotonicity, and diversity,
supported by theoretical analyses. We propose the Multiple Monotonic
Diversified Integrated Gradients (MuMoDIG) attack, which can generate highly
transferable adversarial examples on different CNN and ViT models and defenses.
Experiments validate that MuMoDIG outperforms the latest IG-based attack by up
to 37.3\% and other state-of-the-art attacks by 8.4\%. In general, our study
reveals that migrating established techniques to improve transferability may
require non-trivial efforts. Code is available at
\url{https://github.com/RYC-98/MuMoDIG}.",2024-12-25T09:15:39Z,http://arxiv.org/abs/2412.18844v1,"Yuchen Ren, Zhengyu Zhao, Chenhao Lin, Bo Yang, Lu Zhou, Zhe Liu, Chao Shen"
"Context-Based Semantic-Aware Alignment for Semi-Supervised Multi-Label
  Learning","Due to the lack of extensive precisely-annotated multi-label data in real
word, semi-supervised multi-label learning (SSMLL) has gradually gained
attention. Abundant knowledge embedded in vision-language models (VLMs)
pre-trained on large-scale image-text pairs could alleviate the challenge of
limited labeled data under SSMLL setting.Despite existing methods based on
fine-tuning VLMs have achieved advances in weakly-supervised multi-label
learning, they failed to fully leverage the information from labeled data to
enhance the learning of unlabeled data. In this paper, we propose a
context-based semantic-aware alignment method to solve the SSMLL problem by
leveraging the knowledge of VLMs. To address the challenge of handling multiple
semantics within an image, we introduce a novel framework design to extract
label-specific image features. This design allows us to achieve a more compact
alignment between text features and label-specific image features, leading the
model to generate high-quality pseudo-labels. To incorporate the model with
comprehensive understanding of image, we design a semi-supervised context
identification auxiliary task to enhance the feature representation by
capturing co-occurrence information. Extensive experiments on multiple
benchmark datasets demonstrate the effectiveness of our proposed method.",2024-12-25T09:06:54Z,http://arxiv.org/abs/2412.18842v1,"Heng-Bo Fan, Ming-Kun Xie, Jia-Hao Xiao, Sheng-Jun Huang"
"Advancing NAM-to-Speech Conversion with Novel Methods and the MultiNAM
  Dataset","Current Non-Audible Murmur (NAM)-to-speech techniques rely on voice cloning
to simulate ground-truth speech from paired whispers. However, the simulated
speech often lacks intelligibility and fails to generalize well across
different speakers. To address this issue, we focus on learning phoneme-level
alignments from paired whispers and text and employ a Text-to-Speech (TTS)
system to simulate the ground-truth. To reduce dependence on whispers, we learn
phoneme alignments directly from NAMs, though the quality is constrained by the
available training data. To further mitigate reliance on NAM/whisper data for
ground-truth simulation, we propose incorporating the lip modality to infer
speech and introduce a novel diffusion-based method that leverages recent
advancements in lip-to-speech technology. Additionally, we release the MultiNAM
dataset with over $7.96$ hours of paired NAM, whisper, video, and text data
from two speakers and benchmark all methods on this dataset. Speech samples and
the dataset are available at \url{https://diff-nam.github.io/DiffNAM/}",2024-12-25T08:57:24Z,http://arxiv.org/abs/2412.18839v1,"Neil Shah, Shirish Karande, Vineet Gandhi"
DiFiC: Your Diffusion Model Holds the Secret to Fine-Grained Clustering,"Fine-grained clustering is a practical yet challenging task, whose essence
lies in capturing the subtle differences between instances of different
classes. Such subtle differences can be easily disrupted by data augmentation
or be overwhelmed by redundant information in data, leading to significant
performance degradation for existing clustering methods. In this work, we
introduce DiFiC a fine-grained clustering method building upon the conditional
diffusion model. Distinct from existing works that focus on extracting
discriminative features from images, DiFiC resorts to deducing the textual
conditions used for image generation. To distill more precise and
clustering-favorable object semantics, DiFiC further regularizes the diffusion
target and guides the distillation process utilizing neighborhood similarity.
Extensive experiments demonstrate that DiFiC outperforms both state-of-the-art
discriminative and generative clustering methods on four fine-grained image
clustering benchmarks. We hope the success of DiFiC will inspire future
research to unlock the potential of diffusion models in tasks beyond
generation. The code will be released.",2024-12-25T08:55:48Z,http://arxiv.org/abs/2412.18838v1,"Ruohong Yang, Peng Hu, Xi Peng, Xiting Liu, Yunfan Li"
"MRI2Speech: Speech Synthesis from Articulatory Movements Recorded by
  Real-time MRI","Previous real-time MRI (rtMRI)-based speech synthesis models depend heavily
on noisy ground-truth speech. Applying loss directly over ground truth
mel-spectrograms entangles speech content with MRI noise, resulting in poor
intelligibility. We introduce a novel approach that adapts the multi-modal
self-supervised AV-HuBERT model for text prediction from rtMRI and incorporates
a new flow-based duration predictor for speaker-specific alignment. The
predicted text and durations are then used by a speech decoder to synthesize
aligned speech in any novel voice. We conduct thorough experiments on two
datasets and demonstrate our method's generalization ability to unseen
speakers. We assess our framework's performance by masking parts of the rtMRI
video to evaluate the impact of different articulators on text prediction. Our
method achieves a $15.18\%$ Word Error Rate (WER) on the USC-TIMIT MRI corpus,
marking a huge improvement over the current state-of-the-art. Speech samples
are available at \url{https://mri2speech.github.io/MRI2Speech/}",2024-12-25T08:49:43Z,http://arxiv.org/abs/2412.18836v1,"Neil Shah, Ayan Kashyap, Shirish Karande, Vineet Gandhi"
"Adaptive Rate Control for Deep Video Compression with Rate-Distortion
  Prediction","Deep video compression has made significant progress in recent years,
achieving rate-distortion performance that surpasses that of traditional video
compression methods. However, rate control schemes tailored for deep video
compression have not been well studied. In this paper, we propose a neural
network-based $\lambda$-domain rate control scheme for deep video compression,
which determines the coding parameter $\lambda$ for each to-be-coded frame
based on the rate-distortion-$\lambda$ (R-D-$\lambda$) relationships directly
learned from uncompressed frames, achieving high rate control accuracy
efficiently without the need for pre-encoding. Moreover, this content-aware
scheme is able to mitigate inter-frame quality fluctuations and adapt to abrupt
changes in video content. Specifically, we introduce two neural network-based
predictors to estimate the relationship between bitrate and $\lambda$, as well
as the relationship between distortion and $\lambda$ for each frame. Then we
determine the coding parameter $\lambda$ for each frame to achieve the target
bitrate. Experimental results demonstrate that our approach achieves high rate
control accuracy at the mini-GOP level with low time overhead and mitigates
inter-frame quality fluctuations across video content of varying resolutions.",2024-12-25T08:42:23Z,http://arxiv.org/abs/2412.18834v1,"Bowen Gu, Hao Chen, Ming Lu, Jie Yao, Zhan Ma"
"Federated Learning with Partially Labeled Data: A Conditional
  Distillation Approach","In medical imaging, developing generalized segmentation models that can
handle multiple organs and lesions is crucial. However, the scarcity of fully
annotated datasets and strict privacy regulations present significant barriers
to data sharing. Federated Learning (FL) allows decentralized model training,
but existing FL methods often struggle with partial labeling, leading to model
divergence and catastrophic forgetting. We propose ConDistFL, a novel FL
framework incorporating conditional distillation to address these challenges.
ConDistFL enables effective learning from partially labeled datasets,
significantly improving segmentation accuracy across distributed and
non-uniform datasets. In addition to its superior segmentation performance,
ConDistFL maintains computational and communication efficiency, ensuring its
scalability for real-world applications. Furthermore, ConDistFL demonstrates
remarkable generalizability, significantly outperforming existing FL methods in
out-of-federation tests, even adapting to unseen contrast phases (e.g.,
non-contrast CT images) in our experiments. Extensive evaluations on 3D CT and
2D chest X-ray datasets show that ConDistFL is an efficient, adaptable solution
for collaborative medical image segmentation in privacy-constrained settings.",2024-12-25T08:40:03Z,http://arxiv.org/abs/2412.18833v1,"Pochuan Wang, Chen Shen, Masahiro Oda, Chiou-Shann Fuh, Kensaku Mori, Weichung Wang, Holger R. Roth"
"Structured Speaker-Deficiency Adaptation of Foundation Models for
  Dysarthric and Elderly Speech Recognition","Data-intensive fine-tuning of speech foundation models (SFMs) to scarce and
diverse dysarthric and elderly speech leads to data bias and poor
generalization to unseen speakers. This paper proposes novel structured
speaker-deficiency adaptation approaches for SSL pre-trained SFMs on such data.
Speaker and speech deficiency invariant SFMs were constructed in their
supervised adaptive fine-tuning stage to reduce undue bias to training data
speakers, and serves as a more neutral and robust starting point for test time
unsupervised adaptation. Speech variability attributed to speaker identity and
speech impairment severity, or aging induced neurocognitive decline, are
modelled using separate adapters that can be combined together to model any
seen or unseen speaker. Experiments on the UASpeech dysarthric and DementiaBank
Pitt elderly speech corpora suggest structured speaker-deficiency adaptation of
HuBERT and Wav2vec2-conformer models consistently outperforms baseline SFMs
using either: a) no adapters; b) global adapters shared among all speakers; or
c) single attribute adapters modelling speaker or deficiency labels alone by
statistically significant WER reductions up to 3.01% and 1.50% absolute (10.86%
and 6.94% relative) on the two tasks respectively. The lowest published WER of
19.45% (49.34% on very low intelligibility, 33.17% on unseen words) is obtained
on the UASpeech test set of 16 dysarthric speakers.",2024-12-25T08:39:02Z,http://arxiv.org/abs/2412.18832v1,"Shujie Hu, Xurong Xie, Mengzhe Geng, Jiajun Deng, Zengrui Jin, Tianzi Wang, Mingyu Cui, Guinan Li, Zhaoqing Li, Helen Meng, Xunying Liu"
"PhyloGen: Language Model-Enhanced Phylogenetic Inference via Graph
  Structure Generation","Phylogenetic trees elucidate evolutionary relationships among species, but
phylogenetic inference remains challenging due to the complexity of combining
continuous (branch lengths) and discrete parameters (tree topology).
Traditional Markov Chain Monte Carlo methods face slow convergence and
computational burdens. Existing Variational Inference methods, which require
pre-generated topologies and typically treat tree structures and branch lengths
independently, may overlook critical sequence features, limiting their accuracy
and flexibility. We propose PhyloGen, a novel method leveraging a pre-trained
genomic language model to generate and optimize phylogenetic trees without
dependence on evolutionary models or aligned sequence constraints. PhyloGen
views phylogenetic inference as a conditionally constrained tree structure
generation problem, jointly optimizing tree topology and branch lengths through
three core modules: (i) Feature Extraction, (ii) PhyloTree Construction, and
(iii) PhyloTree Structure Modeling. Meanwhile, we introduce a Scoring Function
to guide the model towards a more stable gradient descent. We demonstrate the
effectiveness and robustness of PhyloGen on eight real-world benchmark
datasets. Visualization results confirm PhyloGen provides deeper insights into
phylogenetic relationships.",2024-12-25T08:33:05Z,http://arxiv.org/abs/2412.18827v1,"ChenRui Duan, Zelin Zang, Siyuan Li, Yongjie Xu, Stan Z. Li"
"CausalTAD: Causal Implicit Generative Model for Debiased Online
  Trajectory Anomaly Detection","Trajectory anomaly detection, aiming to estimate the anomaly risk of
trajectories given the Source-Destination (SD) pairs, has become a critical
problem for many real-world applications. Existing solutions directly train a
generative model for observed trajectories and calculate the conditional
generative probability $P({T}|{C})$ as the anomaly risk, where ${T}$ and ${C}$
represent the trajectory and SD pair respectively. However, we argue that the
observed trajectories are confounded by road network preference which is a
common cause of both SD distribution and trajectories. Existing methods ignore
this issue limiting their generalization ability on out-of-distribution
trajectories. In this paper, we define the debiased trajectory anomaly
detection problem and propose a causal implicit generative model, namely
CausalTAD, to solve it. CausalTAD adopts do-calculus to eliminate the
confounding bias of road network preference and estimates $P({T}|do({C}))$ as
the anomaly criterion. Extensive experiments show that CausalTAD can not only
achieve superior performance on trained trajectories but also generally improve
the performance of out-of-distribution data, with improvements of $2.1\% \sim
5.7\%$ and $10.6\% \sim 32.7\%$ respectively.",2024-12-25T08:20:52Z,http://arxiv.org/abs/2412.18820v1,"Wenbin Li, Di Yao, Chang Gong, Xiaokai Chu, Quanliang Jing, Xiaolei Zhou, Yuxuan Zhang, Yunxia Fan, Jingping Bi"
LLM-assisted vector similarity search,"As data retrieval demands become increasingly complex, traditional search
methods often fall short in addressing nuanced and conceptual queries. Vector
similarity search has emerged as a promising technique for finding semantically
similar information efficiently. However, its effectiveness diminishes when
handling intricate queries with contextual nuances. This paper explores a
hybrid approach combining vector similarity search with Large Language Models
(LLMs) to enhance search accuracy and relevance. The proposed two-step solution
first employs vector similarity search to shortlist potential matches, followed
by an LLM for context-aware ranking of the results. Experiments on structured
datasets demonstrate that while vector similarity search alone performs well
for straightforward queries, the LLM-assisted approach excels in processing
complex queries involving constraints, negations, or conceptual requirements.
By leveraging the natural language understanding capabilities of LLMs, this
method improves the accuracy of search results for complex tasks without
sacrificing efficiency. We also discuss real-world applications and propose
directions for future research to refine and scale this technique for diverse
datasets and use cases.
  Original article:
https://engineering.grab.com/llm-assisted-vector-similarity-search",2024-12-25T08:17:37Z,http://arxiv.org/abs/2412.18819v1,"Md Riyadh, Muqi Li, Felix Haryanto Lie, Jia Long Loh, Haotian Mi, Sayam Bohra"
GSAVS: Gaussian Splatting-based Autonomous Vehicle Simulator,"Modern autonomous vehicle simulators feature an ever-growing library of
assets, including vehicles, buildings, roads, pedestrians, and more. While this
level of customization proves beneficial when creating virtual urban
environments, this process becomes cumbersome when intending to train within a
digital twin or a duplicate of a real scene. Gaussian splatting emerged as a
powerful technique in scene reconstruction and novel view synthesis, boasting
high fidelity and rendering speeds. In this paper, we introduce GSAVS, an
autonomous vehicle simulator that supports the creation and development of
autonomous vehicle models. Every asset within the simulator is a 3D Gaussian
splat, including the vehicles and the environment. However, the simulator runs
within a classical 3D engine, rendering 3D Gaussian splats in real-time. This
allows the simulator to utilize the photorealism that 3D Gaussian splatting
boasts while providing the customization and ease of use of a classical 3D
engine.",2024-12-25T07:52:09Z,http://arxiv.org/abs/2412.18816v1,Rami Wilson
"Distortion-Aware Adversarial Attacks on Bounding Boxes of Object
  Detectors","Deep learning-based object detection has become ubiquitous in the last decade
due to its high accuracy in many real-world applications. With this growing
trend, these models are interested in being attacked by adversaries, with most
of the results being on classifiers, which do not match the context of
practical object detection. In this work, we propose a novel method to fool
object detectors, expose the vulnerability of state-of-the-art detectors, and
promote later works to build more robust detectors to adversarial examples. Our
method aims to generate adversarial images by perturbing object confidence
scores during training, which is crucial in predicting confidence for each
class in the testing phase. Herein, we provide a more intuitive technique to
embed additive noises based on detected objects' masks and the training loss
with distortion control over the original image by leveraging the gradient of
iterative images. To verify the proposed method, we perform adversarial attacks
against different object detectors, including the most recent state-of-the-art
models like YOLOv8, Faster R-CNN, RetinaNet, and Swin Transformer. We also
evaluate our technique on MS COCO 2017 and PASCAL VOC 2012 datasets and analyze
the trade-off between success attack rate and image distortion. Our experiments
show that the achievable success attack rate is up to $100$\% and up to $98$\%
when performing white-box and black-box attacks, respectively. The source code
and relevant documentation for this work are available at the following link:
https://github.com/anonymous20210106/attack_detector",2024-12-25T07:51:57Z,http://arxiv.org/abs/2412.18815v1,"Pham Phuc, Son Vuong, Khang Nguyen, Tuan Dang"
"DebiasDiff: Debiasing Text-to-image Diffusion Models with
  Self-discovering Latent Attribute Directions","While Diffusion Models (DM) exhibit remarkable performance across various
image generative tasks, they nonetheless reflect the inherent bias presented in
the training set. As DMs are now widely used in real-world applications, these
biases could perpetuate a distorted worldview and hinder opportunities for
minority groups. Existing methods on debiasing DMs usually requires model
re-training with a human-crafted reference dataset or additional classifiers,
which suffer from two major limitations: (1) collecting reference datasets
causes expensive annotation cost; (2) the debiasing performance is heavily
constrained by the quality of the reference dataset or the additional
classifier. To address the above limitations, we propose DebiasDiff, a
plug-and-play method that learns attribute latent directions in a
self-discovering manner, thus eliminating the reliance on such reference
dataset. Specifically, DebiasDiff consists of two parts: a set of attribute
adapters and a distribution indicator. Each adapter in the set aims to learn an
attribute latent direction, and is optimized via noise composition through a
self-discovering process. Then, the distribution indicator is multiplied by the
set of adapters to guide the generation process towards the prescribed
distribution. Our method enables debiasing multiple attributes in DMs
simultaneously, while remaining lightweight and easily integrable with other
DMs, eliminating the need for re-training. Extensive experiments on debiasing
gender, racial, and their intersectional biases show that our method
outperforms previous SOTA by a large margin.",2024-12-25T07:30:20Z,http://arxiv.org/abs/2412.18810v1,"Yilei Jiang, Weihong Li, Yiyuan Zhang, Minghong Cai, Xiangyu Yue"
Provable Uncertainty Decomposition via Higher-Order Calibration,"We give a principled method for decomposing the predictive uncertainty of a
model into aleatoric and epistemic components with explicit semantics relating
them to the real-world data distribution. While many works in the literature
have proposed such decompositions, they lack the type of formal guarantees we
provide. Our method is based on the new notion of higher-order calibration,
which generalizes ordinary calibration to the setting of higher-order
predictors that predict mixtures over label distributions at every point. We
show how to measure as well as achieve higher-order calibration using access to
$k$-snapshots, namely examples where each point has $k$ independent conditional
labels. Under higher-order calibration, the estimated aleatoric uncertainty at
a point is guaranteed to match the real-world aleatoric uncertainty averaged
over all points where the prediction is made. To our knowledge, this is the
first formal guarantee of this type that places no assumptions whatsoever on
the real-world data distribution. Importantly, higher-order calibration is also
applicable to existing higher-order predictors such as Bayesian and ensemble
models and provides a natural evaluation metric for such models. We demonstrate
through experiments that our method produces meaningful uncertainty
decompositions for image classification.",2024-12-25T07:26:36Z,http://arxiv.org/abs/2412.18808v1,"Gustaf Ahdritz, Aravind Gollakota, Parikshit Gopalan, Charlotte Peale, Udi Wieder"
FOR: Finetuning for Object Level Open Vocabulary Image Retrieval,"As working with large datasets becomes standard, the task of accurately
retrieving images containing objects of interest by an open set textual query
gains practical importance. The current leading approach utilizes a pre-trained
CLIP model without any adaptation to the target domain, balancing accuracy and
efficiency through additional post-processing. In this work, we propose FOR:
Finetuning for Object-centric Open-vocabulary Image Retrieval, which allows
finetuning on a target dataset using closed-set labels while keeping the
visual-language association crucial for open vocabulary retrieval. FOR is based
on two design elements: a specialized decoder variant of the CLIP head
customized for the intended task, and its coupling within a multi-objective
training framework. Together, these design choices result in a significant
increase in accuracy, showcasing improvements of up to 8 mAP@50 points over
SoTA across three datasets. Additionally, we demonstrate that FOR is also
effective in a semi-supervised setting, achieving impressive results even when
only a small portion of the dataset is labeled.",2024-12-25T07:08:51Z,http://arxiv.org/abs/2412.18806v1,"Hila Levi, Guy Heller, Dan Levi"
Time-Periodic Solutions for Hyperbolic-Parabolic Systems,"Time-periodic weak solutions for a coupled hyperbolic-parabolic system are
obtained. A linear heat and wave equation are considered on two respective
$d$-dimensional spatial domains that share a common $(d-1)$-dimensional
interface $\Gamma$. The system is only partially damped, leading to an
indeterminate case for existing theory (Galdi et al., 2014). We construct
periodic solutions by obtaining novel a priori estimates for the coupled
system, reconstructing the total energy via the interface $\Gamma$. As a
byproduct, geometric constraints manifest on the wave domain which are
reminiscent of classical boundary control conditions for wave stabilizability.
We note a ``loss"" of regularity between the forcing and solution which is
greater than that associated with the heat-wave Cauchy problem. However, we
consider a broader class of spatial domains and mitigate this regularity loss
by trading time and space differentiations, a feature unique to the periodic
setting. This seems to be the first constructive result addressing existence
and uniqueness of periodic solutions in the heat-wave context, where no
dissipation is present in the wave interior. Our results speak to the open
problem of the (non-)emergence of resonance in complex systems, and are readily
generalizable to related systems and certain nonlinear cases.",2024-12-25T06:42:30Z,http://arxiv.org/abs/2412.18801v1,"Stanislav Mosny, Boris Muha, Sebastian Schwarzacher, Justin T. Webster"
"Improving Generated and Retrieved Knowledge Combination Through
  Zero-shot Generation","Open-domain Question Answering (QA) has garnered substantial interest by
combining the advantages of faithfully retrieved passages and relevant passages
generated through Large Language Models (LLMs). However, there is a lack of
definitive labels available to pair these sources of knowledge. In order to
address this issue, we propose an unsupervised and simple framework called
Bi-Reranking for Merging Generated and Retrieved Knowledge (BRMGR), which
utilizes re-ranking methods for both retrieved passages and LLM-generated
passages. We pair the two types of passages using two separate re-ranking
methods and then combine them through greedy matching. We demonstrate that
BRMGR is equivalent to employing a bipartite matching loss when assigning each
retrieved passage with a corresponding LLM-generated passage. The application
of our model yielded experimental results from three datasets, improving their
performance by +1.7 and +1.6 on NQ and WebQ datasets, respectively, and
obtaining comparable result on TriviaQA dataset when compared to competitive
baselines.",2024-12-25T06:40:36Z,http://arxiv.org/abs/2412.18800v1,"Xinkai Du, Quanjie Han, Chao Lv, Yan Liu, Yalin Sun, Hao Shu, Hongbo Shan, Maosong Sun"
Quantifying the Risk of Pastoral Conflict in 4 Central African Countries,"Climate change is becoming a widely recognized risk factor of farmer-herder
conflict in Africa. Using an 8 year dataset (Jan 2015 to Sep 2022) of detailed
weather and terrain data across four African nations, we apply statistical and
machine learning methods to analyze pastoral conflict. We test hypotheses
linking these variables with pastoral conflict within each country using
geospatial and statistical analysis. Complementing this analysis are risk maps
automatically updated for decision-makers. Our models estimate which cells have
a high likelihood of experiencing pastoral conflict with high predictive
accuracy and study the variation of this accuracy with the granularity of the
cells.",2024-12-25T06:37:29Z,http://arxiv.org/abs/2412.18799v1,"Lirika Solaa, Youdinghuan Chen, Samantha K. Murphy, V. S. Subrahmanian"
"Ister: Inverted Seasonal-Trend Decomposition Transformer for Explainable
  Multivariate Time Series Forecasting","In long-term time series forecasting, Transformer-based models have achieved
great success, due to its ability to capture long-range dependencies. However,
existing transformer-based methods face challenges in accurately identifying
which variables play a pivotal role in the prediction process and tend to
overemphasize noisy channels, thereby limiting the interpretability and
practical effectiveness of the models. Besides, it faces scalability issues due
to quadratic computational complexity of self-attention. In this paper, we
propose a new model named Inverted Seasonal-Trend Decomposition Transformer
(Ister), which addresses these challenges in long-term multivariate time series
forecasting by designing an improved Transformer-based structure. Ister firstly
decomposes original time series into seasonal and trend components. Then we
propose a new Dot-attention mechanism to process the seasonal component, which
improves both accuracy, computation complexity and interpretability. Upon
completion of the training phase, it allows users to intuitively visualize the
significance of each feature in the overall prediction. We conduct
comprehensive experiments, and the results show that Ister achieves
state-of-the-art (SOTA) performance on multiple datasets, surpassing existing
models in long-term prediction tasks.",2024-12-25T06:37:19Z,http://arxiv.org/abs/2412.18798v1,"Fanpu Cao, Shu Yang, Zhengjian Chen, Ye Liu, Laizhong Cui"
"DRDM: A Disentangled Representations Diffusion Model for Synthesizing
  Realistic Person Images","Person image synthesis with controllable body poses and appearances is an
essential task owing to the practical needs in the context of virtual try-on,
image editing and video production. However, existing methods face significant
challenges with details missing, limbs distortion and the garment style
deviation. To address these issues, we propose a Disentangled Representations
Diffusion Model (DRDM) to generate photo-realistic images from source portraits
in specific desired poses and appearances. First, a pose encoder is responsible
for encoding pose features into a high-dimensional space to guide the
generation of person images. Second, a body-part subspace decoupling block
(BSDB) disentangles features from the different body parts of a source figure
and feeds them to the various layers of the noise prediction block, thereby
supplying the network with rich disentangled features for generating a
realistic target image. Moreover, during inference, we develop a parsing
map-based disentangled classifier-free guided sampling method, which amplifies
the conditional signals of texture and pose. Extensive experimental results on
the Deepfashion dataset demonstrate the effectiveness of our approach in
achieving pose transfer and appearance control.",2024-12-25T06:36:24Z,http://arxiv.org/abs/2412.18797v1,"Enbo Huang, Yuan Zhang, Faliang Huang, Guangyu Zhang, Yang Liu"
"Unveiling the local elemental arrangements across the interfaces inside
  CdSe/Cd1-xZnxS core-shell and CdSe/CdS/ Cd1-xZnxS core-crown-shell quantum
  wells","We report on a systematic study of the Cd, Zn, Se, and S elemental
distributions across the interfaces in CdSe/Cd1-xZnxS core-shell and
CdSe/CdS/Cd1-xZnxS core-crown-shell quantum wells with the CdSe core thickness
ranging from 3.5 to 5.5 ML. By processing the XAS data, we observe that the
Cd-Se bonds dominate at the CdSe/Cd1-xZnxS core-shell interface of structures
with the 3.5 ML cores, while the Cd-Se bonds were more abundant in the cases of
the 4.5 and 5.5 ML cores. The complementary information about prevailing bonds
were extracted for other constituting elements, thus, describing the
distribution of the elements at the core-shell interface of CdSe-based NPLs.
The naked CdSe cores are covered with an organic shell via bridging oxygen
atoms. Also, we address the issue of stability of such core-shell systems over
the time. We demonstrate that after a half year of aging of the
commercial-ready 4.5 ML CdSe/CdxZn1-xS NPLs, the Cd-Se bonds become more
evident due to the partial degradation of the Cd-S bonds. This is the first
experimental assessment of prevailing interatomic bonds at the core-shell
interface in the CdSe-based NPLs of incremental structural heterogeneity,
providing factual evidences about the elemental arrangement inside the
core-crown-shell NPLs and the growth path of crowns and shells.",2024-12-25T06:28:08Z,http://arxiv.org/abs/2412.18795v1,"Tatiana Lastovina, Oleg Usoltsev, Furkan Isik, Andriy Budnyk, Messaoud Harfouche, Betul Canimkurbey, Hilmi Volkan Demir"
Torque-Aware Momentum,"Efficiently exploring complex loss landscapes is key to the performance of
deep neural networks. While momentum-based optimizers are widely used in
state-of-the-art setups, classical momentum can still struggle with large,
misaligned gradients, leading to oscillations. To address this, we propose
Torque-Aware Momentum (TAM), which introduces a damping factor based on the
angle between the new gradients and previous momentum, stabilizing the update
direction during training. Empirical results show that TAM, which can be
combined with both SGD and Adam, enhances exploration, handles distribution
shifts more effectively, and improves generalization performance across various
tasks, including image classification and large language model fine-tuning,
when compared to classical momentum-based optimizers.",2024-12-25T05:58:07Z,http://arxiv.org/abs/2412.18790v1,"Pranshu Malviya, Goncalo Mordido, Aristide Baratin, Reza Babanezhad Harikandeh, Gintare Karolina Dziugaite, Razvan Pascanu, Sarath Chandar"
On Improved Regret Bounds In Bayesian Optimization with Gaussian Noise,"Bayesian optimization (BO) with Gaussian process (GP) surrogate models is a
powerful black-box optimization method. Acquisition functions are a critical
part of a BO algorithm as they determine how the new samples are selected. Some
of the most widely used acquisition functions include upper confidence bound
(UCB) and Thompson sampling (TS). The convergence analysis of BO algorithms has
focused on the cumulative regret under both the Bayesian and frequentist
settings for the objective. In this paper, we establish new pointwise bounds on
the prediction error of GP under the frequentist setting with Gaussian noise.
Consequently, we prove improved convergence rates of cumulative regret bound
for both GP-UCB and GP-TS. Of note, the new prediction error bound under
Gaussian noise can be applied to general BO algorithms and convergence
analysis, e.g., the asymptotic convergence of expected improvement (EI) with
noise.",2024-12-25T05:57:27Z,http://arxiv.org/abs/2412.18789v1,"Jingyi Wang, Haowei Wang, Cosmin G. Petra, Nai-Yuan Chiang"
"Computational Analysis of Yaredawi YeZema Silt in Ethiopian Orthodox
  Tewahedo Church Chants","Despite its musicological, cultural, and religious significance, the
Ethiopian Orthodox Tewahedo Church (EOTC) chant is relatively underrepresented
in music research. Historical records, including manuscripts, research papers,
and oral traditions, confirm Saint Yared's establishment of three canonical
EOTC chanting modes during the 6th century. This paper attempts to investigate
the EOTC chants using music information retrieval (MIR) techniques. Among the
research questions regarding the analysis and understanding of EOTC chants,
Yaredawi YeZema Silt, namely the mode of chanting adhering to Saint Yared's
standards, is of primary importance. Therefore, we consider the task of
Yaredawi YeZema Silt classification in EOTC chants by introducing a new dataset
and showcasing a series of classification experiments for this task. Results
show that using the distribution of stabilized pitch contours as the feature
representation on a simple neural network-based classifier becomes an effective
solution. The musicological implications and insights of such results are
further discussed through a comparative study with the previous ethnomusicology
literature on EOTC chants. By making this dataset publicly accessible, we aim
to promote future exploration and analysis of EOTC chants and highlight
potential directions for further research, thereby fostering a deeper
understanding and preservation of this unique spiritual and cultural heritage.",2024-12-25T05:42:56Z,http://arxiv.org/abs/2412.18788v1,"Mequanent Argaw Muluneh, Yan-Tsung Peng, Li Su"
"Thermal-Mechanical Physics Informed Deep Learning For Fast Prediction of
  Thermal Stress Evolution in Laser Metal Deposition","Understanding thermal stress evolution in metal additive manufacturing (AM)
is crucial for producing high-quality components. Recent advancements in
machine learning (ML) have shown great potential for modeling complex
multiphysics problems in metal AM. While physics-based simulations face the
challenge of high computational costs, conventional data-driven ML models
require large, labeled training datasets to achieve accurate predictions.
Unfortunately, generating large datasets for ML model training through
time-consuming experiments or high-fidelity simulations is highly expensive in
metal AM. To address these challenges, this study introduces a physics-informed
neural network (PINN) framework that incorporates governing physical laws into
deep neural networks (NNs) to predict temperature and thermal stress evolution
during the laser metal deposition (LMD) process. The study also discusses the
enhanced accuracy and efficiency of the PINN model when supplemented with small
simulation data. Furthermore, it highlights the PINN transferability, enabling
fast predictions with a set of new process parameters using a pre-trained PINN
model as an online soft sensor, significantly reducing computation time
compared to physics-based numerical models while maintaining accuracy.",2024-12-25T05:37:48Z,http://arxiv.org/abs/2412.18786v1,"R. Sharma, Y. B. Guo"
"Simultaneously Recovering Multi-Person Meshes and Multi-View Cameras
  with Human Semantics","Dynamic multi-person mesh recovery has broad applications in sports
broadcasting, virtual reality, and video games. However, current multi-view
frameworks rely on a time-consuming camera calibration procedure. In this work,
we focus on multi-person motion capture with uncalibrated cameras, which mainly
faces two challenges: one is that inter-person interactions and occlusions
introduce inherent ambiguities for both camera calibration and motion capture;
the other is that a lack of dense correspondences can be used to constrain
sparse camera geometries in a dynamic multi-person scene. Our key idea is to
incorporate motion prior knowledge to simultaneously estimate camera parameters
and human meshes from noisy human semantics. We first utilize human information
from 2D images to initialize intrinsic and extrinsic parameters. Thus, the
approach does not rely on any other calibration tools or background features.
Then, a pose-geometry consistency is introduced to associate the detected
humans from different views. Finally, a latent motion prior is proposed to
refine the camera parameters and human motions. Experimental results show that
accurate camera parameters and human motions can be obtained through a one-step
reconstruction. The code are publicly available
at~\url{https://github.com/boycehbz/DMMR}.",2024-12-25T05:35:30Z,http://arxiv.org/abs/2412.18785v1,"Buzhen Huang, Jingyi Ju, Yuan Shu, Yangang Wang"
"Robustness Evaluation of Offline Reinforcement Learning for Robot
  Control Against Action Perturbations","Offline reinforcement learning, which learns solely from datasets without
environmental interaction, has gained attention. This approach, similar to
traditional online deep reinforcement learning, is particularly promising for
robot control applications. Nevertheless, its robustness against real-world
challenges, such as joint actuator faults in robots, remains a critical
concern. This study evaluates the robustness of existing offline reinforcement
learning methods using legged robots from OpenAI Gym based on average episodic
rewards. For robustness evaluation, we simulate failures by incorporating both
random and adversarial perturbations, representing worst-case scenarios, into
the joint torque signals. Our experiments show that existing offline
reinforcement learning methods exhibit significant vulnerabilities to these
action perturbations and are more vulnerable than online reinforcement learning
methods, highlighting the need for more robust approaches in this field.",2024-12-25T05:02:22Z,http://arxiv.org/abs/2412.18781v1,"Shingo Ayabe, Takuto Otomo, Hiroshi Kera, Kazuhiko Kawamoto"
"Skeleton-based Action Recognition with Non-linear Dependency Modeling
  and Hilbert-Schmidt Independence Criterion","Human skeleton-based action recognition has long been an indispensable aspect
of artificial intelligence. Current state-of-the-art methods tend to consider
only the dependencies between connected skeletal joints, limiting their ability
to capture non-linear dependencies between physically distant joints. Moreover,
most existing approaches distinguish action classes by estimating the
probability density of motion representations, yet the high-dimensional nature
of human motions invokes inherent difficulties in accomplishing such
measurements. In this paper, we seek to tackle these challenges from two
directions: (1) We propose a novel dependency refinement approach that
explicitly models dependencies between any pair of joints, effectively
transcending the limitations imposed by joint distance. (2) We further propose
a framework that utilizes the Hilbert-Schmidt Independence Criterion to
differentiate action classes without being affected by data dimensionality, and
mathematically derive learning objectives guaranteeing precise recognition.
Empirically, our approach sets the state-of-the-art performance on NTU RGB+D,
NTU RGB+D 120, and Northwestern-UCLA datasets.",2024-12-25T05:02:11Z,http://arxiv.org/abs/2412.18780v1,Yuheng Yang
"Integrating Zero-Shot Classification to Advance Long COVID Literature: A
  Systematic Social Media-Centered Review","Long COVID continues to challenge public health by affecting a significant
segment of individuals who have recovered from acute SARS-CoV-2 infection yet
endure prolonged and often debilitating symptoms. Social media has emerged as a
vital resource for those seeking real-time information, peer support, and
validating their health concerns related to Long COVID. This paper examines
recent works focusing on mining, analyzing, and interpreting user-generated
content on social media platforms such as Twitter, Reddit, Facebook, and
YouTube to capture the broader discourse on persistent post-COVID conditions. A
novel transformer-based zero-shot learning approach serves as the foundation
for classifying research papers in this area into four primary categories:
Clinical or Symptom Characterization, Advanced NLP or Computational Methods,
Policy, Advocacy, or Public Health Communication, and Online Communities and
Social Support. This methodology showcases the adaptability of advanced
language models in categorizing research papers without predefined training
labels, thus enabling a more rapid and scalable assessment of existing
literature. This review highlights the multifaceted nature of Long COVID
research, where computational techniques applied to social media data reveal
insights into narratives of individuals suffering from Long COVID. This review
also demonstrates the capacity of social media analytics to inform clinical
practice and contribute to policy-making related to Long COVID.",2024-12-25T05:01:17Z,http://arxiv.org/abs/2412.18779v1,Nirmalya Thakur
"Unified Local and Global Attention Interaction Modeling for Vision
  Transformers","We present a novel method that extends the self-attention mechanism of a
vision transformer (ViT) for more accurate object detection across diverse
datasets. ViTs show strong capability for image understanding tasks such as
object detection, segmentation, and classification. This is due in part to
their ability to leverage global information from interactions among visual
tokens. However, the self-attention mechanism in ViTs are limited because they
do not allow visual tokens to exchange local or global information with
neighboring features before computing global attention. This is problematic
because tokens are treated in isolation when attending (matching) to other
tokens, and valuable spatial relationships are overlooked. This isolation is
further compounded by dot-product similarity operations that make tokens from
different semantic classes appear visually similar. To address these
limitations, we introduce two modifications to the traditional self-attention
framework; a novel aggressive convolution pooling strategy for local feature
mixing, and a new conceptual attention transformation to facilitate interaction
and feature exchange between semantic concepts. Experimental results
demonstrate that local and global information exchange among visual features
before self-attention significantly improves performance on challenging object
detection tasks and generalizes across multiple benchmark datasets and
challenging medical datasets. We publish source code and a novel dataset of
cancerous tumors (chimeric cell clusters).",2024-12-25T04:53:19Z,http://arxiv.org/abs/2412.18778v1,"Tan Nguyen, Coy D. Heldermon, Corey Toler-Franklin"
ObitoNet: Multimodal High-Resolution Point Cloud Reconstruction,"ObitoNet employs a Cross Attention mechanism to integrate multimodal inputs,
where Vision Transformers (ViT) extract semantic features from images and a
point cloud tokenizer processes geometric information using Farthest Point
Sampling (FPS) and K Nearest Neighbors (KNN) for spatial structure capture. The
learned multimodal features are fed into a transformer-based decoder for
high-resolution point cloud reconstruction. This approach leverages the
complementary strengths of both modalities rich image features and precise
geometric details ensuring robust point cloud generation even in challenging
conditions such as sparse or noisy data.",2024-12-25T04:34:22Z,http://arxiv.org/abs/2412.18775v1,"Apoorv Thapliyal, Vinay Lanka, Swathi Baskaran"
Learning Broken Symmetries with Approximate Invariance,"Recognizing symmetries in data allows for significant boosts in neural
network training, which is especially important where training data are
limited. In many cases, however, the exact underlying symmetry is present only
in an idealized dataset, and is broken in actual data, due to asymmetries in
the detector, or varying response resolution as a function of particle
momentum. Standard approaches, such as data augmentation or equivariant
networks fail to represent the nature of the full, broken symmetry, effectively
overconstraining the response of the neural network. We propose a learning
model which balances the generality and asymptotic performance of unconstrained
networks with the rapid learning of constrained networks. This is achieved
through a dual-subnet structure, where one network is constrained by the
symmetry and the other is not, along with a learned symmetry factor. In a
simplified toy example that demonstrates violation of Lorentz invariance, our
model learns as rapidly as symmetry-constrained networks but escapes its
performance limitations.",2024-12-25T04:29:04Z,http://arxiv.org/abs/2412.18773v1,"Seth Nabat, Aishik Ghosh, Edmund Witkowski, Gregor Kasieczka, Daniel Whiteson"
Hierarchical Multi-Graphs Learning for Robust Group Re-Identification,"Group Re-identification (G-ReID) faces greater complexity than individual
Re-identification (ReID) due to challenges like mutual occlusion, dynamic
member interactions, and evolving group structures. Prior graph-based
approaches have aimed to capture these dynamics by modeling the group as a
single topological structure. However, these methods struggle to generalize
across diverse group compositions, as they fail to fully represent the
multifaceted relationships within the group.
  In this study, we introduce a Hierarchical Multi-Graphs Learning (HMGL)
framework to address these challenges. Our approach models the group as a
collection of multi-relational graphs, leveraging both explicit features (such
as occlusion, appearance, and foreground information) and implicit dependencies
between members. This hierarchical representation, encoded via a Multi-Graphs
Neural Network (MGNN), allows us to resolve ambiguities in member
relationships, particularly in complex, densely populated scenes. To further
enhance matching accuracy, we propose a Multi-Scale Matching (MSM) algorithm,
which mitigates issues of member information ambiguity and sensitivity to hard
samples, improving robustness in challenging scenarios.
  Our method achieves state-of-the-art performance on two standard benchmarks,
CSG and RoadGroup, with Rank-1/mAP scores of 95.3%/94.4% and 93.9%/95.4%,
respectively. These results mark notable improvements of 1.7% and 2.5% in
Rank-1 accuracy over existing approaches.",2024-12-25T03:33:43Z,http://arxiv.org/abs/2412.18766v1,"Ruiqi Liu, Xingyu Liu, Xiaohao Xu, Yixuan Zhang, Yongxin Ge, Lubin Weng"
"Evaluating authorship disambiguation quality through anomaly analysis on
  researchers' career transition","Authorship disambiguation is crucial for advancing studies in science of
science. However, assessing the quality of authorship disambiguation in
large-scale databases remains challenging since it is difficult to manually
curate a gold-standard dataset that contains disambiguated authors. Through
estimating the timing of when 5.8 million biomedical researchers became
independent Principal Investigators (PIs) with authorship metadata extracted
from the OpenAlex -- the largest open-source bibliometric database -- we
unexpectedly discovered an anomaly: over 60% of researchers appeared as the
last authors in their first career year. We hypothesized that this improbable
finding results from poor name disambiguation, suggesting that such an anomaly
may serve as an indicator of low-quality authorship disambiguation. Our
findings indicated that authors who lack affiliation information, which makes
it more difficult to disambiguate, were far more likely to exhibit this anomaly
compared to those who included their affiliation information. In contrast,
authors with Open Researcher and Contributor ID (ORCID) -- expected to have
higher quality disambiguation -- showed significantly lower anomaly rates. We
further applied this approach to examine the authorship disambiguation quality
by gender over time, and we found that the quality of disambiguation for female
authors was lower than that for male authors before 2010, suggesting that
gender disparity findings based on pre-2010 data may require careful
reexamination. Our results provide a framework for systematically evaluating
authorship disambiguation quality in various contexts, facilitating future
improvements in efforts to authorship disambiguation.",2024-12-25T03:04:46Z,http://arxiv.org/abs/2412.18757v1,"Huaxia Zhou, Mengyi Sun"
"Towards a Statistical Understanding of Neural Networks: Beyond the
  Neural Tangent Kernel Theories","A primary advantage of neural networks lies in their feature learning
characteristics, which is challenging to theoretically analyze due to the
complexity of their training dynamics. We propose a new paradigm for studying
feature learning and the resulting benefits in generalizability. After
reviewing the neural tangent kernel (NTK) theory and recent results in kernel
regression, which address the generalization issue of sufficiently wide neural
networks, we examine limitations and implications of the fixed kernel theory
(as the NTK theory) and review recent theoretical advancements in feature
learning. Moving beyond the fixed kernel/feature theory, we consider neural
networks as adaptive feature models. Finally, we propose an over-parameterized
Gaussian sequence model as a prototype model to study the feature learning
characteristics of neural networks.",2024-12-25T03:03:58Z,http://arxiv.org/abs/2412.18756v1,"Haobo Zhang, Jianfa Lai, Yicheng Li, Qian Lin, Jun S. Liu"
