{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3489d3da-4139-459b-9afa-cf22018e1887",
   "metadata": {},
   "source": [
    "# Scraping Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "661c1f5f-824b-40a4-9563-d3af730be9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from beautifulsoup4) (2.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\HP 840 G8\\Desktop\\skripsi\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (5.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\HP 840 G8\\Desktop\\skripsi\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\HP 840 G8\\Desktop\\skripsi\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4\n",
    "!pip install lxml\n",
    "!pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c2ce05-0aae-477c-be14-76354acd51a0",
   "metadata": {},
   "source": [
    "## requests adalah library Python yang digunakan untuk mengirim permintaan HTTP dan menerima respons dari server. Fungsinya termasuk:\n",
    "\n",
    "Mengambil data dari URL: Anda dapat menggunakan requests untuk mengunduh konten halaman web (HTML) atau API endpoint.\n",
    "\n",
    "Fitur penting:\n",
    "\n",
    "Mendukung berbagai metode HTTP: GET, POST, PUT, DELETE, dll.\n",
    "Mendukung pengaturan headers, cookies, dan parameters.\n",
    "Menangani koneksi HTTPS secara otomatis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f28c1ef-fe89-4702-acd2-67c79ec87daa",
   "metadata": {},
   "source": [
    "# ArXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0546cea9-dab6-4eef-bfa7-415e374d62c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "658aed32-dc8f-41b3-9242-bc58b291efc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_arxiv_by_date(query, start_date, end_date, total_results=5000, batch_size=100):\n",
    "    base_url = \"https://export.arxiv.org/api/query\"\n",
    "    all_articles = []\n",
    "    \n",
    "    # Membagi permintaan menjadi batch berdasarkan total hasil dan batch size\n",
    "    for start in range(0, total_results, batch_size):\n",
    "        params = {\n",
    "            \"search_query\": f\"all:{query}\",\n",
    "            \"start\": start,\n",
    "            \"max_results\": batch_size,\n",
    "            \"sortBy\": \"relevance\",\n",
    "            \"sortOrder\": \"descending\",\n",
    "            \"from_date\": start_date,\n",
    "            \"to_date\": end_date\n",
    "        }\n",
    "        \n",
    "        # Mengirim permintaan GET\n",
    "        response = requests.get(base_url, params=params)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch data: {response.status_code}\")\n",
    "            return []\n",
    "\n",
    "        # Menggunakan xml.etree.ElementTree untuk memparsing XML\n",
    "        root = ET.fromstring(response.text)\n",
    "        entries = root.findall(\"{http://www.w3.org/2005/Atom}entry\")\n",
    "        \n",
    "        # Mengolah artikel-artikel yang didapatkan\n",
    "        for entry in entries:\n",
    "            title = entry.find(\"{http://www.w3.org/2005/Atom}title\").text\n",
    "            summary = entry.find(\"{http://www.w3.org/2005/Atom}summary\").text\n",
    "            published = entry.find(\"{http://www.w3.org/2005/Atom}published\").text\n",
    "            link = entry.find(\"{http://www.w3.org/2005/Atom}id\").text\n",
    "\n",
    "            # Menambahkan informasi author dan affiliation jika ada\n",
    "            authors = entry.findall(\"{http://www.w3.org/2005/Atom}author\")\n",
    "            author_list = []\n",
    "            institution_list = []\n",
    "            for author in authors:\n",
    "                name = author.find(\"{http://www.w3.org/2005/Atom}name\")\n",
    "                if name is not None:\n",
    "                    author_list.append(name.text.strip())\n",
    "                # Jika ada informasi institusi, dapat diambil dari elemen affiliation\n",
    "                affiliation = author.find(\"{http://arxiv.org/schemas/atom}affiliation\")\n",
    "                if affiliation is not None:\n",
    "                    institution_list.append(affiliation.text.strip())\n",
    "            \n",
    "            # Jika tidak ada affiliation yang ditemukan, bisa menambahkan \"Unknown\"\n",
    "            if not institution_list:\n",
    "                institution_list.append(\"Unknown\")\n",
    "\n",
    "            # Menambahkan artikel ke dalam daftar hasil\n",
    "            article = {\n",
    "                \"Title\": title,\n",
    "                \"Summary\": summary,\n",
    "                \"Published\": published,\n",
    "                \"Link\": link,\n",
    "                \"Authors\": \", \".join(author_list),\n",
    "                \"Institutions\": \", \".join(institution_list)\n",
    "            }\n",
    "            all_articles.append(article)\n",
    "\n",
    "        if len(entries) < batch_size:\n",
    "            break  # Jika jumlah artikel kurang dari batch_size, hentikan permintaan lebih lanjut\n",
    "\n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fd5081c3-c3bf-4ef8-86f1-55b2bb1e10b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"neural network OR machine learning OR AI OR artificial intelligence OR data mining\"\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = \"2024-12-29\"\n",
    "total_results = 5000\n",
    "articles = scrape_arxiv_by_date(query, start_date, end_date, total_results=total_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ed39bc3a-cab6-4f5f-b095-930a4685c523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Behave-XAI: Deep Explainable Learning of Behavioral Representational\n",
      "  Data\n",
      "Summary:   According to the latest trend of artificial intelligence, AI-systems needs to\n",
      "clarify regarding general,specific decisions,services provided by it. Only\n",
      "consumer is satisfied, with explanation , for example, why any classification\n",
      "result is the outcome of any given time. This actually motivates us using\n",
      "explainable or human understandable AI for a behavioral mining scenario, where\n",
      "users engagement on digital platform is determined from context, such as\n",
      "emotion, activity, weather, etc. However, the output of AI-system is not always\n",
      "systematically correct, and often systematically correct, but apparently\n",
      "not-perfect and thereby creating confusions, such as, why the decision is\n",
      "given? What is the reason underneath? In this context, we first formulate the\n",
      "behavioral mining problem in deep convolutional neural network architecture.\n",
      "Eventually, we apply a recursive neural network due to the presence of\n",
      "time-series data from users physiological and environmental sensor-readings.\n",
      "Once the model is developed, explanations are presented with the advent of XAI\n",
      "models in front of users. This critical step involves extensive trial with\n",
      "users preference on explanations over conventional AI, judgement of credibility\n",
      "of explanation.\n",
      "\n",
      "Published: 2022-12-30T18:08:48Z\n",
      "Link: http://arxiv.org/abs/2301.00016v2\n",
      "Authors: Rossi Kamal, Zuzana Kubincova\n",
      "Institutions: Unknown\n",
      "\n",
      "Title: Deep Neural Mobile Networking\n",
      "Summary:   The next generation of mobile networks is set to become increasingly complex,\n",
      "as these struggle to accommodate tremendous data traffic demands generated by\n",
      "ever-more connected devices that have diverse performance requirements in terms\n",
      "of throughput, latency, and reliability. This makes monitoring and managing the\n",
      "multitude of network elements intractable with existing tools and impractical\n",
      "for traditional machine learning algorithms that rely on hand-crafted feature\n",
      "engineering. In this context, embedding machine intelligence into mobile\n",
      "networks becomes necessary, as this enables systematic mining of valuable\n",
      "information from mobile big data and automatically uncovering correlations that\n",
      "would otherwise have been too difficult to extract by human experts. In\n",
      "particular, deep learning based solutions can automatically extract features\n",
      "from raw data, without human expertise. The performance of artificial\n",
      "intelligence (AI) has achieved in other domains draws unprecedented interest\n",
      "from both academia and industry in employing deep learning approaches to\n",
      "address technical challenges in mobile networks. This thesis attacks important\n",
      "problems in the mobile networking area from various perspectives by harnessing\n",
      "recent advances in deep neural networks.\n",
      "\n",
      "Published: 2020-10-23T09:23:36Z\n",
      "Link: http://arxiv.org/abs/2011.05267v1\n",
      "Authors: Chaoyun Zhang\n",
      "Institutions: Unknown\n",
      "\n",
      "Title: Learning Random Numbers to Realize Appendable Memory System for\n",
      "  Artificial Intelligence to Acquire New Knowledge after Deployment\n",
      "Summary:   In this study, we developed a learning method for constructing a neural\n",
      "network system capable of memorizing data and recalling it without parameter\n",
      "updates. The system we built using this method is called the Appendable Memory\n",
      "system. The Appendable Memory system enables an artificial intelligence (AI) to\n",
      "acquire new knowledge even after deployment. It consists of two AIs: the\n",
      "Memorizer and the Recaller. This system is a key-value store built using neural\n",
      "networks. The Memorizer receives data and stores it in the Appendable Memory\n",
      "vector, which is dynamically updated when the AI acquires new knowledge.\n",
      "Meanwhile, the Recaller retrieves information from the Appendable Memory\n",
      "vector. What we want to teach AI in this study are the operations of memorizing\n",
      "and recalling information. However, traditional machine learning methods make\n",
      "AI learn features inherent in the learning dataset. We demonstrate that the\n",
      "systems we intend to create cannot be realized by current machine learning\n",
      "methods, that is, by merely repeating the input and output learning sequences\n",
      "with AI. Instead, we propose a method to teach AI to learn operations, by\n",
      "completely removing the features contained in the learning dataset.\n",
      "Specifically, we probabilized all the data involved in learning. This measure\n",
      "prevented AI from learning the features of the data. The learning method\n",
      "proposed in the study differs from traditional machine learning methods and\n",
      "provides fundamental approaches for building an AI system that can store\n",
      "information in a finite memory and recall it at a later date.\n",
      "\n",
      "Published: 2024-07-29T17:24:35Z\n",
      "Link: http://arxiv.org/abs/2407.20197v1\n",
      "Authors: Kazunori D Yamada\n",
      "Institutions: Unknown\n",
      "\n",
      "Title: Modeling the EdNet Dataset with Logistic Regression\n",
      "Summary:   Many of these challenges are won by neural network models created by\n",
      "full-time artificial intelligence scientists. Due to this origin, they have a\n",
      "black-box character that makes their use and application less clear to learning\n",
      "scientists. We describe our experience with competition from the perspective of\n",
      "educational data mining, a field founded in the learning sciences and connected\n",
      "with roots in psychology and statistics. We describe our efforts from the\n",
      "perspectives of learning scientists and the challenges to our methods, some\n",
      "real and some imagined. We also discuss some basic results in the Kaggle system\n",
      "and our thoughts on how those results may have been improved. Finally, we\n",
      "describe how learner model predictions are used to make pedagogical decisions\n",
      "for students. Their practical use entails a) model predictions and b) a\n",
      "decision rule (based on the predictions). We point out how increased model\n",
      "accuracy can be of limited practical utility, especially when paired with\n",
      "simple decision rules and argue instead for the need to further investigate\n",
      "optimal decision rules.\n",
      "\n",
      "Published: 2021-05-17T20:30:36Z\n",
      "Link: http://arxiv.org/abs/2105.08150v1\n",
      "Authors: Philip I. Pavlik Jr, Luke G. Eglington\n",
      "Institutions: Unknown\n",
      "\n",
      "Title: Causal Relationship Network of Risk Factors Impacting Workday Loss in\n",
      "  Underground Coal Mines\n",
      "Summary:   This study aims to establish the causal relationship network between various\n",
      "factors leading to workday loss in underground coal mines using a novel causal\n",
      "artificial intelligence (AI) method. The analysis utilizes data obtained from\n",
      "the National Institute for Occupational Safety and Health (NIOSH). A total of\n",
      "101,010 injury records from 3,982 unique underground coal mines spanning the\n",
      "years from 1990 to 2020 were extracted from the NIOSH database. Causal\n",
      "relationships were analyzed and visualized using a novel causal AI method\n",
      "called Grouped Greedy Equivalence Search (GGES). The impact of each variable on\n",
      "workday loss was assessed through intervention do-calculus adjustment (IDA)\n",
      "scores. Model training and validation were performed using the 10-fold\n",
      "cross-validation technique. Performance metrics, including adjacency precision\n",
      "(AP), adjacency recall (AR), arrowhead precision (AHP), and arrowhead recall\n",
      "(AHR), were utilized to evaluate the models. Findings revealed that after 2006,\n",
      "key direct causes of workday loss among mining employees included total mining\n",
      "experience, mean office employees, mean underground employees, county, and\n",
      "total mining experience (years). Total mining experience emerged as the most\n",
      "influential factor, whereas mean employees per mine exhibited the least\n",
      "influence. The analyses emphasized the significant role of total mining\n",
      "experience in determining workday loss. The models achieved optimal\n",
      "performance, with AP, AR, AHP, and AHR values measuring 0.694, 0.653, 0.386,\n",
      "and 0.345, respectively. This study demonstrates the feasibility of utilizing\n",
      "the new GGES method to clarify the causal factors behind the workday loss by\n",
      "analyzing employment demographics and injury records and establish their causal\n",
      "relationship network.\n",
      "\n",
      "Published: 2024-01-24T22:45:34Z\n",
      "Link: http://arxiv.org/abs/2402.05940v1\n",
      "Authors: Shangsi Ren, Cameron A. Beeche, Zhiyi Shi, Maria Acevedo Garcia, Katherine Zychowski, Shuguang Leng, Pedram Roghanchi, Jiantao Pu\n",
      "Institutions: Unknown\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Contoh artikel\n",
    "for article in articles[:5]:\n",
    "    print(f\"Title: {article['Title']}\")\n",
    "    print(f\"Summary: {article['Summary']}\")\n",
    "    print(f\"Published: {article['Published']}\")\n",
    "    print(f\"Link: {article['Link']}\")\n",
    "    print(f\"Authors: {article['Authors']}\")\n",
    "    print(f\"Institutions: {article['Institutions']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0b3b6858-2af9-468e-bbcf-453a97934468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data berhasil disimpan ke arxiv_articles.csv\n"
     ]
    }
   ],
   "source": [
    "def save_to_csv(articles, filename='arxiv_articles.csv'):\n",
    "    # Menentukan nama kolom yang akan disimpan ke dalam CSV\n",
    "    fieldnames = [\"Title\", \"Summary\", \"Published\", \"Link\", \"Authors\", \"Institutions\"]\n",
    "    \n",
    "    # Membuka file CSV untuk menulis\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        \n",
    "        # Menulis header\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Menulis data artikel ke dalam CSV\n",
    "        for article in articles:\n",
    "            writer.writerow(article)\n",
    "    \n",
    "    print(f\"Data berhasil disimpan ke {filename}\")\n",
    "\n",
    "# Menyimpan artikel ke CSV\n",
    "save_to_csv(articles, 'arxiv_articles.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dd84fb-44a4-4ec7-8bbe-05976df64ae1",
   "metadata": {},
   "source": [
    "## Query 2 : bert, biLSTM, information extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "204dcb9a-bdcb-44d8-8f0a-e1d1e4a52157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c157bdf0-84cc-4bc0-9ecd-1ffe6a6ce787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_arxiv(query, start_date, end_date, max_results=5000, batch_size=100):\n",
    "    base_url = \"http://export.arxiv.org/api/query\"\n",
    "    all_articles = []\n",
    "    total_fetched = 0\n",
    "    start_index = 0\n",
    "\n",
    "    while total_fetched < max_results:\n",
    "        # Build query parameters\n",
    "        params = {\n",
    "            \"search_query\": query,\n",
    "            \"start\": start_index,\n",
    "            \"max_results\": batch_size,\n",
    "            \"sortBy\": \"submittedDate\",\n",
    "            \"sortOrder\": \"descending\"\n",
    "        }\n",
    "        print(f\"Fetching articles {start_index + 1} to {start_index + batch_size}...\")\n",
    "        \n",
    "        # Make the request\n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: HTTP {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        # Parse the XML response\n",
    "        data = response.text\n",
    "        entries = data.split(\"<entry>\")\n",
    "        if len(entries) <= 1:\n",
    "            print(\"No more articles found.\")\n",
    "            break\n",
    "        \n",
    "        for entry in entries[1:]:\n",
    "            try:\n",
    "                title = entry.split(\"<title>\")[1].split(\"</title>\")[0].strip()\n",
    "                summary = entry.split(\"<summary>\")[1].split(\"</summary>\")[0].strip()\n",
    "                published = entry.split(\"<published>\")[1].split(\"</published>\")[0].strip()\n",
    "                link = entry.split('<id>')[1].split('</id>')[0].strip()\n",
    "                authors = \", \".join(\n",
    "                    author.split(\"</name>\")[0].split(\"<name>\")[1].strip()\n",
    "                    for author in entry.split(\"<author>\")[1:]\n",
    "                )\n",
    "                all_articles.append({\n",
    "                    \"Title\": title,\n",
    "                    \"Summary\": summary,\n",
    "                    \"Published\": published,\n",
    "                    \"Link\": link,\n",
    "                    \"Authors\": authors\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing entry: {e}\")\n",
    "        \n",
    "        # Update counters\n",
    "        num_articles = len(entries) - 1\n",
    "        total_fetched += num_articles\n",
    "        start_index += num_articles\n",
    "        \n",
    "        # Stop if no more articles\n",
    "        if num_articles < batch_size:\n",
    "            break\n",
    "        \n",
    "        # Delay to avoid overwhelming the server\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(f\"Fetched a total of {total_fetched} articles.\")\n",
    "    return pd.DataFrame(all_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f281d664-b4be-4e0b-accc-8608da732fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query and parameters\n",
    "query = \"information extraction OR BERT OR BiLSTM\"\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "max_results = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee77e15a-d249-4bf6-86a8-f21cb7f24ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles 1 to 100...\n",
      "Fetching articles 101 to 200...\n",
      "No more articles found.\n",
      "Fetched a total of 100 articles.\n"
     ]
    }
   ],
   "source": [
    "# Scrape data\n",
    "articles_df = scrape_arxiv(query, start_date, end_date, max_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdb25491-a8e1-4fbe-b92d-809d2a931609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'arxiv_articles(2).csv'\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV\n",
    "articles_df.to_csv(\"arxiv_articles(2).csv\", index=False)\n",
    "print(\"Data saved to 'arxiv_articles(2).csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a687bd-4970-4d78-a683-a24f6e0b39aa",
   "metadata": {},
   "source": [
    "## Query 3 : NLP, Transformers, RNN, CNN, big data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efd96b3c-ec9f-4906-9518-a623bafcffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_arxiv(query, start_date, end_date, max_results=5000, batch_size=100):\n",
    "    base_url = \"http://export.arxiv.org/api/query\"\n",
    "    all_articles = []\n",
    "    total_fetched = 0\n",
    "    start_index = 0\n",
    "\n",
    "    while total_fetched < max_results:\n",
    "        # Build query parameters\n",
    "        params = {\n",
    "            \"search_query\": query,\n",
    "            \"start\": start_index,\n",
    "            \"max_results\": batch_size,\n",
    "            \"sortBy\": \"submittedDate\",\n",
    "            \"sortOrder\": \"descending\"\n",
    "        }\n",
    "        print(f\"Fetching articles {start_index + 1} to {start_index + batch_size}...\")\n",
    "        \n",
    "        # Make the request\n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: HTTP {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        # Parse the XML response\n",
    "        data = response.text\n",
    "        entries = data.split(\"<entry>\")\n",
    "        if len(entries) <= 1:\n",
    "            print(\"No more articles found.\")\n",
    "            break\n",
    "        \n",
    "        for entry in entries[1:]:\n",
    "            try:\n",
    "                title = entry.split(\"<title>\")[1].split(\"</title>\")[0].strip()\n",
    "                summary = entry.split(\"<summary>\")[1].split(\"</summary>\")[0].strip()\n",
    "                published = entry.split(\"<published>\")[1].split(\"</published>\")[0].strip()\n",
    "                link = entry.split('<id>')[1].split('</id>')[0].strip()\n",
    "                authors = \", \".join(\n",
    "                    author.split(\"</name>\")[0].split(\"<name>\")[1].strip()\n",
    "                    for author in entry.split(\"<author>\")[1:]\n",
    "                )\n",
    "                all_articles.append({\n",
    "                    \"Title\": title,\n",
    "                    \"Summary\": summary,\n",
    "                    \"Published\": published,\n",
    "                    \"Link\": link,\n",
    "                    \"Authors\": authors\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing entry: {e}\")\n",
    "        \n",
    "        # Update counters\n",
    "        num_articles = len(entries) - 1\n",
    "        total_fetched += num_articles\n",
    "        start_index += num_articles\n",
    "        \n",
    "        # Stop if no more articles\n",
    "        if num_articles < batch_size:\n",
    "            break\n",
    "        \n",
    "        # Delay to avoid overwhelming the server\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(f\"Fetched a total of {total_fetched} articles.\")\n",
    "    return pd.DataFrame(all_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b8cecdd-a2ec-4e0c-8175-4441e512bdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query and parameters\n",
    "query = \"NLP OR RNN OR CNN OR NN OR big data OR transformers OR data structure\"\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "max_results = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99d38f27-7e71-4dfd-90d7-5991f3f7c202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles 1 to 100...\n",
      "Fetching articles 101 to 200...\n",
      "No more articles found.\n",
      "Fetched a total of 100 articles.\n"
     ]
    }
   ],
   "source": [
    "# Scrape data\n",
    "articles_df = scrape_arxiv(query, start_date, end_date, max_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc67d492-7a25-4a7f-8b96-1cbe73756bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'arxiv_articles(3).csv'\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV\n",
    "articles_df.to_csv(\"arxiv_articles(3).csv\", index=False)\n",
    "print(\"Data saved to 'arxiv_articles(3).csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdd12eb-3108-45a6-9b9f-f91d0c643f0c",
   "metadata": {},
   "source": [
    "## Query 4 : text mining, pattern recognition, business intelligence, data visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f14740a1-bb02-4df8-869f-c97aae12fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query and parameters\n",
    "query = \"business inteligence OR data visualization\"\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "max_results = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "47a9643c-5aa4-4547-bc0d-684ca6adebec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles 1 to 100...\n",
      "Fetching articles 101 to 200...\n",
      "Fetching articles 201 to 300...\n",
      "Fetched a total of 205 articles.\n"
     ]
    }
   ],
   "source": [
    "# Scrape data\n",
    "articles_df = scrape_arxiv(query, start_date, end_date, max_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "628dd9db-7ee1-48e2-8716-243ece7e7a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'arxiv_articles(4).csv'\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV\n",
    "articles_df.to_csv(\"arxiv_articles(4).csv\", index=False)\n",
    "print(\"Data saved to 'arxiv_articles(4).csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00e2511-bdb0-42c5-81b3-6d1887b495d1",
   "metadata": {},
   "source": [
    "## Query 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c5be2dfb-d790-419b-9d48-4bf491f35c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query and parameters\n",
    "query = \"forecasting OR classification OR regression OR random forest OR XGBoost\"\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "max_results = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a0d42675-5edb-43d5-83b0-bbd2c62c1936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles 1 to 100...\n",
      "Fetching articles 101 to 200...\n",
      "Fetching articles 201 to 300...\n",
      "No more articles found.\n",
      "Fetched a total of 200 articles.\n"
     ]
    }
   ],
   "source": [
    "# Scrape data\n",
    "articles_df = scrape_arxiv(query, start_date, end_date, max_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "03fce321-c864-4743-b2ba-f2a6c6708953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'arxiv_articles(5).csv'\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV\n",
    "articles_df.to_csv(\"arxiv_articles(5).csv\", index=False)\n",
    "print(\"Data saved to 'arxiv_articles(5).csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6a88c3-81fa-45a9-998c-c8321a82e113",
   "metadata": {},
   "source": [
    "## Query 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "45282551-5648-4891-8428-3609a153bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query and parameters\n",
    "query = \"data warehousing OR database\"\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "max_results = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "66206728-567b-4846-b0e7-df51fa92f8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles 1 to 100...\n",
      "Fetching articles 101 to 200...\n",
      "Fetching articles 201 to 300...\n",
      "No more articles found.\n",
      "Fetched a total of 200 articles.\n"
     ]
    }
   ],
   "source": [
    "# Scrape data\n",
    "articles_df = scrape_arxiv(query, start_date, end_date, max_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "254e561f-5094-44b7-9ad0-25be23a1823b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'arxiv_articles(6).csv'\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV\n",
    "articles_df.to_csv(\"arxiv_articles(6).csv\", index=False)\n",
    "print(\"Data saved to 'arxiv_articles(6).csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765d8090-27b2-4f91-a5cd-a64e6cdd8092",
   "metadata": {},
   "source": [
    "## Query 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "10e97614-b7bb-4460-a41d-c276315d25f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query and parameters\n",
    "query = \"supervised learning OR unsupervised learning OR feature extraction\"\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "max_results = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "83d4bad7-191b-4810-8b60-48d45237f7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles 1 to 100...\n",
      "Fetching articles 101 to 200...\n",
      "Fetching articles 201 to 300...\n",
      "Fetching articles 301 to 400...\n",
      "Fetching articles 401 to 500...\n",
      "No more articles found.\n",
      "Fetched a total of 400 articles.\n"
     ]
    }
   ],
   "source": [
    "# Scrape data\n",
    "articles_df = scrape_arxiv(query, start_date, end_date, max_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4e88ec6d-72dc-4c75-9a17-73c3945df687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'arxiv_articles(7).csv'\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV\n",
    "articles_df.to_csv(\"arxiv_articles(7).csv\", index=False)\n",
    "print(\"Data saved to 'arxiv_articles(7).csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c60af6a-a530-4da4-a6f8-9e9ee60b819a",
   "metadata": {},
   "source": [
    "## Query 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "af7564eb-ee2b-4e90-a578-639c71d34aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query and parameters\n",
    "query = \"recommendation systems OR SQL OR generative AI\"\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "max_results = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d5360b2e-bdb6-4319-a8dd-0038cb817b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching articles 1 to 100...\n",
      "Fetching articles 101 to 200...\n",
      "Fetching articles 201 to 300...\n",
      "No more articles found.\n",
      "Fetched a total of 200 articles.\n"
     ]
    }
   ],
   "source": [
    "# Scrape data\n",
    "articles_df = scrape_arxiv(query, start_date, end_date, max_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "484481a7-07f9-42cb-b8af-f7084d95d9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'arxiv_articles(8).csv'\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV\n",
    "articles_df.to_csv(\"arxiv_articles(8).csv\", index=False)\n",
    "print(\"Data saved to 'arxiv_articles(8).csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3b29f4-7e83-4198-becc-b8ef9a6ec827",
   "metadata": {},
   "source": [
    "# Combine Datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "500fc0bb-87b7-4932-8be0-a4f90384f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "990dc0fc-2f4d-4086-aaa7-8b6a5e337218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_prepare(file):\n",
    "    # Baca file CSV\n",
    "    df = pd.read_csv(file)\n",
    "    return df\n",
    "\n",
    "# Path ke folder CSV\n",
    "path = \"C:/Users/HP 840 G8/Desktop/skripsi/notebooks/data arxiv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0ae2d544-514b-4525-bcdf-69be864f41e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daftar semua file CSV\n",
    "file_list = [os.path.join(path, file) for file in os.listdir(path) if file.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "23939e73-d36a-478b-ac20-e0234edb3029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baca dan persiapkan semua file\n",
    "data_list = [read_and_prepare(file) for file in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fb384aa2-8982-4a20-a042-cfe025b20582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gabungkan semua data\n",
    "combined_data = pd.concat(data_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ff54069d-d70e-4561-ac8f-43655bbad3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset berhasil digabungkan dan disimpan sebagai 'arxiv_articles_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "# Simpan hasilnya\n",
    "combined_data.to_csv('arxiv_articles_dataset.csv', index=False)\n",
    "print(\"Dataset berhasil digabungkan dan disimpan sebagai 'arxiv_articles_dataset.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743c426e-d268-4e12-bc6d-b2e03082c91a",
   "metadata": {},
   "source": [
    "# DOAJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "315d4f71-746e-4db6-a2bb-bfffdbfc2668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp 840 g8\\desktop\\skripsi\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\HP 840 G8\\Desktop\\skripsi\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2bcddb16-b710-4bc1-b23e-13417a8ced28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a5726273-d2b7-43c2-9280-d5446d1e4d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOAJ Scraping Function\n",
    "def scrape_doaj(query, max_results=5000, start_year=2000, end_year=2024):\n",
    "    base_url = \"https://doaj.org/api/v2/search/articles\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    articles = []\n",
    "    page = 1\n",
    "\n",
    "    while len(articles) < max_results:\n",
    "        print(f\"Scraping DOAJ page {page}...\")\n",
    "        params = {\n",
    "            \"q\": f\"{query} AND created_date:[{start_year} TO {end_year}]\",\n",
    "            \"page\": page,\n",
    "            \"pageSize\": 100  # DOAJ supports up to 100 articles per page\n",
    "        }\n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve page {page}, status code: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        for result in data.get(\"results\", []):\n",
    "            articles.append({\n",
    "                \"Title\": result.get(\"bibjson\", {}).get(\"title\", \"\"),\n",
    "                \"Summary\": result.get(\"bibjson\", {}).get(\"abstract\", \"\"),\n",
    "                \"Published\": result.get(\"created_date\", \"\"),\n",
    "                \"Link\": result.get(\"links\", [{}])[0].get(\"url\", \"\"),\n",
    "                \"Authors\": \", \".join([author.get(\"name\", \"\") for author in result.get(\"bibjson\", {}).get(\"author\", [])]),\n",
    "                \"Institutions\": \", \".join([author.get(\"affiliation\", \"\") for author in result.get(\"bibjson\", {}).get(\"author\", [])]),\n",
    "            })\n",
    "\n",
    "            if len(articles) >= max_results:\n",
    "                break\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(1)  # Prevent server overload\n",
    "\n",
    "    return pd.DataFrame(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fd96cc99-4d12-4a2d-8d89-b6fe6068cc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DOAJ scraping...\n",
      "Scraping DOAJ page 1...\n",
      "Failed to retrieve page 1, status code: 404\n",
      "DOAJ scraping completed. Saved 0 articles to 'doaj_articles.csv'.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define queries\n",
    "    query_doaj = \"data science OR information extraction OR BERT OR BiLSTM\"\n",
    "\n",
    "    # Scrape DOAJ\n",
    "    print(\"Starting DOAJ scraping...\")\n",
    "    doaj_data = scrape_doaj(query_doaj, max_results=5000, start_year=2000, end_year=2024)\n",
    "    doaj_data.to_csv(\"doaj_articles.csv\", index=False)\n",
    "    print(f\"DOAJ scraping completed. Saved {len(doaj_data)} articles to 'doaj_articles.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3a6c75-ae89-4fda-bd79-fc9f89be054e",
   "metadata": {},
   "source": [
    "# ZENODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71b042df-a3ac-45b8-b5d5-a8a9ff98167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dae4fdaf-5e63-4169-bd9f-68c0c83fe5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk scraping Zenodo\n",
    "def scrape_zenodo(query, max_results, start_year, end_year, retries=3, delay=5):\n",
    "    base_url = \"https://zenodo.org/api/records/\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    total_fetched = 0\n",
    "    page = 1\n",
    "    \n",
    "    while total_fetched < max_results:\n",
    "        params = {\n",
    "            \"q\": f\"{query} AND publication_date:[{start_year} TO {end_year}]\",\n",
    "            \"page\": page,\n",
    "            \"size\": 100  # Zenodo hanya mendukung hingga 100 artikel per halaman\n",
    "        }\n",
    "\n",
    "        attempt = 0\n",
    "        while attempt < retries:\n",
    "            try:\n",
    "                print(f\"Scraping Zenodo page {page}...\")\n",
    "                response = requests.get(base_url, params=params, headers=headers, timeout=30)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    for record in data['hits']['hits']:\n",
    "                        title = record['metadata'].get('title', 'No Title')\n",
    "                        link = record['links'].get('self', 'No Link')\n",
    "                        published = record['metadata'].get('publication_date', 'No Date')\n",
    "                        summary = record['metadata'].get('description', 'No Summary')\n",
    "                        authors = ', '.join([author['name'] for author in record['metadata'].get('creators', [])])\n",
    "\n",
    "                        results.append({\n",
    "                            'title': title,\n",
    "                            'link': link,\n",
    "                            'published': published,\n",
    "                            'summary': summary,\n",
    "                            'authors': authors\n",
    "                        })\n",
    "                    \n",
    "                    total_fetched += len(data['hits']['hits'])\n",
    "                    if total_fetched >= max_results:\n",
    "                        break\n",
    "                    page += 1\n",
    "                    break  # Break after successful request\n",
    "                \n",
    "                else:\n",
    "                    print(f\"Failed to retrieve page {page}, status code: {response.status_code}\")\n",
    "                    break  # Break on failure (e.g., status code != 200)\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error occurred: {e}\")\n",
    "                attempt += 1\n",
    "                print(f\"Retrying... Attempt {attempt}/{retries}\")\n",
    "                time.sleep(delay)\n",
    "                \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e55c1ea5-a7a7-4901-86b7-17d3534d4a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Zenodo page 1...\n",
      "Scraping Zenodo page 2...\n",
      "Scraping Zenodo page 3...\n",
      "Scraping Zenodo page 4...\n",
      "Scraping Zenodo page 5...\n",
      "Scraping Zenodo page 6...\n",
      "Scraping Zenodo page 7...\n",
      "Scraping Zenodo page 8...\n",
      "Scraping Zenodo page 9...\n",
      "Scraping Zenodo page 10...\n",
      "Scraping Zenodo page 11...\n",
      "Scraping Zenodo page 12...\n",
      "Scraping Zenodo page 13...\n",
      "Scraping Zenodo page 14...\n",
      "Scraping Zenodo page 15...\n",
      "Scraping Zenodo page 16...\n",
      "Scraping Zenodo page 17...\n",
      "Scraping Zenodo page 18...\n",
      "Scraping Zenodo page 19...\n",
      "Scraping Zenodo page 20...\n",
      "Scraping Zenodo page 21...\n",
      "Scraping Zenodo page 22...\n",
      "Scraping Zenodo page 23...\n",
      "Scraping Zenodo page 24...\n",
      "Scraping Zenodo page 25...\n",
      "Scraping Zenodo page 26...\n",
      "Scraping Zenodo page 27...\n",
      "Scraping Zenodo page 28...\n",
      "Scraping Zenodo page 29...\n",
      "Scraping Zenodo page 30...\n",
      "Scraping Zenodo page 31...\n",
      "Scraping Zenodo page 32...\n",
      "Scraping Zenodo page 33...\n",
      "Scraping Zenodo page 34...\n",
      "Scraping Zenodo page 35...\n",
      "Scraping Zenodo page 36...\n",
      "Scraping Zenodo page 37...\n",
      "Scraping Zenodo page 38...\n",
      "Scraping Zenodo page 39...\n",
      "Scraping Zenodo page 40...\n",
      "Scraping Zenodo page 41...\n",
      "Scraping Zenodo page 42...\n",
      "Scraping Zenodo page 43...\n",
      "Scraping Zenodo page 44...\n",
      "Scraping Zenodo page 45...\n",
      "Scraping Zenodo page 46...\n",
      "Scraping Zenodo page 47...\n",
      "Scraping Zenodo page 48...\n",
      "Scraping Zenodo page 49...\n",
      "Scraping Zenodo page 50...\n",
      "Zenodo scraping completed. Saved 5000 articles to 'zenodo_articles(20.000).csv'.\n"
     ]
    }
   ],
   "source": [
    "# Mengambil artikel dari Zenodo dengan rentang tahun 2014-2024 dan maksimal 20000 artikel\n",
    "query_zenodo = \"data science OR deep learning OR natural language processing OR machine learning\" \n",
    "zenodo_data = scrape_zenodo(query_zenodo, max_results=5000, start_year=2014, end_year=2024)\n",
    "\n",
    "# Menyimpan hasil ke CSV\n",
    "zenodo_data.to_csv(\"zenodo_articles(1).csv\", index=False)\n",
    "print(f\"Zenodo scraping completed. Saved {len(zenodo_data)} articles to 'zenodo_articles(20.000).csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce577855-180d-4d8a-a558-038bef2438ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Zenodo page 1...\n",
      "Scraping Zenodo page 2...\n",
      "Scraping Zenodo page 3...\n",
      "Scraping Zenodo page 4...\n",
      "Scraping Zenodo page 5...\n",
      "Scraping Zenodo page 6...\n",
      "Scraping Zenodo page 7...\n",
      "Scraping Zenodo page 8...\n",
      "Scraping Zenodo page 9...\n",
      "Scraping Zenodo page 10...\n",
      "Scraping Zenodo page 11...\n",
      "Scraping Zenodo page 12...\n",
      "Scraping Zenodo page 13...\n",
      "Scraping Zenodo page 14...\n",
      "Scraping Zenodo page 15...\n",
      "Scraping Zenodo page 16...\n",
      "Scraping Zenodo page 17...\n",
      "Scraping Zenodo page 18...\n",
      "Scraping Zenodo page 19...\n",
      "Scraping Zenodo page 20...\n",
      "Scraping Zenodo page 21...\n",
      "Scraping Zenodo page 22...\n",
      "Scraping Zenodo page 23...\n",
      "Scraping Zenodo page 24...\n",
      "Scraping Zenodo page 25...\n",
      "Scraping Zenodo page 26...\n",
      "Scraping Zenodo page 27...\n",
      "Scraping Zenodo page 28...\n",
      "Scraping Zenodo page 29...\n",
      "Scraping Zenodo page 30...\n",
      "Scraping Zenodo page 31...\n",
      "Scraping Zenodo page 32...\n",
      "Scraping Zenodo page 33...\n",
      "Scraping Zenodo page 34...\n",
      "Scraping Zenodo page 35...\n",
      "Scraping Zenodo page 36...\n",
      "Scraping Zenodo page 37...\n",
      "Scraping Zenodo page 38...\n",
      "Scraping Zenodo page 39...\n",
      "Scraping Zenodo page 40...\n",
      "Scraping Zenodo page 41...\n",
      "Scraping Zenodo page 42...\n",
      "Scraping Zenodo page 43...\n",
      "Scraping Zenodo page 44...\n",
      "Scraping Zenodo page 45...\n",
      "Scraping Zenodo page 46...\n",
      "Scraping Zenodo page 47...\n",
      "Scraping Zenodo page 48...\n",
      "Scraping Zenodo page 49...\n",
      "Scraping Zenodo page 50...\n",
      "Zenodo scraping completed. Saved 5000 articles to 'zenodo_articles(20.000).csv'.\n"
     ]
    }
   ],
   "source": [
    "# Mengambil artikel dari Zenodo dengan rentang tahun 2014-2024 \n",
    "query_zenodo = \"data mining OR BERT OR data structure\" \n",
    "zenodo_data = scrape_zenodo(query_zenodo, max_results=5000, start_year=2014, end_year=2024)\n",
    "\n",
    "# Menyimpan hasil ke CSV\n",
    "zenodo_data.to_csv(\"zenodo_articles(2).csv\", index=False)\n",
    "print(f\"Zenodo scraping completed. Saved {len(zenodo_data)} articles to 'zenodo_articles(20.000).csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40537e25-dade-42de-b59e-70828914f3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Zenodo page 1...\n",
      "Scraping Zenodo page 2...\n",
      "Scraping Zenodo page 3...\n",
      "Scraping Zenodo page 4...\n",
      "Scraping Zenodo page 5...\n",
      "Scraping Zenodo page 6...\n",
      "Scraping Zenodo page 7...\n",
      "Scraping Zenodo page 8...\n",
      "Scraping Zenodo page 9...\n",
      "Scraping Zenodo page 10...\n",
      "Scraping Zenodo page 11...\n",
      "Scraping Zenodo page 12...\n",
      "Scraping Zenodo page 13...\n",
      "Scraping Zenodo page 14...\n",
      "Scraping Zenodo page 15...\n",
      "Scraping Zenodo page 16...\n",
      "Scraping Zenodo page 17...\n",
      "Scraping Zenodo page 18...\n",
      "Scraping Zenodo page 19...\n",
      "Scraping Zenodo page 20...\n",
      "Scraping Zenodo page 21...\n",
      "Scraping Zenodo page 22...\n",
      "Scraping Zenodo page 23...\n",
      "Scraping Zenodo page 24...\n",
      "Scraping Zenodo page 25...\n",
      "Scraping Zenodo page 26...\n",
      "Scraping Zenodo page 27...\n",
      "Scraping Zenodo page 28...\n",
      "Scraping Zenodo page 29...\n",
      "Scraping Zenodo page 30...\n",
      "Scraping Zenodo page 31...\n",
      "Scraping Zenodo page 32...\n",
      "Scraping Zenodo page 33...\n",
      "Scraping Zenodo page 34...\n",
      "Scraping Zenodo page 35...\n",
      "Scraping Zenodo page 36...\n",
      "Scraping Zenodo page 37...\n",
      "Scraping Zenodo page 38...\n",
      "Scraping Zenodo page 39...\n",
      "Scraping Zenodo page 40...\n",
      "Scraping Zenodo page 41...\n",
      "Scraping Zenodo page 42...\n",
      "Scraping Zenodo page 43...\n",
      "Scraping Zenodo page 44...\n",
      "Scraping Zenodo page 45...\n",
      "Scraping Zenodo page 46...\n",
      "Scraping Zenodo page 47...\n",
      "Scraping Zenodo page 48...\n",
      "Scraping Zenodo page 49...\n",
      "Scraping Zenodo page 50...\n",
      "Zenodo scraping completed. Saved 5000 articles to 'zenodo_articles(20.000).csv'.\n"
     ]
    }
   ],
   "source": [
    "# Mengambil artikel dari Zenodo dengan rentang tahun 2014-2024 \n",
    "query_zenodo = \"artificial intelligence OR information extraction OR data visualization\" \n",
    "zenodo_data = scrape_zenodo(query_zenodo, max_results=5000, start_year=2014, end_year=2024)\n",
    "\n",
    "# Menyimpan hasil ke CSV\n",
    "zenodo_data.to_csv(\"zenodo_articles(3).csv\", index=False)\n",
    "print(f\"Zenodo scraping completed. Saved {len(zenodo_data)} articles to 'zenodo_articles(20.000).csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6c92468-0e86-44a8-ab7d-432493aa5976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Zenodo page 1...\n",
      "Scraping Zenodo page 2...\n",
      "Scraping Zenodo page 3...\n",
      "Scraping Zenodo page 4...\n",
      "Scraping Zenodo page 5...\n",
      "Scraping Zenodo page 6...\n",
      "Scraping Zenodo page 7...\n",
      "Scraping Zenodo page 8...\n",
      "Scraping Zenodo page 9...\n",
      "Scraping Zenodo page 10...\n",
      "Scraping Zenodo page 11...\n",
      "Scraping Zenodo page 12...\n",
      "Scraping Zenodo page 13...\n",
      "Scraping Zenodo page 14...\n",
      "Scraping Zenodo page 15...\n",
      "Scraping Zenodo page 16...\n",
      "Scraping Zenodo page 17...\n",
      "Scraping Zenodo page 18...\n",
      "Scraping Zenodo page 19...\n",
      "Scraping Zenodo page 20...\n",
      "Scraping Zenodo page 21...\n",
      "Scraping Zenodo page 22...\n",
      "Scraping Zenodo page 23...\n",
      "Scraping Zenodo page 24...\n",
      "Scraping Zenodo page 25...\n",
      "Scraping Zenodo page 26...\n",
      "Scraping Zenodo page 27...\n",
      "Scraping Zenodo page 28...\n",
      "Scraping Zenodo page 29...\n",
      "Scraping Zenodo page 30...\n",
      "Scraping Zenodo page 31...\n",
      "Scraping Zenodo page 32...\n",
      "Scraping Zenodo page 33...\n",
      "Scraping Zenodo page 34...\n",
      "Scraping Zenodo page 35...\n",
      "Scraping Zenodo page 36...\n",
      "Scraping Zenodo page 37...\n",
      "Scraping Zenodo page 38...\n",
      "Scraping Zenodo page 39...\n",
      "Scraping Zenodo page 40...\n",
      "Scraping Zenodo page 41...\n",
      "Scraping Zenodo page 42...\n",
      "Scraping Zenodo page 43...\n",
      "Scraping Zenodo page 44...\n",
      "Scraping Zenodo page 45...\n",
      "Scraping Zenodo page 46...\n",
      "Scraping Zenodo page 47...\n",
      "Scraping Zenodo page 48...\n",
      "Scraping Zenodo page 49...\n",
      "Scraping Zenodo page 50...\n",
      "Zenodo scraping completed. Saved 5000 articles to 'zenodo_articles.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Mengambil artikel dari Zenodo dengan rentang tahun 2014-2024 \n",
    "query_zenodo = \"big data OR transformer OR reinforcement learning\" \n",
    "zenodo_data = scrape_zenodo(query_zenodo, max_results=5000, start_year=2014, end_year=2024)\n",
    "\n",
    "# Menyimpan hasil ke CSV\n",
    "zenodo_data.to_csv(\"zenodo_articles(4).csv\", index=False)\n",
    "print(f\"Zenodo scraping completed. Saved {len(zenodo_data)} articles to 'zenodo_articles.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64183a0d-d9b5-47f8-82a1-4d41f2a344f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Zenodo page 1...\n",
      "Scraping Zenodo page 2...\n",
      "Scraping Zenodo page 3...\n",
      "Scraping Zenodo page 4...\n",
      "Scraping Zenodo page 5...\n",
      "Scraping Zenodo page 6...\n",
      "Scraping Zenodo page 7...\n",
      "Scraping Zenodo page 8...\n",
      "Scraping Zenodo page 9...\n",
      "Scraping Zenodo page 10...\n",
      "Scraping Zenodo page 11...\n",
      "Scraping Zenodo page 12...\n",
      "Scraping Zenodo page 13...\n",
      "Scraping Zenodo page 14...\n",
      "Scraping Zenodo page 15...\n",
      "Scraping Zenodo page 16...\n",
      "Scraping Zenodo page 17...\n",
      "Scraping Zenodo page 18...\n",
      "Scraping Zenodo page 19...\n",
      "Scraping Zenodo page 20...\n",
      "Scraping Zenodo page 21...\n",
      "Scraping Zenodo page 22...\n",
      "Scraping Zenodo page 23...\n",
      "Scraping Zenodo page 24...\n",
      "Scraping Zenodo page 25...\n",
      "Scraping Zenodo page 26...\n",
      "Scraping Zenodo page 27...\n",
      "Scraping Zenodo page 28...\n",
      "Scraping Zenodo page 29...\n",
      "Scraping Zenodo page 30...\n",
      "Scraping Zenodo page 31...\n",
      "Scraping Zenodo page 32...\n",
      "Scraping Zenodo page 33...\n",
      "Scraping Zenodo page 34...\n",
      "Scraping Zenodo page 35...\n",
      "Scraping Zenodo page 36...\n",
      "Scraping Zenodo page 37...\n",
      "Scraping Zenodo page 38...\n",
      "Scraping Zenodo page 39...\n",
      "Scraping Zenodo page 40...\n",
      "Scraping Zenodo page 41...\n",
      "Scraping Zenodo page 42...\n",
      "Scraping Zenodo page 43...\n",
      "Scraping Zenodo page 44...\n",
      "Scraping Zenodo page 45...\n",
      "Scraping Zenodo page 46...\n",
      "Scraping Zenodo page 47...\n",
      "Scraping Zenodo page 48...\n",
      "Scraping Zenodo page 49...\n",
      "Scraping Zenodo page 50...\n",
      "Zenodo scraping completed. Saved 5000 articles to 'zenodo_articles.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Mengambil artikel dari Zenodo dengan rentang tahun 2014-2024 \n",
    "query_zenodo = \"OR part-of-speech tagging OR text mining\" \n",
    "zenodo_data = scrape_zenodo(query_zenodo, max_results=5000, start_year=2014, end_year=2024)\n",
    "\n",
    "# Menyimpan hasil ke CSV\n",
    "zenodo_data.to_csv(\"zenodo_articles(5).csv\", index=False)\n",
    "print(f\"Zenodo scraping completed. Saved {len(zenodo_data)} articles to 'zenodo_articles.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "654d6528-d64b-4f76-bc79-6efa532a4c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Zenodo page 1...\n",
      "Scraping Zenodo page 2...\n",
      "Scraping Zenodo page 3...\n",
      "Scraping Zenodo page 4...\n",
      "Scraping Zenodo page 5...\n",
      "Scraping Zenodo page 6...\n",
      "Scraping Zenodo page 7...\n",
      "Scraping Zenodo page 8...\n",
      "Scraping Zenodo page 9...\n",
      "Scraping Zenodo page 10...\n",
      "Scraping Zenodo page 11...\n",
      "Scraping Zenodo page 12...\n",
      "Scraping Zenodo page 13...\n",
      "Scraping Zenodo page 14...\n",
      "Scraping Zenodo page 15...\n",
      "Scraping Zenodo page 16...\n",
      "Scraping Zenodo page 17...\n",
      "Scraping Zenodo page 18...\n",
      "Scraping Zenodo page 19...\n",
      "Scraping Zenodo page 20...\n",
      "Scraping Zenodo page 21...\n",
      "Scraping Zenodo page 22...\n",
      "Error occurred: HTTPSConnectionPool(host='zenodo.org', port=443): Read timed out. (read timeout=30)\n",
      "Retrying... Attempt 1/3\n",
      "Scraping Zenodo page 22...\n",
      "Scraping Zenodo page 23...\n",
      "Scraping Zenodo page 24...\n",
      "Scraping Zenodo page 25...\n",
      "Scraping Zenodo page 26...\n",
      "Scraping Zenodo page 27...\n",
      "Scraping Zenodo page 28...\n",
      "Scraping Zenodo page 29...\n",
      "Scraping Zenodo page 30...\n",
      "Scraping Zenodo page 31...\n",
      "Scraping Zenodo page 32...\n",
      "Scraping Zenodo page 33...\n",
      "Scraping Zenodo page 34...\n",
      "Scraping Zenodo page 35...\n",
      "Scraping Zenodo page 36...\n",
      "Scraping Zenodo page 37...\n",
      "Scraping Zenodo page 38...\n",
      "Scraping Zenodo page 39...\n",
      "Scraping Zenodo page 40...\n",
      "Scraping Zenodo page 41...\n",
      "Scraping Zenodo page 42...\n",
      "Scraping Zenodo page 43...\n",
      "Scraping Zenodo page 44...\n",
      "Scraping Zenodo page 45...\n",
      "Scraping Zenodo page 46...\n",
      "Scraping Zenodo page 47...\n",
      "Scraping Zenodo page 48...\n",
      "Scraping Zenodo page 49...\n",
      "Scraping Zenodo page 50...\n",
      "Zenodo scraping completed. Saved 5000 articles to 'zenodo_articles.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Mengambil artikel dari Zenodo dengan rentang tahun 2014-2024 \n",
    "query_zenodo = \"neural netowrk OR clustering OR forecasting OR random forest\" \n",
    "zenodo_data = scrape_zenodo(query_zenodo, max_results=5000, start_year=2014, end_year=2024)\n",
    "\n",
    "# Menyimpan hasil ke CSV\n",
    "zenodo_data.to_csv(\"zenodo_articles(6).csv\", index=False)\n",
    "print(f\"Zenodo scraping completed. Saved {len(zenodo_data)} articles to 'zenodo_articles.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3099e0c8-5cc1-46d4-98a2-35f4d71238af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Zenodo page 1...\n",
      "Scraping Zenodo page 2...\n",
      "Scraping Zenodo page 3...\n",
      "Scraping Zenodo page 4...\n",
      "Scraping Zenodo page 5...\n",
      "Scraping Zenodo page 6...\n",
      "Scraping Zenodo page 7...\n",
      "Scraping Zenodo page 8...\n",
      "Scraping Zenodo page 9...\n",
      "Scraping Zenodo page 10...\n",
      "Scraping Zenodo page 11...\n",
      "Scraping Zenodo page 12...\n",
      "Scraping Zenodo page 13...\n",
      "Scraping Zenodo page 14...\n",
      "Scraping Zenodo page 15...\n",
      "Scraping Zenodo page 16...\n",
      "Scraping Zenodo page 17...\n",
      "Scraping Zenodo page 18...\n",
      "Scraping Zenodo page 19...\n",
      "Scraping Zenodo page 20...\n",
      "Scraping Zenodo page 21...\n",
      "Scraping Zenodo page 22...\n",
      "Scraping Zenodo page 23...\n",
      "Scraping Zenodo page 24...\n",
      "Scraping Zenodo page 25...\n",
      "Scraping Zenodo page 26...\n",
      "Scraping Zenodo page 27...\n",
      "Scraping Zenodo page 28...\n",
      "Scraping Zenodo page 29...\n",
      "Scraping Zenodo page 30...\n",
      "Scraping Zenodo page 31...\n",
      "Scraping Zenodo page 32...\n",
      "Scraping Zenodo page 33...\n",
      "Scraping Zenodo page 34...\n",
      "Scraping Zenodo page 35...\n",
      "Scraping Zenodo page 36...\n",
      "Scraping Zenodo page 37...\n",
      "Scraping Zenodo page 38...\n",
      "Scraping Zenodo page 39...\n",
      "Scraping Zenodo page 40...\n",
      "Scraping Zenodo page 41...\n",
      "Scraping Zenodo page 42...\n",
      "Scraping Zenodo page 43...\n",
      "Scraping Zenodo page 44...\n",
      "Scraping Zenodo page 45...\n",
      "Scraping Zenodo page 46...\n",
      "Scraping Zenodo page 47...\n",
      "Scraping Zenodo page 48...\n",
      "Scraping Zenodo page 49...\n",
      "Scraping Zenodo page 50...\n",
      "Zenodo scraping completed. Saved 5000 articles to 'zenodo_articles.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Mengambil artikel dari Zenodo dengan rentang tahun 2014-2024 \n",
    "query_zenodo = \"decision tree OR cloud computing OR classification\" \n",
    "zenodo_data = scrape_zenodo(query_zenodo, max_results=5000, start_year=2014, end_year=2024)\n",
    "\n",
    "# Menyimpan hasil ke CSV\n",
    "zenodo_data.to_csv(\"zenodo_articles(7).csv\", index=False)\n",
    "print(f\"Zenodo scraping completed. Saved {len(zenodo_data)} articles to 'zenodo_articles.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7c66c38-8b3f-4ab4-b4dc-33c2c03004b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Zenodo page 1...\n",
      "Scraping Zenodo page 2...\n",
      "Scraping Zenodo page 3...\n",
      "Scraping Zenodo page 4...\n",
      "Scraping Zenodo page 5...\n",
      "Scraping Zenodo page 6...\n",
      "Scraping Zenodo page 7...\n",
      "Scraping Zenodo page 8...\n",
      "Scraping Zenodo page 9...\n",
      "Scraping Zenodo page 10...\n",
      "Scraping Zenodo page 11...\n",
      "Scraping Zenodo page 12...\n",
      "Scraping Zenodo page 13...\n",
      "Scraping Zenodo page 14...\n",
      "Scraping Zenodo page 15...\n",
      "Scraping Zenodo page 16...\n",
      "Scraping Zenodo page 17...\n",
      "Scraping Zenodo page 18...\n",
      "Scraping Zenodo page 19...\n",
      "Scraping Zenodo page 20...\n",
      "Scraping Zenodo page 21...\n",
      "Scraping Zenodo page 22...\n",
      "Scraping Zenodo page 23...\n",
      "Scraping Zenodo page 24...\n",
      "Scraping Zenodo page 25...\n",
      "Scraping Zenodo page 26...\n",
      "Scraping Zenodo page 27...\n",
      "Scraping Zenodo page 28...\n",
      "Scraping Zenodo page 29...\n",
      "Scraping Zenodo page 30...\n",
      "Scraping Zenodo page 31...\n",
      "Scraping Zenodo page 32...\n",
      "Scraping Zenodo page 33...\n",
      "Scraping Zenodo page 34...\n",
      "Scraping Zenodo page 35...\n",
      "Scraping Zenodo page 36...\n",
      "Scraping Zenodo page 37...\n",
      "Scraping Zenodo page 38...\n",
      "Scraping Zenodo page 39...\n",
      "Scraping Zenodo page 40...\n",
      "Scraping Zenodo page 41...\n",
      "Scraping Zenodo page 42...\n",
      "Scraping Zenodo page 43...\n",
      "Scraping Zenodo page 44...\n",
      "Scraping Zenodo page 45...\n",
      "Scraping Zenodo page 46...\n",
      "Scraping Zenodo page 47...\n",
      "Scraping Zenodo page 48...\n",
      "Scraping Zenodo page 49...\n",
      "Scraping Zenodo page 50...\n",
      "Zenodo scraping completed. Saved 5000 articles to 'zenodo_articles.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Mengambil artikel dari Zenodo dengan rentang tahun 2014-2024 \n",
    "query_zenodo = \"predictive modeling OR data analysis OR time series \" \n",
    "zenodo_data = scrape_zenodo(query_zenodo, max_results=5000, start_year=2014, end_year=2024)\n",
    "\n",
    "# Menyimpan hasil ke CSV\n",
    "zenodo_data.to_csv(\"zenodo_articles(8).csv\", index=False)\n",
    "print(f\"Zenodo scraping completed. Saved {len(zenodo_data)} articles to 'zenodo_articles.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9d6be9-d237-4ddc-8558-523a3d3d85a0",
   "metadata": {},
   "source": [
    "# Combine Zenodo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2139ce5-f979-4400-89e3-622729302fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "791ff9a5-2608-4298-b78b-ed0cab04ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder yang berisi file CSV\n",
    "folder_path = \"C:/Users/HP 840 G8/Desktop/skripsi/notebooks/data zenodo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2bf660ac-e596-49b9-96cf-18a5c5b7162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat daftar file CSV\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d22b2e8-a5a4-4a36-8b0f-4890beb84c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membaca dan menggabungkan semua file CSV\n",
    "combined_data = pd.DataFrame()  # DataFrame kosong\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    data = pd.read_csv(file_path)  # Membaca setiap file CSV\n",
    "    combined_data = pd.concat([combined_data, data], ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f15dcd0-3ddd-44ea-9c0a-79d22536e275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File berhasil digabungkan dan disimpan sebagai 'articles_zenodo_dataset.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Menyimpan hasil gabungan ke file CSV baru\n",
    "output_file = \"articles_zenodo_dataset.csv\"\n",
    "combined_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"File berhasil digabungkan dan disimpan sebagai '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0d872c-4f92-4f7a-9239-fd7507be74e6",
   "metadata": {},
   "source": [
    "## Cleaning Data Zenodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1404a414-0702-4294-b86c-b33169c9c1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>published</th>\n",
       "      <th>summary</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hate Speech Detection in Twitter: Natural Lang...</td>\n",
       "      <td>https://zenodo.org/api/records/11178221</td>\n",
       "      <td>2023-11-22</td>\n",
       "      <td>&lt;p&gt;The proliferation of social media platforms...</td>\n",
       "      <td>Egode, Kelly, Oraegbunam, Linda, Oyatunji, Ade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Job Prediction based on Skills and Years of ex...</td>\n",
       "      <td>https://zenodo.org/api/records/10325357</td>\n",
       "      <td>2023-12-09</td>\n",
       "      <td>&lt;h2&gt;&lt;strong&gt;Purpose:&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;This data...</td>\n",
       "      <td>Pettugani, Hotragn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Improving Medical Coding Processes with Data A...</td>\n",
       "      <td>https://zenodo.org/api/records/11079994</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>&lt;p&gt;&lt;span&gt;In this study, we explore the pressin...</td>\n",
       "      <td>Arun Chandramouli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deep Learning: Theoretical and Practical Approach</td>\n",
       "      <td>https://zenodo.org/api/records/6672318</td>\n",
       "      <td>2022-06-24</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;Deep Learning Course Book In Persia...</td>\n",
       "      <td>Sina Ranjbar Kooh Farhadi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Application of NLP and Machine Learning for Me...</td>\n",
       "      <td>https://zenodo.org/api/records/8036627</td>\n",
       "      <td>2022-08-30</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;Abstract: &lt;/strong&gt;Humans&amp;#39; most...</td>\n",
       "      <td>Trinayan Borah, S. Ganesh Kumar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Hate Speech Detection in Twitter: Natural Lang...   \n",
       "1  Job Prediction based on Skills and Years of ex...   \n",
       "2  Improving Medical Coding Processes with Data A...   \n",
       "3  Deep Learning: Theoretical and Practical Approach   \n",
       "4  Application of NLP and Machine Learning for Me...   \n",
       "\n",
       "                                      link   published  \\\n",
       "0  https://zenodo.org/api/records/11178221  2023-11-22   \n",
       "1  https://zenodo.org/api/records/10325357  2023-12-09   \n",
       "2  https://zenodo.org/api/records/11079994  2021-12-31   \n",
       "3   https://zenodo.org/api/records/6672318  2022-06-24   \n",
       "4   https://zenodo.org/api/records/8036627  2022-08-30   \n",
       "\n",
       "                                             summary  \\\n",
       "0  <p>The proliferation of social media platforms...   \n",
       "1  <h2><strong>Purpose:</strong></h2><p>This data...   \n",
       "2  <p><span>In this study, we explore the pressin...   \n",
       "3  <p><strong>Deep Learning Course Book In Persia...   \n",
       "4  <p><strong>Abstract: </strong>Humans&#39; most...   \n",
       "\n",
       "                                             authors  \n",
       "0  Egode, Kelly, Oraegbunam, Linda, Oyatunji, Ade...  \n",
       "1                                 Pettugani, Hotragn  \n",
       "2                                  Arun Chandramouli  \n",
       "3                          Sina Ranjbar Kooh Farhadi  \n",
       "4                    Trinayan Borah, S. Ganesh Kumar  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Membaca file CSV\n",
    "data = pd.read_csv('articles_zenodo_dataset.csv')\n",
    "\n",
    "# Menampilkan 5 baris pertama\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52965bfd-2b81-469e-b892-950941c335b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   title      40000 non-null  object\n",
      " 1   link       40000 non-null  object\n",
      " 2   published  40000 non-null  object\n",
      " 3   summary    39924 non-null  object\n",
      " 4   authors    39999 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 1.5+ MB\n",
      "None\n",
      "                                                    title  \\\n",
      "count                                               40000   \n",
      "unique                                              35167   \n",
      "top     Reproduction of: Retiring Adult - New Datasets...   \n",
      "freq                                                    8   \n",
      "\n",
      "                                          link   published  \\\n",
      "count                                    40000       40000   \n",
      "unique                                   35942        3852   \n",
      "top     https://zenodo.org/api/records/8050516  2018-04-01   \n",
      "freq                                         5         378   \n",
      "\n",
      "                                                  summary  \\\n",
      "count                                               39924   \n",
      "unique                                              34524   \n",
      "top     <p>Project Tycho datasets contain case counts ...   \n",
      "freq                                                  360   \n",
      "\n",
      "                                                authors  \n",
      "count                                             39999  \n",
      "unique                                            29941  \n",
      "top     Van Panhuis, Willem, Cross, Anne, Burke, Donald  \n",
      "freq                                                339  \n",
      "title         0\n",
      "link          0\n",
      "published     0\n",
      "summary      76\n",
      "authors       1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Memeriksa informasi data\n",
    "\n",
    "## Menampilkan informasi data\n",
    "print(data.info())\n",
    "\n",
    "## Menampilkan statistik deskriptif\n",
    "print(data.describe())\n",
    "\n",
    "## Melihat apakah ada nilai yang hilang (NaN)\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "edbff973-246f-4d31-9b55-a0ac37faff0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris sebelum penghapusan: 40000\n"
     ]
    }
   ],
   "source": [
    "# Menampilkan jumlah baris sebelum penghapusan\n",
    "print(\"Jumlah baris sebelum penghapusan:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eaaad365-bca9-489a-a5c6-c097fcb6da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghapus baris yang memiliki nilai kosong di kolom 'summary' atau 'authors'\n",
    "data_cleaned = data.dropna(subset=['summary', 'authors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f361fcac-86f1-4e80-90f4-f26041db0e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris setelah penghapusan: 39923\n"
     ]
    }
   ],
   "source": [
    "# Menampilkan jumlah baris setelah penghapusan\n",
    "print(\"Jumlah baris setelah penghapusan:\", len(data_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9eff39b0-85c8-477b-9cba-4b9d9cc7f4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data yang telah dibersihkan disimpan ke 'zenodo_articles_cleaned.csv'.\n"
     ]
    }
   ],
   "source": [
    "#  Menyimpan data yang telah dibersihkan ke file baru\n",
    "data_cleaned.to_csv('zenodo_articles_cleaned.csv', index=False)\n",
    "\n",
    "print(\"Data yang telah dibersihkan disimpan ke 'zenodo_articles_cleaned.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041b64ae-b752-4341-ae4b-3f2d01bbd406",
   "metadata": {},
   "source": [
    "## Penyamaan format published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ecad40f3-5c09-469b-a1da-fbc29ac3fedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebelum format disamakan:\n",
      "0    2023-11-22\n",
      "1    2023-12-09\n",
      "2    2021-12-31\n",
      "3    2022-06-24\n",
      "4    2022-08-30\n",
      "Name: published, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Menampilkan beberapa baris sebelum pengubahan format tanggal\n",
    "print(\"Sebelum format disamakan:\")\n",
    "print(data['published'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ae3a22c-b533-43fc-8d1d-3c58f2f99f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menambahkan tanggal default untuk format yang tidak lengkap (hanya tahun dan bulan)\n",
    "data['published'] = data['published'].apply(\n",
    "    lambda x: f\"{x}-01\" if isinstance(x, str) and len(x) == 7 else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f2ac22f-8e2b-466b-ad9a-3f8840a24f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengonversi ke format datetime\n",
    "data['published'] = pd.to_datetime(data['published'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d4af910-8ae7-4875-935b-0843f88e2e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baris dengan tanggal tidak valid:\n",
      "                                                   title  \\\n",
      "136    Near-real-time Country-wide Estimation of Susc...   \n",
      "148    Zero-Shot Cross-lingual Aphasia Detection usin...   \n",
      "354    Conquering catastrophic forgetting in machine ...   \n",
      "608    Advanced Data Analysis for Machine Learning-po...   \n",
      "634    Enabling Deep Learning on IoT Devices: A Compr...   \n",
      "...                                                  ...   \n",
      "39620  Data pertaining to 'Dunedin groundwater monito...   \n",
      "39659  A novel feature engineering approach for high-...   \n",
      "39747  DUAREM: Full-scale experimental validation of ...   \n",
      "39913  A transfer entropy analysis of leader-follower...   \n",
      "39984  Social skills, autism and technologies: An ana...   \n",
      "\n",
      "                                          link published  \\\n",
      "136     https://zenodo.org/api/records/8099812       NaT   \n",
      "148    https://zenodo.org/api/records/10089640       NaT   \n",
      "354    https://zenodo.org/api/records/14057039       NaT   \n",
      "608    https://zenodo.org/api/records/14475984       NaT   \n",
      "634    https://zenodo.org/api/records/14189797       NaT   \n",
      "...                                        ...       ...   \n",
      "39620  https://zenodo.org/api/records/10275216       NaT   \n",
      "39659  https://zenodo.org/api/records/10605550       NaT   \n",
      "39747  https://zenodo.org/api/records/10720600       NaT   \n",
      "39913  https://zenodo.org/api/records/13438291       NaT   \n",
      "39984  https://zenodo.org/api/records/10549038       NaT   \n",
      "\n",
      "                                                 summary  \\\n",
      "136    <p>This Zenodo repository contains the code, d...   \n",
      "148    <p>Aphasia is a common speech and language dis...   \n",
      "354    <p>This is the code repository of the manuscri...   \n",
      "608    <div>Recommender systems (RSs) are key machine...   \n",
      "634    <p>Deep learning, while powerful, often demand...   \n",
      "...                                                  ...   \n",
      "39620  <p>Dunedin City in the South Island of New Zea...   \n",
      "39659  <p>Feature engineering for high-frequency fina...   \n",
      "39747  <p>Conventional seismic design philosophy is b...   \n",
      "39913  (Uploaded by Plazi for the Bat Literature Proj...   \n",
      "39984  <p><em>Students with Autism Spectrum Disorder ...   \n",
      "\n",
      "                                                 authors  \n",
      "136    Dimasaka, Joshua, Selvakumaran, Sivasakthy, Ma...  \n",
      "148    Chatzoudis, Gerasimos, Plitsis, Manos, Stamoul...  \n",
      "354                                              Du, Jin  \n",
      "608         Antal, Lidia-Monica, Iantovics, Laszlo Barna  \n",
      "634                                 Mr. Kottureswara M S  \n",
      "...                                                  ...  \n",
      "39620  Cox, Simon C., Ettema, Marc, Chambers, Lee, Ea...  \n",
      "39659                    Mantilla, P., Dormido-Canto, S.  \n",
      "39747                                        Dubina, Dan  \n",
      "39913                              Orange, N., Abaid, N.  \n",
      "39984  Cored Bandrs, Sergio, Vzquez Toledo, Sandra,...  \n",
      "\n",
      "[475 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Menampilkan baris dengan nilai NaT setelah konversi\n",
    "missing_dates = data[data['published'].isna()]\n",
    "if not missing_dates.empty:\n",
    "    print(\"\\nBaris dengan tanggal tidak valid:\")\n",
    "    print(missing_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02a63fa5-8b82-4e78-be68-e618025bd7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data dengan format tanggal yang disamakan telah disimpan ke 'zenodo_articles_uniform_dates.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Menyimpan data yang telah diperbarui ke file baru\n",
    "data.to_csv('zenodo_articles_uniform_dates.csv', index=False)\n",
    "\n",
    "print(\"\\nData dengan format tanggal yang disamakan telah disimpan ke 'zenodo_articles_uniform_dates.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
